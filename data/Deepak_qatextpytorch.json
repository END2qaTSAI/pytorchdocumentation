[
    {
        "answer": "zero",
        "question": "How many dimensions does each input tensor have?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "answer": "3-dimensional",
        "question": "What dimension is the view of each input tensor with zero dimensions?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "answer": "three or more",
        "question": "Input tensors with how many dimensions are returned as-is?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "answer": "output",
        "question": "What is another name for a tuple of Tensors?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "answer": "a 3-dimensional view",
        "question": "What is returned of each input tensor with zero dimensions?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "answer": "as-is",
        "question": "How are input tensors with three or more dimensions returned?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "answer": "output",
        "question": "What is the output of a tuple of Tensors?",
        "context": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "answer": "Computes the bitwise XOR",
        "question": "What does the input tensor do?",
        "context": "Computes the bitwise XOR of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR. input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor"
    },
    {
        "answer": "integral or Boolean types",
        "question": "The input tensor must be of what types?",
        "context": "Computes the bitwise XOR of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR. input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor"
    },
    {
        "answer": "bool tensors",
        "question": "For what type of tensor does it compute the logical XOR?",
        "context": "Computes the bitwise XOR of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR. input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor"
    },
    {
        "answer": "output tensor",
        "question": "What is the second input tensor out?",
        "context": "Computes the bitwise XOR of input and other. The input tensor must be of\nintegral or Boolean types. For bool tensors, it computes the logical XOR. input \u2013 the first input tensor other \u2013 the second input tensor out (Tensor, optional) \u2013 the output tensor. Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor"
    },
    {
        "answer": "Computes the base two exponential function of input",
        "question": "What is the output tensor?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "logit",
        "question": "Returns a new tensor with what of the elements of input?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "1-D",
        "question": "What is the input tensor?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "output tensor",
        "question": "Out (Tensor, optional) - what is the output tensor?",
        "context": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "answer": "optional",
        "question": "What type of tensor is the output tensor?",
        "context": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "answer": "torch.sinh",
        "question": "What may use the Sleef library when input is on the CPU?",
        "context": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "answer": "here",
        "question": "Where can you find details about the Sleef library?",
        "context": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "answer": "a new tensor",
        "question": "Returns what with the hyperbolic sine of the elements of input?",
        "context": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "answer": "Sleef library",
        "question": "When input is on the CPU, the implementation of torch.sinh may use what library?",
        "context": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "answer": "details",
        "question": "What does the Sleef library provide?",
        "context": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "answer": "Roll the tensor along the given dimension(s).",
        "question": "What does a tensor do when a dimension is not specified?",
        "context": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "answer": "Elements that are shifted beyond the last position",
        "question": "What is re-introduced at the first position?",
        "context": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "answer": "flattened before rolling",
        "question": "If a dimension is not specified, the tensor will be what?",
        "context": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "answer": "Roll the tensor",
        "question": "What does the tensor do along the given dimension(s)?",
        "context": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "answer": "python:ints",
        "question": "What is shifts a tuple of?",
        "context": "input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "answer": "a tuple of the same size",
        "question": "If shifts is a tuple, dims must be what?",
        "context": "input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "answer": "shifts",
        "question": "What is the number of places by which the elements of the tensor are shifted?",
        "context": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "answer": "tuple",
        "question": "If shifts is what, dims must be a tuple of the same size?",
        "context": "Roll the tensor along the given dimension(s). Elements that are shifted beyond the\nlast position are re-introduced at the first position. If a dimension is not\nspecified, the tensor will be flattened before rolling and then restored\nto the original shape. input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "answer": "tensor",
        "question": "What is constructed by repeating the elements of input?",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "The reps argument",
        "question": "What specifies the number of repetitions in each dimension?",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "fewer",
        "question": "If reps specifies how many dimensions than input has, then ones are prepended to reps until all dimensions are specified?",
        "context": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "(1, 1, 2, 2).",
        "question": "If input has shape (8, 6, 4, 2) and reps is (2, 2), reps is treated as what?",
        "context": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "by repeating the elements of input",
        "question": "How is a tensor constructed?",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "fewer",
        "question": "If reps specifies how many dimensions than input has, then ones are prepended to reps until all dimensions are specified.",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "(1, 1, 2, 2)",
        "question": "What are reps treated as if input has shape and reps is 2?",
        "context": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "reps",
        "question": "What specifies fewer dimensions than input has?",
        "context": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "k largest elements",
        "question": "What is the size of the input tensor along a given dimension?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "dim",
        "question": "If what is not given, the last dimension of the input is chosen?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "False",
        "question": "If largest is what, the smallest elements of the input tensor are returned?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "A namedtuple of (values, indices)",
        "question": "What is returned, where the indices are the indices of the elements in the original input tensor?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "input (Tensor)",
        "question": "The boolean option sorted if True will make sure that the returned k elements are themselves sorted?",
        "context": "The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "k",
        "question": "What is the k in \"top-k\"?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "k largest elements",
        "question": "Returns what of the given input tensor along a given dimension?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "False",
        "question": "If largest is what, the k smallest elements are returned?",
        "context": "If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "A namedtuple of (values, indices)",
        "question": "What returns the indices of the elements in the original input tensor?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "sorted input",
        "question": "The boolean option sorted if True, will make sure that the returned k elements are themselves what?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "k (int)",
        "question": "What is the k in \u201ctop-k\u201d?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "False",
        "question": "If largest is what, the smallest elements are returned?",
        "context": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "input tensor",
        "question": "The boolean option sorted if True will make sure that the returned k elements are themselves sorted input (Tensor)",
        "context": "If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "k",
        "question": "What is the k in \"top-k\" dim?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "the last dimension of the input",
        "question": "What is chosen if dim is not given?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "A namedtuple of",
        "question": "What is returned if largest is False?",
        "context": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "k",
        "question": "What is the dimension to sort along with the input tensor?",
        "context": "If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "input (Tensor)",
        "question": "The boolean option sorted if True will make sure that the returned k elements are themselves sorted what?",
        "context": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "k",
        "question": "Which dimension controls whether to return largest or smallest elements?",
        "context": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "k smallest elements",
        "question": "What are returned if largest is False?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "indices",
        "question": "A namedtuple of (values, indices, etc.) is returned, where the indices are the indices of",
        "context": "A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "boolean",
        "question": "What option makes sure that the returned k elements are themselves sorted input?",
        "context": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "A namedtuple of (values, indices)",
        "question": "What is returned where the indices are the indices of the elements in the original input tensor?",
        "context": "A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "k",
        "question": "The dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or smallest elements sorted (bool, optional)",
        "context": "The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "k",
        "question": "The boolean option sorted if True, will ma what?",
        "context": "The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "answer": "1D convolution",
        "question": "What type of convolution is applied over an input signal composed of several input planes?",
        "context": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D convolution",
        "question": "What type of convolution is applied over an input image composed of several input planes?",
        "context": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "What is another name for a 1D transposed convolution operator over an input signal composed of several input planes?",
        "context": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D",
        "question": "What type of convolution over an input image composed of several input planes?",
        "context": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D transposed convolution operator",
        "question": "What type of operator is applied over an input signal composed of several input planes?",
        "context": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D transposed convolution operator",
        "question": "What type of operator is used over an input image composed of several input planes?",
        "context": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "What is another name for a 2D transposed convolution operator?",
        "context": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 3D convolution over an input image composed of what?",
        "context": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "What is a 3D transposed convolution operator over an input image composed of several input planes sometimes called?",
        "context": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D",
        "question": "What type of transposed convolution operator is used over an input image composed of several input planes?",
        "context": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "batched input tensor",
        "question": "What type of input tensor is used to extract sliding local blocks?",
        "context": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "What is another name for a 1D transposed convolution operator?",
        "context": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "What is another name for a 3D transposed convolution operator?",
        "context": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D transposed convolution operator",
        "question": "What type of operator is applied over an input image composed of several input planes?",
        "context": "  Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "batched",
        "question": "What type of input tensor is the input tensor?",
        "context": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Combines an array of sliding local blocks into a large containing tensor",
        "question": "What does a 3D transposed convolution operator do?",
        "context": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D average pooling",
        "question": "What is applied over an input signal composed of several input planes?",
        "context": "Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called what?",
        "context": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "large containing tensor",
        "question": "What is combined an array of sliding local blocks into?",
        "context": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D max pooling",
        "question": "What type of pooling is applied over an input signal composed of several input planes?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D average-pooling operation",
        "question": "What type of operation is applied in kHkWkH times kWkHkW regions?",
        "context": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D average-pooling operation",
        "question": "What type of operation is applied in kHkWkH times kWkHkW regions by step size?",
        "context": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device",
        "question": "What does this do?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "2D",
        "question": "What type of average-pooling operation is applied in kHkWkH times kWkHkW regions?",
        "context": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D average-pooling operation",
        "question": "What is applied in kTkHkWkT times kH times kWkTk",
        "context": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D average pooling",
        "question": "Applies what over an input signal composed of several input planes?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D average-pooling",
        "question": "What type of operation does kHkWkH times kWkHkW regions by step size?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D average-pooling",
        "question": "What type of operation is applied in kTkHkWkT times kH times kWk",
        "context": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "tensor",
        "question": "What is a large containing?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D",
        "question": "What type of average-pooling operation does kHkWkH times kWkHkW regions by step size?",
        "context": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D average-pooling operation",
        "question": "What does kTkHkWkT times kH times kWkTkH",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D",
        "question": "What type of average-pooling operation does kTkHkWkT times kH times ",
        "context": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D average-pooling",
        "question": "What type of pooling operation does kHkWkH times kWkHkW regions by step size?",
        "context": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D average-pooling",
        "question": "What operation does kHkWkH times kWkHkW regions by step size sHsWs",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 2D max pooling over an input signal composed of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 3D max pooling over an input signal composed of what?",
        "context": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D",
        "question": "What is the max pooling over an input signal composed of several input planes?",
        "context": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MaxPool3d",
        "question": "Computes a partial inverse of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D average-pooling",
        "question": "What operation does kTkHkWkT times kH times kWkTkH",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "A 3D max pooling over an input signal composed of what?",
        "context": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What does a 3D max pooling over an input signal composed of several input planes do?",
        "context": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool2d",
        "question": "What does the 3D pooling over an input signal consist of several input planes do?",
        "context": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What does a 3D pooling over an input signal consist of several input planes do?",
        "context": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool2d",
        "question": "What does a partial inverse of MaxPool1d do?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What does a partial inverse of MaxPool2d do?",
        "context": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D power-average",
        "question": "What is the pooling over an input signal composed of several input planes?",
        "context": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D",
        "question": "What type of max pooling is applied over an input signal composed of several input planes?",
        "context": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What does the 3D max pooling over an input signal consist of?",
        "context": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What is the result of Computes a partial inverse of MaxPool3d?",
        "context": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What does a 3D max pooling over an input signal consist of several input planes do?",
        "context": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "A 2D power-average pooling over an input signal composed of what?",
        "context": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D",
        "question": "What is the adaptive max pooling over an input signal composed of several input planes?",
        "context": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What does the 3D max pooling over an input signal consist of several input planes do?",
        "context": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool2d",
        "question": "What is the result of the 3D max pooling over an input signal composed of several input planes?",
        "context": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D",
        "question": "What is the power-average pooling over an input signal composed of several input planes?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "tensor",
        "question": "An array of sliding local blocks is combined into a large containing what?",
        "context": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes called what?",
        "context": "  Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called what?",
        "context": "  Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "A 2D adaptive max pooling over an input signal composed of what?",
        "context": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse",
        "question": "What does MaxPool1d do?",
        "context": "Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool2d",
        "question": "What does Computes a partial inverse of MaxPool2d?",
        "context": "Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool2d",
        "question": "What does Computes a partial inverse of MaxPool3d?",
        "context": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D",
        "question": "What type of power-average pooling over an input signal composed of several input planes?",
        "context": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 1D adaptive max pooling over an input signal composed of what?",
        "context": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 2D adaptive max pooling over an input signal composed of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What does MaxPool2d do?",
        "context": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What does Compute a partial inverse of MaxPool3d do?",
        "context": "Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D adaptive max pooling",
        "question": "What does MaxPool3d apply over an input signal composed of several input planes?",
        "context": "Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D",
        "question": "What type of adaptive max pooling over an input signal composed of several input planes?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What does MaxPool3d do?",
        "context": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "A 1D adaptive max pooling over an input signal composed of what?",
        "context": "Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D",
        "question": "What type of adaptive max pooling is applied over an input signal composed of several input planes?",
        "context": "Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D",
        "question": "How large is the adaptive max pooling over an input signal composed of several input planes?",
        "context": "Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "A 2D adaptive max pooling over an input signal is composed of what?",
        "context": "Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D",
        "question": "What is the adaptive average pooling over an input signal composed of several input planes?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 1D adaptive average pooling over an input signal composed of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D",
        "question": "What type of adaptive average pooling over an input signal composed of several input planes?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "A 1D adaptive average pooling over an input signal composed of what?",
        "context": "Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "A 2D adaptive average pooling over an input signal composed of what?",
        "context": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D",
        "question": "Applies a 3D adaptive max pooling over an input signal composed of several input planes?",
        "context": "Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D",
        "question": "How large is the adaptive average pooling over an input signal composed of several input planes?",
        "context": "Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What is the 3D adaptive average pooling over an input signal composed of?",
        "context": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 3D adaptive max pooling over an input signal composed of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 2D adaptive average pooling over an input signal composed of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "A 3D adaptive average pooling over an input signal composed of what?",
        "context": "Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D fractional max pooling over an input signal composed of several input planes",
        "question": "What does a 3D adaptive average pooling over an input signal composed of several input planes do?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 3D adaptive average pooling over an input signal composed of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Thresholds",
        "question": "What is the name of each element of the input Tensor?",
        "context": "Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "hardtanh",
        "question": "What is the in-place version of?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "rectified linear unit function",
        "question": "What does threshold apply element-wise?",
        "context": "Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "relu()",
        "question": "What is the in-place version of threshold()?",
        "context": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "hardswish function",
        "question": "What is the name of the function that is element-wise?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "In-place",
        "question": "What version of threshold() is used?",
        "context": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "rectified linear unit function",
        "question": "What element-wise function does threshold() use?",
        "context": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "elu()",
        "question": "In-place version of what function?",
        "context": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   Applies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).   Applies a softmin function.   Applies a softmax function.   Applies the soft shrinkage function elementwise   Samples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "hardswish function",
        "question": "What is the element-wise function of the input Tensor?",
        "context": "Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "rectified linear unit function",
        "question": "What does threshold() apply element-wise?",
        "context": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "rectified linear unit function",
        "question": "What element-wise function does relu() use?",
        "context": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "hardswish function",
        "question": "What is the element-wise function of the HardTanh function?",
        "context": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "rectified linear unit function",
        "question": "What does relu do element-wise?",
        "context": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "relu()",
        "question": "What is the in-place version of the rectified linear unit function?",
        "context": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "hardtanh()",
        "question": "What is the in-place version of HardTanh?",
        "context": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "HardTanh",
        "question": "What is the in-place version of hardtanh()?",
        "context": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "hardtanh()",
        "question": "What is the in-place version of relu()?",
        "context": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "hardswish function",
        "question": "What is the element-wise function that is described in the paper?",
        "context": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "element-wise",
        "question": "How does elu(x) apply?",
        "context": "Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "element-wise",
        "question": "In-place version of elu(). Applies what?",
        "context": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "element-wise",
        "question": "How does SELU(x)=scale(max(0,x)+min(0,(exp(x",
        "context": "Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "CELU(x)",
        "question": "What =max(0,x)+min(0,(exp(x/)1))?",
        "context": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "LeakyReLU(x)",
        "question": "What =max(0,x)+negative_slopemin(0,x)?",
        "context": "Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "LeakyReLU",
        "question": "What does x =max(0,x)+negative_slopemin(0,x)textLeaky",
        "context": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "0,x",
        "question": "What is the value of the function PReLU(x)?",
        "context": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "gated linear unit",
        "question": "What is the in-place version of rrelu()?",
        "context": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "rrelu()",
        "question": "What is the in-place version of Randomized leaky ReLU?",
        "context": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "gated linear unit",
        "question": "What type of unit is the LeakyReLU?",
        "context": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "weight",
        "question": "What does PReLU(x)=max(0,x)+?",
        "context": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "elu()",
        "question": "What is the in-place version of leaky_relu()?",
        "context": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "linear",
        "question": "What type of transformation does y=xAT+by = xAT + by=xAT+b?",
        "context": "  Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b.   Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "bilinear",
        "question": "What type of transformation does y=x1TAx2+by apply to the incoming data?",
        "context": "  Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b.   Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "probability p",
        "question": "What is the probability of zeroing some of the elements of the input tensor?",
        "context": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "alpha dropout",
        "question": "What is applied to the input?",
        "context": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "feature map",
        "question": "What is a channel?",
        "context": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D feature map",
        "question": "What type of map is a channel?",
        "context": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "a Bernoulli distribution",
        "question": "Randomly zeroes some of the elements of the input tensor with probability p using samples from what?",
        "context": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "alpha dropout",
        "question": "What type of dropout is applied to the input?",
        "context": "Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "alpha dropout",
        "question": "Applies what to the input?",
        "context": "Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Randomly masks out entire channels",
        "question": "What happens when a channel is a feature map?",
        "context": "  Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D feature map",
        "question": "A channel is a what?",
        "context": "  Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D feature map",
        "question": "What is a channel called?",
        "context": "  During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor). ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "a fixed dictionary and size",
        "question": "A simple lookup table that looks up embeddings in what?",
        "context": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "instantiating the intermediate embeddings",
        "question": "Computes sums, means or maxes of bags of embeddings without what?",
        "context": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "zeros",
        "question": "What does the tensor of shape have everywhere except where the index of last dimension matches the corresponding value of the input tensor?",
        "context": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "A simple lookup table",
        "question": "What looks up embeddings in a fixed dictionary and size?",
        "context": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes sums, means or maxes",
        "question": "What does the lookup table do without instantiating the intermediate embeddings?",
        "context": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "num_classes",
        "question": "What is the name of the tensor that returns a tensor of shape?",
        "context": "  A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "dim",
        "question": "Cosine similarity between x1 and x2 is computed along what?",
        "context": "  See torch.nn.PairwiseDistance for details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "p-norm",
        "question": "What is the distance between every pair of row vectors in the input?",
        "context": "  See torch.nn.PairwiseDistance for details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "cosine",
        "question": "What type of similarity between x1 and x2 is returned by torch.nn.PairwiseDistance?",
        "context": "  See torch.nn.PairwiseDistance for details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "p-norm",
        "question": "Computes the distance between every pair of row vectors in the input?",
        "context": "  See torch.nn.PairwiseDistance for details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "the target and the output",
        "question": "What is the Binary Cross Entropy between?",
        "context": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Binary Cross Entropy",
        "question": "What does the function measure between target and output logits?",
        "context": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Poisson",
        "question": "What type of negative log likelihood loss does CosineEmbeddingLoss combine?",
        "context": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "CosineEmbeddingLoss",
        "question": "What criterion combines log_softmax and nll_loss in a single function?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "log_softmax and nll_loss",
        "question": "CosineEmbeddingLoss combines what two criterions in a single function?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "CosineEmbeddingLoss",
        "question": "What is the criterion that combines log_softmax and nll_loss in a single function?",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Gaussian",
        "question": "What type of negative log likelihood loss is HingeEmbeddingLoss?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Connectionist Temporal Classification loss",
        "question": "What is the Gaussian negative log likelihood loss?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Kullback-Leibler divergence Loss",
        "question": "What is the name of the function that measures the Binary Cross Entropy between the target and the output?",
        "context": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Binary Cross Entropy",
        "question": "What does the function measure between the target and the output?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Binary Cross Entropy",
        "question": "What does the function measure between the target and output logits?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Poisson negative log likelihood loss",
        "question": "What is a function that measures Binary Cross Entropy between target and output logits?",
        "context": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "CosineEmbeddingLoss",
        "question": "What is the name of the function that measures Poisson negative log likelihood loss?",
        "context": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "log_softmax and nll_loss",
        "question": "What criterion combines in a single function?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Connectionist Temporal Classification loss",
        "question": "What is a Gaussian negative log likelihood loss?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Gaussian negative log likelihood loss",
        "question": "What type of loss is HingeEmbeddingLoss?",
        "context": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Kullback-Leibler",
        "question": "What divergence Loss?",
        "context": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Poisson",
        "question": "What type of negative log likelihood loss does CosineEmbeddingLoss measure?",
        "context": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "log_softmax and nll_loss",
        "question": "CosineEmbeddingLoss combines what two criterion in a single function?",
        "context": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Connectionist Temporal Classification loss",
        "question": "What is the criterion for Gaussian negative log likelihood loss?",
        "context": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "The Kullback-Leibler divergence Loss Function",
        "question": "What takes the mean element-wise absolute value difference?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Poisson negative log likelihood loss",
        "question": "What is the function that measures Binary Cross Entropy between target and output logits?",
        "context": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "CosineEmbeddingLoss",
        "question": "What is the name of the function that measures Binary Cross Entropy between target and output logits?",
        "context": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Connectionist Temporal Classification loss",
        "question": "What is the term for the loss of log_softmax and nll_loss in a single function?",
        "context": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Gaussian negative log likelihood loss",
        "question": "What type of loss does HingeEmbeddingLoss measure?",
        "context": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Kullback-Leibler",
        "question": "What divergence Loss Function takes the mean element-wise absolute value difference?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Poisson",
        "question": "CosineEmbeddingLoss combines log_softmax and nll_loss in a single function?",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Connectionist Temporal Classification loss",
        "question": "What is the name of the criterion that combines log_softmax and nll_loss in a single function?",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Measures the element-wise mean squared error",
        "question": "What does the Kullback-Leibler divergence Loss Function measure?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MarginRankingLoss",
        "question": "What is the Kullback-Leibler divergence Loss Function that measures the element-wise mean squared error?",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Poisson negative log likelihood loss",
        "question": "What is the loss that combines log_softmax and nll_loss in a single function?",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "CosineEmbeddingLoss",
        "question": "What is the name of the function that combines log_softmax and nll_loss in a single function?",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Gaussian",
        "question": "What type of negative log likelihood loss?",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MarginRankingLoss",
        "question": "What function measures the element-wise mean squared error?",
        "context": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "HingeEmbeddingLoss",
        "question": "What is the name of the Gaussian negative log likelihood loss function?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelMarginLoss",
        "question": "What is the Kullback-Leibler divergence Loss Function called?",
        "context": "The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelSoftMarginLoss",
        "question": "What does the Kullback-Leibler divergence Loss Function use for details?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "log_softmax and nll_loss",
        "question": "CosineEmbeddingLoss combines what in a single function?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Gaussian negative log likelihood loss",
        "question": "What is another term for the Connectionist Temporal Classification loss?",
        "context": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelMarginLoss",
        "question": "What is the name of the function that takes the mean element-wise absolute value difference?",
        "context": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelMarginLoss",
        "question": "What is the name of the function that measures the mean element-wise mean squared error?",
        "context": "Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "log_softmax and nll_loss",
        "question": "What does this criterion combine in a single function?",
        "context": "This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelMarginLoss",
        "question": "What is the Kullback-Leibler divergence Loss Function that takes the mean element-wise absolute value difference?",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelSoftMarginLoss",
        "question": "What is the Kullback-Leibler divergence Loss Function?",
        "context": "The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelSoftMarginLoss",
        "question": "What is the name of the MultiLabelSoftMarginLoss function?",
        "context": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "nll_loss",
        "question": "This criterion combines log_softmax and what else in a single function?",
        "context": "This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Gaussian negative log likelihood loss",
        "question": "What is the Connectionist Temporal Classification loss?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelSoftMarginLoss",
        "question": "What is the name of the function that measures the element-wise mean squared error?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Gaussian negative log likelihood loss",
        "question": "What is HingeEmbeddingLoss for details?",
        "context": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelMarginLoss",
        "question": "What is the name of the Loss Function that takes the mean element-wise absolute value difference?",
        "context": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "HingeEmbeddingLoss",
        "question": "What is another name for Gaussian negative log likelihood loss?",
        "context": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelSoftMarginLoss",
        "question": "What is another name for MultiLabelSoftMarginLoss?",
        "context": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "multi_margin_loss",
        "question": "What is the term for input, target, p=1, margin=1, weight=None, size_average=None?",
        "context": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "multi_margin_loss",
        "question": "What function measures the negative log likelihood loss?",
        "context": "Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Gaussian negative log likelihood loss",
        "question": "What is HingeEmbeddingLoss?",
        "context": "Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelMarginLoss",
        "question": "What is the name of the function that measures the mean squared error?",
        "context": "See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "negative log likelihood loss",
        "question": "What is multi_margin_loss?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "negative log likelihood loss",
        "question": "Multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=N",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "negative log likelihood loss",
        "question": "What is multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average",
        "context": "See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "negative log likelihood loss",
        "question": "What is the result of multi_margin_loss?",
        "context": "The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "the mean element-wise absolute value difference",
        "question": "What does the function take?",
        "context": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Measures the element-wise mean squared error",
        "question": "What does MarginRankingLoss do?",
        "context": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelSoftMarginLoss",
        "question": "What function uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise?",
        "context": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise",
        "question": "When does a squared term use a function that uses a squared term?",
        "context": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "the element-wise mean squared error",
        "question": "What does the function measure?",
        "context": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "negative log likelihood loss",
        "question": "What does multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "squared term",
        "question": "What term is used if the absolute element-wise error falls below delta?",
        "context": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "the element-wise mean squared error",
        "question": "What does MarginRankingLoss measure?",
        "context": "Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "if the absolute element-wise error falls below beta and an L1 term otherwise",
        "question": "When does a function use a squared term?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "negative log likelihood loss",
        "question": "What does multi_margin_loss measure?",
        "context": "Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "TripletMarginLoss",
        "question": "What is the name of the function that uses a squared term if the absolute element-wise error falls below beta?",
        "context": "  See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "SoftMarginLoss",
        "question": "What is the name of the function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "a squared term",
        "question": "What does a function use if the absolute element-wise error falls below delta?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "a squared term",
        "question": "What does a function use if the absolute element-wise error falls below beta and an L1 term otherwise?",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelSoftMarginLoss",
        "question": "What is the name of the function that uses a squared term if the absolute element-wise error falls below delta?",
        "context": "  See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "squared term",
        "question": "What term is used if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "beta",
        "question": "What is the absolute element-wise error that uses a squared term if the absolute element-wise error falls below?",
        "context": "  See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "delta",
        "question": "What does the absolute element-wise error fall below?",
        "context": "  See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "a squared term",
        "question": "What does a function use if the absolute element-wise error falls below beta?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "delta",
        "question": "What is the absolute element-wise error below?",
        "context": "See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "MultiLabelSoftMarginLoss",
        "question": "What is the name of the function that uses a squared term if the absolute element-wise error falls below delta and a delta-scale",
        "context": "  See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "upscale_factor",
        "question": "What is r?",
        "context": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "PixelShuffle",
        "question": "What operation is reversed by rearranging elements in a tensor of shape?",
        "context": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Pads tensor",
        "question": "What is the downscale_factor of a tensor of shape?",
        "context": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "r",
        "question": "What ranges elements in a tensor of shape?",
        "context": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "PixelShuffle",
        "question": "Reverses what operation by rearranging elements in a tensor of shape?",
        "context": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Pads",
        "question": "What is the tensor of a tensor?",
        "context": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "bilinear upsampling",
        "question": "What is used to upsample the input?",
        "context": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "r",
        "question": "What is the downscale_factor of the PixelShuffle operation?",
        "context": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Pads",
        "question": "What is the tensor of the PixelShuffle operation?",
        "context": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Tensor",
        "question": "What is the name of the tensor?",
        "context": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Down/up",
        "question": "What is the name of the step that samples the input to the given size or the given scale_factor?",
        "context": "  Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "input values and pixel locations",
        "question": "What is used to compute the output of a flow-field grid?",
        "context": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "affine matrices theta",
        "question": "What is given a batch of to generate a 2D or 3D flow field?",
        "context": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Pads tensor",
        "question": "What is the name of the tensor that Down/up samples the input to either the given size or the given scale_factor?",
        "context": "  Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "bilinear upsampling",
        "question": "Upsamples the input using what?",
        "context": "  Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "input values and pixel locations",
        "question": "What is computed from a flow-field grid?",
        "context": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "affine matrices",
        "question": "What type of theta is used to generate a flow field?",
        "context": "  Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "device_ids",
        "question": "What are the GPUs given in?",
        "context": "torch.nn.parallel.data_parallel Evaluates module(input) in parallel across the GPUs given in device_ids. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "module",
        "question": "What does torch.nn.parallel.data_parallel evaluate in parallel across the GPUs given in device_ids?",
        "context": "torch.nn.parallel.data_parallel Evaluates module(input) in parallel across the GPUs given in device_ids. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "device_ids",
        "question": "What does torch.nn.parallel.data_parallel use to evaluate module(input) in parallel across GPUs?",
        "context": "torch.nn.parallel.data_parallel Evaluates module(input) in parallel across the GPUs given in device_ids. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Matrix product of two tensors",
        "question": "What does the behavior depend on the dimensionality of the tensors as follows?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "dot product",
        "question": "What is returned if both tensors are 1-dimensional?",
        "context": "If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix-matrix product",
        "question": "What is returned if both arguments are 2-dimensional?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1",
        "question": "What is the dimension of the first argument?",
        "context": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the prepended dimension is removed",
        "question": "What happens after the matrix multiply?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "Matrix product",
        "question": "What product of two tensors is returned?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix-matrix",
        "question": "What product is returned if both arguments are 2-dimensional?",
        "context": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1",
        "question": "What is the dimensionality of the first argument?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the prepended dimension",
        "question": "What is removed after the matrix multiply?",
        "context": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix multiply",
        "question": "When is the prepended dimension removed?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "a 1",
        "question": "If the first argument is 1-dimensional and the second argument is 2-dimensional, what is prepended to its dimension for the purpose of the matrix multiply?",
        "context": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix-vector product",
        "question": "What is returned if the first argument is 2-dimensional and the second argument is 1-dimensional?",
        "context": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix-vector",
        "question": "What product is returned if the first argument is 2-dimensional and the second argument is 1-dimensional?",
        "context": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1",
        "question": "If the first argument is 2-dimensional and the second is 2-dimensional, how many dimensions does the matrix-matrix product return?",
        "context": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "removed",
        "question": "What happens to the prepended dimension after the matrix multiply?",
        "context": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "2-dimensional",
        "question": "The matrix-vector product is returned if the first argument is what?",
        "context": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If the first argument is 2-dimensional and the second argument is what?",
        "context": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the broadcasting logic only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions",
        "question": "What does the broadcasting logic only look at when determining if the inputs are broadcastable?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the final two dimensions",
        "question": "What are different in the matrix dimensions?",
        "context": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix dimensions",
        "question": "What are the final two dimensions?",
        "context": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "TensorFloat32",
        "question": "What does this operator support?",
        "context": "This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "What dimension does the dot product version of this function not support an out parameter?",
        "context": "This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "torch.acos",
        "question": "What is an alias for?",
        "context": "Alias for torch.acos(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos"
    },
    {
        "answer": "torch.mul()",
        "question": "What is an Alias for?",
        "context": "Alias for torch.mul(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply"
    },
    {
        "answer": "Counts the number of non-zero values in the tensor input along the given dim",
        "question": "What does it do when a tensor input is not specified?",
        "context": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "answer": "all non-zeros",
        "question": "What is counted if no dim is specified?",
        "context": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "answer": "python:ints",
        "question": "What is a tuple of dims along which to count non-zeros?",
        "context": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "answer": "Example:",
        "question": "What is an example of a tuple of dims along which to count non-zeros?",
        "context": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "answer": "Algorithm 5.1",
        "question": "What is the implementation based on?",
        "context": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "pseudorandom number generator",
        "question": "To obtain repeatable results, reset the seed for what?",
        "context": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "10-fold",
        "question": "How much higher performance is torch.linalg.svd() compared to the full-rank SVD implementation?",
        "context": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "sparse matrices",
        "question": "What type of matrices will low-rank SVD be useful for?",
        "context": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "2009",
        "question": "When was the Algorithm 5.1 implemented?",
        "context": "The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "low-rank matrix",
        "question": "What is the input assumed to be?",
        "context": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "10-fold higher performance characteristics",
        "question": "Why should you use the full-rank SVD implementation for dense matrices?",
        "context": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "huge sparse matrices",
        "question": "What type of matrices is low-rank SVD useful for?",
        "context": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "10-fold",
        "question": "How much higher performance is torch.linalg.svd() compared to full-rank SVD?",
        "context": "The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "A",
        "question": "What is the input tensor of size?",
        "context": "To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "huge sparse matrices",
        "question": "What is the low-rank SVD useful for?",
        "context": "Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "q",
        "question": "What is a slightly overestimated rank of A?",
        "context": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "niter",
        "question": "What must be a nonnegative integer?",
        "context": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "huge sparse matrices",
        "question": "The low-rank SVD is useful for what type of matrices that torch.linalg.svd() cannot handle?",
        "context": "The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "10-fold",
        "question": "How much higher performance characteristics do full-rank SVD implementations have?",
        "context": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "low-rank SVD",
        "question": "What will be useful for huge sparse matrices that torch.linalg.svd() cannot handle?",
        "context": "Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "low-rank SVD",
        "question": "What is useful for huge sparse matrices that torch.linalg.svd() cannot handle?",
        "context": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "a slightly overestimated rank of A. conduct",
        "question": "What does q mean?",
        "context": "A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "arXiv:0909.4061",
        "question": "What is the name of the book that Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp published in 2009?",
        "context": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "iii th",
        "question": "What grid is defined by expanding the iii th input over dimensions defined by other inputs?",
        "context": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "scalar or 1-dimensional vector",
        "question": "What can NNN tensors be?",
        "context": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "list of scalars or 1 dimensional tensors",
        "question": "What are tensors?",
        "context": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "Scalars",
        "question": "What will be treated as tensors of size automatically?",
        "context": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "list of scalars or 1 dimensional tensors",
        "question": "What is tensors?",
        "context": "tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "Scalars",
        "question": "What will be treated as tensors of size?",
        "context": "tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "seq",
        "question": "What is an example of a sequence of Tensors?",
        "context": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "kkk tensors",
        "question": "If the input has what of size (N1,),(N2,),...,(Nk,)(N_1,), (N_2,",
        "context": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "seq",
        "question": "What is the name for the sequence of Tensors?",
        "context": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "the number of threads",
        "question": "Returns what number of threads used for inter-op parallelism on CPU?",
        "context": "Returns the number of threads used for inter-op parallelism on CPU\n(e.g. in JIT interpreter) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads"
    },
    {
        "answer": "JIT interpreter",
        "question": "In what is the number of threads used for inter-op parallelism on CPU used?",
        "context": "Returns the number of threads used for inter-op parallelism on CPU\n(e.g. in JIT interpreter) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads"
    },
    {
        "answer": "if the input is a single element tensor",
        "question": "Returns True what if the input is a single element tensor?",
        "context": "Returns True if the input is a single element tensor which is not equal to zero\nafter type conversions.\ni.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or\ntorch.tensor([False]).\nThrows a RuntimeError if torch.numel() != 1 (even in case\nof sparse tensors). input (Tensor) \u2013 the input tensor. Examples: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero"
    },
    {
        "answer": "torch.tensor([0]) or torch.tensor([False])",
        "question": "Returns True if the input is a single element tensor which is not equal to what?",
        "context": "Returns True if the input is a single element tensor which is not equal to zero\nafter type conversions.\ni.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or\ntorch.tensor([False]).\nThrows a RuntimeError if torch.numel() != 1 (even in case\nof sparse tensors). input (Tensor) \u2013 the input tensor. Examples: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero"
    },
    {
        "answer": "RuntimeError",
        "question": "What throws if torch.numel()!= 1?",
        "context": "Returns True if the input is a single element tensor which is not equal to zero\nafter type conversions.\ni.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or\ntorch.tensor([False]).\nThrows a RuntimeError if torch.numel() != 1 (even in case\nof sparse tensors). input (Tensor) \u2013 the input tensor. Examples: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero"
    },
    {
        "answer": "A torch.Tensor",
        "question": "What is a multi-dimensional matrix containing elements of a single data type?",
        "context": "A torch.Tensor is a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "10 tensor types with CPU and GPU variants",
        "question": "How many tensor types does Torch define?",
        "context": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "CPU and GPU",
        "question": "What are the two variants of Torch's tensor types?",
        "context": "A torch.Tensor is a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32",
        "question": "What is the bit floating point torch?",
        "context": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "128-bit",
        "question": "What is the complex of a BFloat16Tensor torch?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit floating point torch",
        "question": "What is the data type dtype CPU tensor GPU tensor?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dtype",
        "question": "What is the name of the CPU tensor GPU tensor 32-bit floating point torch?",
        "context": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dtype CPU tensor GPU tensor",
        "question": "What type of tensor is a 32-bit floating point torch?",
        "context": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit floating point torch",
        "question": "What type of torch is a GPU tensor?",
        "context": "CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit floating point torch",
        "question": "What is the name of the GPU tensor?",
        "context": "CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit floating point torch",
        "question": "What is the GPU tensor?",
        "context": "GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32",
        "question": "What bit floating point torch is GPU tensor?",
        "context": "GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch",
        "question": "What type of torch is float32?",
        "context": "torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "unsigned",
        "question": "What is a double 8-bit integer?",
        "context": "32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "bfloat16 torch",
        "question": "What is the name of the torch?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "double torch",
        "question": "What is another name for a double-tensor torch?",
        "context": "torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit",
        "question": "What type of complex torch is included in the BFloat16Tensor 32-bit complex torch?",
        "context": "torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "double torch",
        "question": "What is a double torch?",
        "context": "torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "double torch",
        "question": "What is another name for a double torch?",
        "context": "torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit",
        "question": "What is the floating point of a DoubleTensor torch?",
        "context": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point 1 torch",
        "question": "What is a half torch?",
        "context": "16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point",
        "question": "What is 1 torch.float16 or torch.half torch?",
        "context": "16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "half torch",
        "question": "What type of torch is a HalfTensor torch?",
        "context": "torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.float16 or torch.half torch",
        "question": "What are the names of the two types of torch?",
        "context": "torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32",
        "question": "How many bits does a BFloat16Tensor have?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32",
        "question": "How many byte complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or",
        "context": "torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point 2 torch",
        "question": "What type of torch is a HalfTensor?",
        "context": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.cuda",
        "question": "What is the name of HalfTensor 16-bit floating point 2 torch?",
        "context": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point 2 torch",
        "question": "What type of torch is a bfloat16 torch?",
        "context": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point 2",
        "question": "What type of torch is bfloat16?",
        "context": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit complex torch.complex32 64-bit complex torch.complex64",
        "question": "What is the 32-bit complex torch.complex32 64-bit complex torch.complex64?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "64-bit",
        "question": "What type of complex torch is a BFloat16Tensor 32-bit complex torch?",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit complex torch.complex32 64-bit complex torch.complex64",
        "question": "What is a torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.byteTensor",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit",
        "question": "What type of complex torch is a BFloat16Tensor?",
        "context": "64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32",
        "question": "How many bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch",
        "context": "torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit",
        "question": "What is the most common type of complex torch?",
        "context": "32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "LongTensor",
        "question": "What is the name of the torch that has a 64-bit integer?",
        "context": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "long torch",
        "question": "What type of torch is LongTensor?",
        "context": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "64-bit",
        "question": "What type of complex torch is a torch?",
        "context": "64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "LongTensor Boolean",
        "question": "What is the name of the Boolean?",
        "context": "64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Boolean torch",
        "question": "What type of torch is a LongTensor?",
        "context": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "LongTensor torch.cuda",
        "question": "What is a LongTensor Boolean torch.bool?",
        "context": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "128",
        "question": "How many bits is a complex torch?",
        "context": "128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "128-bit complex torch",
        "question": "What type of torch is.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch",
        "context": "128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.cdouble",
        "question": "What is another name for unsigned 8-bit integer torch.uint8 torch?",
        "context": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit",
        "question": "What type of integer is a torch?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What is a torch.uint8 torch?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What is a ByteTensor?",
        "context": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What is the sign of a ByteTensor torch?",
        "context": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What type of torch is a ByteTensor?",
        "context": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What does ByteTensor stand for?",
        "context": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit integer",
        "question": "What is a torch?",
        "context": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit integer",
        "question": "What is the sign of a torch?",
        "context": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16",
        "question": "How many bits is a CharTensor?",
        "context": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.int8",
        "question": "What is the name of the torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed)",
        "context": "torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.int16 or torch.short torch",
        "question": "What is a CharTensor 16-bit integer (signed)?",
        "context": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit",
        "question": "What type of integer is a short torch?",
        "context": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16",
        "question": "Int16 or torch.short torch.ShortTensor torch.cuda.IntTensor 32-bit",
        "context": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit integer",
        "question": "What is a shorttensor?",
        "context": "torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "3",
        "question": "How many 2-bit integers are in a torch?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "4-bit integer",
        "question": "What is the sign of an IntTensor?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit integer",
        "question": "What is a torch.int32 or torch.int torch?",
        "context": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Python",
        "question": "What language can a tensor be constructed from?",
        "context": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "requires_grad_() or detach()",
        "question": "What are two ways to avoid a copy of a Tensor?",
        "context": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.as_tensor()",
        "question": "What is used if you have a numpy array and want to avoid a copy?",
        "context": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "by passing a torch.dtype and/or a torch",
        "question": "How can a tensor of specific data type be constructed?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.tensor()",
        "question": "A tensor can be constructed from a Python list or sequence using what constructor?",
        "context": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "requires_grad_() or detach()",
        "question": "What are two ways to avoid a copy of a tensor?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.as_tensor()",
        "question": "If you have a numpy array and want to avoid a copy, use what?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.dtype",
        "question": "A tensor of specific data type can be constructed by passing a what?",
        "context": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning torch.tensor()",
        "question": "What always copies data?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "numpy array",
        "question": "What type of data does torch.as_tensor() copy?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Creation Ops",
        "question": "For more information about building Tensors, see what?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "always copies data",
        "question": "What does torch.tensor() do?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Python",
        "question": "What language can be used to access and modify the contents of a tensor?",
        "context": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "indexing",
        "question": "The contents of a tensor can be accessed and modified using Python\u2019s what?",
        "context": "A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "indexing and slicing notation",
        "question": "The contents of a tensor can be accessed and modified using Python's what?",
        "context": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "requires_grad=True",
        "question": "What is required to create a tensor?",
        "context": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "an associated torch.Storage",
        "question": "What holds data for each tensor?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "multi-dimensional, strided view of a storage",
        "question": "What does the tensor class provide?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Note",
        "question": "What is the name of the tensor class that defines numeric operations on it?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a tensor",
        "question": "What does torch.Tensor.item() get a Python number from?",
        "context": "Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.Storage",
        "question": "What holds a tensor's data?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "defines numeric operations on it",
        "question": "What does the tensor class do?",
        "context": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Note",
        "question": "What is another name for a tensor class?",
        "context": "Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor Views",
        "question": "For more information on tensor views, see what?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor Attributes",
        "question": "For more information on the torch.dtype, torch.device, and torch.layout attributes of a torch.Tensor,",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "an underscore suffix",
        "question": "Methods that mutate a tensor are marked with what?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a new tensor",
        "question": "What does torch.FloatTensor.abs() compute the result in?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Note",
        "question": "What does torch.FloatTensor.abs() do?",
        "context": "Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore suffix",
        "question": "Methods which mutate a tensor are marked with what?",
        "context": "For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore suffix",
        "question": "What are methods that mutate a tensor marked with?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "to() method",
        "question": "What method is used to change an existing tensor's torch.device and/or torch.dtype?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What does the to() method do to an existing tensor?",
        "context": "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor Attributes",
        "question": "For more information on the torch.Tensor, see what?",
        "context": "For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore",
        "question": "What suffix mark methods that mutate a tensor?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "computes the absolute value in-place and returns the modified tensor",
        "question": "What does torch.FloatTensor.abs_() do?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What is the name of the warning that is issued when a tensor is changed?",
        "context": "For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "to() method",
        "question": "What method can be used to change an existing tensor's torch.device and/or torch.dtype?",
        "context": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore",
        "question": "Methods which mutate a tensor are marked with what suffix?",
        "context": "For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore",
        "question": "What suffix is used to mark methods that mutate a tensor?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.FloatTensor.abs_()",
        "question": "What computes the absolute value in-place and returns the modified tensor?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What is a warning about a mutated tensor?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What does the to() method do?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What is the name of the warning that a method that mutates a tensor is marked with an underscore suffix?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What does the to() method do to change an existing tensor's torch.device and/or torch.dtype?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "unexpectedly high",
        "question": "What kind of memory usage might be caused by the current implementation of torch.Tensor?",
        "context": "Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "one large structure",
        "question": "What should you use if you have a lot of tiny tensors?",
        "context": "Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "memory overhead",
        "question": "What does the current implementation of torch.Tensor introduce?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "one large structure",
        "question": "What should you use if you have many tiny tensors?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor",
        "question": "What is the main way to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "pre-existing data",
        "question": "What does torch.tensor() create a tensor with?",
        "context": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "specific size",
        "question": "What do you need to create a tensor with?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Creation Ops",
        "question": "What is the name of the tensor creation ops?",
        "context": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor with the same size (and similar types) as another tensor",
        "question": "What type of tensor does torch create?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor",
        "question": "What type of tensor is similar to another tensor but different in size?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor",
        "question": "What type of tensor is created with the same size as another tensor?",
        "context": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor",
        "question": "What type of tensor is created with the same type but different size as another tensor?",
        "context": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "its dimensions",
        "question": "What is reversed in a tensor?",
        "context": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "specific size",
        "question": "What does torch create a tensor with?",
        "context": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Creation Ops",
        "question": "What is another name for tensor creation ops?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch",
        "question": "What is used to create a tensor with the same size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor.new_* creation ops",
        "question": "To create a tensor with similar type but different size as another tensor, use what?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "reversed",
        "question": "Is this Tensor with its dimensions reversed or reversed?",
        "context": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "x.permute",
        "question": "If n is the number of dimensions in x, x.T is equivalent to what?",
        "context": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.new_tensor",
        "question": "What is used to create a tensor with similar type but different size?",
        "context": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor.new_* creation ops",
        "question": "What is used to create a tensor with similar type but different size as another tensor?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "x",
        "question": "If n is the number of dimensions in what integer, x.T is equivalent to x.permute?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor",
        "question": "What returns a new Tensor with data as the tensor data?",
        "context": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "data",
        "question": "What does Tensor.new_tensor return as the tensor data?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "fill_value",
        "question": "What does tensor.new_full return a Tensor of size size filled with?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What does tensor.new_empty return a Tensor of size size filled with?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.device",
        "question": "What returns a Tensor of size size filled with uninitialized data?",
        "context": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "fill_value",
        "question": "What does Tensor.new_full return a Tensor of size size filled with?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.new_ones",
        "question": "What returns a tensor of size size filled with uninitialized data?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "x",
        "question": "If n is the number of dimensions in what, x.T is equivalent to x.permute(n-1, n-2",
        "context": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "data",
        "question": "What is returned as the tensor data?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What does the Tensor.new_empty return a Tensor of size size filled with?",
        "context": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "1",
        "question": "Tensor.new_ones Returns a Tensor of size size filled with what value?",
        "context": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "0.",
        "question": "Tensor.new_zeros Returns a Tensor of size size filled with what value?",
        "context": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.is_cuda",
        "question": "What is the name of the Tensor with its dimensions reversed?",
        "context": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What data does Tensor.new_empty return a Tensor of size size filled with?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "1",
        "question": "Tensor.new_ones Returns a Tensor of size size filled with what?",
        "context": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "0.",
        "question": "Tensor.new_zeros Returns a Tensor of size size filled with what?",
        "context": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.is_cuda",
        "question": "What returns a Tensor of size size filled with 0.",
        "context": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "x.permute",
        "question": "What is x.T equivalent to?",
        "context": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What does Tensor.new_empty return a Tensor of size size filled with?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True",
        "question": "Is the Tensor.is_cuda True or False if the Tensor is stored on the GPU?",
        "context": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What does a Tensor.new_empty return a Tensor of size size filled with?",
        "context": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "0.",
        "question": "What is the tensor of size size filled with?",
        "context": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True",
        "question": "Is Tensor.is_cuda True or False if the Tensor is stored on the GPU?",
        "context": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "data",
        "question": "What does the new Tensor return as the tensor data?",
        "context": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "GPU",
        "question": "Tensor.is_cuda Is True if the Tensor is stored on what?",
        "context": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True",
        "question": "Is the Tensor.is_quantized True or False if the Tensor is quantized?",
        "context": "Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.is_cuda",
        "question": "What is true if the Tensor is stored on the GPU?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.is_quantized",
        "question": "What is true if the Tensor is quantized?",
        "context": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "the GPU",
        "question": "Where is the Tensor stored?",
        "context": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a new Tensor",
        "question": "Returns what with data as the tensor data?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta tensor",
        "question": "What is the Tensor true if it is a meta tensor?",
        "context": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta",
        "question": "What is True if the Tensor is a meta tensor?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta",
        "question": "What is true if the Tensor is a meta tensor?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What is the Tensor of size size filled with?",
        "context": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True",
        "question": "Tensor.is_quantized Is what if the Tensor is quantized?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta tensor",
        "question": "What type of Tensor is True if the Tensor is a meta tensor?",
        "context": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "1",
        "question": "Returns a Tensor of size size filled with what?",
        "context": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch",
        "question": "What is the device where the Tensor is located?",
        "context": "Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "the torch.device where this Tensor is",
        "question": "What is the torch.device where the Tensor is located?",
        "context": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch",
        "question": "What device is used to store the Tensor?",
        "context": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.grad",
        "question": "What is the torch.device where the Tensor is?",
        "context": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor of size size filled with 1.",
        "question": "What does Tensor.new_ones return?",
        "context": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "0.",
        "question": "Tensor.new_zeros Returns a Tensor of size size filled with how many zeros?",
        "context": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch",
        "question": "What device is used to store a Tensor?",
        "context": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch",
        "question": "What is the device where the Tensor is stored?",
        "context": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "1",
        "question": "What is the size size of a Tensor of size size filled with?",
        "context": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch",
        "question": "What is the device where the Tensor is?",
        "context": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.grad",
        "question": "What is the torch.device where this Tensor is?",
        "context": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor of size size filled with 0.",
        "question": "What does Tensor.new_zeros return?",
        "context": "Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "None",
        "question": "What is the default value of Tensor.grad?",
        "context": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dim",
        "question": "What Alias for dim() Tensor.real?",
        "context": "Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "0.",
        "question": "What is the size of the Tensor?",
        "context": "Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta tensor",
        "question": "Is True if the Tensor is a what?",
        "context": "Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dim",
        "question": "What Alias for dim() Tensor.n Returns a new tensor containing real values of the self tens",
        "context": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real",
        "question": "What returns a new tensor containing real values of the self tensor?",
        "context": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True",
        "question": "Is Tensor.is_quantized True or False if the Tensor is quantized?",
        "context": "Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.device",
        "question": "What is the Tensor.device?",
        "context": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dim",
        "question": "What Alias for dim() Tensor.real Returns a new tensor containing real values of the self tens",
        "context": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True",
        "question": "Is the Tensor quantized?",
        "context": "Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "None",
        "question": "What is the default value of the attribute Tensor.grad?",
        "context": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values of the self tensor",
        "question": "What does the imag return a new tensor containing?",
        "context": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.imag",
        "question": "What returns a new tensor containing imaginary values of the self tensor?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "quantized",
        "question": "Is True if the Tensor is what?",
        "context": "Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.device",
        "question": "What is the name of the device where the Tensor is?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "None",
        "question": "What is the default attribute of Tensor.grad?",
        "context": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of the self tensor",
        "question": "What does Tensor.ndim return a new tensor containing?",
        "context": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values of the self tensor",
        "question": "What does Tensor.imag return a new tensor containing?",
        "context": "Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True",
        "question": "Is the Tensor.is_meta True or False if the Tensor is a meta tensor?",
        "context": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.device",
        "question": "Where is the Tensor?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values of the self tensor",
        "question": "What does imag return a new tensor containing?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs()",
        "question": "What is the name of the In-place version of abs()?",
        "context": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True",
        "question": "Is Tensor.is_meta True or False if the Tensor is a meta tensor?",
        "context": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs()",
        "question": "What is the In-place version of abs()?",
        "context": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True",
        "question": "Is the Tensor a meta tensor?",
        "context": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.device",
        "question": "What is the name of the device where the Tensor is located?",
        "context": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs() Tensor.abs",
        "question": "What is the in-place version of abs() Tensor.absolute?",
        "context": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "if the Tensor is a meta tensor",
        "question": "Is True if the Tensor is a meta tensor?",
        "context": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs() Tensor.absolute",
        "question": "What is the torch.abs() Tensor.abs_ In-place version of?",
        "context": "Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias",
        "question": "What is used for abs() Tensor.absolute?",
        "context": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.device",
        "question": "What is the name of the device where this Tensor is?",
        "context": "Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of the self tensor",
        "question": "What does Tensor.ndim Alias for dim() Tensor.real Return a new tensor containing?",
        "context": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.device",
        "question": "Where is the Tensor located?",
        "context": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.device",
        "question": "Where is this Tensor located?",
        "context": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs_() Tensor.acos",
        "question": "What Alias does torch.acos() provide for?",
        "context": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.abs",
        "question": "What does torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for",
        "context": "Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "None",
        "question": "What is the default value of the attribute that becomes a Tensor the first time a call to backward() computes gradients for self",
        "context": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.acos",
        "question": "What is the name of the Alias for abs?",
        "context": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "None by default",
        "question": "What is the default value of this attribute?",
        "context": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs() Tensor.absolute",
        "question": "What Alias for abs() Tensor.absolute_ In-place version of absolute()?",
        "context": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "add",
        "question": "What is another name for a tensor?",
        "context": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values of the self tensor",
        "question": "What does the new tensor contain?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs",
        "question": "What Alias for abs() Tensor.absolute Returns a new tensor containing real values of the self",
        "context": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of the self tensor",
        "question": "Alias for dim() Tensor.real Returns a new tensor containing what?",
        "context": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "add",
        "question": "What does a new tensor do?",
        "context": "Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real",
        "question": "What Returns a new tensor containing real values of the self tensor?",
        "context": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.abs",
        "question": "What Returns a new tensor containing imaginary values of the self tensor?",
        "context": "Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values of the self tensor",
        "question": "Returns a new tensor containing what?",
        "context": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a scalar or tensor",
        "question": "What does add to self tensor?",
        "context": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "scalar",
        "question": "What type of tensor can be added to a self tensor?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "add",
        "question": "What is the name of the In-place version of add() Tensor?",
        "context": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "scalar or tensor",
        "question": "What is added to self tensor?",
        "context": "Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "add() Tensor",
        "question": "What is the In-place version of Tensor.add?",
        "context": "Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "scalar or tensor",
        "question": "What does add add to self tensor?",
        "context": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs() Tensor",
        "question": "What is the In-place version of abs() Tensor?",
        "context": "Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is the name of the In-place version of addbmm() Tensor?",
        "context": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is the In-place version of addbmm() Tensor?",
        "context": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs() Tensor",
        "question": "What is the name of the in-place version of abs() Tensor?",
        "context": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs() Tensor",
        "question": "Abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolut",
        "context": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs() Tensor",
        "question": "What is the in-place version of Tensor?",
        "context": "Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is the name of the in-place version of add() Tensor?",
        "context": "In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "absolute",
        "question": "What Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_()",
        "context": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "add",
        "question": "What is the In-place version of add() Tensor?",
        "context": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs() Tensor.absolute",
        "question": "What is the In-place version of Alias for abs() Tensor.absolute?",
        "context": "In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias",
        "question": "What does Tensor.absolute use for abs?",
        "context": "Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addcdiv",
        "question": "What is the name of the In-place version of addcdiv?",
        "context": "Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a scalar or tensor to self tensor",
        "question": "What do you add to a scalar or tensor to self tensor?",
        "context": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias for abs() Tensor",
        "question": "What is the name of the in-place version of absolute() Alias for abs_() Tensor?",
        "context": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addcmul",
        "question": "What is the name of the function that adds a scalar or tensor to self tensor?",
        "context": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a scalar or tensor to self tensor",
        "question": "What does add add add?",
        "context": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "absolute",
        "question": "What is the In-place version of absolute() Alias for abs() Tensor?",
        "context": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias",
        "question": "What is used for abs_() Tensor?",
        "context": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addcmul",
        "question": "What is the name of the in-place version of addcdiv?",
        "context": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "acos() Tensor.arccos",
        "question": "What is the In-place version of absolute() Alias for abs_() Tensor.acos?",
        "context": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "absolute",
        "question": "What is the In-place version of absolute() Alias for abs_() Tensor?",
        "context": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.mul()",
        "question": "Alias for what?",
        "context": "Alias for torch.mul(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply"
    },
    {
        "answer": "non-zero",
        "question": "When talking about storing only what type of elements of a sparse array, the usage of adjective \"non-zero\" is not",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "array elements that are actually stored",
        "question": "What do we use \"specified elements\" for?",
        "context": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero value",
        "question": "What are unspecified elements assumed to have?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What does the term \"fill value\" mean?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "non-zero",
        "question": "When talking about storing only what elements of a sparse array, the usage of adjective \u201cnon-zero\u201d is not strict:",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "specified elements",
        "question": "What do we use for those array elements that are actually stored?",
        "context": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "fill value",
        "question": "What term is used to denote unspecified elements?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse storage format",
        "question": "What can be advantageous only when the size and sparsity levels of arrays are high?",
        "context": "Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "contiguous memory storage format",
        "question": "What is the most efficient way to store low-sparsity arrays?",
        "context": "Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "beta",
        "question": "The PyTorch API of sparse tensors is in what state?",
        "context": "Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse arrays",
        "question": "What can a sparse storage format be advantageous for storing?",
        "context": "Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "contiguous memory storage format",
        "question": "What is the most efficient way to store sparse arrays?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch API",
        "question": "What API of sparse tensors is in beta?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "when the size and sparsity levels of arrays are high",
        "question": "When can using a sparse storage format be advantageous?",
        "context": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "contiguous memory storage format",
        "question": "What is the most efficient approach for small-sized or low-sparsity arrays?",
        "context": "Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "beta",
        "question": "In what state is the PyTorch API of sparse tensors?",
        "context": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Coordinate format",
        "question": "What does PyTorch implement as one of the storage formats for implementing sparse tensors?",
        "context": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO format",
        "question": "What is another name for Coordinate format?",
        "context": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tuples of element indices and the corresponding values",
        "question": "What are the specified elements stored as in COO format?",
        "context": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices tensor of size",
        "question": "In what format are the indices of specified elements collected?",
        "context": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Coordinate format",
        "question": "What is the storage format for sparse tensors?",
        "context": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices tensor of size",
        "question": "What are the indices of specified elements collected in?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.int64",
        "question": "What is the name of the indices of specified elements in COO format?",
        "context": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch",
        "question": "Which API of sparse tensors is in beta?",
        "context": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Coordinate format",
        "question": "What does PyTorch implement?",
        "context": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Coordinate format",
        "question": "PyTorch implements what format for sparse tensors?",
        "context": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch",
        "question": "What is the name of the element type in the COO format?",
        "context": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "ndim",
        "question": "What is the dimensionality of the tensor and nse is the number of specified elements?",
        "context": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse",
        "question": "The indices of specified elements are collected in indices tensor of size (ndim) and with element type torch.int",
        "context": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse",
        "question": "What is another name for indices tensor of size?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "ndim",
        "question": "In indices tensor of size, what is the dimensionality of the tensor?",
        "context": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse",
        "question": "What is the tensor of size?",
        "context": "the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "ndim",
        "question": "What is the dimensionality of the tensor?",
        "context": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse bytes",
        "question": "What is the memory consumption of a sparse COO tensor?",
        "context": "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse bytes",
        "question": "The memory consumption of a sparse COO tensor is at least what?",
        "context": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "product",
        "question": "What is the memory consumption of a strided tensor?",
        "context": "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse",
        "question": "What is the number of specified elements in a tensor?",
        "context": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensor",
        "question": "What tensor is at least product(tensor shape>) * size of element type in bytes>?",
        "context": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensor",
        "question": "What type of tensor is at least product(tensor shape>) * size of element type in bytes>?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "100 000 = 2 000 000 bytes",
        "question": "What is the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "200 fold",
        "question": "How much memory saving occurs when using the COO storage format?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "2 000 000 bytes",
        "question": "What is the memory consumption of a 10 000 x 10 000 tensor with COO tensor layout?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "200 fold",
        "question": "What is the memory saving from using the COO storage format?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "by providing the two tensors of indices and values, as well as the size of the sparse tensor",
        "question": "How can a sparse COO tensor be constructed?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1, 0",
        "question": "What is the default value of the sparse tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "What is the default value of a sparse COO tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "write",
        "question": "What would we do to define a sparse COO tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "entry 3",
        "question": "What is the location of the sparse tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "What is the default value of the fill value?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the input i is NOT a list of index tuples",
        "question": "What should we note about the input i?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "specifying its size",
        "question": "How can an empty sparse COO tensor be constructed?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "What is the default value of a sparse tensor?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "i",
        "question": "What would we write to define a sparse tensor?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "index tuples",
        "question": "The input i is NOT a list of what?",
        "context": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Pytorch",
        "question": "What implements an extension of sparse tensors with scalar values to sparse tensors with (",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "hybrid tensors",
        "question": "What are sparse tensors with contiguous tensor values called?",
        "context": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "scalar",
        "question": "Pytorch implements an extension of sparse tensors with what values?",
        "context": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "hybrid tensors",
        "question": "What are sparse tensors with (contiguous) tensor values called?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices tensor of size",
        "question": "The indices of specified elements are collected in what?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "empty sparse COO tensor",
        "question": "What can be constructed by specifying its size only?",
        "context": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "scalar values",
        "question": "What do Pytorch extend sparse tensors with?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch",
        "question": "Which hybrid COO tensor extends the sparse COO tensor?",
        "context": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What does PyTorch hybrid COO tensor do?",
        "context": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch hybrid COO tensor",
        "question": "What extends the sparse COO tensor?",
        "context": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor",
        "question": "The indices of specified elements are collected in what indices of size?",
        "context": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse",
        "question": "The indices of specified elements are collected in indices tensor of size (sparse_dims, dense_",
        "context": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M + K",
        "question": "What is the name of the dimensional tensor to denote a N-dimensional hybrid sparse tensor?",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M + K",
        "question": "What are the numbers of sparse and dense dimensions?",
        "context": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor",
        "question": "What are the corresponding values of nse, dense_dims, and with an arbitrary integer or floating point number element type?",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse, dense_dims",
        "question": "What are the values tensor of size?",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M + K",
        "question": "What is the number of sparse and dense dimensions?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "entry [3, 4] at location (0, 2)",
        "question": "What is the name of the entry in a 2 + 1-dimensional tensor?",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "s.sparse_dim()",
        "question": "What does M =?",
        "context": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where is the entry [5, 6] in a sparse tensor?",
        "context": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse COO tensor",
        "question": "What type of tensor is s?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "s.sparse_dim()",
        "question": "What is the name of the sparse COO tensor?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "What is the location of a sparse tensor?",
        "context": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dimensionality",
        "question": "What is the sum of the number of sparse and dense dimensions?",
        "context": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where is the entry [5, 6] in a 2 + 1-dimensional tensor?",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "If s is a COO tensor and M = s.sparse_dim(), K = s",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "s.values().layout == torch.strided - values are stored as what?",
        "context": "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What is the meaning of strided tensors?",
        "context": "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "s.ndim",
        "question": "What is the dimensionality of a tensor the sum of the number of sparse and dense dimensions?",
        "context": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse dimensions",
        "question": "Dense dimensions always follow what?",
        "context": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided",
        "question": "What are values stored as strided tensors?",
        "context": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "What are values stored as?",
        "context": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse dimensions",
        "question": "What do dense dimensions always follow?",
        "context": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dense",
        "question": "What is not supported when mixing sparse and dense dimensions?",
        "context": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch sparse COO tensor format",
        "question": "What permits uncoalesced sparse tensors?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D",
        "question": "What is an uncoalesced tensor?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D",
        "question": "What is the uncoalesced tensor?",
        "context": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D",
        "question": "What uncoalesced tensor can be created when multiple values are specified for the same index?",
        "context": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the sum of all duplicate value entries",
        "question": "What is the value at an uncoalesced sparse COO tensor?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "summation",
        "question": "What does the coalescing process use to accumulate multi-valued elements into a single value?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the coalescing process",
        "question": "What process will accumulate the multi-valued elements into a single value using summation?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What does the torch.Tensor.is_coalesced() return True?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the indices of specified tensor elements are unique",
        "question": "What are the properties of the output of torch.Tensor.coalesce() method?",
        "context": "s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What is the result of the torch.Tensor.is_coalesced() method?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "True",
        "question": "What does torch.Tensor.is_coalesced() return?",
        "context": "torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or uncoalesced sparse tensor",
        "question": "What type of sparse tensor will most operations work identically given?",
        "context": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical order",
        "question": "In what order are the indices of specified tensor elements sorted?",
        "context": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "most operations will work identically",
        "question": "What happens when a sparse tensor is coalesced or uncoalesced?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or not",
        "question": "What is a sparse tensor?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced tensors",
        "question": "Some operations can be implemented more efficiently on uncoalesced tensors, and some on coalesced tensors",
        "context": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensor",
        "question": "What type of tensor is coalesced?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced tensors",
        "question": "Some operations can be implemented more efficiently on what?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical order",
        "question": "In what order are the indices sorted?",
        "context": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced tensors",
        "question": "Some operations can be implemented more efficiently on uncoalesced what?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "concatenating the indices and values tensors",
        "question": "How is addition of sparse COO tensors implemented?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or not",
        "question": "What should you not care about a sparse tensor being?",
        "context": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "addition of sparse COO tensors",
        "question": "What is implemented by simply concatenating the indices and values tensors?",
        "context": "s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical ordering",
        "question": "What can be advantageous for implementing algorithms that involve many element selection operations?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesce",
        "question": "What should you do to your sparse tensors to prevent them from growing too large?",
        "context": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Let\u2019s consider the following example",
        "question": "What is an example of a lexicographical ordering of indices?",
        "context": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesce",
        "question": "What should you do with sparse tensors to prevent them from growing too large?",
        "context": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "example",
        "question": "What is an example of lexicographical ordering of indices?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse COO tensor",
        "question": "What is a torch.Tensor instance?",
        "context": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch",
        "question": "What is the name of a sparse COO tensor?",
        "context": "Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "methods torch.Tensor.indices() and torch.Tensor.values()",
        "question": "What methods can be used to acquire the COO format data of a sparse COO tensor?",
        "context": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What does a sparse COO tensor do?",
        "context": "As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "methods torch.Tensor.sparse_dim() and torch.Tensor.dense_dim()",
        "question": "How can the number of sparse and dense dimensions be acquired?",
        "context": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced",
        "question": "Currently, one can acquire the COO format data only when the tensor instance is what?",
        "context": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse and dense",
        "question": "What dimensions can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.d",
        "context": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse COO tensor",
        "question": "If s is a what, then its COO format data can be acquired using methods torch.Tensor.indices() and torch",
        "context": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced",
        "question": "For acquiring the COO format data of what tensor, use torch.Tensor._values() and torch.T",
        "context": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced",
        "question": "When can one acquire the COO format data?",
        "context": "Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced",
        "question": "What type of tensor is uncoalesced?",
        "context": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced copy",
        "question": "What can one construct of a sparse COO tensor using the torch.Tensor.coalesce() method?",
        "context": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.Tensor.coalesce()",
        "question": "What method can be used to construct a coalesced copy of a sparse COO tensor?",
        "context": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the torch",
        "question": "What can be used to construct a coalesced copy of a sparse COO tensor?",
        "context": "but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch",
        "question": "What method can be used to create a coalesced copy of a sparse COO tensor?",
        "context": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced copy",
        "question": "What can one construct of a sparse COO tensor using the torch?",
        "context": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch",
        "question": "What can be used to build a coalesced copy of a sparse COO tensor?",
        "context": "but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Slicing",
        "question": "What is supported only for dense dimensions?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "In PyTorch, the fill value of a sparse tensor is assumed to be what?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "there exists operations that may interpret the fill value differently",
        "question": "Is the fill value of a sparse tensor assumed to be zero in general?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.sparse.softmax",
        "question": "What computes the softmax with the assumption that the fill value is negative infinity?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dense dimensions",
        "question": "Slicing of a sparse COO tensor is supported only for what dimensions?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "What is the fill value of a sparse tensor assumed to be?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "operations that may interpret the fill value differently",
        "question": "What can be done with the fill value of a sparse tensor?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "The CSR (Compressed Sparse Row) sparse tensor format",
        "question": "What implements the CSR format for storage of 2 dimensional tensors?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "MKL and MAGMA backends",
        "question": "What two backends are used in sparse matrix-vector multiplication?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CUDA",
        "question": "What does not exist as of now?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "crow_indices, col_indices and values",
        "question": "What are the three 1-D tensors in a CSR sparse tensor?",
        "context": "A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Compressed Sparse Row",
        "question": "What does CSR stand for?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "better use of storage and much faster computation operations",
        "question": "What is the primary advantage over the COO format?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "compressed row indices",
        "question": "What does the crow_indices tensor consist of?",
        "context": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor of size size[0] + 1",
        "question": "What is the crow_indices tensor?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "number of non-zeros",
        "question": "What is the last element of a CSR sparse tensor?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor",
        "question": "What encodes the index in values and col_indices depending on where the given row starts?",
        "context": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the number of elements in a given row",
        "question": "What does each successive number in the tensor subtracted by the number before it denote?",
        "context": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor",
        "question": "What is the size of the crow_indices tensor?",
        "context": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "where the given row starts",
        "question": "The crow_indices tensor encodes the index in values and col_indices depending on what?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "number of non-zeros",
        "question": "What is the last element of the crow_indices tensor?",
        "context": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "col_indices",
        "question": "The crow_indices tensor encodes the index in values and what else depending on where the given row starts?",
        "context": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "col_indices tensor",
        "question": "What contains the column indices of each value?",
        "context": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor of size nnz",
        "question": "What is the col_indices tensor?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "number of elements in a given row",
        "question": "Each successive number in the tensor subtracted by the number before it denotes what?",
        "context": "A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column indices of each value",
        "question": "What does the col_indices tensor contain?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "col_indices",
        "question": "What tensor contains the column indices of each value?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "values tensor",
        "question": "What contains the values of the CSR tensor?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor",
        "question": "What is the CSR tensor?",
        "context": "The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.int64",
        "question": "The index tensors crow_indices and col_indices should have element type either what?",
        "context": "The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.int32",
        "question": "If you want to use MKL-enabled matrix operations, use what?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "32 bit integer indexing",
        "question": "What does MKL LP64 use?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor",
        "question": "What is the col_indices tensor of size nnz?",
        "context": "The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "values",
        "question": "What tensor contains the values of the CSR tensor?",
        "context": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D",
        "question": "What is the size of the CSR tensor?",
        "context": "The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "MKL LP64",
        "question": "What is the default linking of pytorch?",
        "context": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.int32",
        "question": "If you want to use MKL-enabled matrix operations, what should you use?",
        "context": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.int64",
        "question": "What is the default element type for crow_indices and col_indices?",
        "context": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.int64",
        "question": "What is the default element type for index tensors?",
        "context": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.int64",
        "question": "The index tensors crow_indices and col_indices should have element type what?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch",
        "question": "What is the name of the method used to construct sparse CSR matrices?",
        "context": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "row and column indices and values tensors",
        "question": "What must the user supply separately?",
        "context": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "crow_indices and col_indices",
        "question": "The size argument is optional and will be deduced from what if it is not present?",
        "context": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor",
        "question": "What is the simplest way of constructing a sparse CSR from a strided or sparse COO tens",
        "context": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "missing values",
        "question": "Any zeros in the (strided) tensor will be interpreted as what in the sparse tensor?",
        "context": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch",
        "question": "What method can be used to construct sparse CSR matrices?",
        "context": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "optional",
        "question": "Is the size argument optional or optional?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "missing values",
        "question": "Any zeros in the strided tensor will be interpreted as what in the sparse tensor?",
        "context": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor._to_sparse_csr()",
        "question": "What is the simplest way of constructing a sparse CSR tensor from a strided or sparse CO",
        "context": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor.matmul()",
        "question": "The sparse matrix-vector multiplication can be performed with what method?",
        "context": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CSR tensors",
        "question": "The sparse matrix-vector multiplication is the only math operation supported on what?",
        "context": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "missing values",
        "question": "Any zeros in the strided tensor will be interpreted as what?",
        "context": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "math operation",
        "question": "The sparse matrix-vector multiplication is currently the only what operation supported on CSR tensors?",
        "context": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch._sparse_csr_tensor() method",
        "question": "What method can be used to construct Sparse CSR matrices?",
        "context": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Linear Algebra operations",
        "question": "The following table summarizes supported what on sparse matrices?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "T",
        "question": "What denotes a tensor with a given layout?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix",
        "question": "What does M[layout] denote?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "f",
        "question": "What denotes a scalar?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Sparse grad?",
        "question": "What is a PyTorch operation?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is Layout signature torch.mv()?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "supported Linear Algebra operations",
        "question": "The following table summarizes what on sparse matrices where the operands layouts may vary?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a tensor",
        "question": "What does T[layout] denote with a given layout?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Sparse grad?",
        "question": "What is the name of the PyTorch operation?",
        "context": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.mv()",
        "question": "What is the name of the Layout signature?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is the default value for M[sparse_coo] at V[strided] -> V[stride",
        "context": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch operation Sparse grad",
        "question": "What is the name of the PyTorch operation Sparse grad?",
        "context": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Layout signature",
        "question": "What does torch.mv() no M[sparse_coo] @ V[strided] -> V[s",
        "context": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Sparse grad",
        "question": "What is the term for a hybrid sparse?",
        "context": "Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What is the name of the Sparse grad at V[strided] -> V[strided] torch?",
        "context": "Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is the value of M[sparse_coo] at V[strided] -> V[strided",
        "context": "Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What is not present at V[strided] -> V[strided] torch?",
        "context": "Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What does torch.mv() say about M[sparse_coo] at V[strided] -> V[",
        "context": "torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does torch.mv() no?",
        "context": "torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is the number of M[sparse_coo] at V[strided] -> V[strided",
        "context": "no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does no M[sparse_coo] @ V[strided] -> V[strided] torch",
        "context": "no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo] @ V[strided] -> V[strided] torch",
        "question": "What does.mv() no M[sparse_csr] @ V[strided] -> V[",
        "context": "M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does M[sparse_coo] do at V[strided] -> V[strided] torch",
        "context": "M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is the value of the M[sparse_csr] at V[strided] -> V[stri",
        "context": "torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "addmm",
        "question": "What is the name of the node that does not have a T[sparse_coo] at T[strided",
        "context": "torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is the number of M[sparse_csr] at V[strided] -> V[stride",
        "context": "no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "addmm",
        "question": "What does no M[sparse_coo] @ M[strided] -> M[strided] torch",
        "context": "no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]",
        "question": "What does matmul() no M[sparse_coo] @ M[strided] -> M[stride",
        "context": "M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "Matmul() no what @ M[strided] -> M[strided] torch?",
        "context": "M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[strided]",
        "question": "Where does M[sparse_coo] come from?",
        "context": "torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]",
        "question": "What does torch.matmul() no?",
        "context": "torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[strided]",
        "question": "When does no M[sparse_coo] occur?",
        "context": "no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does M[sparse_coo] do at M[strided] -> M[strided] torch",
        "context": "M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[strided]",
        "question": "Where does M[sparse_csr] come from?",
        "context": "M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[strided]",
        "question": "When does no M[sparse_csr] occur?",
        "context": "no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]",
        "question": "What does M[sparse_csr] do at M[strided] -> M[strided]",
        "context": "M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]",
        "question": "What @ M[strided] -> M[strided] torch?",
        "context": "M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does torch.mm() no?",
        "context": "torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does M[sparse_coo] @ M[strided] -> M[strided] torch?",
        "context": "no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does torch.sparse.mm() say at M[strided] -> M[strided] torch?",
        "context": "torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does torch.sparse.mm() yes?",
        "context": "torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does M[strided] -> M[strided] torch have?",
        "context": "M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lobpcg",
        "question": "What does lobpcg stand for?",
        "context": "yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does torch.smm() no at M[strided] -> M[sparse_coo] torch?",
        "context": "torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does torch.smm() no?",
        "context": "torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does not exist at M[strided] -> M[sparse_coo] torch?",
        "context": "no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does M[sparse_coo] do at M[strided] -> M[sparse_co",
        "context": "no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does M[sparse_coo] mean?",
        "context": "M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[strided]",
        "question": "Where is M[sparse_coo] at?",
        "context": "M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What @ M[strided] -> M[sparse_coo] torch?",
        "context": "M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does torch.hspmm() no?",
        "context": "torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does not exist at M[strided] -> M[hybrid sparse_coo] torch?",
        "context": "no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What does M[sparse_coo] @ M[strided] -> M[hybrid sparse",
        "context": "no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does M[hybrid sparse_coo] mean?",
        "context": "M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What @ M[strided] -> M[hybrid sparse_coo] torch?",
        "context": "M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does f * mean?",
        "context": "f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "f * M[sparse_coo]",
        "question": "What does sspaddmm() no?",
        "context": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is the answer to T[sparse_coo] at T[strided]?",
        "context": "no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is T[sparse_coo] @ T[strided] -> T[strided] torch?",
        "context": "no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PCA",
        "question": "What is the acronym for \"M[sparse_coo]\"?",
        "context": "T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does M[strided] + f * mean at M[strided]?",
        "context": "no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does f * M[strided] + f * (M[sparse_coo] @ M[s",
        "context": "f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does f * M[strided] + f * mean at M[strided]?",
        "context": "torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "SVD",
        "question": "What is M[sparse_coo]?",
        "context": "yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "SVD",
        "question": "What does M[sparse_coo] stand for?",
        "context": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does torch.sspaddmm() no f *?",
        "context": "torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does torch.sspaddmm() no f * (M[sparse_coo] @ M[stride",
        "context": "torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.is_sparse Is True",
        "question": "What is true if the Tensor uses sparse storage layout?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.dense_dim",
        "question": "What returns the number of dense dimensions in a sparse tensor self?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_dim",
        "question": "What returns the number of sparse dimensions in a sparse tensor self?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices of the sparse tensor mask",
        "question": "What are the values of the strided tensor self filtered by?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse copy of the tensor",
        "question": "What does Tensor.to_sparse return?",
        "context": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse storage layout",
        "question": "What does Tensor.is_sparse use if the Tensor uses?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dense",
        "question": "What is the number of dense dimensions in a sparse tensor self?",
        "context": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "number of dense dimensions",
        "question": "What is returned in a sparse tensor self?",
        "context": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices of the sparse tensor mask",
        "question": "What are the values from a strided tensor self filtered by?",
        "context": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices",
        "question": "What are the values of the sparse tensor mask filtered by?",
        "context": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "compressed row storage format",
        "question": "Tensor._to_sparse_csr Convert a tensor to what format?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "False",
        "question": "Is True if the Tensor uses sparse storage layout?",
        "context": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What _dim Return the number of sparse dimensions in a sparse tensor self?",
        "context": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "compressed row storage format",
        "question": "What format does Tensor._to_sparse_csr convert a tensor to?",
        "context": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_mask",
        "question": "What returns a new sparse tensor with values from a strided tensor self filtered by the indice",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.indices",
        "question": "What is used to convert a tensor to compressed row storage format?",
        "context": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices",
        "question": "What does the sparse tensor mask filter?",
        "context": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor._to_sparse_csr",
        "question": "What Converts a tensor to compressed row storage format?",
        "context": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a strided tensor self filtered by the indices of the sparse tensor mask",
        "question": "What does the new sparse tensor return values from?",
        "context": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.indices",
        "question": "What returns the indices tensor of a sparse COO tensor?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.dense_dim",
        "question": "What Returns the number of dense dimensions in a sparse tensor self?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What return the number of sparse dimensions in a sparse tensor self?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor._to_sparse_csr",
        "question": "What Convert a tensor to compressed row storage format?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the number of sparse dimensions",
        "question": "What does a sparse tensor self return?",
        "context": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.to_sparse",
        "question": "What returns a sparse copy of the tensor?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "Return the number of dense dimensions in a what tensor self?",
        "context": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.values",
        "question": "What returns the values tensor of a sparse COO tensor?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the number of sparse dimensions in a sparse tensor self",
        "question": "What does Tensor.sparse_dim return?",
        "context": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.indices",
        "question": "What Returns the indices tensor of a sparse COO tensor?",
        "context": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.values",
        "question": "What Returns the values tensor of a sparse COO tensor?",
        "context": "Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "number of sparse dimensions in a sparse tensor self",
        "question": "What is the return value of a sparse tensor?",
        "context": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.coalesce",
        "question": "What Tensor method is specific to sparse COO tensors?",
        "context": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a strided tensor self filtered by the indices of the sparse tensor mask",
        "question": "Where are the values of a new sparse tensor retrieved from?",
        "context": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices",
        "question": "What is the sparse tensor mask filtered by?",
        "context": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.coalesce",
        "question": "What are the following Tensor methods specific to sparse COO tensors?",
        "context": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced",
        "question": "What type of copy of self does Tensor.coalesce return if self is an uncoalesced tensor?",
        "context": "Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_",
        "question": "What Tensor method returns a coalesced copy of self if self is an uncoalesced tensor?",
        "context": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "self",
        "question": "What is returned if self is an uncoalesced tensor?",
        "context": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.coalesce",
        "question": "What returns a coalesced copy of self if self is an uncoalesced tensor?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What type of copy of the tensor does Tensor._to_sparse_csr return?",
        "context": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize",
        "question": "What resizes self sparse tensor to the desired size and the number of sparse and dense dimensions?",
        "context": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse copy",
        "question": "What type of tensor does Tensor._to_sparse_csr return?",
        "context": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced",
        "question": "Returns a coalesced copy of self if self is an what type of tensor?",
        "context": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced",
        "question": "What type of tensor is self?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the desired size and the number of sparse and dense dimensions",
        "question": "What does Tensor.sparse_resize_ Resize self sparse tensor to?",
        "context": "Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What is the name of the Tensor method that resizes a sparse tensor?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "if self is an uncoalesced tensor",
        "question": "If self is an uncoalesced tensor, what is returned?",
        "context": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What is the name of the tensor method that resizes a sparse tensor?",
        "context": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "compressed row storage format",
        "question": "What format can a tensor be converted to?",
        "context": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Convert a tensor to compressed row storage format",
        "question": "What is one way to convert a tensor to a compressed row storage format?",
        "context": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "self sparse tensor",
        "question": "What does Tensor.sparse_resize_ Resize?",
        "context": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What is the name of the Tensor method that resizes a sparse COO tensor?",
        "context": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What is another name for sparse tensors?",
        "context": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices",
        "question": "What is the tensor of a sparse COO tensor?",
        "context": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices",
        "question": "Return the tensor of a sparse COO tensor?",
        "context": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "self sparse tensor",
        "question": "What does Tensor.sparse_resize resize?",
        "context": "Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "resizes self to the desired size and the number of sparse and dense dimensions",
        "question": "What happens when a sparse tensor is removed?",
        "context": "Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Removes all specified elements",
        "question": "What does Tensor.sparse_resize_and_clear_ do?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "values tensor of a sparse COO tensor",
        "question": "What does return the value of a sparse COO tensor?",
        "context": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What removes all specified elements from a sparse tensor?",
        "context": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Return the values tensor of a sparse COO tensor",
        "question": "What is returned by the value tensor of a sparse COO tensor?",
        "context": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced",
        "question": "Returns a coalesced copy of self if self is an what tensor?",
        "context": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced",
        "question": "If self is a sparse COO tensor, what type of tensor is it?",
        "context": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.is",
        "question": "What is _coalesced?",
        "context": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "if self is an uncoalesced tensor",
        "question": "When does Tensor.coalesce return a coalesced copy of self?",
        "context": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "False",
        "question": "If self is a sparse COO tensor that is coalesced, what returns true?",
        "context": "Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.to_dense",
        "question": "What returns true if self is a sparse COO tensor that is coalesced?",
        "context": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced",
        "question": "If self is a sparse COO tensor that is coalesced, what type of tensor is returned?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "True",
        "question": "If self is a sparse COO tensor that is coalesced, what return does Tensor.is_co",
        "context": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced",
        "question": "What type of copy of self is returned if self is an uncoalesced tensor?",
        "context": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided",
        "question": "What type of copy of self does Tensor.to_dense create?",
        "context": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced copy",
        "question": "Returns what type of self if self is an uncoalesced tensor?",
        "context": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Removes all specified elements",
        "question": "What does Tensor.sparse_resize_and_clear_ do to a sparse tensor?",
        "context": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided copy of self",
        "question": "What does Tensor.to_dense create?",
        "context": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse CSR tensors",
        "question": "The following methods are specific to what?",
        "context": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "True",
        "question": "If self is a sparse COO tensor that is coalesced, what value does Tensor.is_co",
        "context": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "self sparse tensor",
        "question": "What resizes to the desired size and the number of sparse and dense dimensions?",
        "context": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "False",
        "question": "Is self a sparse COO tensor that is coalesced?",
        "context": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.crow_indices",
        "question": "What methods are specific to sparse CSR tensors?",
        "context": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the desired size",
        "question": "Resizes self sparse tensor to what size?",
        "context": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.crow_indices",
        "question": "What method is specific to sparse CSR tensors?",
        "context": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "resizes self to the desired size and the number of sparse and dense dimensions",
        "question": "What happens to a sparse tensor?",
        "context": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "False",
        "question": "If self is a sparse COO tensor that is coalesced, what return value is returned?",
        "context": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CSR tensors",
        "question": "The following methods are specific to sparse what?",
        "context": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What removes all specified elements from a sparse tensor self?",
        "context": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Removes all specified elements",
        "question": "What does resize self to the desired size and the number of sparse and dense dimensions?",
        "context": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column indices of the self tensor",
        "question": "What does tensor.col_indices return when self is a sparse CSR tensor of layout sparse",
        "context": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column indices",
        "question": "What does the tensor return when self is a sparse CSR tensor of layout sparse_csr",
        "context": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "False",
        "question": "Returns True if self is a sparse COO tensor that is coalesced, what else?",
        "context": "Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.to_dense",
        "question": "What creates a strided copy of self?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.col_indices",
        "question": "What returns the tensor containing the compressed row indices of the self tensor when self is a sparse C",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "True",
        "question": "Returns what if self is a sparse COO tensor that is coalesced?",
        "context": "Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.col_indices",
        "question": "What returns the tensor containing the column indices of the self tensor when self is a sparse CSR",
        "context": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor containing the compressed row indices of the self tensor",
        "question": "What does Tensor.crow_indices return when self is a sparse CSR tensor of layout sparse",
        "context": "Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column indices of the self tensor",
        "question": "What does Tensor.col_indices return when self is a sparse CSR tensor of layout sparse_",
        "context": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO tensors",
        "question": "The following Tensor methods support sparse what?",
        "context": "Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "The following methods are specific to what type of CSR tensors?",
        "context": "Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column",
        "question": "What indices of the self tensor is returned when self is a sparse CSR tensor of layout spars",
        "context": "Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "The following Tensor methods support sparse COO tensors",
        "question": "What do the following Tensor methods support?",
        "context": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided",
        "question": "What type of copy of self is created?",
        "context": "Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided copy",
        "question": "Creates what copy of self?",
        "context": "Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column",
        "question": "What indices does the tensor return when self is a sparse CSR tensor of layout sparse_",
        "context": "Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse_csr",
        "question": "When self is a sparse CSR tensor of layout what?",
        "context": "Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the tensor containing the compressed row indices",
        "question": "What does return when self is a sparse CSR tensor of layout sparse_csr?",
        "context": "Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column",
        "question": "Returns the tensor containing what indices of the self tensor when self is a sparse CSR ",
        "context": "Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "layout sparse_csr",
        "question": "Self is a sparse CSR tensor of what?",
        "context": "Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse_csr",
        "question": "When self is a sparse CSR tensor of layout, what does Tensor.col_indices return?",
        "context": "Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "self",
        "question": "What is a sparse CSR tensor of layout sparse_csr?",
        "context": "Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the tensor containing the column indices of the self tensor",
        "question": "Returns what when self is a sparse CSR tensor of layout sparse_csr?",
        "context": "Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "t_()",
        "question": "What is the name of the method that supports sparse COO tensors?",
        "context": "The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor methods",
        "question": "What methods support sparse COO tensors?",
        "context": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO(rdinate) format",
        "question": "In what format is a sparse tensor constructed?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "crow_indices and col_indices",
        "question": "What are the specified values for a sparse tensor in CSR?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dim",
        "question": "What ensions dim does sparse.sum return the sum of each row of the sparse tensor input in?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.addmm()",
        "question": "What does sparse.addmm do exactly the same thing as?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse.mm",
        "question": "What is the name of the function that supports backward for sparse matrix mat1?",
        "context": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "crow_indices and col_indices",
        "question": "What are the specified values for the sparse tensor?",
        "context": "Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dim",
        "question": "What type of ensions dim does sparse.sum return?",
        "context": "Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix multiplication",
        "question": "What does sparse.mm perform?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse.sum",
        "question": "What returns the sum of each row of the sparse tensor input in the given dimensions dim?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse matrix mat1",
        "question": "What does sparse.addmm support backward for?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse matrix mat1 and the (sparse or strided) matrix mat2",
        "question": "What does sparse.mm perform a matrix multiplication of?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Matrix",
        "question": "What multiplies a sparse tensor mat1 with a dense tensor mat2?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dim",
        "question": "Returns the sum of each row of the sparse tensor input in what ensions dim?",
        "context": "Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the sum of each row of the sparse tensor input",
        "question": "What does sparse.addmm return?",
        "context": "Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix multiplication",
        "question": "What does sparse.mm perform of the sparse matrix mat1 and the (sparse or strided) matrix mat2",
        "context": "sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix multiplication",
        "question": "What performs a sparse COO matrix mat1 and a strided matrix mat2?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a strided matrix mat2",
        "question": "Matrix performs a matrix multiplication of a sparse COO matrix mat1 and what else?",
        "context": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.addmm()",
        "question": "What does this function do exactly the same thing as in the forward?",
        "context": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What type of matrix does matrix multiplication perform?",
        "context": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse matrix mat1",
        "question": "What does this function support backward for?",
        "context": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse.mm",
        "question": "What performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2",
        "context": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix multiplication",
        "question": "What does sparse.mm perform of a sparse COO matrix mat1 and a strided matrix mat2?",
        "context": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dense matrix mat",
        "question": "What is used to perform a matrix multiplication of the sparse matrix input?",
        "context": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "softmax function",
        "question": "What does sparse.softmax apply?",
        "context": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "softmax",
        "question": "What does sparse.log_apply?",
        "context": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse COO matrix mat1 and a strided matrix mat2",
        "question": "What do sparse.mm perform a matrix multiplication of?",
        "context": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse matrix input",
        "question": "What does sparse.mm perform a matrix multiplication of with the dense matrix mat?",
        "context": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse.softmax",
        "question": "What Applies a softmax function?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "softmax",
        "question": "What applies a softmax function?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix multiplication",
        "question": "What is performed of the sparse matrix mat1 and the (sparse or strided) matrix mat2?",
        "context": "Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse COO matrix mat1 and a strided matrix mat2",
        "question": "Matrix performs a matrix multiplication of what?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dense matrix mat",
        "question": "Matrix multiplies sparse matrix input with what?",
        "context": "Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse.log_softmax",
        "question": "What Applies a softmax function followed by logarithm?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "hstack()",
        "question": "Which torch function supports sparse tensors?",
        "context": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "same_size()",
        "question": "What is_signed() is_tensor()?",
        "context": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "svd_lowrank()",
        "question": "What is the name of the torch function that supports sparse tensors?",
        "context": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensors",
        "question": "What do the following torch functions support?",
        "context": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a new tensor",
        "question": "Returns what with the sine of the elements of input?",
        "context": "Returns a new tensor with the sine of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin"
    },
    {
        "answer": "fill_value",
        "question": "What does torch.full_like(input, fill_value, layout=input.layout, device=input.device",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "torch.full_like",
        "question": "What is equivalent to torch.full?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "input",
        "question": "What is the size of input that determines the size of the output tensor?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "fill_value",
        "question": "What is the number to fill the output tensor with?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "dtype",
        "question": "What is the desired data type of returned Tensor?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "if None",
        "question": "What defaults to the dtype of input?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "tensor",
        "question": "What is returned with the same size as input filled with fill_value?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "torch.full",
        "question": "What is equivalent to torch.full(input.size(), fill_value, dtype=input.dtype, layout=",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "the number to fill the output tensor with",
        "question": "What is fill_value?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "desired data type of returned tensor",
        "question": "dtype (torch.dtype, optional) \u2013 what?",
        "context": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "defaults to the dtype of input",
        "question": "What happens if None?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "input",
        "question": "What determines size of the output tensor?",
        "context": "input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "layout",
        "question": "What is the desired layout of returned tensor?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "if None",
        "question": "What defaults to the layout of input?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "device",
        "question": "What is the desired device of returned tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "if None",
        "question": "What defaults to the device of input?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "None",
        "question": "Default: if what, defaults to the dtype of input?",
        "context": "fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "None",
        "question": "If none, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned ten",
        "context": "fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "None",
        "question": "Default: if what, defaults to the device of input?",
        "context": "fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "autograd",
        "question": "What should record operations on the returned tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "False",
        "question": "What is the default value for autograd to record operations on the returned tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "Default",
        "question": "If None, defaults to the dtype of input?",
        "context": "dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "None",
        "question": "Default: if what, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "defaults",
        "question": "What does None do to the device of input?",
        "context": "dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "False",
        "question": "What is the default setting for autograd to record operations on the returned tensor?",
        "context": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "memory_format",
        "question": "What is the desired memory format of returned Tensor?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "torch.preserve_format",
        "question": "What is the default memory format of returned Tensor?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "defaults",
        "question": "Default: if None, what is the default to the device of input?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "requires_grad",
        "question": "If autograd should record operations on the returned tensor, what is optional?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.preserve_format",
        "question": "What is the default memory format of returned tensor?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "Short-time Fourier transform",
        "question": "What is STFT?",
        "context": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex",
        "question": "What must always be given explicitly for real inputs?",
        "context": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex=True",
        "question": "What function will only return complex tensors?",
        "context": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "torch.view_as_real()",
        "question": "What can be used to recover a real tensor with an extra last dimension for real and imaginary components?",
        "context": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Fourier transform",
        "question": "What does the STFT compute of short overlapping windows of the input?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "frequency components of the signal",
        "question": "What does the Fourier transform of short overlapping windows of the input give?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "librosa stft function",
        "question": "The interface of the STFT is modeled after what?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "optional batch dimension",
        "question": "Ignoring what, the STFT computes the following expression?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "The STFT",
        "question": "What computes the Fourier transform of short overlapping windows of the input?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the following expression",
        "question": "Ignoring the optional batch dimension, this method computes what?",
        "context": "Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "frequency components",
        "question": "The STFT computes the Fourier transform of short overlapping windows of the input to give what of the signal as they change over time?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the following expression",
        "question": "What does the STFT compute without the optional batch dimension?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "mmm",
        "question": "What is the index of the sliding window?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "Input must be either a time sequence or a 2-D batch of time sequences?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "floor(n_fft / 4)",
        "question": "If hop_length is None, it is treated as equal to what?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "Input must be either a what?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "floor(n_fft / 4)",
        "question": "If hop_length is None (default), it is treated as equal to what?",
        "context": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "Input must be a what?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "None",
        "question": "If win_length is what, it is treated as equal to n_fft?",
        "context": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "If win_length is None (default), it is treated as equal to what?",
        "context": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "What must input be?",
        "context": "Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "If win_length is None, it is treated as equal to what?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D tensor of size",
        "question": "What is the win_length of a window?",
        "context": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "111",
        "question": "If window is None (default), it is treated as if having what number everywhere in the window?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "padded on both sides to length n_fft",
        "question": "What happens to window if win_lengthn_ffttextwin_lengthn_fft?",
        "context": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D tensor",
        "question": "What can a window be a size of win_length?",
        "context": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "padded",
        "question": "If win_lengthn_ffttextwin_lengthn_fft, window will be what on both sides?",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "padded on both sides to length n_fft",
        "question": "What happens to window if win_lengthn_ffttextwin_length  textn",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D tensor",
        "question": "What can a window be of size win_length?",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "padded",
        "question": "If win_lengthn_ffttextwin_lengthn_fft, window will be what on both sides",
        "context": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "111",
        "question": "If window is None (default), it is treated as if it has what number everywhere in the window?",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "None",
        "question": "If window is what (default) value, it is treated as if having 111 everywhere in the window?",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ttt-th frame",
        "question": "What is centered at time thop_lengtht times texthop_lengththop_length",
        "context": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ttt-th frame",
        "question": "What begins at time thop_lengtht times texthop_lengththop_length?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "pad_mode",
        "question": "What determines the padding method used on input when center is True?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "torch.nn.functional.pad()",
        "question": "What is the name of the function that determines the padding method used on input when center is True?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "reflect",
        "question": "What is the default padding method used when center is True?",
        "context": "pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ttt-th frame",
        "question": "If center is True, input will be padded on both sides so that which frame is centered at time thop_lengtht ",
        "context": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "torch.nn.functional.pad()",
        "question": "What is the name of the padding method used on input when center is True?",
        "context": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "\"reflect\"",
        "question": "What is the default setting for padding when center is True?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "reflect",
        "question": "What is the default value of pad_mode?",
        "context": "pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "True",
        "question": "What is the default for real input?",
        "context": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "onesided output",
        "question": "What is not possible if the input or window tensors are complex?",
        "context": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "True",
        "question": "If onesided is what (default for real input)?",
        "context": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "normalized is True",
        "question": "If what is true, the function returns the normalized STFT results?",
        "context": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "input.dim()",
        "question": "If return_complex is True, the return is what?",
        "context": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "input.dim()",
        "question": "If return_complex is False, the output is what?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "False",
        "question": "What is the default value of normalized?",
        "context": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the last dimension",
        "question": "What represents the real and imaginary components?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "input.dim()",
        "question": "What is the return if return_complex is True?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the last dimension represents the real and imaginary components",
        "question": "What is the last dimension of the input.dim() + 2 dimensional real tensor?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex",
        "question": "If what is true, returns a complex tensor of size?",
        "context": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "TTT",
        "question": "What is the total number of frames used?",
        "context": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "version 0.4.1",
        "question": "When did this function change signature?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Calling with the previous signature",
        "question": "What may cause error or return incorrect result?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "if return_complex is true",
        "question": "Returns either a complex tensor of size (NT)(* times N times T)",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the total number of frames",
        "question": "What is TTT?",
        "context": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "error or return incorrect",
        "question": "Calling with the previous signature may cause what?",
        "context": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "0.4.1",
        "question": "What version of the function changed signature?",
        "context": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Calling with the previous signature may cause error or return incorrect result",
        "question": "What is a warning when calling with the previous signature?",
        "context": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Fourier transform hop_length",
        "question": "What is the size of the distance between neighboring sliding window frames?",
        "context": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "None",
        "question": "What is the default value of the input tensor?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "floor(n_fft / 4)",
        "question": "What is the default value for Win_length?",
        "context": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "None",
        "question": "What is the default value for the size of window frame and STFT filter?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Calling with the previous signature may cause error or return incorrect result",
        "question": "What can happen if a function is called with a previous signature?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "optional window function",
        "question": "What is window (Tensor, optional)?",
        "context": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "111",
        "question": "Default: None (treated as window of all how many s)",
        "context": "hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "win_length",
        "question": "What is the size of window frame and STFT filter?",
        "context": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "None",
        "question": "What is the default window of all 111 s?",
        "context": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Fourier transform hop_length",
        "question": "n_fft (int) \u2013 size of what?",
        "context": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "floor",
        "question": "What is n_fft equal to?",
        "context": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "What is the size of Fourier transform hop_length?",
        "context": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "optional",
        "question": "Default: None (treated as equal to n_fft) window (Tensor, what) \u2013 the optional window function",
        "context": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "hop_length",
        "question": "What is the distance between neighboring sliding window frames?",
        "context": "hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "floor(n_fft / 4)",
        "question": "What is hop_length equal to?",
        "context": "hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "111 s",
        "question": "Default: None (treated as window of all?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "True pad_mode",
        "question": "What controls the padding method used when center is True?",
        "context": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "\"reflect\"",
        "question": "What is the default name of the padding method used when center is True?",
        "context": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "None",
        "question": "What is the default window function?",
        "context": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "reflect",
        "question": "What is the default value for the padding method used when center is True?",
        "context": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "optional",
        "question": "What is the window function?",
        "context": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "\"reflect\" normalized",
        "question": "What controls whether to return the normalized STFT results?",
        "context": "hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "optional",
        "question": "Window (Tensor) \u2013 the optional window function. Default: None (treated as window of all 111 s)",
        "context": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "False",
        "question": "What is the default setting for return normalized STFT results?",
        "context": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "whether to pad input on both sides",
        "question": "What is center?",
        "context": "center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "normalized",
        "question": "What type of STFT results does \"reflect\" return?",
        "context": "center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "redundancy",
        "question": "What do you want to avoid with real inputs?",
        "context": "pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "True",
        "question": "What is the default for real input and window?",
        "context": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "False",
        "question": "What is the default value for normalized STFT results?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "half",
        "question": "Default: False onesided (bool, optional) \u2013 controls whether to return what percentage of results?",
        "context": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "True",
        "question": "What is the default value for real input and window?",
        "context": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex",
        "question": "What returns a complex tensor or a real tensor with an extra last dimension for the real and imaginary components?",
        "context": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "shape",
        "question": "A tensor containing the STFT result with what described above Tensor containing the STFT result with?",
        "context": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "redundancy",
        "question": "Default: False onesided (bool, optional) \u2013 controls whether to return half of results to avoid what for real inputs?",
        "context": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex",
        "question": "What bool controls whether to return a complex tensor, or a real tensor with an extra last dimension for the",
        "context": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "tensor",
        "question": "What containing the STFT result with shape described above?",
        "context": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "NumPy\u2019s gradient function",
        "question": "What is this function analogous to?",
        "context": "This function is analogous to NumPy\u2019s gradient function. {input} \u2013  spacing (scalar, list of scalar, list of Tensor, optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at (the) \u2013  dim (int, list of python:int, optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order (int, optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "answer": "the dimension or dimensions to approximate the gradient over",
        "question": "What does dim represent?",
        "context": "This function is analogous to NumPy\u2019s gradient function. {input} \u2013  spacing (scalar, list of scalar, list of Tensor, optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at (the) \u2013  dim (int, list of python:int, optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order (int, optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "answer": "unsupported",
        "question": "What is edge_order?",
        "context": "This function is analogous to NumPy\u2019s gradient function. {input} \u2013  spacing (scalar, list of scalar, list of Tensor, optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at (the) \u2013  dim (int, list of python:int, optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order (int, optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "answer": "Example",
        "question": "What is the default value of edge_order?",
        "context": "This function is analogous to NumPy\u2019s gradient function. {input} \u2013  spacing (scalar, list of scalar, list of Tensor, optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at (the) \u2013  dim (int, list of python:int, optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order (int, optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "answer": "Count the frequency of each value in an array of non-negative ints",
        "question": "What does the number of bins in an array of non-negative ints do?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "size 1",
        "question": "What is the size of the number of bins in an array of non-negative ints?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "minlength",
        "question": "If what is specified, the number of bins is at least minlength and if input is empty, the result is tensor of size",
        "context": "The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "n",
        "question": "What is the value at position i?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "Note",
        "question": "What does the number of bins in an array of non-negative ints need to be one larger than the largest value in input?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "tensor of size 0.",
        "question": "What is the result of the number of bins (size 1) being one larger than the largest value in input unless input is empty?",
        "context": "The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "Note",
        "question": "What does the number of bins (size 1) have to be larger than the largest value in input?",
        "context": "The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "CUDA",
        "question": "When given tensors on what device may produce nondeterministic gradients?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "Reproducibility",
        "question": "What is the name of the operation that may produce nondeterministic gradients when given tensors on a CUDA device?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "input tensor",
        "question": "What is input (Tensor)?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "same size",
        "question": "What should the input tensor be of?",
        "context": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "minlength",
        "question": "What is optional, minimum number of bins?",
        "context": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "non-negative",
        "question": "What should the minimum number of bins be?",
        "context": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "if input is non-empty",
        "question": "What happens to a tensor of shape Size([max(input) + 1])?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "random number generator",
        "question": "Sets what state?",
        "context": "Sets the random number generator state. new_state (torch.ByteTensor) \u2013 The desired state ",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_rng_state.html#torch.set_rng_state"
    },
    {
        "answer": "new_state",
        "question": "What is the desired state of the random number generator?",
        "context": "Sets the random number generator state. new_state (torch.ByteTensor) \u2013 The desired state ",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_rng_state.html#torch.set_rng_state"
    },
    {
        "answer": "Computes the element-wise conjugate",
        "question": "What does torch.conj() do?",
        "context": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"
    },
    {
        "answer": "input",
        "question": "What has a non-complex dtype?",
        "context": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"
    },
    {
        "answer": "torch.conj()",
        "question": "What may return a non-writeable view for an input of non-complex dtype?",
        "context": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"
    },
    {
        "answer": "torch.conj()",
        "question": "What function returns a non-writeable view for an input of non-complex dtype?",
        "context": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"
    },
    {
        "answer": "torch.vstack()",
        "question": "What is the name of the alias?",
        "context": "Alias of torch.vstack(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.row_stack.html#torch.row_stack"
    },
    {
        "answer": "mean and std",
        "question": "Fills self tensor with elements samples from the normal distribution parameterized by what?",
        "context": "Fills self tensor with elements samples from the normal distribution\nparameterized by mean and std. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"
    },
    {
        "answer": "self tensor",
        "question": "What fills with elements from the normal distribution?",
        "context": "Fills self tensor with elements samples from the normal distribution\nparameterized by mean and std. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"
    },
    {
        "answer": "self tensor",
        "question": "Fills what with elements samples from the normal distribution parameterized by mean and std?",
        "context": "Fills self tensor with elements samples from the normal distribution\nparameterized by mean and std. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"
    },
    {
        "answer": "NumPy ndarray",
        "question": "What does torch.as_tensor() copy data from?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "a Tensor data",
        "question": "What does torch.tensor() always copy data?",
        "context": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor",
        "question": "What type of data does torch.Tensor.requires_grad_() or torch.Tensor.detach",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "NumPy ndarray",
        "question": "What does torch.as_tensor() use if you want to avoid a copy of data?",
        "context": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "Warning",
        "question": "What does torch.tensor always copy data?",
        "context": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "always copies data",
        "question": "What does torch.tensor do?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "Warning",
        "question": "What is a warning about a copy of data by a Tensor?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "torch.Tensor.requires_grad_() or torch.Tensor.detach()",
        "question": "What do you use if you want to avoid a copy of a Tensor data?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "NumPy",
        "question": "What ndarray does torch.as_tensor() avoid a copy of?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "Warning",
        "question": "What does torch.as_tensor() do?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "leaf",
        "question": "What variable does torch.tensor() construct when data is a tensor x?",
        "context": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "torch.tensor(x)",
        "question": "What is equivalent to x.clone().detach()?",
        "context": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "clone() and detach()",
        "question": "What are the equivalents of torch.tensor?",
        "context": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "array_like",
        "question": "What is the initial data for the tensor?",
        "context": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "a list, tuple, NumPy ndarray, scalar, and other types",
        "question": "What can data (array_like) \u2013 Initial data for the tensor be?",
        "context": "Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "a list, tuple, NumPy ndarray, scalar, and other types",
        "question": "What can initial data for the tensor be?",
        "context": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "dtype",
        "question": "What is the desired data type of returned tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "if None",
        "question": "What infers data type from data?",
        "context": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "if None",
        "question": "What default uses the current device for the default tensor type?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "CPU",
        "question": "What will device be for CPU tensor types?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "None",
        "question": "Default: if what, infers data type from data. device (torch.device, optional) \u2013 the desired device",
        "context": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "current device",
        "question": "Default: if None, uses what for the default tensor type?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "CPU",
        "question": "What is the device for CPU tensor types?",
        "context": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "current device",
        "question": "What is the default tensor type?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "pin_memory",
        "question": "If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors.",
        "context": "requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "CPU tensors",
        "question": "What does pin_memory only work for?",
        "context": "requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "False",
        "question": "What is the default setting for pin_memory?",
        "context": "requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "Example:",
        "question": "What is an example of a return tensor?",
        "context": "device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "column coordinates",
        "question": "What does the second row of a 2-by-N Tensor contain?",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "rows and then columns",
        "question": "Indices are ordered based on what?",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "the elements on and above the diagonal",
        "question": "What is the upper triangular part of a matrix defined as?",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "the indices",
        "question": "Returns what of the upper triangular part of a row by col matrix in a 2-by-N Tensor?",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "the elements on and above the diagonal",
        "question": "What is the upper triangular part of the matrix defined as?",
        "context": "The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "CUDA",
        "question": "When running on what platform, row * col must be less than 259259259 to prevent overflow during calculation?",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "row",
        "question": "What is the number of rows in the 2-D matrix?",
        "context": "row (int) \u2013 number of rows in the 2-D matrix. col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "col",
        "question": "What is the name of the number of columns in the 2-D matrix?",
        "context": "row (int) \u2013 number of rows in the 2-D matrix. col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "offset",
        "question": "What is the diagonal offset from the main diagonal?",
        "context": "offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Default",
        "question": "If not provided, what is the default?",
        "context": "col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Default",
        "question": "What is the default data type of returned tensor?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "CUDA",
        "question": "When running on what platform must row * col be less than 259259259 to prevent overflow during calculation?",
        "context": "When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Default",
        "question": "What is the default value for the offset from the main diagonal?",
        "context": "row (int) \u2013 number of rows in the 2-D matrix. col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "offset",
        "question": "What is the name of the diagonal offset from the main diagonal?",
        "context": "offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "if None",
        "question": "What is the default value of torch.long?",
        "context": "col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "col",
        "question": "What is the number of columns in the 2-D matrix?",
        "context": "col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "diagonal offset",
        "question": "What is offset from the main diagonal?",
        "context": "col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "if not provided",
        "question": "What is the default value for the offset?",
        "context": "col (int) \u2013 number of columns in the 2-D matrix. offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Default",
        "question": "What is the default value for the data type of returned tensor?",
        "context": "Constructs a tensor with data. Warning torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "CPU",
        "question": "What is the device used for CPU tensor types?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Default",
        "question": "What is the default value of offset?",
        "context": "offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "CPU",
        "question": "What is the current CUDA device for CUDA tensor types?",
        "context": "offset (int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, torch.long. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "complementary error function of input",
        "question": "Computes what?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "integer and floating point numbers",
        "question": "The dividend and divisor may contain both for what?",
        "context": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "sign",
        "question": "The remainder of division has the same what as the divisor other?",
        "context": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "broadcasting",
        "question": "What is supported to a common shape, type promotion, and integer and float inputs?",
        "context": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "Complex inputs",
        "question": "What inputs are not supported?",
        "context": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "not mathematically possible",
        "question": "In some cases, it is what to satisfy the definition of a modulo operation with complex numbers?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "torch.fmod()",
        "question": "What is the name of the function that handles division by zero?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "dividend and divisor",
        "question": "What may contain both integer and floating point numbers?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "The remainder",
        "question": "What has the same sign as the divisor other?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "broadcasting",
        "question": "Supports what type of programming?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "Tensor",
        "question": "What is the input for the dividend?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "broadcasting",
        "question": "Supports what to a common shape, type promotion, and integer and float inputs?",
        "context": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "complex numbers",
        "question": "In some cases, it is not mathematically possible to satisfy the definition of a modulo operation with what?",
        "context": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "the dividend",
        "question": "What is the input (Tensor) \u2013 the divisor out (Tensor, optional) \u2013 the output tens",
        "context": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "Complex inputs",
        "question": "What is not supported?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "it is not mathematically possible to satisfy the definition of a modulo operation with complex numbers",
        "question": "Why are complex inputs not supported?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "input tensor",
        "question": "What is the input (Tensor)?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "torch.fmod()",
        "question": "What computes the element-wise remainder of division equivalently to the C library function fmod()?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "the dividend",
        "question": "What is the input (Tensor) referred to as?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "a random number on all GPUs",
        "question": "Sets the seed for generating random numbers to what?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "64",
        "question": "What is the bit number used to seed the RNG?",
        "context": "Sets the seed for generating random numbers to a non-deterministic\nrandom number. Returns a 64 bit number used to seed the RNG. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.seed.html#torch.seed"
    },
    {
        "answer": "intraop parallelism",
        "question": "What is the number of threads used for on a CPU?",
        "context": "Sets the number of threads used for intraop parallelism on CPU. Warning To ensure that the correct number of threads is used, set_num_threads\nmust be called before running eager, JIT or autograd code. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_num_threads.html#torch.set_num_threads"
    },
    {
        "answer": "set_num_threads",
        "question": "What must be called before running eager, JIT or autograd code?",
        "context": "Sets the number of threads used for intraop parallelism on CPU. Warning To ensure that the correct number of threads is used, set_num_threads\nmust be called before running eager, JIT or autograd code. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_num_threads.html#torch.set_num_threads"
    },
    {
        "answer": "the number of threads",
        "question": "What does set set for intraop parallelism on CPU?",
        "context": "Sets the number of threads used for intraop parallelism on CPU. Warning To ensure that the correct number of threads is used, set_num_threads\nmust be called before running eager, JIT or autograd code. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_num_threads.html#torch.set_num_threads"
    },
    {
        "answer": "2-D",
        "question": "What type of tensor does torch return?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "n",
        "question": "What is the number of rows in the output tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "if None",
        "question": "What is the default value of torch.set_default_tensor_type()?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "layout",
        "question": "What is the desired layout of returned Tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default setting of the returned Tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "a 2-D tensor",
        "question": "What is returned with ones on the diagonal and zeros elsewhere?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "n out",
        "question": "What is the default number of columns in the output tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "global",
        "question": "Default: if None, uses a what default?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default setting for a 2-D tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "the output tensor",
        "question": "What does n out represent?",
        "context": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "if None",
        "question": "What uses a global default?",
        "context": "beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default setting for the returned Tensor?",
        "context": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "m",
        "question": "n (int) \u2013 the nu of rows m (int, optional) \u2013 the number of columns with default being n",
        "context": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "Default",
        "question": "If None, uses a global default (see torch.set_default_tensor_type()).",
        "context": "out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default setting of returned Tensor?",
        "context": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "output tensor",
        "question": "What is the name of the output tensor?",
        "context": "m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default layout of the returned Tensor?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default layout of returned Tensor?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "output tensor",
        "question": "Out (Tensor, optional) - what is the name of the output tensor?",
        "context": "out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default setting for the output tensor?",
        "context": "out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "if None",
        "question": "What is the default setting for the current device for the default tensor type?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "2-D",
        "question": "What type of tensor has ones on the diagonal and zeros elsewhere?",
        "context": "device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "if None",
        "question": "What is the default setting for the default tensor type?",
        "context": "device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "2-D tensor",
        "question": "What is a tensor with ones on the diagonal and zeros elsewhere?",
        "context": "device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "a new tensor",
        "question": "Returns what with the truncated integer values of the elements of input?",
        "context": "Returns a new tensor with the truncated integer values of\nthe elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc"
    },
    {
        "answer": "Example",
        "question": "What is an example of a new tensor?",
        "context": "Returns a new tensor with the truncated integer values of\nthe elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc"
    },
    {
        "answer": "a new tensor",
        "question": "Returns what with the logarithm to the base 2 of the elements of input?",
        "context": "Returns a new tensor with the logarithm to the base 2 of the elements\nof input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2"
    },
    {
        "answer": "Example:",
        "question": "What is an example of an output tensor?",
        "context": "Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma"
    },
    {
        "answer": "torch.special.expm1",
        "question": "What is the name of the Alias for?",
        "context": "Alias for torch.special.expm1(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1"
    },
    {
        "answer": "window length",
        "question": "What is the window_length?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "shape parameter beta",
        "question": "Computes the Kaiser window with window length window_length and what other parameter?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Let I_0 be the zeroth order modified Bessel function of the first kind",
        "question": "What does torch.i0() do?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device",
        "question": "What does this function do?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "window length",
        "question": "Computes the Kaiser window with what parameter?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "the zeroth order modified Bessel function of the first kind",
        "question": "What is I_0?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "L",
        "question": "What is the zeroth order modified Bessel function of the first kind?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Calling torch.kaiser_window",
        "question": "What is equivalent to calling torch.kaiser_window?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "a helpful shorthand",
        "question": "What is the periodic argument intended for?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Note",
        "question": "What is the periodic argument intended to produce a periodic window as input to functions like torch.stft()?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "L",
        "question": "What is the name of the zeroth order modified Bessel function of the first kind?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.kaiser_window(L, B, periodic=True)",
        "question": "What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1]?",
        "context": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Note",
        "question": "What is the periodic argument intended as a useful shorthand to produce a periodic window as input to functions like torch.stft()?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Calling torch.kaiser_window",
        "question": "What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)?",
        "context": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "a single element tensor containing a one",
        "question": "What is the returned window if window_length is one?",
        "context": "Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "window_length (int)",
        "question": "What is the length of the window?",
        "context": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.stft()",
        "question": "The periodic argument is intended as a useful shorthand to produce a periodic window as input to functions like what?",
        "context": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "one",
        "question": "If window_length is what, then the returned window is a single element tensor containing a one?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "length of the window",
        "question": "What is window_length?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "a single element tensor",
        "question": "What is the window returned if window_length is one?",
        "context": "Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "length of the window",
        "question": "What does window_length (int) mean?",
        "context": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "periodic",
        "question": "What is the window suitable for use in spectral analysis?",
        "context": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "filter design",
        "question": "If False, returns a symmetric window suitable for use in what?",
        "context": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "shape parameter",
        "question": "What does beta (float, optional) represent for the window?",
        "context": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "symmetric",
        "question": "If False, returns a window suitable for use in filter design.",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "beta",
        "question": "What is the shape parameter for the window?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "a single element tensor containing a one",
        "question": "If window_length is one, what is the returned window?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "periodic",
        "question": "If True, returns a periodic window suitable for use in spectral analysis. If False, returns a symmetric window suitable for use",
        "context": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "periodic",
        "question": "What is the name of the window that is suitable for use in spectral analysis?",
        "context": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "If False",
        "question": "What returns a symmetric window suitable for use in filter design?",
        "context": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "global default",
        "question": "What does torch.set_default_tensor_type() use?",
        "context": "periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "spectral analysis",
        "question": "What is the periodic window suitable for?",
        "context": "periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "shape parameter",
        "question": "What does beta (float, optional) provide for the window?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "the desired data type of returned tensor",
        "question": "What is dtype?",
        "context": "periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "layout",
        "question": "What is the desired layout of returned window tensor?",
        "context": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.strided",
        "question": "What is the only option that supports dense layout?",
        "context": "beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "shape parameter",
        "question": "What parameter does beta (float, optional) provide for the window?",
        "context": "beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "global default",
        "question": "Default: if None, uses a what?",
        "context": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.strided",
        "question": "What is the term for dense layout?",
        "context": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Default",
        "question": "What is the default type of returned tensor?",
        "context": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.strided",
        "question": "What is the name for a dense layout?",
        "context": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.layout",
        "question": "What is the name of the desired layout of returned window tensor?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default layout of a window tensor?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "if None",
        "question": "What uses the current device for the default tensor type?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "desired layout",
        "question": "What is the layout of returned window tensor?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default layout of returned window tensor?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "CPU",
        "question": "Device will be what for CPU tensor types and the current CUDA device for CUDA tensor types?",
        "context": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Common linear algebra operations",
        "question": "What is a vector or matrix norm?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "matrix norm",
        "question": "What is the determinant of a square matrix?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "vector norm",
        "question": "What is a common linear algebra operation?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "determinant of a square matrix",
        "question": "Computes the sign and natural logarithm of the absolute value of what?",
        "context": "Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the sign and natural logarithm",
        "question": "What is the absolute value of the determinant of a square matrix?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the condition number of a matrix with respect to a matrix norm",
        "question": "What does it compute?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "numerical rank",
        "question": "What does the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix compute?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Cholesky",
        "question": "Who decomposes a complex Hermitian or real symmetric positive-definite matrix?",
        "context": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Common linear algebra operations",
        "question": "What operations are used to compute a vector or matrix norm?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "condition number",
        "question": "Computes what of a matrix with respect to a matrix norm?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "singular values",
        "question": "Computes what of a matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Cholesky",
        "question": "What decomposition of a complex Hermitian or real symmetric positive-definite matrix is computed?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "a vector or matrix norm",
        "question": "What type of norm is computed?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "vector norm",
        "question": "What is a matrix norm?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "determinant of a square matrix",
        "question": "What does Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the condition number of a matrix with respect to a matrix norm",
        "question": "What does the computation of the numerical rank of a matrix consist of?",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "QR",
        "question": "What decomposition of a matrix is computed?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "determinant",
        "question": "Computes the absolute value of what of a square matrix?",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the QR decomposition of a matrix",
        "question": "What does Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "context": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "matrix norm",
        "question": "Computes a vector norm.",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "determinant of a square matrix",
        "question": "Computes the absolute value of what?",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "matrix norm",
        "question": "Computes the condition number of a matrix with respect to what?",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue",
        "question": "What is the decomposition of a square matrix if it exists?",
        "context": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "determinant",
        "question": "Computes the sign and natural logarithm of the absolute value of what of a square matrix?",
        "context": "Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues",
        "question": "What does the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix if it exists?",
        "context": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the sign and natural logarithm",
        "question": "Computes what of the absolute value of the determinant of a square matrix?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue",
        "question": "What decomposition of a square matrix if it exists?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues",
        "question": "Computes the eigenvalue decomposition of a square matrix if it exists. Computes what of a square matrix?",
        "context": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix",
        "question": "What does the Cholesky decomposition of a square matrix if it exists?",
        "context": "Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix",
        "question": "What does the Cholesky decomposition of a complex Hermitian or real symmetric matrix compute?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "if it exists",
        "question": "What is the eigenvalue decomposition of a square matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes the eigenvalues of a square matrix",
        "question": "Computes the eigenvalue decomposition of a square matrix if it exists.",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue",
        "question": "Computes the decomposition of a complex Hermitian or real symmetric matrix?",
        "context": "Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a square matrix",
        "question": "What does the Cholesky decomposition of a complex Hermitian or real symmetric matrix compute if it exists?",
        "context": "Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues",
        "question": "Computes what of a square matrix?",
        "context": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix",
        "question": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.",
        "context": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues",
        "question": "Computes what of a complex Hermitian or real symmetric matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "singular value decomposition",
        "question": "What does SVD stand for?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Cholesky",
        "question": "Computes the decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a square matrix",
        "question": "What does it compute if it exists?",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues of a square matrix",
        "question": "What does the eigenvalue decomposition of a complex Hermitian or real symmetric matrix compute?",
        "context": "  Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix",
        "question": "What is the eigenvalue decomposition of a complex Hermitian or real symmetric matrix?",
        "context": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues of a square matrix",
        "question": "What does the SVD of a matrix compute?",
        "context": "Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "if it exists",
        "question": "Computes the eigenvalue decomposition of a square matrix?",
        "context": "Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "a square system of linear equations with a unique solution",
        "question": "What is the solution of?",
        "context": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes the eigenvalues of a square matrix",
        "question": "What does Computes the eigenvalue decomposition of a square matrix if it exists?",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "singular values of a matrix",
        "question": "Computes the singular value decomposition (SVD) of a matrix.",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "unique solution",
        "question": "Computes the solution of a square system of linear equations with what?",
        "context": "  Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix",
        "question": "What does the SVD of a matrix do?",
        "context": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "a square system of linear equations with a unique solution",
        "question": "What solves the least squares problem of a system of linear equations?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "a system of linear equations",
        "question": "Computes a solution to the least squares problem of what?",
        "context": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the least squares problem",
        "question": "Computes a solution to what problem of a system of linear equations?",
        "context": "  Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "What is the name of a square matrix if it exists?",
        "context": "Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "Computes what of a square matrix if it exists?",
        "context": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "pseudoinverse",
        "question": "What does Moore-Penrose inverse stand for?",
        "context": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Moore-Penrose",
        "question": "What is the pseudoinverse of a matrix?",
        "context": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "matrix norm",
        "question": "Computes a what?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "a square system of linear equations with a unique solution",
        "question": "Computes the solution of what?",
        "context": "  Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "Computes the what of a square matrix if it exists?",
        "context": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "n-th power",
        "question": "Computes what of a square matrix for an integer n?",
        "context": "  Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "first n columns",
        "question": "What is the first column of a product of Householder matrices?",
        "context": "  Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "n-th power",
        "question": "What is the power of a square matrix for an integer n?",
        "context": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "reordering",
        "question": "Efficiently multiplies two or more matrices by what?",
        "context": "  Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "first n columns",
        "question": "Computes what of a product of Householder matrices?",
        "context": "  Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "multiplicative inverse",
        "question": "What does torch.tensordot() compute?",
        "context": "  Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "B",
        "question": "Computes the solution X to the system torch.tensordot(A, X) = what?",
        "context": "  Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Cholesky",
        "question": "What is the name of the decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix if it is in",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Cholesky",
        "question": "Computes what decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the inverse",
        "question": "Computes what of a square matrix if it is invertible?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "COO(rdinate)",
        "question": "What format is used to construct a sparse tensor?",
        "context": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given\nindices. Note This function returns an uncoalesced tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "uncoalesced tensor",
        "question": "What type of tensor does this function return?",
        "context": "Note This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "COO(rdinate)",
        "question": "Constructs a sparse tensor in what format?",
        "context": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given\nindices. Note This function returns an uncoalesced tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "returns an uncoalesced tensor",
        "question": "What does this function return?",
        "context": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "a list, tuple, NumPy ndarray, scalar, and other types",
        "question": "What types of indices can be used for the tensor?",
        "context": "Note This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "torch",
        "question": "What is cast to the LongTensor internally?",
        "context": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "the number of tensor dimensions",
        "question": "What is the first dimension of the indices?",
        "context": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "a list, tuple, NumPy ndarray, scalar, and other types",
        "question": "What can indices be?",
        "context": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "LongTensor",
        "question": "What is the name of the internal tensor?",
        "context": "Note This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "the coordinates of the non-zero values in the matrix",
        "question": "What are the indices?",
        "context": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "tuple",
        "question": "Can be a list, NumPy ndarray, scalar, and other types?",
        "context": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "LongTensor",
        "question": "What is the name of the tensor internally?",
        "context": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "tuple, NumPy ndarray, scalar",
        "question": "What are some other types of indices?",
        "context": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "torch",
        "question": "What will be cast to the LongTensor internally?",
        "context": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "tensor",
        "question": "What is the initial value for?",
        "context": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "tuple, NumPy ndarray, scalar",
        "question": "What are some other types of initial values for the tensor?",
        "context": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "size",
        "question": "What is optional for a sparse tensor?",
        "context": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "the minimum size big enough to hold all non-zero elements",
        "question": "What is the size of the sparse tensor inferred as if not provided?",
        "context": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "if None",
        "question": "What default infers data type from values?",
        "context": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "size",
        "question": "What is the size of the sparse tensor?",
        "context": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "minimum size big enough to hold all non-zero elements",
        "question": "What is the size of the sparse tensor if not provided?",
        "context": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "None",
        "question": "Default: if what, infers data type from values?",
        "context": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "if None",
        "question": "What infers data type from values?",
        "context": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "None",
        "question": "Default: if what, infers data type from values. device (torch.device, optional) \u2013 the desired device",
        "context": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "Computes the element-wise logical OR",
        "question": "What does compute the element-wise logical OR of the given input tensors do?",
        "context": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "answer": "Zeros",
        "question": "What is treated as False?",
        "context": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "answer": "other",
        "question": "What is the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor?",
        "context": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "answer": "Example",
        "question": "What is an example of a tensor to compute OR with out?",
        "context": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "answer": "LU",
        "question": "What contains L and U factors for LU factorization of A?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "2D",
        "question": "What type of inputs can torch.solve(B, A) take?",
        "context": "LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "batched outputs solution",
        "question": "What does torch.solve(B, A) return if the inputs are batches?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "real-valued and complex-valued inputs",
        "question": "What types of inputs does torch.solve(B, A) support?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Warning",
        "question": "What does torch.solve(B, A) do?",
        "context": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "namedtuple solution",
        "question": "What is the LU factorization of A in order?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "L and U factors",
        "question": "What does LU contain for LU factorization of A?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Warning",
        "question": "What does torch.solve(B, A) do when it takes inputs that are batches of 2D matrices?",
        "context": "LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "2D",
        "question": "What type of inputs can torch.solve(B, A) take in?",
        "context": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "batched outputs solution",
        "question": "What does torch.solve return if the inputs are batches?",
        "context": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "real-valued and complex-valued",
        "question": "What types of inputs does torch.solve support?",
        "context": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "2D inputs B, A or inputs that are batches of 2D matrices",
        "question": "What can torch.solve(B, A) take in?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Warning",
        "question": "What does torch.solve(B, A) provide?",
        "context": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "real-valued and complex-valued inputs",
        "question": "What types of inputs does torch.solve() support?",
        "context": "Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Warning torch.solve()",
        "question": "What is deprecated in favor of torch.linalg.solve()?",
        "context": "Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "LU factorization",
        "question": "What does torch.linalg.solve() not return?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "torch.lu()",
        "question": "What can be used with torch.lu_solve() and torch.lu_unpack() to get the LU factorization?",
        "context": "Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Note",
        "question": "What should torch.solve(B, A) be replaced with?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "real-valued and complex-valued inputs",
        "question": "What types of inputs does PyTorch support?",
        "context": "Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "torch.linalg.solve()",
        "question": "What is the replacement for torch.solve()?",
        "context": "Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "does not return the LU factorization of the input",
        "question": "What does torch.linalg.solve() do?",
        "context": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "torch.lu()",
        "question": "What is used to get the LU factorization of the input?",
        "context": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Note",
        "question": "What should X = torch.solve(B, A)solution be replaced with?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "torch.lu()",
        "question": "What is used to get the LU factorization?",
        "context": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "torch.linalg.solve()",
        "question": "What is torch.solve() replaced with?",
        "context": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "torch.linalg.solve()",
        "question": "What is torch.solve() replaced by?",
        "context": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "B.contiguous().transpose(-1, -2)",
        "question": "What is the name of the strides used to transpose the returned matrices?",
        "context": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "zero or more batch dimensions",
        "question": "What is *?",
        "context": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "A",
        "question": "What is the input square matrix of size (,m,m)(*, m, m)(,m,m)",
        "context": "X = torch.solve(B, A).solution should be replaced with Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "the returned matrices solution and LU",
        "question": "What will be transposed regardless of the original strides?",
        "context": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "A",
        "question": "What is the name of the input square matrix of size (,m,m)(*, m, m)(,m",
        "context": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "optional",
        "question": "Out ((Tensor, Tensor)) \u2013 optional output tuple.",
        "context": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "GPUs",
        "question": "What do CUDA tensor types use for computation?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "is_available()",
        "question": "What is used to determine if your system supports CUDA?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA semantics",
        "question": "What has more details about working with CUDA?",
        "context": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager StreamContext",
        "question": "What selects a given stream?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Checks if peer access between two devices is possible",
        "question": "What does the Context-manager do?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cublasHandle_t",
        "question": "What is the pointer to current cuBLAS handle?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "peer access between two devices",
        "question": "What is checked if the Context-manager selects a given stream?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current cuBLAS handle",
        "question": "What does cublasHandle_t pointer to?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cublasHandle_t pointer",
        "question": "What does cublasHandle_t pointer return to current cuBLAS handle?",
        "context": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selected Stream",
        "question": "What does cublasHandle_t return for a given device?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a dictionary of CUDA memory allocator statistics",
        "question": "What does this return for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "peer access between two devices is possible",
        "question": "Checks if what is possible?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory occupied by tensors in bytes",
        "question": "What is returned for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager",
        "question": "What changes the selected device?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "GPUs",
        "question": "Returns the number of what?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current cuBLAS handle",
        "question": "Returns cublasHandle_t pointer to what?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "number of GPUs available",
        "question": "What does the Context-manager return?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cublasHandle_t pointer",
        "question": "What is returned to current cuBLAS handle?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager",
        "question": "What changes the current device to that of given object?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager",
        "question": "Who changes the selected device?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "number of GPUs available",
        "question": "What does the Context-manager that changes the selected device return?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the currently selected Stream",
        "question": "What does the cublasHandle_t pointer return for a given device?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA architectures",
        "question": "What does this library return a list of?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA architectures",
        "question": "Returns list of what library was compiled for?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "What does this library return?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What is returned when a library is compiled?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "index",
        "question": "What is the name of a currently selected device?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "default Stream",
        "question": "What does the Context-manager return for a given device?",
        "context": "Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the name of a device",
        "question": "What is the name of a device?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the name of a device",
        "question": "What does the cuda capability of a device return?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "What do you get from a device?",
        "context": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selected device",
        "question": "Returns the index of what?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "number of GPUs available",
        "question": "What does Context-manager return?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "Gets what of a device?",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the cuda capability of a device",
        "question": "What does this library get?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "What does the library get from a device?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "What was this library compiled with?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "default Stream",
        "question": "What does PyTorch return for a given device?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "What does PyTorch get from a device?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the name of a device",
        "question": "What does PyTorch get?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "What does PyTorch's CUDA state return?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "PyTorch",
        "question": "What library initializes its CUDA state?",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA architectures",
        "question": "What is the list this library was compiled for?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda capability",
        "question": "What is the name of a device. Gets the name of a device. Gets the properties of a device.",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the properties of a device",
        "question": "What does PyTorch do?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "What does PyTorch return?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Force",
        "question": "What collects GPU memory after it has been released by CUDA IPC?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "Returns what flags this library was compiled with?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Initialize PyTorch\u2019s CUDA state",
        "question": "What does Force collect GPU memory after it has been released by CUDA IPC?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "PyTorch",
        "question": "What library initializes CUDA state?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "bool",
        "question": "What indicates if CUDA is currently available?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current random seed of the current GPU",
        "question": "Returns what?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "bool",
        "question": "Returns what indicating if CUDA is currently available?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "What did PyTorch's CUDA state return?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the name of a device",
        "question": "What does Gets the cuda capability of a device?",
        "context": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA architectures",
        "question": "What does this library list?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Initialize PyTorch\u2019s CUDA state",
        "question": "What does the Force collect GPU memory after it has been released by CUDA IPC?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory managed by the caching allocator in bytes for a given device",
        "question": "What does this return?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA architectures",
        "question": "Returns list of what architectures this library was compiled for?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current device",
        "question": "Sets what if PyTorch\u2019s CUDA state has been initialized?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Initialize",
        "question": "What does PyTorch do with its CUDA state?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the current device",
        "question": "What is a wrapper API to set the stream?",
        "context": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "wrapper API",
        "question": "What type of API is used to set the stream?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda capability",
        "question": "What does a device get?",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What does the bool indicating if PyTorch\u2019s CUDA state has been initialized return?",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "random number generator state of all devices",
        "question": "Sets what?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the current device",
        "question": "What is a wrapper API to set the current stream?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "wrapper API",
        "question": "What is this to set the stream?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper",
        "question": "What is the name of the API that selects a given stream?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "set the stream",
        "question": "This is a wrapper API to what?",
        "context": "Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager StreamContext",
        "question": "Wrapper around what that selects a given stream?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "ByteTensor",
        "question": "Returns the random number generator state of the specified GPU as a what?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a list of ByteTensor",
        "question": "Returns what representation of the random number states of all devices?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does the ByteTensor do?",
        "context": "Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all devices",
        "question": "Sets the random number generator state of what?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU",
        "question": "Sets the seed for generating random numbers for what?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all GPUs",
        "question": "Sets the seed for generating random numbers on what?",
        "context": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU",
        "question": "Sets the seed for generating random numbers to a random number for what?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "ByteTensor",
        "question": "What represents the random number states of all devices?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the random number generator state of all devices",
        "question": "What does the ByteTensor set?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU",
        "question": "What is the seed for generating random numbers for?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all GPUs",
        "question": "The seed for generating random numbers is set on what?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a random number on all GPUs",
        "question": "What is the seed for generating random numbers set to?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all GPUs",
        "question": "The seed for generating random numbers is set to a random number on what?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "ByteTensor",
        "question": "Returns a list of what representing the random number states of all devices?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "generating random numbers",
        "question": "Sets the seed for what on all GPUs?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a random number",
        "question": "Sets the seed for generating random numbers to what on all GPUs?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "random number generator state",
        "question": "Sets what state of the specified GPU?",
        "context": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all GPUs",
        "question": "Sets the seed for generating random numbers to a random number on what?",
        "context": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current random seed of the current GPU",
        "question": "What does Sets the seed for generating random numbers return?",
        "context": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast Broadcasts",
        "question": "What sends a tensor to specified GPU devices?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast_coalesced Broadcasts",
        "question": "What is a sequence of tensors to the specified GPUs?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sums",
        "question": "What does comm.reduce_add tensors from multiple GPUs do?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.scatter",
        "question": "What Scatters tensor across multiple GPUs?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.gather",
        "question": "What gathers tensors from multiple GPU devices?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast",
        "question": "What Broadcasts a tensor to specified GPU devices?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast_coalesced",
        "question": "What Broadcasts a sequence tensors to the specified GPUs?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.reduce_add",
        "question": "What Sums tensors from multiple GPUs?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a CUDA event",
        "question": "Wrapper around what?",
        "context": "  Wrapper around a CUDA stream.   Wrapper around a CUDA event. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA stream",
        "question": "What is Wrapper around a CUDA event?",
        "context": "  Wrapper around a CUDA stream.   Wrapper around a CUDA event. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA event",
        "question": "What is Wrapper around a CUDA stream?",
        "context": "  Wrapper around a CUDA stream.   Wrapper around a CUDA event. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "unoccupied cached memory",
        "question": "Releases all what currently held by the caching allocator?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA",
        "question": "What type of memory allocator statistics does nvidia-smi return?",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Releases all unoccupied cached memory currently held by the caching allocator",
        "question": "What does it do to the unoccupied cached memory currently held by the caching allocator?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a human-readable printout",
        "question": "What is returned of the running processes and their GPU memory use for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator statistics",
        "question": "Returns a dictionary of what for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current memory allocator statistics",
        "question": "Returns a human-readable printout of what for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "snapshot",
        "question": "Returns a what of the CUDA memory allocator state across all devices?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "tensors in bytes",
        "question": "What is the current GPU memory occupied by for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU memory occupied by tensors in bytes",
        "question": "What does Returns a snapshot of the CUDA memory allocator state across all devices?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a dictionary of CUDA memory allocator statistics",
        "question": "What does return for a given device?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a human-readable printout",
        "question": "What is the current memory allocator statistics for a given device?",
        "context": "Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "snapshot",
        "question": "What is the CUDA memory allocator state across all devices?",
        "context": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory occupied by tensors in bytes",
        "question": "What does Returns the current GPU memory occupied by tensors in bytes for a given device?",
        "context": "Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator state",
        "question": "Returns a snapshot of what across all devices?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point",
        "question": "What does it do in tracking maximum GPU memory occupied by tensors for a given device?",
        "context": "Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA",
        "question": "What is the name of the memory allocator state across all devices?",
        "context": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator state",
        "question": "Returns a snapshot of what state across all devices?",
        "context": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory occupied by tensors in bytes",
        "question": "Returns the current GPU memory occupied by tensors in bytes for a given device.",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors",
        "question": "What does this function do for a given device?",
        "context": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU memory managed by the caching allocator in bytes",
        "question": "Returns the current GPU memory managed by the caching allocator in bytes for a given device.",
        "context": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device",
        "question": "What does it do to the starting point in tracking maximum GPU memory occupied by tensors for a given device?",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "caching allocator",
        "question": "Returns the current GPU memory managed by what for a given device?",
        "context": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory",
        "question": "What is occupied by tensors in bytes for a given device?",
        "context": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU memory managed by the caching allocator in bytes",
        "question": "What does Returns the starting point in tracking maximum GPU memory occupied by tensors for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the caching allocator in bytes",
        "question": "Returns the maximum GPU memory managed by what for a given device?",
        "context": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction",
        "question": "What does a process do for a process?",
        "context": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "tensors",
        "question": "What is the maximum GPU memory occupied by?",
        "context": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction for a process",
        "question": "What does memory_reserved do?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Deprecated",
        "question": "What is memory_reserved()?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "max_memory_reserved",
        "question": "What is a deprecated function that returns the maximum GPU memory occupied by tensors in bytes for a given device",
        "context": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "tensors",
        "question": "Returns the maximum GPU memory occupied by what in bytes for a given device?",
        "context": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors",
        "question": "What does it do for a given device?",
        "context": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction",
        "question": "What does memory_reserved() do for a process?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "memory_reserved()",
        "question": "Set memory fraction for a process. Deprecated; see what?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "max_memory_reserved()",
        "question": "Deprecated; see what?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "max_memory_reserved()",
        "question": "What is a deprecated function that returns the maximum GPU memory occupied by tensors for a given device?",
        "context": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "tensors",
        "question": "Resets the starting point in tracking maximum GPU memory occupied by what for a given device?",
        "context": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "default Stream",
        "question": "Returns what for a given device?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "memory fraction",
        "question": "Set what for a process?",
        "context": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction",
        "question": "What does memory_reserved do for a process?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "max_memory_reserved()",
        "question": "What is the name of the deprecated function that returns the maximum GPU memory managed by the caching allocator for a given device",
        "context": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA",
        "question": "Which memory allocator resets the \"peak\" stats?",
        "context": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory managed by the caching allocator",
        "question": "What is returned in bytes for a given device?",
        "context": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "starting point",
        "question": "What is reset in tracking maximum GPU memory managed by the caching allocator for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator",
        "question": "What does the \u201cpeak\u201d stats track?",
        "context": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "instantaneous",
        "question": "nvtx.mark Describe an event that occurred at some point.",
        "context": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "nvtx.range_push",
        "question": "What pushes a range onto a stack of nested range spans?",
        "context": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "nvtx.range_pop",
        "question": "What pops a range off of a stack of nested range spans?",
        "context": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "an instantaneous event",
        "question": "What does nvtx.mark describe?",
        "context": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "The torch.special module",
        "question": "What is modeled after SciPy's special module?",
        "context": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "BETA",
        "question": "What version of PyTorch is the torch.special module in?",
        "context": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "New functions are still being added",
        "question": "What is the status of the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the documentation of each function",
        "question": "Where can you find details of the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "elementwise",
        "question": "Computes the entropy on input in what way?",
        "context": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the error function of input",
        "question": "What does the torch.special module do?",
        "context": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor) \u2013 the input tensor",
        "question": "What is the error function of input defined as?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "SciPy\u2019s special module",
        "question": "What is the torch.special module modeled after?",
        "context": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "New functions",
        "question": "What is still being added to the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "documentation",
        "question": "What is the name of each function in the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "entropy",
        "question": "What does the torch.special module compute on input?",
        "context": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "Out (Tensor, optional) \u2013 what?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the error function of input",
        "question": "What does the torch.special module compute?",
        "context": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "BETA",
        "question": "What version of PyTorch is this module in?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "PyTorch releases",
        "question": "When may some functions change?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "documentation",
        "question": "What is the name of each function in PyTorch?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "entropy",
        "question": "Computes what on input?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "optional",
        "question": "Out (Tensor, optional) \u2013 the output tensor.",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the error function of input",
        "question": "What is the function that computes the error function of input?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the complementary error function of input",
        "question": "What is the error function of input?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the complementary error function of input",
        "question": "What does Compute the error function of input do?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor) \u2013 the input tensor",
        "question": "What is the complementary error function of input defined as?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the complementary error function of input",
        "question": "What is an example of a function that computes the error function of input?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "What error function is defined in the range (1,1)(-1, 1)(1,1)?",
        "context": "Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "What is defined in the range (1,1)(-1, 1)(1,1)?",
        "context": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the inverse error function of input",
        "question": "What is the complementary error function of input?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "What is out (Tensor, optional) defined as?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor)",
        "question": "What is the inverse error function of input defined as?",
        "context": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "expit",
        "question": "What is another name for the logistic sigmoid function?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "Computes what error function of input?",
        "context": "Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "What is defined in the range (1,1)(-1, 1)(1,1) as: input (Tensor) \u2013 the input",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the logistic sigmoid function",
        "question": "What is the expit also known as?",
        "context": "Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor)",
        "question": "What is the name of the input tensor?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the inverse error function of input",
        "question": "What is the inverse error function of input?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor)",
        "question": "What is the inverse error function defined as?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "1",
        "question": "Computes the exponential of the elements minus what number of input?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Note",
        "question": "What is the name of the function that computes the exponential of the elements minus 1 of input?",
        "context": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "What function is defined in the range (1,1)(-1, 1)(1,1)?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponential",
        "question": "Computes what of the elements minus 1 of input?",
        "context": "Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Note",
        "question": "What does the exponential of the elements minus 1 of input do?",
        "context": "Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor)",
        "question": "What is the inverse error function of input called?",
        "context": "Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "Out (Tensor, optional) is what?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "What function is defined in the range (1,1)(-1, 1)(1,1) as: input (Tensor) \u2013 the",
        "context": "Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "base two exponential function",
        "question": "Computes what function of input?",
        "context": "Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "expit",
        "question": "What is the logistic sigmoid function?",
        "context": "Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "1",
        "question": "Computes the exponential of the elements minus what amount of input?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exp(x) - 1",
        "question": "The exponential of the elements minus 1 of input provides greater precision than what for small values of x?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "base two",
        "question": "What is the base of the exponential function of input?",
        "context": "Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "optional",
        "question": "Out (Tensor) - the output tensor.",
        "context": "Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "greater precision",
        "question": "What does the exponential of the elements minus 1 of input provide?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the logistic sigmoid function",
        "question": "Computes the expit also known as what?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "1",
        "question": "Computes the exponential of the elements minus what of input?",
        "context": "Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exp(x) - 1",
        "question": "This function provides greater precision than what for small values of x?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "logarithm",
        "question": "Computes the natural what of the absolute value of the gamma function on input?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the exponential of the elements minus 1 of input",
        "question": "What function provides greater precision than exp(x) - 1 for small values of x?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "logarithm",
        "question": "What is the natural value of the absolute value of the gamma function on input?",
        "context": "Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "greater precision",
        "question": "What does this function provide than exp(x) - 1 for small values of x?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the natural logarithm of the absolute value of the gamma function on input",
        "question": "What does Compute?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponentially scaled zeroth order modified Bessel function of the first kind (as defined below) for each element of input",
        "question": "What type of function is computed for each element of input?",
        "context": "Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "first kind",
        "question": "Computes the exponentially scaled zeroth order modified Bessel function of what kind?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the base two exponential function of input",
        "question": "What is an example of a function that computes the base two exponential function of input?",
        "context": "Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "logarithm",
        "question": "What natural function is computed of the absolute value of the gamma function on input?",
        "context": "This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "zeroth",
        "question": "What order modified Bessel function of the first kind is computed for each element of input?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the first kind",
        "question": "What is the exponentially scaled zeroth order modified Bessel function of?",
        "context": "Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "* log1p(other)",
        "question": "What is the base two exponential function of input?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "optional",
        "question": "What type of Tensor is the output tensor?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "base two exponential function",
        "question": "Computes what of input?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "natural logarithm",
        "question": "Computes what of the absolute value of the gamma function on input?",
        "context": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What is computed for each element of input?",
        "context": "Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "natural logarithm",
        "question": "What is the absolute value of the gamma function on input?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "optional",
        "question": "Out (Tensor, optional) - the output tensor.",
        "context": "Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "zeroth",
        "question": "What order modified Bessel function is computed for each element of input?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "eps",
        "question": "What is input clamped to when eps is not None?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "NaN",
        "question": "When eps is None and input  0 or input > 1, the function will yield what?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the exponentially scaled zeroth order modified Bessel function of the first kind",
        "question": "What does the function do for each element of input?",
        "context": "Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "float",
        "question": "What is the epsilon for input clamp bound?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "None",
        "question": "What is the default value of the output tensor?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "* log1p(other)",
        "question": "What is the input with the following cases?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "SciPy",
        "question": "What is scipy.special.xlog1py similar to?",
        "context": "Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "None out",
        "question": "What is the default output tensor?",
        "context": "Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "* log1p(other)",
        "question": "What does the function compute with the following cases?",
        "context": "Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Multiplier",
        "question": "What does input (Number or Tensor) represent?",
        "context": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Vandermonde",
        "question": "What is the name of the matrix that generates?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "elementwise powers",
        "question": "What are the columns of the output matrix?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "True",
        "question": "If increasing is true, the order of the columns is reversed x0,x1,...,x(N1),x(N",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Alexandre-Theophile Vandermonde",
        "question": "Who is a Vandermonde matrix named for?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "1-D",
        "question": "What type of input tensor is x (Tensor)?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "True",
        "question": "If increasing is what, the order of the columns is reversed?",
        "context": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Alexandre-Theophile Vandermonde",
        "question": "Who is a matrix with a geometric progression in each row named for?",
        "context": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "elementwise",
        "question": "What are the powers of the input vector?",
        "context": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "True",
        "question": "If increasing is what, the order of the columns is reversed x0,x1,...,x(N1)x0, ",
        "context": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "1-D input tensor",
        "question": "What is x (Tensor)?",
        "context": "x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "N",
        "question": "What is the number of columns in the output?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "N",
        "question": "If what is not specified, a square array is returned?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Order of the powers of the columns",
        "question": "What is increasing (bool, optional)?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "True",
        "question": "If what, the powers increase from left to right?",
        "context": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Number of columns in the output",
        "question": "What does N stand for?",
        "context": "x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "a square array",
        "question": "What is returned if N is not specified?",
        "context": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "increasing",
        "question": "What is the order of the powers of columns?",
        "context": "x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "the powers increase from left to right",
        "question": "What happens if True?",
        "context": "x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Number of columns in the output",
        "question": "What is N (int, optional)?",
        "context": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Number of columns in the output",
        "question": "What is N?",
        "context": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Order of the powers of the columns",
        "question": "What does increasing (bool, optional) mean?",
        "context": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "the powers increase from left to right",
        "question": "If True, what happens to the powers of columns?",
        "context": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "fault-tolerant and elastic",
        "question": "What does PyTorch make?",
        "context": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "Usage API Advanced Plugins",
        "question": "What makes distributed PyTorch fault-tolerant?",
        "context": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "fault-tolerant",
        "question": "Is distributed PyTorch fault-tolerant or elastic?",
        "context": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "elastic",
        "question": "Makes distributed PyTorch fault-tolerant and what else?",
        "context": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "Usage API Advanced Plugins",
        "question": "What makes distributed PyTorch fault-tolerant and elastic?",
        "context": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "Usage",
        "question": "What is the most important aspect of a computer?",
        "context": "Usage ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "Usage",
        "question": "What is the term for what?",
        "context": "Usage ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "API Advanced Plugins",
        "question": "What kind of plugins are available?",
        "context": "API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "API Advanced Plugins",
        "question": "What is the name of what?",
        "context": "API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "Decomposes input",
        "question": "What does mantissa and exponent tensors do?",
        "context": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "answer": "open interval",
        "question": "What is the range of mantissa?",
        "context": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "answer": "float inputs",
        "question": "What type of input does mantissa support?",
        "context": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "answer": "input (Tensor)",
        "question": "What is the input tensor out?",
        "context": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "answer": "tensor2",
        "question": "Performs the element-wise division of tensor1 by what?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "scalar value",
        "question": "What is the result of the element-wise division of tensor1 by tensor2 multiplied by?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "Warning",
        "question": "What is the name of the function that performs the element-wise division of tensor1 by tensor2?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "multiply the result by the scalar value",
        "question": "How does the element-wise division of tensor1 by tensor2 work?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "Warning",
        "question": "What is the result of the element-wise division of tensor1 by tensor2?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "Warning Integer",
        "question": "What division with addcdiv is no longer supported?",
        "context": "Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "torch.trunc",
        "question": "What is the historic addcdiv behavior implemented as?",
        "context": "Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "dtypes",
        "question": "The future addcdiv behavior is the same for all what?",
        "context": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "addcdiv",
        "question": "What is no longer supported for integer division?",
        "context": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "float inputs",
        "question": "What inputs can be implemented as (input + value * tensor1 / tensor2)?",
        "context": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "all dtypes",
        "question": "What does the future addcdiv behavior apply to?",
        "context": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "torch.trunc",
        "question": "The historic addcdiv behavior can be implemented as (input + value * what?",
        "context": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "broadcastable",
        "question": "The shapes of input, tensor1, and tensor2 must be what?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "real number",
        "question": "For inputs of type FloatTensor or DoubleTensor, value must be a what?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "tensor",
        "question": "Input (Tensor) \u2013 what is to be added tensor1 (Tensor)?",
        "context": "The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "Example",
        "question": "What is an example of a tensor that must be broadcastable?",
        "context": "The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "asynchronous execution",
        "question": "What does the Future type encapsulate?",
        "context": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Distributed RPC Framework",
        "question": "What is the Future type primarily used by?",
        "context": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "rpc_async()",
        "question": "What encapsulates an asynchronous execution of a callable?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "rpc_async()",
        "question": "What is an example of an asynchronous execution of a callable?",
        "context": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "APIs",
        "question": "What does the Future type expose to add callback functions and set results?",
        "context": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Warning GPU support",
        "question": "What is a beta feature?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "asynchronous execution of a callable",
        "question": "What does torch._C.Future encapsulate?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "APIs",
        "question": "What does torch._C.Future expose to add callback functions and set results?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "beta",
        "question": "What version of Warning GPU support is subject to changes?",
        "context": "Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Warning GPU support",
        "question": "What is a beta feature, subject to changes?",
        "context": "Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "when the Future is completed",
        "question": "When will the given callback function run?",
        "context": "GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Multiple callbacks",
        "question": "What can be added to the same Future, but the order in which they will be executed cannot be guaranteed?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "one argument",
        "question": "How many arguments must a callback take?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "value() method",
        "question": "What can the callback function use to get the value?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inline",
        "question": "If the Future is already completed, the given callback will be run what?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Warning",
        "question": "What is GPU support a beta feature?",
        "context": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "when the Future is completed",
        "question": "When will the given callback function be run?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the order in which they will be executed",
        "question": "What cannot be guaranteed when multiple callbacks are added to the same Future?",
        "context": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "value()",
        "question": "What method can the callback function use to get the value?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "GPU",
        "question": "What GPU support is a beta feature?",
        "context": "GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "value() method",
        "question": "What can a callback function use to get the value of a Future?",
        "context": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "then()",
        "question": "Which method provides a way to synchronize after your callback has completed?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "add_done_callback",
        "question": "What can be cheaper if your callback does not return anything?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "same",
        "question": "What kind of callback registration API do both then() and add_done_callback use?",
        "context": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "GPU tensors",
        "question": "What does add_done_callback behave in the same way as then()?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "callback",
        "question": "What is a Callable that takes in one argument, is the reference to this Future?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What is the name of the callable that takes in one argument?",
        "context": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "then()",
        "question": "What method provides a way to synchronize after your callback has completed?",
        "context": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "same",
        "question": "What do both then() and add_done_callback use under the hood?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future",
        "question": "What is the name of the callback that takes in one argument?",
        "context": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "which",
        "question": "What is the name of the callback that takes in one argument, is the reference to this Future?",
        "context": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "then()",
        "question": "What method behaves in the same way as GPU tensors?",
        "context": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "GPU tensors",
        "question": "What does this method behave in the same way as then()?",
        "context": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What is the name of the Callable that references the Future?",
        "context": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future",
        "question": "What is a callback for a Callable that takes in one argument, is the reference to this Future?",
        "context": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future",
        "question": "What is a callback that takes in one argument, is the reference to this Future?",
        "context": "callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What is a callback that takes in one argument?",
        "context": "callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future",
        "question": "What is the reference to?",
        "context": "is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "error handling",
        "question": "What must be carefully taken care of if the callback function throws?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the user is responsible for handling completion/waiting on those futures independently",
        "question": "What is the user responsible for if the callback later completes additional futures?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completion/waiting",
        "question": "What is the user responsible for handling if the callback later completes additional futures?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completion/waiting",
        "question": "What is the user responsible for handling on futures that are not marked as completed with an error?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "True",
        "question": "What does Future.done() return if the Future is done?",
        "context": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a result or an exception",
        "question": "A Future is done if it has what?",
        "context": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "asynchronous kernels",
        "question": "What is populating the tensors that reside on GPUs?",
        "context": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "True",
        "question": "What does Future.done() return if this Future is done?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "if it has a result or an exception",
        "question": "When is a Future done?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "synchronizations",
        "question": "If the result is already usable, what does Future.done() do?",
        "context": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If the value contains what that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are popul",
        "context": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future.done()",
        "question": "What will return True if the value contains tensors that reside on GPUs?",
        "context": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "synchronizations",
        "question": "What does Future.done() do to ensure that the result is already usable?",
        "context": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future.done()",
        "question": "What function returns True if the value contains tensors that reside on GPUs?",
        "context": "callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future",
        "question": "What will mark this Future as completed with an error and trigger all attached callbacks?",
        "context": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inline",
        "question": "When calling wait()/value() on this Future, the exception set here will be raised what?",
        "context": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "result",
        "question": "What is the exception for this Future?",
        "context": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completed",
        "question": "Set the result for this Future, which will mark this Future as what?",
        "context": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a Future cannot be marked completed twice",
        "question": "What does a Future cannot be marked completed twice?",
        "context": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "mark this Future as completed",
        "question": "What does the result for this Future do?",
        "context": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Set the result for this Future",
        "question": "What will mark this Future as completed and trigger all attached callbacks?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "twice",
        "question": "How many times can a Future be marked completed?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "trigger all attached callbacks",
        "question": "What happens when a Future is marked as completed?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a Future cannot be marked completed twice",
        "question": "What does it mean that a Future cannot be marked completed twice?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "object",
        "question": "What is the result object of the Future?",
        "context": "result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "object",
        "question": "What is the result object of this Future?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "What does a callback return that reside on a GPU?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "re-synchronize them with the original streams",
        "question": "What must one do if one wants to change streams?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Callable",
        "question": "What is a Callable that takes this Future as the only argument?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "return value",
        "question": "A new Future object that holds what of the callback will be marked as completed when the given callback finishes?",
        "context": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What does a Callable that takes this Future as the only argument do?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future",
        "question": "What is the only argument for a callback?",
        "context": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completed",
        "question": "When a new Future object holds the return value of the callback is marked as what?",
        "context": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What does a Callable that takes this Future object hold the return value of the callback and will be marked as completed when the given callback finishes",
        "context": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What is a new Future object that holds the return value of a callback that will be marked as completed when the given callback finishes?",
        "context": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completed when the given callback finishes",
        "question": "When will a new Future object be marked as completed?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "A new Future object",
        "question": "What holds the return value of the callback and will be marked as completed when the given callback finishes?",
        "context": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What is a new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes?",
        "context": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the encountered error",
        "question": "The future returned by fut.wait() will be marked appropriately with what?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the future returned by then will be marked appropriately",
        "question": "What happens if the callback function throws an error?",
        "context": "Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the future returned by then",
        "question": "What will be marked appropriately with the encountered error if the callback function throws?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "an already-completed future",
        "question": "What is the value of?",
        "context": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait() has completed",
        "question": "Obtain the value of an already-completed future. This method should only be called after a call to what?",
        "context": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future may not yet hold a value",
        "question": "What could happen if the method is called after a call to wait() has completed?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Obtain the value of an already-completed future",
        "question": "What is the purpose of this method?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "Obtain the value of an already-completed future. This method should only be called after a call to what function has completed?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inside a callback function",
        "question": "Where should a method be called to obtain the value of an already-completed future?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "after a call to wait() has completed",
        "question": "When should this method be called?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future",
        "question": "What may not yet hold a value?",
        "context": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "GPUs",
        "question": "Where do tensors reside?",
        "context": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "What method should be used to perform additional synchronization if the value contains tensors that reside on GPUs?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inside a callback function",
        "question": "Where should this method be called after a call to wait() has completed?",
        "context": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future may not yet hold a value",
        "question": "What could happen if a call to value() is called after a call to wait() has completed?",
        "context": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "synchronization",
        "question": "If the value contains tensors that reside on GPUs, this method will not perform any additional what?",
        "context": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "If the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done",
        "context": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "The synchronization of tensors that reside on GPUs should be done separately through a call to what?",
        "context": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future",
        "question": "What is the value held by?",
        "context": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the function (callback or RPC) creating the value has thrown an error",
        "question": "What happens to the value held by this Future?",
        "context": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Block",
        "question": "What does the value() method do until the value of this Future is ready?",
        "context": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "What is the name of the method that performs synchronization of tensors that reside on GPUs?",
        "context": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "The value held by this Future",
        "question": "What is the value held by this Future?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait method",
        "question": "What method will throw an error if the function creating the value has thrown an error?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "until the value of this Future is ready",
        "question": "How long should the value() method block?",
        "context": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "value() method",
        "question": "What method throws an error if the function creating the value throws an error?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "until the value of this Future is ready",
        "question": "How long is the value held by this Future blocked?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Block",
        "question": "What is the name of the block until the value of the Future is ready?",
        "context": "Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Block",
        "question": "How long until the value of this Future is ready?",
        "context": "Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "The value held by this Future",
        "question": "What does the wait method return if the function creating the value has thrown an error?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "callback or RPC",
        "question": "What functions create the value of a Future?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed",
        "question": "What is the function that collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "list",
        "question": "What is a list of Future objects?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Returns a Future object to a list of the passed in Futures",
        "question": "What does this return a Future object to?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "value held by this Future",
        "question": "What is the value held by a Future?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "callback or RPC",
        "question": "What function created the value has thrown an error?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Collects the provided Future objects into a single combined Future",
        "question": "What is done when all of the sub-futures are completed?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a list of the passed in Futures",
        "question": "Returns a Future object to what?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "when all of the sub-futures are completed",
        "question": "When is a single combined Future completed?",
        "context": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "list of Future object",
        "question": "What is a futures (list)?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Returns a Future object to a list of the passed in Futures",
        "question": "What does futures return?",
        "context": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Waits for all provided futures to be complete",
        "question": "What does the method do that returns the list of completed values?",
        "context": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "exit early",
        "question": "What happens if any of the futures encounter an error?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Waits for all provided futures to be complete",
        "question": "What does the method do?",
        "context": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "exit early",
        "question": "If any of the futures encounter an error, the method will do what?",
        "context": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a list of Future object",
        "question": "What is futures (list)?",
        "context": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Returns a Future object to a list of the passed in Futures",
        "question": "What does this method do?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "exit early",
        "question": "What happens if a future encounters an error?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completed Future results",
        "question": "What is a list of in futures?",
        "context": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "if wait on any Future throws",
        "question": "When will this method throw an error?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a Future object",
        "question": "Returns what to a list of the passed in Futures?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completed Future results",
        "question": "Futures (list) \u2013 a list of what?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "throw an error",
        "question": "What does this method do if wait on any Future throws?",
        "context": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the element-wise remainder of division",
        "question": "What does the dividend and divisor compute?",
        "context": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "dividend input",
        "question": "The remainder has the same sign as what?",
        "context": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "broadcasting",
        "question": "What does the dividend support?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "NaN",
        "question": "When the divisor is zero, returns what for floating point dtypes on both CPU and GPU?",
        "context": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "input (Tensor)",
        "question": "What is the dividend input?",
        "context": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "dividend input",
        "question": "The remainder of the dividend and divisor has the same sign as what?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "the dividend",
        "question": "What is input (Tensor) \u2013?",
        "context": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "p-norm",
        "question": "Returns what of (input - other) The shapes of input and other must be broadcastable?",
        "context": "Returns the p-norm of (input - other) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the Right-hand-side input tensor p (float, optional) \u2013 the norm to be computed Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist"
    },
    {
        "answer": "the Right-hand-side input tensor",
        "question": "What is the other (Tensor)?",
        "context": "Returns the p-norm of (input - other) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the Right-hand-side input tensor p (float, optional) \u2013 the norm to be computed Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist"
    },
    {
        "answer": "(1, 1, 2, 2).",
        "question": "What is reps treated as if input has shape and reps is 2?",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "if input has fewer dimensions than reps specifies",
        "question": "If input has fewer dimensions than reps specifies, then input is treated as if it were unsqueezed at dimension zero until it has",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "4, 2)",
        "question": "If input has shape (what is the number of dimensions) and reps is (3, 3, 2, 2), then input is treated as if it had",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "NumPy\u2019s tile function",
        "question": "What is this function similar to?",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "input",
        "question": "What is the tensor whose elements to repeat?",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "the number of repetitions per dimension",
        "question": "What does reps represent?",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "Example",
        "question": "What is an example of a tensor whose elements to repeat?",
        "context": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "answer": "deconvolution",
        "question": "What is another name for a 3D transposed convolution operator over an input image composed of several input planes?",
        "context": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes called what?",
        "context": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D",
        "question": "What type of transposed convolution operator is applied over an input image composed of several input planes?",
        "context": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "large containing tensor",
        "question": "What does deconvolution combine an array of sliding local blocks into?",
        "context": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What is a partial inverse of MaxPool3d?",
        "context": "Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What does Compute a partial inverse of MaxPool2d?",
        "context": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "sliding local blocks",
        "question": "What is extracted from a batched input tensor?",
        "context": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool2d",
        "question": "What does Computes a partial inverse of MaxPool1d?",
        "context": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 1D power-average pooling over an input signal composed of what?",
        "context": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "a large containing tensor",
        "question": "What does the array of sliding local blocks combine into?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What does a 3D pooling over an input signal comprised of several input planes do?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What does the 3D pooling function do?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D power-average",
        "question": "What pooling over an input signal composed of several input planes?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 2D power-average pooling over an input signal composed of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies a 1D max pooling over an input signal composed of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What is a partial inverse of MaxPool2d?",
        "context": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D average-pooling",
        "question": "Applies what operation in kTkHkWkT times kH times kWkT",
        "context": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What does the 3D pooling over an input signal comprised of several input planes do?",
        "context": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What is an input signal composed of?",
        "context": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "What is a 3D transposed convolution operator called?",
        "context": "  Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What is a 1D adaptive max pooling over an input signal composed of?",
        "context": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "A 3D adaptive max pooling over an input signal composed of what?",
        "context": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What is a 1D adaptive average pooling over an input signal composed of?",
        "context": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D fractional max pooling",
        "question": "What does 3D fractional max pooling over an input signal composed of several input planes?",
        "context": "  Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies 3D fractional max pooling over an input signal composed of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "hardswish",
        "question": "Applies the element-wise function ReLU6(x)=min(max(0,x),6)textReLU6",
        "context": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "ELU(x)",
        "question": "What is the name of the element-wise function?",
        "context": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "elu()",
        "question": "What is the in-place version of elu()?",
        "context": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1.0507009873554804934193349852946",
        "question": "What is the scale of elu()?",
        "context": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "LeakyReLU(x)",
        "question": "What is the name of the in-place version of elu()?",
        "context": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Gaussian negative log likelihood loss",
        "question": "What is another term for Gaussian negative log likelihood loss?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "beta",
        "question": "A squared term is used if the absolute element-wise error falls below what?",
        "context": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Poisson negative log likelihood loss",
        "question": "What is the name of the loss that combines log_softmax and nll_loss in a single function?",
        "context": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "below beta and an L1 term otherwise",
        "question": "Where does the absolute element-wise error fall?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "SoftMarginLoss",
        "question": "What function uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise?",
        "context": "  See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "bilinear upsampling",
        "question": "Upsamples the input, using nearest neighbours\u2019 pixel values. Upsamples the input using what?",
        "context": "  Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1",
        "question": "What is the first argument of the matrix-matrix product?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If the first argument is 2-dimensional and the second argument is what, the matrix-vector product is returned?",
        "context": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "batched matrix multiply",
        "question": "What is returned if both arguments are at least 1-dimensional and at least one argument is N-dimensional?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1",
        "question": "If the first argument is 2-dimensional and the second argument is 1-dimensional, what dimension is prepended to its dimension for the purpose of the batched matrix",
        "context": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If the second argument is what, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "non-matrix",
        "question": "What type of dimensions are broadcasted?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "batch",
        "question": "What is another term for non-matrix dimensions?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1",
        "question": "What dimension is the first argument in a batched matrix multiply?",
        "context": "If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If the second argument is at least what dimension, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after",
        "context": "If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "U, S, V",
        "question": "Return the singular value decomposition of a matrix, batches of matrices, or a sparse matrix AAA such that A",
        "context": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "SVD",
        "question": "What is computed for the matrix AMA - MAM?",
        "context": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "huge sparse matrices",
        "question": "What is low-rank SVD useful for?",
        "context": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "a nonnegative integer",
        "question": "What must niter be?",
        "context": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "NNN N-dimensional grids",
        "question": "What do NNN tensors create?",
        "context": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "multi-dimensional matrix containing elements of a single data type",
        "question": "What is a torch.Tensor?",
        "context": "A torch.Tensor is a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "LongTensor Boolean",
        "question": "What type of tensor does Torch define?",
        "context": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dtype",
        "question": "What is the data type of CPU tensor GPU tensor 32-bit floating point torch?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dtype CPU tensor GPU tensor",
        "question": "What is the name of the 32-bit floating point torch?",
        "context": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit floating point torch",
        "question": "What is the name of the floating point torch?",
        "context": "32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "float32",
        "question": "What is another name for torch?",
        "context": "torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "double torch",
        "question": "What is another name for a doubletensor torch?",
        "context": "torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "4-bit integer",
        "question": "What is the sign of an IntTensor 6?",
        "context": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point 2 torch",
        "question": "What is a bfloat16 torch?",
        "context": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "/ quantized 4-bit integer",
        "question": "What is the unsigned name of the IntTensor torch?",
        "context": "torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "1 sign, 5 exponent, and 10 significand bits",
        "question": "What does binary16 use?",
        "context": "128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "when precision is important at the expense of range",
        "question": "When is binary16 useful?",
        "context": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Brain Floating Point",
        "question": "What is binary16 sometimes referred to as?",
        "context": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "range is important",
        "question": "When is Brain Floating Point useful?",
        "context": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What is the sign for torch.int8 torch?",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is the quantized 4-bit integer stored as?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "EmbeddingBag operator",
        "question": "What is Brain Floating Point only supported in?",
        "context": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is float32 quantized 4-bit integer stored as?",
        "context": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.Tensor",
        "question": "What is an alias for the default tensor type?",
        "context": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "requires_grad=True",
        "question": "What can be used to create a tensor?",
        "context": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "A tensor of specific data type",
        "question": "What can be constructed by passing a torch.dtype and/or a torch.device to a constructor or tensor",
        "context": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "indexing and slicing notation",
        "question": "What are the contents of a tensor accessed and modified using?",
        "context": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "holds its data",
        "question": "What does torch.Storage do?",
        "context": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.dtype, torch.device, and torch.layout attributes",
        "question": "What are the three attributes of a torch.Tensor?",
        "context": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "indexing",
        "question": "What is Indexing, Slicing, Joining, Mutating Ops?",
        "context": "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "associated torch.Storage",
        "question": "What holds each tensor's data?",
        "context": "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "multi-dimensional, strided view",
        "question": "What does the tensor class provide of a storage?",
        "context": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.dtype, torch.device, and torch.layout attributes",
        "question": "What are some attributes of a torch.Tensor?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What does to() method on a tensor do?",
        "context": "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "requires_grad=True",
        "question": "What can a tensor be created with?",
        "context": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.Storage",
        "question": "What holds the data of a tensor?",
        "context": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What does the to() method on a tensor do?",
        "context": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "multi-dimensional",
        "question": "What kind of view does the tensor class provide?",
        "context": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.new_tensor",
        "question": "What Returns a new Tensor with data as the tensor data?",
        "context": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values",
        "question": "Tensor.imag Returns a new tensor containing what values of the self tensor?",
        "context": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs()",
        "question": "What Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs()?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "fill_value",
        "question": "Tensor.new_full Returns a Tensor of size size filled with what?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What is returned a Tensor of size size filled with?",
        "context": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs() Tensor.absolute",
        "question": "In-place version of what Alias for abs() Tensor.absolute?",
        "context": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.device",
        "question": "What is the name of the device where the Tensor is stored?",
        "context": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a scalar or tensor",
        "question": "What can be added to a self tensor?",
        "context": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is the name of the tensor that is stored on the GPU?",
        "context": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "scalar or tensor",
        "question": "Add what to self tensor?",
        "context": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "add",
        "question": "What Add a scalar or tensor to self tensor?",
        "context": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.device",
        "question": "Where is the Tensor.device located?",
        "context": "Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of the self tensor",
        "question": "What does the Alias for dim() Tensor.ndim return?",
        "context": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "abs() Tensor",
        "question": "What does abs() Tensor.absolute Alias for abs() Tensor.absolute Alias for ab",
        "context": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values",
        "question": "What does Tensor.imag return?",
        "context": "Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "PyTorch",
        "question": "What provides torch.Tensor to represent a multi-dimensional array containing elements of a single data type?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "array elements",
        "question": "What is stored contiguously in memory?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse arrays",
        "question": "What class of multi-dimensional arrays have a property of having a vast portion of elements being equal to zero?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Sparse arrays",
        "question": "What has a property of having a vast portion of elements being equal to zero?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO, CSR/CSC, LIL",
        "question": "What are some sparse storage formats?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "specific operations on the arrays",
        "question": "What are sparse storage formats optimized for?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What is a sparse storage format?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "specified elements",
        "question": "What do we use when talking about storing only non-zero elements of a sparse array?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "fill value",
        "question": "What term is used to denote the unspecified elements of a sparse array?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse storage",
        "question": "What format can be advantageous only when the size and sparsity levels of arrays are high?",
        "context": "Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Coordinate format",
        "question": "What is the name of the storage format for sparse tensors implemented by PyTorch?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "ndim",
        "question": "What is the indices of specified elements called?",
        "context": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse",
        "question": "What is the number of specified elements in a sparse COO tensor?",
        "context": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensor",
        "question": "What is at least product(tensor shape>) * size of element type in bytes>?",
        "context": "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "200 fold",
        "question": "How much memory saving does a sparse COO tensor have from using the COO storage format?",
        "context": "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "2 000 000 bytes",
        "question": "What is the memory consumption of a 10 000 x 10 000 tensor with non-zero 32-bit floating point numbers?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "What is the default value of the fill value in a sparse tensor?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the input i",
        "question": "What is NOT a list of index tuples?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "transpose",
        "question": "If you want to write your indices this way, you should do what before passing them to the sparse constructor?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "i",
        "question": "What is the input i of a sparse COO tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "i",
        "question": "What would we use to define a sparse tensor?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "index tuples",
        "question": "What is the input i NOT a list of?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch",
        "question": "What extends the sparse COO tensor by allowing the values tensor to be a multi-dimensional ten",
        "context": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "allowing the values tensor to be a multi-dimensional tensor",
        "question": "How does PyTorch hybrid COO tensor extend the sparse COO tensor?",
        "context": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "multi-dimensional tensor",
        "question": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing the values tens",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where is the entry [5, 6] in a dimensional tensor?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "write",
        "question": "What would we do to create a 2 + 1-dimensional tensor?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where is the entry [5, 6] in the tensor?",
        "context": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "s.sparse_dim(), K = s.dense_dim()",
        "question": "What are the invariants of a sparse COO tensor?",
        "context": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "What is the location of the entry [5, 6] in a 2 + 1-dimensional tensor?",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "arbitrary integer or floating point number element type",
        "question": "What are the corresponding tensor values collected with?",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where do we want to create a 2 + 1-dimensional tensor?",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "K",
        "question": "If s is a sparse COO tensor and M = s.sparse_dim(), what",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "What are the values of a hybrid sparse tensor stored as?",
        "context": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "What are the values of a hybrid tensor stored as?",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced sparse tensors",
        "question": "What does PyTorch sparse COO tensor format permit?",
        "context": "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "duplicate coordinates",
        "question": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors where there may",
        "context": "s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the coalescing process",
        "question": "What will accumulate the multi-valued elements into a single value using summation?",
        "context": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "unique",
        "question": "What are the indices of specified tensor elements?",
        "context": "s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced sparse tensors",
        "question": "PyTorch sparse COO tensor format permits what?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1",
        "question": "How many values can be specified for the same index?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "3 and 4",
        "question": "What are two values that lead to a 1-D uncoalesced tensor?",
        "context": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "3 and 4",
        "question": "How many values can be specified for the same index 1?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order",
        "question": "What are the properties of a sparse tensor?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "to prevent them from growing too large",
        "question": "Why should you coalesce sparse tensors?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensor",
        "question": "What tensor is coalesced?",
        "context": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical",
        "question": "In what order are indices sorted?",
        "context": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensor",
        "question": "What is coalesced?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "by simply concatenating the indices and values tensors",
        "question": "How are sparse COO tensors implemented?",
        "context": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.Tensor",
        "question": "What is a sparse COO tensor a instance of?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesce",
        "question": "What should you do to prevent sparse tensors from growing too large?",
        "context": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What type of COO tensor is a torch.Tensor instance?",
        "context": "For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO format data",
        "question": "What can be acquired using methods torch.Tensor.indices() and torch.Tensor.values()?",
        "context": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical ordering",
        "question": "What can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products?",
        "context": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What dimension can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.d",
        "context": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "multiplying all the uncoalesced values with the scalar",
        "question": "How could the scalar multiplication on an uncoalesced sparse tensor be implemented?",
        "context": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sqrt(a + b) == sqrt(a) + sqrt(b)",
        "question": "What does not hold in general?",
        "context": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "terms of a sum",
        "question": "The values of the same indices are what?",
        "context": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the additive nature of uncoalesced data",
        "question": "What must one take into account when working with uncoalesced sparse COO tensors?",
        "context": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dense dimensions",
        "question": "Slicing (with positive step) of a sparse COO tensor is supported only for what?",
        "context": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Indexing",
        "question": "What is supported for both sparse and dense dimensions?",
        "context": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the terms of a sum",
        "question": "What are the values of the same indices?",
        "context": "but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "additive nature of uncoalesced data",
        "question": "When working with uncoalesced sparse COO tensors, one must take into account what?",
        "context": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CSR",
        "question": "What format implements the CSR format for storage of 2 dimensional tensors?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor",
        "question": "The values tensor contains the values of the CSR tensor. This is a what?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor",
        "question": "What is the CSR tensor of size nnz?",
        "context": "The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.int64",
        "question": "What is the default element type for index tensors crow_indices and col_indices?",
        "context": "The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Sparse CSR matrices",
        "question": "What can be directly constructed by using the torch._sparse_csr_tensor() method?",
        "context": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zeros",
        "question": "What will be interpreted as missing values in the sparse tensor?",
        "context": "The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CSR tensors",
        "question": "The sparse matrix-vector multiplication is currently the only math operation supported on what?",
        "context": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch operation Sparse grad",
        "question": "What is a PyTorch operation Sparse grad?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "addmm()",
        "question": "What function is no longer used for Linear Algebra operations on sparse matrices?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "SVD",
        "question": "What does svd_lowrank() yes indicate if the PyTorch operation supports backward with respect to sparse matrix argument",
        "context": "torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "All PyTorch operations",
        "question": "What does the PyTorch operation support backward with respect to sparse matrix argument?",
        "context": "no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch",
        "question": "What is the only PyTorch operation that supports backward with respect to sparse matrix argument?",
        "context": "M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided matrix arguments",
        "question": "All PyTorch operations, except torch.smm(), support backward with respect to what?",
        "context": "torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[strided] @ M[sparse_coo]",
        "question": "PyTorch does not support matrix multiplication with what layout signature?",
        "context": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix relation",
        "question": "PyTorch applications can still compute this using what?",
        "context": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a strided tensor self",
        "question": "Where are the values of a sparse tensor filtered by the indices of the sparse tensor mask",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize",
        "question": "What _ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a new sparse tensor",
        "question": "What does Tensor.sparse_mask return?",
        "context": "Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What is the name of the method that resizes a sparse tensor?",
        "context": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What is the name of the number of sparse dimensions in a sparse tensor self?",
        "context": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a new sparse tensor",
        "question": "What does the Tensor.sparse_mask return?",
        "context": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensor",
        "question": "Return the number of dense dimensions in a what self?",
        "context": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse copy of the tensor",
        "question": "What does Tensor.to_sparse Return?",
        "context": "Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_mask",
        "question": "What Returns a new sparse tensor with values from a strided tensor self filtered by the ",
        "context": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced",
        "question": "If self is a sparse COO tensor that is coalesced, return a coalesced copy of self",
        "context": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a new sparse tensor",
        "question": "Returns what with values from a strided tensor self filtered by the indices of the sparse tens",
        "context": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.to_sparse",
        "question": "What Returns a sparse copy of the tensor?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced",
        "question": "If self is a sparse COO tensor, what type of tensor is returned?",
        "context": "Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.col_indices",
        "question": "What returns the indices of the self tensor when self is a sparse CSR tensor of layout spars",
        "context": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "compressed row storage format",
        "question": "Convert a tensor to what format?",
        "context": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices",
        "question": "Return the tensor of a sparse COO tensor. Tensor.values Return the values tens",
        "context": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced",
        "question": "If self is a sparse COO tensor, return a coalesced copy of self if self is what?",
        "context": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.crow_indices",
        "question": "What Returns the tensor containing the compressed row indices of the self tensor when self is a sparse",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor",
        "question": "Return the values of what of a sparse COO tensor?",
        "context": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "The following methods are specific to what CSR tensors?",
        "context": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Removes all specified elements",
        "question": "What does a sparse tensor self do?",
        "context": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.col_indices",
        "question": "What Returns the tensor containing the column indices of the self tensor when self is a sparse C",
        "context": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse matrix input with the dense matrix mat",
        "question": "Performs a matrix multiplication of what?",
        "context": "  Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "defaults to the dtype of input",
        "question": "Default: if None, what does torch.full_like(input.size(), fill_value, input.dtype",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "None",
        "question": "If None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned ten",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "memory_format",
        "question": "What is the desired memory format of returned tensor?",
        "context": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "return_complex=True",
        "question": "What is the name of the short-time Fourier transform function that will only return complex tensors?",
        "context": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "The STFT computes the Fourier transform",
        "question": "What does STFT compute of short overlapping windows of the input?",
        "context": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "librosa stft function",
        "question": "The interface of the STFT computes the Fourier transform of short overlapping windows of the input is modeled after what?",
        "context": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "What must the input be?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D tensor of size win_length",
        "question": "What can a window be?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "111",
        "question": "If window is None (default), it is treated as if having what everywhere in the window?",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "If win_length is None, window will be padded on both sides to length n_fft before being applied.",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "If win_length is None, window will be padded on both sides to length n_fft before being applied?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "True",
        "question": "If center is what, input will be padded on both sides so that the ttt-th frame is centered?",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "reflect",
        "question": "What is the default value for input when center is True?",
        "context": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "center",
        "question": "If True, input will be padded on both sides so that the ttt-th frame is centered at time thop_",
        "context": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "reflect",
        "question": "What is the default value for the padding method used on input when center is True?",
        "context": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "center is True",
        "question": "If what is the default, input will be padded on both sides so that the ttt-th frame is centered?",
        "context": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "reflect",
        "question": "What is the default setting for a window when center is True?",
        "context": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "If win_length  textn_fftwin_lengthn_fft, window will be",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "torch.nn.functional.pad()",
        "question": "What is the name of all available options?",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "\"reflect\"",
        "question": "What is the default setting for the padding method used on input when center is True?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "If normalized is True",
        "question": "If normalized is True, what is the default?",
        "context": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "If normalized is True",
        "question": "When does the function return the normalized STFT results?",
        "context": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "a complex tensor of size",
        "question": "Returns what if return_complex is true?",
        "context": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "NNN",
        "question": "What is the number of frequencies where STFT is applied?",
        "context": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "previous signature",
        "question": "Calling with what may cause error or return incorrect result?",
        "context": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Fourier transform",
        "question": "The input tensor n_fft (int) \u2013 the input tensor n_fft (",
        "context": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "a real tensor of size",
        "question": "Returns either a complex tensor of size (NT)(* times N times T ",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "None",
        "question": "What is the default value for the window function?",
        "context": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "None",
        "question": "What is the default value for window of all 111 s?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ttt-th",
        "question": "What frame is centered at time thop_lengtht times texthop_lengththop_",
        "context": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "False",
        "question": "What is the default value for return normalized STFT results?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "False onesided",
        "question": "What is the default to return half of results to avoid redundancy for real inputs?",
        "context": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "window",
        "question": "What is the optional window function?",
        "context": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "\"reflect\" normalized",
        "question": "What is the default for STFT results?",
        "context": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex",
        "question": "What is the name of the tensor that returns a complex tensor?",
        "context": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Count the frequency of each value in an array of non-negative ints",
        "question": "What is one way to count the frequency of each value in an array of non-negative ints?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "a tensor of size 0.",
        "question": "What is the result if the number of bins is one larger than the largest value in input unless input is empty?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "minlength",
        "question": "If what is specified, the number of bins is at least minlength and if input is empty, the result is a tensor",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "1-d int tensor weights",
        "question": "Input (Tensor) \u2013 optional, weight for each value in the input tensor. Should be of same size as input",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "same size",
        "question": "What should the weight for each value in the input tensor be of?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "optional, minimum number of bins",
        "question": "Minlength (int) \u2013 what?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "non-negative",
        "question": "Minlength (int) \u2013 optional, minimum number of bins. Should be what?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "a tensor",
        "question": "What does torch.tensor() construct with data?",
        "context": "Constructs a tensor with data. Warning torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "NumPy ndarray",
        "question": "What type of data does torch.as_tensor() avoid a copy of?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "a leaf variable",
        "question": "What does torch.tensor() construct when data is a tensor x?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "torch.Tensor.requires_grad_() or torch.Tensor.detach()",
        "question": "What do you use if you have a Tensor data and want to avoid a copy?",
        "context": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "The upper triangular part",
        "question": "What is defined as the elements on and above the diagonal?",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "offset",
        "question": "What controls which diagonal to consider?",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "offset = 0,",
        "question": "What value controls which diagonal to consider?",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "A positive value",
        "question": "What excludes just as many diagonals above the main diagonal?",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "main diagonal",
        "question": "What is the set of indices (i,i)lbrace (i, i) rbrace",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "row (int) \u2013 number of rows in the 2-D matrix",
        "question": "What does row (int) mean?",
        "context": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "zeros",
        "question": "Returns a 2-D tensor with ones on the diagonal and what else?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "n out",
        "question": "What is the default number of columns in a 2-D tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "Default",
        "question": "What is the global default of a 2-D tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default setting of a 2-D tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "False",
        "question": "What is the default value for autograd?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "zeros",
        "question": "A 2-D tensor with ones on the diagonal and what else?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "window length",
        "question": "Computes the Kaiser window with what?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.kaiser_window(L, B, periodic=True)",
        "question": "What is equivalent to calling torch.kaiser_window(L, B, periodic=True)[:-1])?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "periodic",
        "question": "What is the name of the window suitable for use in spectral analysis?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "symmetric",
        "question": "What type of window is returned if False?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "global default",
        "question": "Default: if None, uses what?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.kaiser_window(L, B, periodic=True)",
        "question": "What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1])?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Default",
        "question": "What is the global default of the returned tensor?",
        "context": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default layout of a returned window tensor?",
        "context": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "current CUDA device",
        "question": "What device will be the CPU for CPU tensor types?",
        "context": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "False",
        "question": "If autograd should record operations on the returned tensor, what is the default?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "eigenvalues",
        "question": "Computes the eigenvalue decomposition of a square matrix if it exists. Computes the eigenvalue decom",
        "context": "Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues of a square matrix",
        "question": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "singular values of a matrix",
        "question": "Computes the singular value decomposition (SVD) of a matrix. Computes the solution of a square system of linear equation",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition",
        "question": "Computes the inverse of a square matrix if it exists. Computes the eigenvalues of a square matrix.",
        "context": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix",
        "question": "What does Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix?",
        "context": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes the singular values of a matrix",
        "question": "What is the singular value decomposition of a matrix?",
        "context": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "n-th power",
        "question": "Computes the what of a square matrix for an integer n?",
        "context": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "Computes the eigenvalue decomposition of a square matrix if it exists. Computes the pseudoinverse (Moore-",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "torch.tensordot()",
        "question": "Computes the multiplicative inverse of what?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "X",
        "question": "Computes the solution to the system torch.tensordot(A, X) = B.",
        "context": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix",
        "question": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix?",
        "context": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "torch.linalg.solve()",
        "question": "What function replaces torch.solve()?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "torch.lu()",
        "question": "What function returns the LU factorization of the input?",
        "context": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What is the name of a device. Gets the properties of a device?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the properties of a device",
        "question": "What does Gets the properties of a device?",
        "context": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the number of GPUs available",
        "question": "What does Context-manager that changes the selected device return?",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "Returns a bool indicating if CUDA is currently available.",
        "context": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "peer access between two devices",
        "question": "Checks if what is possible. Returns cublasHandle_t pointer to current cuBLAS handle. Returns the index",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "name",
        "question": "Gets the cuda capability of a device. Gets the properties of a device.",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a bool",
        "question": "What indicating if CUDA is currently available?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "Returns a bool indicating if PyTorch\u2019s CUDA state has been initialized.",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current stream",
        "question": "Sets the current device. This is a wrapper API to set the stream.",
        "context": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "set the stream",
        "question": "Sets the current device. This is a wrapper API to what?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "StreamContext",
        "question": "What is the wrapper around that selects a given stream?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What does Returns a bool indicating if PyTorch\u2019s CUDA state has been initialized?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What is the cuda capability of a device?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "What happens when a CUDA device is selected?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "ByteTensor",
        "question": "What is the random number generator state of the specified GPU?",
        "context": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the number of GPUs available",
        "question": "What does Return?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What is returned when a bool indicating if CUDA is currently available?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does Set the random number generator state of the specified GPU do?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What does Returns a bool indicating if CUDA is currently available?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "What happens when all kernels in all streams on a CUDA device complete?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does Sets the random number generator state of the specified GPU do?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "random number generator state",
        "question": "Sets what of all devices?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What does the bool indicating if CUDA is currently available return?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of all devices",
        "question": "What does Sets the seed for generating random numbers for the current GPU do?",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers for the current GPU",
        "question": "What does Sets the seed for generating random numbers for the current GPU?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers on all GPUs",
        "question": "What does Sets the seed for generating random numbers on all GPUs?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does it do when a ByteTensor returns a list of ByteTensors representing the random number",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "generating random numbers",
        "question": "Sets the seed for what to a random number on all GPUs?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current random seed of the current GPU",
        "question": "What does Sets the seed for generating random numbers on all GPUs return?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "tensors",
        "question": "Returns the current GPU memory occupied by what in bytes for a given device?",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU memory managed by the caching allocator in bytes",
        "question": "Returns the maximum GPU memory managed by the caching allocator for a given device.",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction",
        "question": "What does the caching allocator do for a process?",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors",
        "question": "Returns the starting point in tracking maximum GPU memory occupied by tensors for a given device.",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU memory managed by the caching allocator in bytes",
        "question": "Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.",
        "context": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors",
        "question": "Returns the starting point in tracking maximum GPU memory occupied by tensors in bytes for a given device.",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the maximum GPU memory managed by the caching allocator in bytes for a given device",
        "question": "What does Returns the current GPU memory managed by the caching allocator in bytes for a given device?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction for a process",
        "question": "What does memory_reserved() do?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "\u201cpeak\u201d stats",
        "question": "Resets what stats tracked by the CUDA memory allocator?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "documentation",
        "question": "What is the name of each function that may change in future PyTorch releases?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the complementary error function of input",
        "question": "What does Computes the error function of input do?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the exponential of the elements minus 1 of input",
        "question": "What does Computes the exponential of the elements minus 1 of input?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the complementary error function of input",
        "question": "What is an example of a complementary error function?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor) \u2013 the input tensor",
        "question": "What is the complementary error function defined as?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the exponential of the elements minus 1 of input",
        "question": "What is the function that provides greater precision than exp(x) - 1 for small values of x?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "greater precision",
        "question": "What does Computes the exponential of the elements minus 1 of input provide?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the exponential of the elements minus 1 of input",
        "question": "What is an example of a function that provides greater precision than exp(x) - 1 for small values of x?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the first kind",
        "question": "What is the exponentially scaled zeroth order modified Bessel function?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "scalar",
        "question": "What value does addcdiv multiply the result of the element-wise division of tensor1 by tensor2?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "Warning Integer division",
        "question": "What is no longer supported with addcdiv?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "float inputs",
        "question": "For what type of inputs can addcdiv be implemented as (input + value * tensor1 / tensor2)",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "future addcdiv behavior",
        "question": "What is just the latter implementation of addcdiv?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "input",
        "question": "What is the tensor to be added tensor1 (Tensor)?",
        "context": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "APIs",
        "question": "What does the torch._C.Future expose to add callback functions and set results?",
        "context": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "asynchronous execution of a callable",
        "question": "What does the torch._C.Future encapsulate?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "APIs",
        "question": "What does the torch._C.Future expose?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Append the given callback function to this Future",
        "question": "What will be run when the Future is completed?",
        "context": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "callback registration",
        "question": "What API does add_done_callback use?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future",
        "question": "Append the given callback function to what?",
        "context": "Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "beta",
        "question": "What version of GPU support is subject to changes?",
        "context": "GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Append",
        "question": "What is the name of the callback function that will be run when the Future is completed?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inline",
        "question": "If this Future is already completed, the given callback will be run what?",
        "context": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future",
        "question": "What is another name for callback?",
        "context": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completion/waiting",
        "question": "What is the user responsible for handling if a callback later completes additional futures?",
        "context": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Return True",
        "question": "What happens if a Future is done?",
        "context": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Return True",
        "question": "What happens if this Future is done?",
        "context": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future.done()",
        "question": "If the value contains tensors that reside on GPUs, what will return True even if the asynchronous kernels that are popul",
        "context": "is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a Future cannot be marked completed twice",
        "question": "What does a Future not do?",
        "context": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "synchronizations",
        "question": "If the result is already usable, what can be performed?",
        "context": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "mark this Future as completed and trigger all attached callbacks",
        "question": "What happens when the result is set for this Future?",
        "context": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If the result contains what that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those ",
        "context": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "change streams in between",
        "question": "What is safe to call this method immediately after launching kernels?",
        "context": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "all the relevant current streams",
        "question": "This method will record events on what?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "mark this Future as completed and trigger all attached callbacks",
        "question": "Set the result for this Future, which will do what?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a Future cannot be marked completed twice",
        "question": "Is a Future able to be completed twice?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "current ones",
        "question": "What are the streams on which asynchronous kernels were enqueued set as when this method is called?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "immediately after launching those kernels",
        "question": "When is it safe to call this method?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "all the relevant current streams",
        "question": "What will this method record events on?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "chaining",
        "question": "What is a way to enforce a certain order?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inline",
        "question": "If the Future is already completed, the given callback will be run immediately what?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "What does the Future's value contain that reside on GPUs?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "current",
        "question": "What are the dedicated streams set as?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "after the kernels complete",
        "question": "When will any operation performed by the callback on tensors be scheduled on the device?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "switch streams",
        "question": "What does the callback not do?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "What non-blocking behavior is similar to the non-blocking behavior of?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "A new Future object",
        "question": "What holds the return value of the callback?",
        "context": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Obtain the value of an already-completed future",
        "question": "What does a callback do?",
        "context": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "after a call to wait() has completed, or inside a callback function passed to then()",
        "question": "When should Obtain the value of an already-completed future be called?",
        "context": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future may not yet hold a value",
        "question": "What could cause calling value() to fail?",
        "context": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Obtain the value of an already-completed future",
        "question": "What is a method that should only be called after a call to wait() has completed?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future may not yet hold a value",
        "question": "What could cause value() to fail?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Obtain the value of an already-completed future",
        "question": "What does this method do to obtain the value of an already-completed future?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "after a call to wait() has completed",
        "question": "When should the Obtain the value of an already-completed future be called?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future may not yet hold a value",
        "question": "What could happen if the method is called after a call to wait() has completed or inside a callback function passed to then()?",
        "context": "Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future may not yet hold a value",
        "question": "What could happen if a call to value() fails?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If the value contains what that reside on GPUs, this method will not perform any additional synchronization?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Block until the value of this Future is ready",
        "question": "What does the value() method do?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "until the value of this Future is ready",
        "question": "How long should the value() method be blocked?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future may not yet hold a value",
        "question": "What could cause the call to value() to fail?",
        "context": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "until the value of this Future is ready",
        "question": "How long should the value() method be called?",
        "context": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this wait method",
        "question": "What method will throw an error if the function (callback or RPC) creating the value has thrown an error?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "until the value of this Future is ready",
        "question": "When does Block occur?",
        "context": "Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If the value contains what that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "What will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the as",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "No further synchronization",
        "question": "What is required when accessing and using the values, as long as one doesn't change streams?",
        "context": "Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this wait method",
        "question": "If the function (callback or RPC) creating the value has thrown an error, what method will also throw an error?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "synchronization",
        "question": "What is performed if the value contains tensors that reside on GPUs?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "No further synchronization",
        "question": "What is required when accessing and using the values, as long as one doesn\u2019t change streams?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Collects the provided Future objects into a single combined Future",
        "question": "What happens when all of the sub-futures are completed?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "No further synchronization is required",
        "question": "What is required when accessing and using the values?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "The value held by this Future",
        "question": "What does the wait method throw an error on?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Waits for all provided futures to be complete",
        "question": "What does the wait method do?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "exit early",
        "question": "If any of the futures encounter an error, the method will report the error not waiting for other futures to complete?",
        "context": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "1D convolution",
        "question": "What type of convolution is applied to an input signal composed of several input planes?",
        "context": "Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "large containing tensor",
        "question": "Combines an array of sliding local blocks into what?",
        "context": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D average",
        "question": "Applies what pooling over an input signal composed of several input planes?",
        "context": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D average-pooling",
        "question": "Applies what operation in kTkHkWkT times kWkTkHkW regions by step",
        "context": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D max",
        "question": "Applies a pooling over an input signal composed of several input planes. Applies a pooling over an input signal composed of several input",
        "context": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What is the result of a 2D max pooling over an input signal composed of several input planes?",
        "context": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D adaptive max",
        "question": "What is the name of the pooling over an input signal composed of several input planes?",
        "context": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D",
        "question": "Applies what type of convolution over an input image composed of several input planes?",
        "context": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "deconvolution",
        "question": "What is another name for a 2D transposed convolution operator over an input image composed of several input planes?",
        "context": "Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D average-pooling",
        "question": "Applies what operation in kHkWkH times kWkHkW regions by step size sHs",
        "context": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D convolution",
        "question": "Applies what over an input image composed of several input planes?",
        "context": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D average-pooling",
        "question": "What operation in kTkHkWkT times kWkTkHkW regions by step size ",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What does a 3D max pooling over an input signal consist of?",
        "context": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What is the result of a 3D max pooling over an input signal composed of several input planes?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D adaptive max",
        "question": "What is the name of the pooling over an input signal?",
        "context": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D average-pooling",
        "question": "What operation in kHkWkH times kWkHkW regions by step size sHsWs",
        "context": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What is the result of a partial inverse of MaxPool1d?",
        "context": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D power-average",
        "question": "What type of pooling over an input signal composed of several input planes?",
        "context": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "large containing tensor",
        "question": "What does a 3D transposed convolution operator combine an array of sliding local blocks into?",
        "context": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What does a 2D max pooling over an input signal consist of?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "2D power-average pooling over an input signal composed of what?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "2D fractional max pooling over",
        "question": "What is a 2D fractional max pooling over an input signal composed of several input planes?",
        "context": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "a large containing tensor",
        "question": "What does an array of sliding local blocks combine into?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What is the input signal composed of?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What does a 1D adaptive max pooling over an input signal consist of?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What does a 2D adaptive max pooling over an input signal consist of?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D",
        "question": "What type of fractional max pooling over an input signal composed of several input planes?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Thresholds",
        "question": "What does each element of the input Tensor have?",
        "context": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Thresholds",
        "question": "What is each element of the input Tensor?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "In-place version of threshold()",
        "question": "What is the In-place version of threshold()?",
        "context": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "rectified linear unit function element-wise",
        "question": "What type of linear unit function does threshold() apply?",
        "context": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "In-place",
        "question": "What is the rectified linear unit function element-wise?",
        "context": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool2d",
        "question": "Computes a partial inverse of MaxPool2d. Computes a partial inverse of MaxPool",
        "context": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "rectified linear unit function",
        "question": "What is element-wise?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "HardTanh",
        "question": "What function is element-wise?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "hardswish function",
        "question": "Applies what element-wise function?",
        "context": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "3D",
        "question": "What type of max pooling over an input signal composed of several input planes?",
        "context": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What is the result of the 2D max pooling over an input signal composed of several input planes?",
        "context": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "2D adaptive max pooling over an input signal composed of what?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What does a 3D adaptive max pooling over an input signal consist of?",
        "context": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "What does a 1D adaptive average pooling over an input signal consist of?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "2D adaptive average pooling over an input signal composed of what?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "HardTanh",
        "question": "Which function is element-wise?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "HardTanh",
        "question": "What function element-wise. In-place version of hardtanh(). Applies the hardswish function, element-wise?",
        "context": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "3D adaptive max pooling over an input signal composed of what?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "3D adaptive average pooling over an input signal composed of what?",
        "context": "  Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool1d",
        "question": "What does a 3D pooling over an input signal composed of several input planes do?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Computes a partial inverse of MaxPool3d",
        "question": "What does a partial inverse of MaxPool1d. Computes a partial inverse of MaxPool2",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "1D power-average pooling",
        "question": "What is applied to an input signal composed of several input planes?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "several input planes",
        "question": "Applies 2D fractional max pooling over an input signal composed of what?",
        "context": "  Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "leaky_relu()",
        "question": "In-place version of what?",
        "context": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   Applies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).   Applies a softmin function.   Applies a softmax function.   Applies the soft shrinkage function elementwise   Samples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.   ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Poisson negative log likelihood loss",
        "question": "What is a function that measures the Binary Cross Entropy between target and output logits?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "CosineEmbeddingLoss",
        "question": "What is the name of the function that measures the Poisson negative log likelihood loss?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "SoftMarginLoss",
        "question": "What function uses a squared term if the absolute element-wise error falls below delta and an L1 term otherwise?",
        "context": "  Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details. ",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "answer": "Matrix product of two tensors",
        "question": "What is the result of two tensors?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If both tensors are what dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional,",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix-matrix",
        "question": "If both arguments are 2-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, what product is returned?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix multiply",
        "question": "If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of what?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the prepended dimension is removed",
        "question": "After the matrix multiply, what happens to the prepended dimension?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "batched matrix multiply",
        "question": "If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), what is returned?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If the first argument is what dimension, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If the second argument is what dimension, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "broadcasted",
        "question": "The non-matrix dimensions are what?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the broadcasting logic",
        "question": "What only looks at the batch dimensions when determining if the inputs are broadcastable?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "Torch",
        "question": "What defines 10 tensor types with CPU and GPU variants?",
        "context": "A torch.Tensor is a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Data type dtype CPU tensor GPU tensor",
        "question": "What is the name of the data type dtype CPU tensor GPU tensor?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dtype CPU tensor GPU tensor",
        "question": "What is a dtype CPU tensor or a GPU tensor?",
        "context": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "CPU tensor GPU tensor",
        "question": "What type of tensor is 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.",
        "context": "CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16",
        "question": "How many bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.Hal",
        "context": "16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "precision",
        "question": "Useful when what is important at the expense of range?",
        "context": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Sometimes referred",
        "question": "What is another term for binary16?",
        "context": "torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "64-bit floating point",
        "question": "What is torch.float64 or torch.double torch?",
        "context": "64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Brain Floating Point",
        "question": "What is the term used to describe a device that uses 1 sign, 5 exponent, and 10 significand bits?",
        "context": "64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Brain Floating Point",
        "question": "What is another term for the use of 1 sign, 8 exponent, and 7 significand bits?",
        "context": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Useful",
        "question": "Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when precision is important at the expense of",
        "context": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16",
        "question": "How many bits does a doubletensor have?",
        "context": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "precision",
        "question": "What is important at the expense of range?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "range is important",
        "question": "Useful when precision is important at the expense of range.",
        "context": "torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Brain Floating Point",
        "question": "What is another name for a sign, 8 exponent, and 7 significand bits?",
        "context": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point 2",
        "question": "What is torch.cuda.HalfTensor?",
        "context": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Brain Floating Point",
        "question": "What is the term for a sign, 8 exponent, and 7 significand bits?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is Brain Floating Point stored as?",
        "context": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "EmbeddingBag operator",
        "question": "Currently it\u2019s only supported in what?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point 2",
        "question": "What is torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor",
        "context": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is the same number of exponent bits as float32 quantized 4-bit integer?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.Tensor",
        "question": "What is the name of the torch.Tensor?",
        "context": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.Tensor",
        "question": "What is an alias?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is a Brain Floating Point stored as?",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "EmbeddingBag operator",
        "question": "Currently it\u2019s only supported in what operator?",
        "context": "torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "default tensor type (torch.FloatTensor)",
        "question": "What is torch.Tensor an alias for?",
        "context": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "binary16",
        "question": "What is the term for ByteTensor?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is stored as a quantized 4-bit integer?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "precision is important at the expense of range",
        "question": "When precision is important at the expense of range?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Python list or sequence using the torch",
        "question": "A tensor can be constructed from a what?",
        "context": "64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "A tensor",
        "question": "What can be constructed from a Python list or sequence using the torch.tensor type?",
        "context": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "128-bit complex",
        "question": "What is torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.By",
        "context": "128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "A tensor",
        "question": "What can be constructed from a Python list or sequence using the torch.tensor() constructor?",
        "context": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "binary16",
        "question": "What is the name of the tensor that uses 1 sign, 5 exponent, and 10 significand bits?",
        "context": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What is unsigned?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tens",
        "question": "What type of Tensor?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "binary16",
        "question": "What is another name for ByteTensor?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "requires_grad flag",
        "question": "What does a Tensor data need to change?",
        "context": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "requires_grad_() or detach()",
        "question": "If you want to change the required_grad flag of a Tensor, use what?",
        "context": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit",
        "question": "What type of integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Python list or sequence using the torch",
        "question": "What can a tensor be constructed from?",
        "context": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "requires_grad_() or detach()",
        "question": "What are two ways to avoid a copy of a Tensor data?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "numpy",
        "question": "If you have a Tensor data and just want to change its requires_grad flag, use requires_grad_() or detach()",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "EmbeddingBag operator",
        "question": "In what operator is Brain Floating Point only supported?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "numpy array",
        "question": "If you have a Tensor data and want to avoid a copy, use requires_grad_() or detach() to avoid ",
        "context": "torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit integer",
        "question": "What is the signature of torch.cuda.CharTensor?",
        "context": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit",
        "question": "What type of integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.",
        "context": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "specific data type",
        "question": "What is a tensor of?",
        "context": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.dtype and/or a torch.device",
        "question": "A tensor of specific data type can be constructed by passing what to a constructor?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch",
        "question": "To create a tensor with specific size, use what?",
        "context": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "creation ops",
        "question": "What does torch. * tensor do?",
        "context": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch",
        "question": "To create a tensor with the same size (and similar types) as another tensor, use what?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "creation ops",
        "question": "What does torch. *_like tensor do?",
        "context": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "its dimensions reversed",
        "question": "Is this Tensor with what?",
        "context": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True if the Tensor is stored on the GPU",
        "question": "Is the Tensor.is_cuda true?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.is_quantized",
        "question": "What is True if the Tensor is quantized?",
        "context": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True if the Tensor is a meta tensor",
        "question": "Is Tensor.is_meta True if the Tensor is a meta tensor?",
        "context": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "None",
        "question": "Tensor.grad This attribute is what by default?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of the self tensor",
        "question": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing what?",
        "context": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values of the self tensor",
        "question": "Tensor.imag Returns a new tensor containing what?",
        "context": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.abs",
        "question": "What does torch.abs() Tensor.abs_ In-place version of abs() Tensor.abs_ In-place version of",
        "context": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Creation Ops",
        "question": "What is another name for creation ops?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of the self tensor",
        "question": "What does Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True if the Tensor is stored on the GPU",
        "question": "Is the Tensor.is_cuda True?",
        "context": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.abs",
        "question": "What does torch.abs() Tensor.abs stand for?",
        "context": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "Tensor.new_empty Returns a Tensor of size size filled with what?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True if the Tensor is stored on the GPU",
        "question": "Tensor.is_cuda Is what?",
        "context": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta tensor",
        "question": "Tensor.is_meta Is True if the Tensor is a what?",
        "context": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.device",
        "question": "Where is the Tensor.device?",
        "context": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Add a scalar or tensor to self tensor",
        "question": "What does a scalar or tensor do to self tensor?",
        "context": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is another name for add() Tensor?",
        "context": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What does new_empty return a Tensor of size size filled with?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.abs",
        "question": "What is the name of the tensor that returns a new Tensor?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What does addbmm stand for?",
        "context": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values",
        "question": "Tensor.imag Returns a new tensor containing what of the self tensor?",
        "context": "Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is the name of the feature that adds a scalar or tensor to self tensor?",
        "context": "Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is another name for addbmm() Tensor?",
        "context": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What type of data does a Tensor.new_empty return a Tensor of size size filled with?",
        "context": "Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.abs",
        "question": "What is another name for Tensor.abs?",
        "context": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta",
        "question": "Is True if the Tensor is a meta tensor, False otherwise?",
        "context": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of the self tensor",
        "question": "Tensor.ndim Alias for dim() Tensor.ndim Returns a new tensor containing what?",
        "context": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "True if the Tensor is a meta tensor",
        "question": "Tensor.is_meta Is what?",
        "context": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of the self tensor",
        "question": "What does Tensor.ndim Alias for dim() Tensor.ndim Returns a new tensor containing",
        "context": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of the self tensor",
        "question": "What does the tensor.ndim Alias for dim() Tensor.real Returns a new tensor ",
        "context": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.abs",
        "question": "What attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self?",
        "context": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is the name of the attribute that becomes a Tensor the first time a call to backward() computes gradients for self?",
        "context": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.imag",
        "question": "Returns a new tensor containing imaginary values of the self tensor. What Returns a new tensor",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a scalar or tensor to self tensor",
        "question": "What is added to the tensor?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.addbmm",
        "question": "What is the In-place version of add()?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.argmax",
        "question": "What is another name for Tensor.argmax?",
        "context": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a scalar or tensor to self tensor",
        "question": "What is added to the self tensor?",
        "context": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.addbmm",
        "question": "What is the name of the tensor that returns a scalar or tensor to self tensor?",
        "context": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor",
        "question": "Which tensor returns a new tensor containing imaginary values of the self tensor?",
        "context": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "add() Tensor.addbmm",
        "question": "What is the Tensor.add_ In-place version of?",
        "context": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "add() Tensor.addbmm",
        "question": "What is a Tensor.add_ In-place version of?",
        "context": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.abs",
        "question": "What _ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolut",
        "context": "Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a scalar or tensor to self tensor",
        "question": "What does add add?",
        "context": "In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a scalar or tensor to self tensor",
        "question": "What is added to the Tensor.absolute Alias for abs() Tensor.absolute Alias for ab",
        "context": "Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "add() Tensor.addbmm",
        "question": "Tensor.add_ In-place version of what add() Tensor.addbmm?",
        "context": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.absolute",
        "question": "What is the In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos",
        "context": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "non-zero",
        "question": "What type of elements can be stored in a sparse array?",
        "context": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "memory",
        "question": "What is the consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "200 fold",
        "question": "How much memory saving does a 10 000 x 10 000 tensor with 100 000 non-zero floating point numbers have?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.sparse_coo_tensor()",
        "question": "What is the function that provides the size of a sparse COO tensor?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1, 0",
        "question": "What is the value of entry 4 in a sparse tensor?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero by default",
        "question": "What is the fill value of a sparse tensor?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1, 0",
        "question": "What is the value of entry 4 in the sparse tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M and K",
        "question": "What are the numbers of sparse and dense dimensions, respectively?",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(2 + 1)-dimensional",
        "question": "What type of tensor would we want to create?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1, 0",
        "question": "Suppose we want to define a sparse tensor with the entry 4 at location?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(2 + 1)",
        "question": "Suppose we want to create a what -dimensional tensor with the entry [3, 4] at location (0, 2), entry [",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "s.sparse_dim()",
        "question": "What does M stand for?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "What is the location of the entry [5, 6] in a 2 + 1)-dimensional tensor?",
        "context": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "empty",
        "question": "What type of sparse COO tensor can be constructed by specifying its size only?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where is the entry [5, 6] in a 2 + 1)-dimensional tensor?",
        "context": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "hybrid tensors",
        "question": "Pytorch implements an extension of sparse tensors with scalar values to sparse tensor",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(2 + 1)",
        "question": "What is a dimensional tensor?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.int64",
        "question": "What is the element type of the indices of specified elements?",
        "context": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch",
        "question": "What is a sparse COO tensor?",
        "context": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "arbitrary integer or floating point number element type",
        "question": "What are the corresponding (tensor) values collected in?",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch",
        "question": "What sparse COO tensor format permits uncoalesced sparse tensors?",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "multiple values",
        "question": "What leads to an 1-D uncoalesced tensor?",
        "context": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(2 + 1)",
        "question": "What is the name of the tensor we want to create?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Dense dimensions",
        "question": "What always follow sparse dimensions?",
        "context": "Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch sparse COO tensor",
        "question": "What format permits uncoalesced sparse tensors?",
        "context": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "multiple values",
        "question": "What can lead to an 1-D uncoalesced tensor?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "What is the location of a 2 + 1-dimensional tensor?",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensor",
        "question": "What is coalesced or not?",
        "context": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "Values are stored as what?",
        "context": "s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D",
        "question": "What type of uncoalesced tensor is created when multiple values are specified for the same index 1?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical ordering of indices",
        "question": "What is the lexicographical ordering of indices?",
        "context": "s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical",
        "question": "What type of ordering of indices can be advantageous for implementing algorithms that involve many element selection operations?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or uncoalesced sparse tensor",
        "question": "Most operations will work identically given a what?",
        "context": "torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "slicing or matrix products",
        "question": "The lexicographical ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as what?",
        "context": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.Tensor",
        "question": "What is a sparse COO tensor a part of?",
        "context": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.Tensor",
        "question": "What instance of a sparse COO tensor is a sparse COO tensor?",
        "context": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO format data",
        "question": "What type of data can a sparse COO tensor acquire?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse tensor",
        "question": "What is the output of torch.Tensor.coalesce() method?",
        "context": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO format data",
        "question": "What can one acquire when the tensor instance is coalesced?",
        "context": "torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices",
        "question": "What of specified tensor elements are unique?",
        "context": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.Tensor",
        "question": "What instance does a sparse COO tensor belong to?",
        "context": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO",
        "question": "What format data can one acquire only when the tensor instance is coalesced?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical order",
        "question": "The indices are sorted in what order?",
        "context": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "to prevent them from growing too large",
        "question": "Why should sparse tensors be coalesced?",
        "context": "torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "most operations will work identically",
        "question": "How will most operations work with a coalesced or uncoalesced sparse tensor?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "not strict",
        "question": "What is the usage of adjective \"non-zero\"?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "specified elements",
        "question": "What does PyTorch use for those array elements that are actually stored?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "when the size and sparsity levels of arrays are high",
        "question": "When can using a sparse storage format for storing sparse arrays be advantageous?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "beta",
        "question": "What beta is the PyTorch API of sparse tensors in?",
        "context": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO format data",
        "question": "What can one acquire only when the tensor instance is coalesced?",
        "context": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "scalar multiplication",
        "question": "What could be implemented by multiplying all the uncoalesced values with the scalar?",
        "context": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nonlinear operation",
        "question": "What type of operation cannot be implemented by a square root?",
        "context": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CUDA",
        "question": "What type of support does not exist as of now?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "three",
        "question": "How many 1-D tensors does a CSR sparse tensor consist of?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Each successive number in the tensor",
        "question": "What is subtracted by the number before it denotes the number of elements in a given row?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch._sparse_csr_tensor() method",
        "question": "How can Sparse CSR matrices be directly constructed?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "separately",
        "question": "The user must supply the row and column indices and values tensors separately or separately?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "simplest way of constructing a sparse CSR",
        "question": "What is the simplest way of constructing a sparse CSR matrices?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.is_sparse",
        "question": "What is True if the Tensor uses sparse storage layout?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "Tensor.dense_dim Return the number of sparse dimensions in a sparse tensor self. Tens",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a new sparse tensor with values from a strided tensor self",
        "question": "What is returned by the indices of the sparse tensor mask?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.indices",
        "question": "What Return the indices tensor of a sparse COO tensor?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.values",
        "question": "What Return the values tensor of a sparse COO tensor?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "True if self is a sparse COO tensor that is coalesced",
        "question": "If self is a sparse COO tensor that is coalesced Returns what?",
        "context": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "True if the Tensor uses sparse storage layout",
        "question": "Tensor.is_sparse Is what?",
        "context": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_dim",
        "question": "What Return the number of sparse dimensions in a sparse tensor self?",
        "context": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "True if self is a sparse COO tensor that is coalesced",
        "question": "Is True if self is a sparse COO tensor that is coalesced?",
        "context": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a new sparse tensor",
        "question": "Tensor.sparse_mask Returns what with values from a strided tensor self filtered by the",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "True if self is a sparse COO tensor that is coalesced",
        "question": "If self is a sparse COO tensor that is coalesced, what does Tensor.is_coales",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.col_indices",
        "question": "What Returns the tensor containing the compressed row indices of the self tensor?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "return_complex=True",
        "question": "What is strongly prefered as in a future pytorch release, this function will only return complex tensors?",
        "context": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Fourier transform",
        "question": "The STFT computes what of short overlapping windows of the input?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "frequency components",
        "question": "The Fourier transform of short overlapping windows of the input gives what of the signal as they change over time?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "optional batch dimension",
        "question": "Ignoring what, this method computes the following expression: where mmm is the index of the sliding window, and omega",
        "context": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "input must be either a what?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D tensor",
        "question": "Window can be a what?",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "center is True",
        "question": "If what is true, input will be padded on both sides to length n_fft before being applied?",
        "context": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1.8.0",
        "question": "When was return_complex required to be given explicitly for real inputs?",
        "context": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex=True",
        "question": "What is the name of the function that will only return complex tensors?",
        "context": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "If win_length is None (default), it is treated as equal to n_fft. If center is True (default), input will",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "center is True",
        "question": "If what is true, input will be padded on both sides so that the n_fft is padded before being applied?",
        "context": "Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1.8.0",
        "question": "From what version did return_complex have to always be given explicitly for real inputs?",
        "context": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "center is True",
        "question": "If what is true, input will be padded on both sides so that the t is the t?",
        "context": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "center is True",
        "question": "If what is true, input will be padded on both sides so that the ttt-th frame is centered at time t",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ttt-th frame",
        "question": "What begins at time thop_lengtht times texthop_length?",
        "context": "Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "If win_length is None, window will be padded on both sides to length n_fft before being applied. If center is True",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "torch.nn.functional.pad()",
        "question": "For all available options, see what for all available options. Default is \"reflect\"?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "onesided is True",
        "question": "If what is true (default for real input) only values for omega are used?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "If win_length is None (default), window will be padded on both sides to length n_fft before being applied. If center",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "\"reflect\"",
        "question": "What is the default setting for the padding method used when center is True?",
        "context": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "True",
        "question": "If onesided is what?",
        "context": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "If win_length is None (default), it is treated as equal to n_fft?",
        "context": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "center is True",
        "question": "If what is true, input will be padded on both sides so that the ttt-th frame is centered?",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "\"reflect\"",
        "question": "What is the default setting for input when center is True?",
        "context": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "onesided",
        "question": "If True (default for real input), only values for omega are returned because the real-to-complex Fourier transform ",
        "context": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "onesided",
        "question": "If what is True (default for real input), only values for omega are returned because the real-to-complex Fourier",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "onesided output is not possible",
        "question": "If the input or window tensors are complex, what is not possible?",
        "context": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "What is win_length?",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "False",
        "question": "If normalized is True, the function returns the normalized STFT results, i.e., normalized STFT results, i.",
        "context": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "onesided",
        "question": "If center is True, only values for omega are returned because the real-to-complex Fourier transform satisfie",
        "context": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "normalized STFT results",
        "question": "If normalized is True, the function returns what?",
        "context": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1 dimensional complex tensor",
        "question": "If return_complex is True, the return is a input.dim() + what?",
        "context": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "input.dim() + 2 dimensional real tensor",
        "question": "If return_complex is True, the output is what?",
        "context": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "a complex tensor of size",
        "question": "Returns either a complex tensor of size or what?",
        "context": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "input.dim() + 2 dimensional real tensor",
        "question": "What is the output if return_complex is True?",
        "context": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "version 0.4.1",
        "question": "At what version did the function change signature?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Fourier transform",
        "question": "n_fft (int) \u2013 the input tensor n_fft (int) \u2013 size",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "None",
        "question": "What is the default value for n_fft?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "True for real input and window",
        "question": "What is the default?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "input.dim() + 2 dimensional real tensor",
        "question": "What is the output if return_complex is False?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "False onesided",
        "question": "What is the default value for return half of results to avoid redundancy for real inputs?",
        "context": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "window_length",
        "question": "What is the window length of the Kaiser window?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "periodic",
        "question": "What argument is intended as a useful shorthand to produce a periodic window as input to functions like torch.stft()?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "torch.strided",
        "question": "What is the default layout of the returned window tensor?",
        "context": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What does this library do?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "Gets the cuda capability of a device. Gets the name of a device. Gets what of a device?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current device",
        "question": "What does Sets the current stream. This is a wrapper API to set the stream?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers for the current GPU",
        "question": "What does Sets the seed for generating random numbers on all GPUs do?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What does a bool indicating if CUDA is currently available return?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "What happens when all kernels in all streams on a CUDA device are selected?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers to a random number for the current GPU",
        "question": "What does Sets the seed for generating random numbers to a random number on all GPUs?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current random seed of the current GPU",
        "question": "What does comm.broadcast return?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast",
        "question": "What is the name of the command that returns the current random seed of the current GPU?",
        "context": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager",
        "question": "Who selects a given stream?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "default Stream",
        "question": "Returns the currently selected Stream for a given device. Returns what for a given device?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda",
        "question": "Gets the capability of a device. Gets the name of a device. Gets the properties of a device. Gets the capability",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the name of a device",
        "question": "Gets the name of a device. Gets the properties of a device. Returns NVCC gencode flags this library was",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "Gets the properties of a device. Gets the cuda capability of a device. Gets the name of a device. Get",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Initialize PyTorch\u2019s CUDA state",
        "question": "Force collects GPU memory after it has been released by CUDA IPC. Returns whether PyTorch\u2019s CUDA state has",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "wrapper API to set the stream",
        "question": "Sets the current stream.This is a what?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around the Context-manager StreamContext",
        "question": "What wrapper wraps around the Context-manager StreamContext that selects a given stream?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "What happens to all kernels in all streams on a CUDA device to complete?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "random number generator state",
        "question": "Sets what of the specified GPU. Sets the random number generator state of all devices. Sets the seed for generating random numbers for the current",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers on all GPUs",
        "question": "Sets the seed for generating random numbers on all GPUs. Sets the seed for generating random numbers on all GPUs. Returns the",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "list CUDA architectures",
        "question": "Returns what this library was compiled for. Gets the cuda capability of a device. Gets the name of a device.",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "Returns what flags this library was compiled with. Initialize PyTorch\u2019s CUDA state. Force collects GPU memory after",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "Waits for all kernels in all streams on a CUDA device to complete?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "Sets the random number generator state of the specified GPU. Sets the seed for generating random numbers for the current GPU. Sets the seed for",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current random seed of the current GPU",
        "question": "Sets the seed for generating random numbers on all GPUs. Sets the seed for generating random numbers on all GPUs. Sets the",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm",
        "question": "What does comm.reduce_add Sums tensors from multiple GPUs?",
        "context": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "name of a device",
        "question": "Gets the cuda capability of a device. Gets what?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.scatter",
        "question": "What Scatters tensors from multiple GPUs?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selected device",
        "question": "Returns the index of a what?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selected Stream",
        "question": "Returns the index of a currently selected device. Returns what for a given device?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda",
        "question": "Gets the capability of a device. Gets the name of a device. Gets the properties of a device.",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "name",
        "question": "Gets the cuda capability of a device. Gets the what of a device?",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "Gets what of a device. Returns NVCC gencode flags this library was compiled with. Initialize PyTorch",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "generating random numbers",
        "question": "Sets the seed for generating random numbers on all GPUs. Sets the seed for what?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.gather",
        "question": "What Gathers tensors?",
        "context": "Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the name of a device",
        "question": "What does Gets the name of a device?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA stream",
        "question": "Wrapper around a CUDA stream <sep>",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA event",
        "question": "Wrapper around what event?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA event",
        "question": "Releases all unoccupied cached memory currently held by CUDA IPC. Releases all unoccupied cached memory currently held by CUDA",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Releases all unoccupied cached memory",
        "question": "What release releases all unoccupied cached memory currently held by CUDA?",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "generating random numbers",
        "question": "Sets the seed for generating random numbers on all GPUs. Sets the seed for generating random numbers to a random number on all GPU",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Releases all unoccupied cached memory",
        "question": "What releases all unoccupied cached memory currently held by the caching allocator?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does Sets the random number generator state of the specified GPU?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda capability",
        "question": "Gets what of a device. Gets the name of a device. Gets the properties of a device. Returns NVCC",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Initialize PyTorch\u2019s CUDA state",
        "question": "What does Force collects GPU memory after it has been released by CUDA IPC?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA stream",
        "question": "Wrapper around a CUDA event. Releases all unoccupied cached memory currently held by the caching allocator?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "human",
        "question": "Returns a what?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA stream",
        "question": "Wrapper around a CUDA event. Releases all unoccupied cached memory currently held by the caching allocator. Releases",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does it do?",
        "context": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a human-readable printout",
        "question": "Returns what of the running processes and their GPU memory use for a given device?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the properties of a device",
        "question": "Gets what?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "Returns what flags this library was compiled with. Initialize PyTorch\u2019s CUDA state.",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA stream",
        "question": "Wrapper around a CUDA event. Releases all unoccupied cached memory currently held by the caching allocator so that those",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator statistics",
        "question": "Returns a dictionary of what?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator",
        "question": "Resets the \u201cpeak\u201d stats tracked by what?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "BETA",
        "question": "What is the current state of this module?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "PyTorch releases",
        "question": "Some functions may change in future what?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "documentation of each function",
        "question": "For details, see what for details. Computes the entropy on input (as defined below), elementwise?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor) \u2013 the input tensor",
        "question": "Computes the entropy on input (as defined below), elementwise. Computes the entropy on input (a",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor) \u2013 the input tensor",
        "question": "Computes the error function of input. The error function is defined as follows: what is the input tensor?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the complementary error function of input",
        "question": "What is the name of the function that Computes the error function of input?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor) \u2013 the input tensor",
        "question": "Computes the complementary error function of input. The complementary error function is defined as follows: what is the input tensor?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the exponential of the elements minus 1 of input",
        "question": "What is the equivalent of Computes the exponential of the elements minus 1 of input?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the natural logarithm of the absolute value of the gamma function on input",
        "question": "Computes the natural logarithm of the absolute value of the gamma function on input?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes",
        "question": "Computes the natural logarithm of the absolute value of the gamma function on input. Example: Computes the inverse",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input (Tensor) \u2013 the input tensor",
        "question": "What is the error function defined as?",
        "context": "Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the inverse error function of input",
        "question": "What is an example of a function that computes the inverse error function of input?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "expit",
        "question": "What is also known as the logistic sigmoid function?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the natural logarithm of the absolute value of the gamma function on input",
        "question": "What is an example of a computation?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "Computes the first kind (as defined below) for each element of input. input (Tensor) \u2013 the input ten",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input tensor",
        "question": "What does input (Tensor) refer to?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "optional",
        "question": "Out (Tensor) is what?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the error function of input",
        "question": "What does the output tensor do?",
        "context": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the complementary error function of input",
        "question": "What is the name of the function that computes the complementary error function of input?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the base two exponential function of input",
        "question": "What is an example?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "a new tensor",
        "question": "Returns what with the logit of the elements of input?",
        "context": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Future type",
        "question": "What encapsulates an asynchronous execution and a set of utility functions to simplify operations on Future objects?",
        "context": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "APIs",
        "question": "What does torch._C.Future expose?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "beta",
        "question": "Warning GPU support is a what?",
        "context": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inline",
        "question": "If the Future is already completed, the given callback will be run what way?",
        "context": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "beta feature",
        "question": "GPU support is a what?",
        "context": "GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inline",
        "question": "When calling wait()/value() on this Future, the exception set here will be raised what way?",
        "context": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "then()",
        "question": "What method behaves in the same way with respect to GPU tensors?",
        "context": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "current ones",
        "question": "What are the streams on which the kernels were enqueued set as when this method is called?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "change streams in between",
        "question": "What is safe to call this method immediately after launching kernels without any additional synchronization?",
        "context": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future",
        "question": "This method will use events on all the relevant current streams to ensure proper scheduling for all the consumers of what?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future",
        "question": "What is the reference to this Future?",
        "context": "is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future",
        "question": "This method will use the events to ensure proper scheduling for all the consumers of what?",
        "context": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "immediately after launching those kernels",
        "question": "When is it safe to call this method if the result contains tensors that reside on GPUs?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "record events on all the relevant current streams",
        "question": "What does Future.done() do?",
        "context": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "chaining",
        "question": "What is used to enforce a certain order?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "one argument",
        "question": "How many arguments must the callback take?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the given callback",
        "question": "What will be run immediately inline if this Future is already completed?",
        "context": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future is already completed",
        "question": "When will the given callback be run immediately inline?",
        "context": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "mark this Future as completed and trigger all attached callbacks",
        "question": "What does setting the result for this Future do?",
        "context": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inline",
        "question": "If this Future is already completed, the given callback will be run immediately what?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a Future cannot be marked completed twice",
        "question": "What does a Future not have to be done twice?",
        "context": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "while the async kernels that are populating those tensors haven\u2019t yet finished executing on the device",
        "question": "If the Future's value contains tensors that reside on GPUs, the callback might be invoked when?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "current",
        "question": "What are the dedicated streams set to when the callback is invoked?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "after the kernels complete",
        "question": "When will any operation performed by the callback on these tensors be scheduled on the device?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "switch streams",
        "question": "If the callback doesn't do what, it can safely manipulate the result without any additional synchronization?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "The non-blocking behavior of what method is similar to the non-blocking behavior of?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If the Future's value contains what that reside on GPUs, the callback might be invoked while the async kernels that are",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "The non-blocking behavior of what is similar to the non-blocking behavior of?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "if the callback returns a value that contains tensors that reside on a GPU",
        "question": "What happens if the callback returns a value that contains tensors that reside on a GPU?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future",
        "question": "What does a callback take as the only argument?",
        "context": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Obtain the value of an already-completed future",
        "question": "What does a callback do to an already-completed future?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Future may not yet hold a value",
        "question": "What could happen if the callback is called after a call to wait() has completed?",
        "context": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If the value contains what that reside on GPUs, this method will not perform any additional synchronization.",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "What method does not perform any additional synchronization if the value contains tensors that reside on GPUs?",
        "context": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Obtain the value of an already-completed future",
        "question": "What does fut.wait() do?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "If the value contains tensors that reside on GPUs, this method will not perform any additional synchronization. This should be done separately",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "What method will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "synchronization",
        "question": "If the value contains tensors that reside on GPUs, what is performed with the kernels?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()",
        "question": "What method inserts the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    }
]