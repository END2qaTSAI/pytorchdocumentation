[
    {
        "answer": "Matrix product of two tensors",
        "question": "What does the behavior depend on the dimensionality of the tensors as follows?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "dot product",
        "question": "What is returned if both tensors are 1-dimensional?",
        "context": "If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix-matrix product",
        "question": "What is returned if both arguments are 2-dimensional?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1",
        "question": "What is the dimension of the first argument?",
        "context": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the prepended dimension is removed",
        "question": "What happens after the matrix multiply?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "Matrix product",
        "question": "What product of two tensors is returned?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix-matrix",
        "question": "What product is returned if both arguments are 2-dimensional?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1",
        "question": "What is the dimensionality of the first argument?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the prepended dimension",
        "question": "What is removed after the matrix multiply?",
        "context": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "After the matrix multiply",
        "question": "When is the prepended dimension removed?",
        "context": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "a 1",
        "question": "If the first argument is 1-dimensional and the second argument is 2-dimensional, what is prepended to its dimension for the purpose of the matrix multiply?",
        "context": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix-vector product",
        "question": "What is returned if the first argument is 2-dimensional and the second argument is 1-dimensional?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix-vector",
        "question": "What product is returned if the first argument is 2-dimensional and the second argument is 1-dimensional?",
        "context": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1",
        "question": "If the first argument is 2-dimensional and the second is 2-dimensional, how many dimensions does the matrix-matrix product return?",
        "context": "If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "removed",
        "question": "What happens to the prepended dimension after the matrix multiply?",
        "context": "If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "2-dimensional",
        "question": "The matrix-vector product is returned if the first argument is what?",
        "context": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If the first argument is 2-dimensional and the second argument is what?",
        "context": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the broadcasting logic only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions",
        "question": "What does the broadcasting logic only look at when determining if the inputs are broadcastable?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix dimensions",
        "question": "What are the final two dimensions?",
        "context": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "TensorFloat32",
        "question": "What does this operator support?",
        "context": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "final two dimensions",
        "question": "What are different in the matrix dimensions?",
        "context": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "What dot product version of this function does not support anoutparameter?",
        "context": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "input(Tensor)",
        "question": "What is the first tensor to be multiplied other(Tensor)?",
        "context": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "out(Tensor,optional) \u2013 the output tensor",
        "question": "What is the output tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Alias fortorch.acos",
        "question": "What is the name of the plant?",
        "context": "Alias fortorch.acos(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos"
    },
    {
        "answer": "Alias fortorch.acos",
        "question": "What is another name for Alias fortorch.acos?",
        "context": "Alias fortorch.acos(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos"
    },
    {
        "answer": "Counts the number of non-zero values in the tensor",
        "question": "What does inputalong the givendim do?",
        "context": "Counts the number of non-zero values in the tensorinputalong the givendim.\nIf no dim is specified then all non-zeros in the tensor are counted. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints,optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "answer": "all non-zeros",
        "question": "What is counted if no dim is specified?",
        "context": "Counts the number of non-zero values in the tensorinputalong the givendim.\nIf no dim is specified then all non-zeros in the tensor are counted. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints,optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "answer": "input(Tensor)",
        "question": "What is the input tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "python:ints",
        "question": "Intortuple of what is optional?",
        "context": "Counts the number of non-zero values in the tensorinputalong the givendim.\nIf no dim is specified then all non-zeros in the tensor are counted. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints,optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "answer": "Example:",
        "question": "What is an example of a tuple of dims along which to count non-zeros?",
        "context": "Counts the number of non-zero values in the tensorinputalong the givendim.\nIf no dim is specified then all non-zeros in the tensor are counted. input(Tensor) \u2013 the input tensor. dim(intortuple of python:ints,optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "answer": "Algorithm 5.1",
        "question": "What is the implementation based on?",
        "context": "Return the singular value decomposition(U,S,V)of a matrix,\nbatches of matrices, or a sparse matrixAAAsuch thatA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In caseMMMis given, then\nSVD is computed for the matrixA\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "pseudorandom number generator",
        "question": "To obtain repeatable results, reset the seed for what?",
        "context": "Return the singular value decomposition(U,S,V)of a matrix,\nbatches of matrices, or a sparse matrixAAAsuch thatA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In caseMMMis given, then\nSVD is computed for the matrixA\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "10-fold",
        "question": "How much higher performance characteristics do full-rank SVD implementations have?",
        "context": "In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "thattorch.linalg.svd()",
        "question": "What cannot handle huge sparse matrices?",
        "context": "Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "2009",
        "question": "When was the Algorithm 5.1 implemented?",
        "context": "The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "low-rank matrix",
        "question": "What is the input assumed to be?",
        "context": "The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "10-fold higher performance characteristics",
        "question": "Why should you use the full-rank SVD implementationtorch.linalg.svd() for dense matrices?",
        "context": "Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "huge sparse matrices",
        "question": "What is the low-rank SVD useful for?",
        "context": "In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "thattorch.linalg.svd()",
        "question": "What can the low-rank SVD be useful for large sparse matrices?",
        "context": "In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "A (Tensor)",
        "question": "What is the input tensor of size(,m,n)(*,m,n)(*,m,n)",
        "context": "The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "tensor",
        "question": "What is the input of A (Tensor)?",
        "context": "The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "A",
        "question": "What is the input tensor of size(,m,n)?",
        "context": "To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "10-fold higher performance characteristics",
        "question": "Why should you use full-rank SVD implementationtorch.linalg.svd() for dense matrices?",
        "context": "To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "q",
        "question": "What is the tensor of size(,m,n)(*, m, n)(,m,n",
        "context": "To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "niter",
        "question": "What must be a nonnegative integer?",
        "context": "Return the singular value decomposition(U,S,V)of a matrix,\nbatches of matrices, or a sparse matrixAAAsuch thatA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In caseMMMis given, then\nSVD is computed for the matrixA\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "huge sparse matrices",
        "question": "What type of matrices can low-rank SVD be useful for?",
        "context": "The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "q",
        "question": "A (Tensor) is the input tensor of size(,m,n)(*, m, n",
        "context": "In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "10-fold",
        "question": "How much higher performance characteristics do full-rank SVD implementations of dense matrices have?",
        "context": "Return the singular value decomposition(U,S,V)of a matrix,\nbatches of matrices, or a sparse matrixAAAsuch thatA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In caseMMMis given, then\nSVD is computed for the matrixA\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "10-fold higher performance characteristics",
        "question": "Why should you use full-rank SVD for dense matrices?",
        "context": "In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "q",
        "question": "What is a slightly overestimated rank of A. conduct?",
        "context": "A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "arXiv:0909.4061",
        "question": "What is the name of the book that Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp published in 2009?",
        "context": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementationtorch.linalg.svd()for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices thattorch.linalg.svd()cannot handle. A (Tensor): the input tensor of size(\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available atarXiv). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "answer": "theiiithgrid",
        "question": "What is defined by expanding theiiithinput over dimensions defined by other inputs?",
        "context": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also havekkktensors,\nwhere all tensors are of size(N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "scalar or 1-dimensional vector",
        "question": "What can NNNtensors be?",
        "context": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "list of scalars or 1 dimensional tensors",
        "question": "What is tensors(list of Tensor)?",
        "context": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also havekkktensors,\nwhere all tensors are of size(N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "Scalars",
        "question": "What will be treated as tensors of size(1,)(1,)(1,)automatically?",
        "context": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also havekkktensors,\nwhere all tensors are of size(N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "expanding theiiithinput over dimensions defined by other inputs",
        "question": "How is theiiithgrid defined?",
        "context": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "list of scalars or 1 dimensional tensors",
        "question": "What are tensors?",
        "context": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "list of scalars or 1 dimensional tensors",
        "question": "What is tensors?",
        "context": "tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also havekkktensors,\nwhere all tensors are of size(N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "seq",
        "question": "What is an example of a sequence of Tensors?",
        "context": "TakeNNNtensors, each of which can be either scalar or 1-dimensional\nvector, and createNNNN-dimensional grids, where theiiithgrid is defined by\nexpanding theiiithinput over dimensions defined by other inputs. tensors(list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size(1,)(1,)(1,)automatically If the input haskkktensors of size(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also havekkktensors,\nwhere all tensors are of size(N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "answer": "the number of threads",
        "question": "Returns what number of threads used for inter-op parallelism on CPU?",
        "context": "Returns the number of threads used for inter-op parallelism on CPU\n(e.g. in JIT interpreter) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads"
    },
    {
        "answer": "JIT interpreter",
        "question": "In what is the number of threads used for inter-op parallelism on CPU used?",
        "context": "Returns the number of threads used for inter-op parallelism on CPU\n(e.g. in JIT interpreter) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.get_num_interop_threads.html#torch.get_num_interop_threads"
    },
    {
        "answer": "Atorch.Tensoris",
        "question": "What is a multi-dimensional matrix containing elements of a single data type?",
        "context": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "10 tensor types with CPU and GPU variants",
        "question": "How many tensor types does Torch define?",
        "context": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "multi-dimensional matrix",
        "question": "What is Atorch.Tensoris?",
        "context": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "bfloat16",
        "question": "What is the name of the tensor type defined by Torch?",
        "context": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32",
        "question": "What is the bit floating point torch?",
        "context": "CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Data type dtype",
        "question": "What is the name of the CPU tensor GPU tensor 32-bit floating point torch?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit floating point torch",
        "question": "What is the data type dtype CPU tensor GPU tensor?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit floating point torch",
        "question": "What type of torch is a dtype CPU tensor GPU tensor?",
        "context": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "CPU tensor GPU tensor",
        "question": "What is a 32-bit floating point torch?",
        "context": "CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit floating point torch",
        "question": "What type of torch is a GPU tensor?",
        "context": "CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit",
        "question": "What type of complex torch is included in a BFloat16Tensor 32-bit complex torch?",
        "context": "GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "unsigned",
        "question": "What is a double 8-bit integer?",
        "context": "32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch",
        "question": "What type of torch is float32ortorch?",
        "context": "torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dtype CPU tensor GPU tensor 32-bit floating point torch",
        "question": "What is the name of the torch?",
        "context": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.float64ortorch",
        "question": "What is a double torch?",
        "context": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8",
        "question": "How many bits does a complex torch have?",
        "context": "torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "double torch",
        "question": "What type of torch is float64ortorch?",
        "context": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "64-bit floating point torch",
        "question": "What type of torch is a double torch?",
        "context": "64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit",
        "question": "What is the floating point of a double-tensor torch?",
        "context": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point1 torch",
        "question": "What type of torch is float16ortorch?",
        "context": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point1 torch",
        "question": "What is a half torch?",
        "context": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16",
        "question": "How many bits floating point1 torch?",
        "context": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "half torch",
        "question": "What type of torch is a HalfTensor torch?",
        "context": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "half torch",
        "question": "What is the term for torch.float16ortorch?",
        "context": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32",
        "question": "How many bits does a BFloat16Tensor have?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32",
        "question": "How many byte complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128or",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point2 torch",
        "question": "What type of torch is a HalfTensor?",
        "context": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32",
        "question": "How many byte complex torch?",
        "context": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point2 torch",
        "question": "What type of torch is a bfloat16 torch?",
        "context": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point2",
        "question": "What type of torch is bfloat16?",
        "context": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit complex torch",
        "question": "What type of torch does bfloat16Tensor have?",
        "context": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "64-bit",
        "question": "What type of complex torch is the BFloat16Tensor 32-bit complex torch?",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "64",
        "question": "How many bits are in a complex torch?",
        "context": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "cdouble 8-bit integer",
        "question": "What is unsigned torch.uint8 torch?",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit",
        "question": "What is the most common type of complex torch?",
        "context": "32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "LongTensor torch.cuda",
        "question": "What is the name of the LongTensor torch?",
        "context": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "64-bit",
        "question": "What type of complex torch is the complex64 128-bit complex torch?",
        "context": "64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "LongTensor torch.cuda",
        "question": "What is the name of the LongTensor Boolean torch?",
        "context": "64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Boolean torch",
        "question": "What type of torch is a bool torch?",
        "context": "torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "128",
        "question": "How many bits is a complex torch?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "128",
        "question": "How many bits complex torch?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit",
        "question": "What type of integer is a torch?",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What is the unsigned part of a torch?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What is a ByteTensor?",
        "context": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What does ByteTensor stand for?",
        "context": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What type of torch is a ByteTensor?",
        "context": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What is the sign of a ByteTensor torch?",
        "context": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What type of torch is a CharTensor torch?",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit integer",
        "question": "What is the sign of a torch?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16",
        "question": "How many bits is a CharTensor?",
        "context": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit integer",
        "question": "What is the signature of a torch?",
        "context": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit",
        "question": "What type of integer is a shortTensor?",
        "context": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit integer",
        "question": "What does CharTensor stand for?",
        "context": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit",
        "question": "What type of integer is a short torch?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit integer",
        "question": "What is a shorttensor?",
        "context": "torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "int16ortorch",
        "question": "What is the short torch?",
        "context": "torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "4-bit integer",
        "question": "What is the sign of the IntTensor 6?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "4-bit integer",
        "question": "What is the sign of an IntTensor?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "4",
        "question": "How many bit integers are in an IntTensor?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "LongTensor",
        "question": "What type of torch.cuda.LongTensor Boolean torch.bool torch.BoolTen",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32-bit integer",
        "question": "What type of torch is a torch?",
        "context": "32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.tensor()always copiesdata",
        "question": "What does torch.tensor() always copydata?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "userequires_grad_()ordetach()",
        "question": "What do you need to change to avoid a copy of a Tensordata?",
        "context": "torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "usetorch.as_tensor()",
        "question": "What is used if you have a numpy array and want to avoid a copy?",
        "context": "torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "atorch.dtype",
        "question": "A tensor of specific data type can be constructed by passing what to a constructor or tensor creation op?",
        "context": "A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Python",
        "question": "What programming language can be used to access and modify the contents of a tensor?",
        "context": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Usetorch.Tensor.item()",
        "question": "What is used to get a Python number from a tensor containing a single value?",
        "context": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "automatic differentiation",
        "question": "What is the purpose of creating a tensor?",
        "context": "For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "change an existing tensor\u2019",
        "question": "What is the purpose of a tensor?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation",
        "question": "How can a tensor be created?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "associatedtorch.Storage",
        "question": "What holds each tensor's data?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "multi-dimensional,stridedview of a storage",
        "question": "What does the tensor class provide?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Note",
        "question": "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage?",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a tensor",
        "question": "What does Usetorch.Tensor.item() get a Python number from?",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "holds its data",
        "question": "What does the associatedtorch.Storage do?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "defines numeric operations on it",
        "question": "What does the tensor class do?",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Note",
        "question": "What is the name of the tensor class that provides multi-dimensional,stridedview of a storage and defines numeric operations",
        "context": "Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Views",
        "question": "For more information on tensor views, see what?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Note",
        "question": "What is another name for tensor views?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Note",
        "question": "What do you do when creating a tensor?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Note",
        "question": "What is the name of the tensor class that holds its data?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "multi-dimensional,stridedview",
        "question": "What does the tensor class provide of a storage?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor views",
        "question": "What are tensor views?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "thetorch.dtype,torch.device, andtorch.layoutattributes",
        "question": "What are some examples of atorch.Tensor attributes?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Note",
        "question": "What is another name for atorch.Tensor?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore suffix",
        "question": "Methods that mutate a tensor are marked with what?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a new tensor",
        "question": "What is the result oftorch.FloatTensor.abs()?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Note",
        "question": "What is a suffix for a method that mutates a tensor?",
        "context": "For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore suffix",
        "question": "Methods which mutate a tensor are marked with what?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a new tensor",
        "question": "What doestorch.FloatTensor.abs() return?",
        "context": "For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "thetorch.dtype,torch.device, andtorch.layoutattributes",
        "question": "What are some of the attributes of atorch.Tensor?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore",
        "question": "What suffix is used to mark methods that mutate a tensor?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a new tensor",
        "question": "What doestorch.FloatTensor.abs()compute the result in?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "to()method",
        "question": "What method is used to change an existing tensor'storch.deviceand/ortorch.dtype?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What is a warning about usingto() method on a tensor?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "atorch.Tensor",
        "question": "What is another name for a tensor?",
        "context": "Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore",
        "question": "Methods which mutate a tensor are marked with what suffix?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "the result in a new tensor",
        "question": "What does torch.FloatTensor.abs()compute?",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What does usingto()method on a tensor do?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "atorch.Tensor",
        "question": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of what",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What does usingto()method on a tensor do to change an existing tensor'storch.deviceand/or",
        "context": "For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What is a warning about a method that mutates a tensor?",
        "context": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore",
        "question": "What suffix mark methods that mutate a tensor?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "the absolute value in-place",
        "question": "What does torch.FloatTensor.abs_() compute?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore",
        "question": "Methods that mutate a tensor are marked with what suffix?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What is a warning?",
        "context": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "unexpectedly high",
        "question": "What kind of memory usage could be caused by the current implementation oftorch.Tensor?",
        "context": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "one large structure",
        "question": "What should you use if you have a lot of tiny tensors?",
        "context": "Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "memory overhead",
        "question": "What does current implementation oftorch.Tensor introduce?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "one large structure",
        "question": "What should you use if you have many tiny tensors?",
        "context": "Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a few main ways to create a tensor",
        "question": "How many ways to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "pre-existing data",
        "question": "What does usetorch.tensor() do to create a tensor with?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "specific size",
        "question": "What do you need to create a tensor with?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "*tensor creation ops",
        "question": "What does usetorch to create a tensor with specific size?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "usetorch",
        "question": "What is the best way to create a tensor with the same size as another tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "*_liketensor creation ops",
        "question": "What do you use to create a tensor with the same size as another tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "usetensor.new_*creation ops",
        "question": "What is used to create a tensor with similar type but different size as another tensor?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "its dimensions",
        "question": "What is reversed in a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor",
        "question": "What is the main way to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "pre-existing data",
        "question": "What does usetorch.tensor() create a tensor with?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "*tensor creation ops",
        "question": "What does usetorch do to create a tensor with specific size?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "same size",
        "question": "What is the size of a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "*_liketensor creation ops",
        "question": "What creates a tensor with the same size as another tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "similar type but different size",
        "question": "What type of tensor is used to create a tensor?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "usetorch.tensor()",
        "question": "What is used to create a tensor with pre-existing data?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "*tensor creation ops",
        "question": "What is used to create a tensor with specific size?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "*_liketensor creation ops",
        "question": "What is used to create a tensor with the same size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "its dimensions reversed",
        "question": "Is this Tensor with what?",
        "context": "To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute",
        "question": "What is the number of dimensions inx,x.Tis equivalent to?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "reversed",
        "question": "Is this Tensor with its dimensions reversed or reversed?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tox.permute",
        "question": "Ifnis the number of dimensions inx,x.Tis equivalent what?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.new_tensor",
        "question": "What is used to create a tensor with similar type but different size?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "specific size",
        "question": "Usetorch to create a tensor with what?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "*tensor creation ops",
        "question": "What do you use to create a tensor with specific size?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.new_tensor",
        "question": "What is used to create a tensor with reversed dimensions?",
        "context": "To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "usetensor.new_*creation ops",
        "question": "What is used to create a tensor with the same type but different size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Ifnis",
        "question": "What is the number of dimensions inx,x.Tis equivalent to tox.permute(n-1,n-2,...,0",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "data",
        "question": "What does Tensor.new_tensor return as the tensor data?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor of sizesizefilled withfill_value",
        "question": "What does Tensor.new_full return?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor",
        "question": "What returns a Tensor of sizesizefilled withfill_value?",
        "context": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "*_liketensor creation ops",
        "question": "What does usetorch do to create a tensor with the same size as another tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "similar type but different size",
        "question": "What type of tensor does usetorch. *_liketensor creation ops create?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "new_tensor",
        "question": "What Returns a new Tensor withdataas the tensor data?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "withfill_value",
        "question": "What value does Tensor.new_full return?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "empty",
        "question": "What type of tensor returns a new Tensor?",
        "context": "To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "reversed",
        "question": "Is the tensor with its dimensions reversed or reversed?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What does Tensor.new_empty return a Tensor of sizesizefilled with?",
        "context": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "1",
        "question": "Tensor.new_ones Returns a Tensor of sizesizefilled with what value?",
        "context": "Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor.new_*creation",
        "question": "What ops is used to create a tensor with similar type but different size as another tensor?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "reversed",
        "question": "Is this tensor with its dimensions reversed or reversed?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "new Tensor",
        "question": "What does Tensor.new_tensor return?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "withfill_value",
        "question": "What value does Tensor.new_full return a Tensor of sizesizefilled?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.new_ones",
        "question": "What returns a Tensor of sizesizefilled with1?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "data",
        "question": "What is returned as the tensor data?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a Tensor of sizesizefilled with0",
        "question": "What does Tensor.new_zeros return?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.is_cuda",
        "question": "What is the name of the Tensor with its dimensions reversed?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tox.permute",
        "question": "Ifnis the number of dimensions inx,x.Tis equivalent to what?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "withfill_value",
        "question": "What returns a Tensor of sizesizefilled?",
        "context": "Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "with1",
        "question": "Tensor.new_ones Returns a Tensor of sizesizefilled what?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Ifnis",
        "question": "What is the number of dimensions inx,x?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor data",
        "question": "What data does the new Tensor return?",
        "context": "Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "GPU",
        "question": "Where is the Tensor stored?",
        "context": "IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "quantized",
        "question": "What is true if the Tensor is stored on the GPU?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor data",
        "question": "What does the new Tensor return withdataas?",
        "context": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "sizesizefilled withfill_value",
        "question": "What does Tensor.new_full return a Tensor of?",
        "context": "Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "0",
        "question": "Tensor.new_zeros Returns a Tensor of sizesizefilled with what value?",
        "context": "Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor data",
        "question": "Returns a new Tensor withdataas what?",
        "context": "Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta",
        "question": "Tensor.is_what IsTrueif the Tensor is a meta tensor?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.device",
        "question": "What is the name of the device that returns a tensor of sizesizefilled withfill_value?",
        "context": "Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "sizesizefilled withfill_value",
        "question": "Returns a Tensor of what?",
        "context": "Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta",
        "question": "What is true if the Tensor is a meta tensor?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "1",
        "question": "Tensor.new_ones Returns a Tensor of sizesizefilled with what?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.grad",
        "question": "What is thetorch.devicewhere the Tensor is?",
        "context": "Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "uninitialized data",
        "question": "What does Tensor.new_ones return a Tensor of sizesizefilled with?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.grad",
        "question": "What is thetorch.devicewhere this Tensor is?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "0",
        "question": "What is the size of the Tensor of sizesizefilled with?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tobackward()",
        "question": "When a Tensor.grad isNoneby default, it becomes a Tensor the first time a call to which function compute",
        "context": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "0",
        "question": "Tensor.new_zeros Returns a Tensor of sizesizefilled with what?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "thetorch.device",
        "question": "Where is the Tensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.ndim",
        "question": "What is the name of the attribute that becomes a Tensor the first time a call tobackward()computes gradients forself",
        "context": "Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "sizesizefilled with0",
        "question": "What is the size of the Tensor returned by Tensor.new_zeros?",
        "context": "Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tobackward()computes gradients forself",
        "question": "What is the first time a Tensor becomes a Tensor?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.ndim Alias fordim() Tensor.real",
        "question": "What is the name of the Alias fordim?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "sizesizefilled with0",
        "question": "What is the size of the Tensor?",
        "context": "Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.is",
        "question": "What is true if the Tensor is quantized?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor",
        "question": "What returns a new tensor containing real values of theselftensor?",
        "context": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of theselftensor",
        "question": "Tensor.real Returns a new tensor containing what?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values",
        "question": "Tensor.imag Returns a new tensor containing what of theselftensor?",
        "context": "Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.imag",
        "question": "What returns a new tensor containing imaginary values of theselftensor?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta",
        "question": "IsTrueif the Tensor is a meta tensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values",
        "question": "What does imag return a new tensor containing?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Seetorch.abs()",
        "question": "What function returns a new tensor containing imaginary values of theselftensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "quantized",
        "question": "IsTrueif the Tensor is what?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "thetorch.device",
        "question": "What is the name of the device where the Tensor is?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.grad",
        "question": "What attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself",
        "context": "Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of theselftensor",
        "question": "What does Tensor.ndim return a new tensor containing?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.imag",
        "question": "What Returns a new tensor containing imaginary values of theselftensor?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.abs",
        "question": "What Seetorch.abs() returns a new tensor containing imaginary values of theselftensor?",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta",
        "question": "What type of tensor is true if the Tensor is a meta tensor?",
        "context": "Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real",
        "question": "What Returns a new tensor containing real values of theselftensor?",
        "context": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "In-place version ofabs() Tensor.abs",
        "question": "What is olute?",
        "context": "Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "meta tensor",
        "question": "IsTrue if the Tensor is a what?",
        "context": "To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias forabs",
        "question": "What is the name of the function that returns the absolute value of the Tensor?",
        "context": "IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias",
        "question": "What is the name of the tensor that returns a new tensor containing real values of theselftensor?",
        "context": "Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "thetorch.device",
        "question": "Where is the Tensor.device located?",
        "context": "Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of theselftensor",
        "question": "What does the Alias fordim() Tensor.real Return a new tensor containing?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.abs",
        "question": "What does Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alia",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "thetorch.device",
        "question": "Where is the Tensor located?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias forabs_()",
        "question": "What is the name of the function that returns the Tensor.absolute?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "thetorch.device",
        "question": "Where is this Tensor located?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias forabs",
        "question": "What is the in-place version ofabs() Tensor.absolute?",
        "context": "Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias",
        "question": "Who is forabs in Tensor.absolute?",
        "context": "Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tobackward()computes gradients forself",
        "question": "When does this attribute become a Tensor?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias",
        "question": "Who forabs() Tensor.absolute Returns a new tensor containing real values of theselftensor?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "isNoneby",
        "question": "What attribute becomes a Tensor the first time a call tobackward()computes gradients forself?",
        "context": "This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values of theselftensor",
        "question": "What does the imag return a new tensor containing?",
        "context": "Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of theselftensor",
        "question": "What does the new tensor contain?",
        "context": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary",
        "question": "What kind of values of theselftensor does Tensor.imag return?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias fordim",
        "question": "Who returns a new tensor containing real values of theselftensor?",
        "context": "Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "scalar or tensor",
        "question": "What is added to the selftensor?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.add_",
        "question": "What adds a scalar or tensor toselftensor?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "scalar",
        "question": "What type of tensor can be added to a selftensor?",
        "context": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.addbmm",
        "question": "What is the name of the tensor that adds a scalar or tensor toselftensor?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values of theselftensor",
        "question": "What does Tensor.imag return a new tensor containing?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "scalar or tensor",
        "question": "What do you add to theselftensor?",
        "context": "Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is another name for Add a scalar or tensor toselftensor?",
        "context": "Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values of theselftensor",
        "question": "Returns a new tensor containing what?",
        "context": "Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "scalar or tensor",
        "question": "What is added to theselftensor?",
        "context": "Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias forabs",
        "question": "What is in-place version ofabs() Tensor.absolute?",
        "context": "Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is the In-place version ofaddbmm() Tensor?",
        "context": "Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias forabs",
        "question": "What is the In-place version ofabs() Tensor.absolute?",
        "context": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias forabs",
        "question": "What does Tensor.absolute stand for?",
        "context": "Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is the In-place version ofadd() Tensor?",
        "context": "Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What does Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor",
        "context": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "scalar or tensor toselftensor",
        "question": "What do you add to a Tensor?",
        "context": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is the In-place version of addbmm() Tensor?",
        "context": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias fortorch.special.logit()",
        "question": "What is the name of the function that is used to log in a system?",
        "context": "Alias fortorch.special.logit(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit"
    },
    {
        "answer": "Alias fortorch.special.logit()",
        "question": "What is the name of the function used by Alias fortorch.special.logit()?",
        "context": "Alias fortorch.special.logit(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit"
    },
    {
        "answer": "non-zero",
        "question": "When talking about storing only what type of elements of a sparse array, the usage of adjective \"non-zero\" is not",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "array elements that are actually stored",
        "question": "What do we use \"specified elements\" for?",
        "context": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero value",
        "question": "What are unspecified elements assumed to have?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What does the term \"fill value\" mean?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "non-zero",
        "question": "When talking about storing only what elements of a sparse array, the usage of adjective \u201cnon-zero\u201d is not strict:",
        "context": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "specified elements",
        "question": "What do we use for those array elements that are actually stored?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "fill value",
        "question": "What term is used to denote unspecified elements?",
        "context": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse storage format",
        "question": "What can be advantageous only when the size and sparsity levels of arrays are high?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "contiguous memory storage format",
        "question": "What is the most efficient way to store low-sparsity arrays?",
        "context": "Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "beta",
        "question": "The PyTorch API of sparse tensors is in what state?",
        "context": "Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse arrays",
        "question": "What can a sparse storage format be advantageous for storing?",
        "context": "Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "contiguous memory storage format",
        "question": "What is the most efficient way to store sparse arrays?",
        "context": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch API",
        "question": "What API of sparse tensors is in beta?",
        "context": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "when the size and sparsity levels of arrays are high",
        "question": "When can using a sparse storage format be advantageous?",
        "context": "Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "contiguous memory storage format",
        "question": "What is the most efficient approach for small-sized or low-sparsity arrays?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Coordinate format",
        "question": "What is another name for COO format?",
        "context": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64, the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tuples",
        "question": "What are the specified elements stored as in COO format?",
        "context": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64, the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "inindicestensor of size(ndim,nse)",
        "question": "What are the indices of specified elements collected in COO format?",
        "context": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64, the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "arbitrary integer or floating point number element type",
        "question": "What are the corresponding values collected invaluestensor of size(nse)and with?",
        "context": "the indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64, the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type, wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a constant overhead",
        "question": "What is added to the memory consumption of a sparse COO tensor from storing other tensor data?",
        "context": "the indices of specified elements are collected inindicestensor of size(ndim,nse)and with element typetorch.int64, the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type, wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "size(nse,)",
        "question": "The corresponding values are collected invaluestensor of what?",
        "context": "the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type, wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a constant overhead",
        "question": "What does storing other tensor data add to the memory consumption of a sparse COO tensor?",
        "context": "wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "at leastproduct",
        "question": "What is the memory consumption of a strided tensor?",
        "context": "The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dimensionality",
        "question": "What does ndimis indicate about a tensor?",
        "context": "wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse COO tensor",
        "question": "What type of tensor is at least(ndim*8+sizeofelementtypeinbytes>)*nse",
        "context": "wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "number of specified elements",
        "question": "What is the dimensionality of the tensor andnseis?",
        "context": "wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nsebytes",
        "question": "What is the memory consumption of a sparse COO tensor?",
        "context": "The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensor",
        "question": "What tensor is at leastproduct(tensorshape>)*sizeofelementtypeinbytes>?",
        "context": "Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided",
        "question": "What type of tensor is at leastproduct(tensorshape>)*sizeofelementtypeinbytes",
        "context": "The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "100 000",
        "question": "What is the memory consumption of a 10 000 x 10 000 tensor with non-zero 32-bit floating point numbers?",
        "context": "The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "200 fold",
        "question": "What is the memory saving from using the COO storage format?",
        "context": "the corresponding values are collected invaluestensor of\nsize(nse,)and with an arbitrary integer or floating point\nnumber element type, wherendimis the dimensionality of the tensor andnseis the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "100 000",
        "question": "How many non-zero 32-bit floating point numbers does a 10 000 x 10 000 tensor have?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "at least(2*8+4)*100000=2000000bytes",
        "question": "What is the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero floating point numbers?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "by providing the two tensors of indices and values, as well as the size of the sparse tensor",
        "question": "How can a sparse COO tensor be constructed?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1, 0",
        "question": "What is the default value of the sparse tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "What is the default value of a sparse COO tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "write",
        "question": "What would we do to define a sparse COO tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "entry 3",
        "question": "What is the location of the sparse tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "What is the default value of the fill value?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "inputiis NOT a list of index tuples",
        "question": "What is the inputi of a sparse tensor?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "specifying its size",
        "question": "How can an empty sparse COO tensor be constructed?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "What is the default value of a sparse tensor?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "index tuples",
        "question": "What is the inputiis NOT a list of?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "inputiis NOT a list of index tuples",
        "question": "What is the inputi?",
        "context": "Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Pytorch",
        "question": "What implements an extension of sparse tensors with scalar values to sparse tensors with (",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "hybrid tensors",
        "question": "What are sparse tensors with contiguous tensor values called?",
        "context": "Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "index tuples",
        "question": "The inputiis NOT a list of what?",
        "context": "Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "scalar",
        "question": "Pytorch implements an extension of sparse tensors with what values?",
        "context": "Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "hybrid tensors",
        "question": "What are sparse tensors with (contiguous) tensor values called?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch",
        "question": "What extends the sparse COO tensor by allowing thevaluestensor to be a multi-dimensional tensor",
        "context": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "empty sparse COO tensor",
        "question": "What can be constructed by specifying its size only?",
        "context": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "scalar values",
        "question": "What do Pytorch extend sparse tensors with?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indicestensor of size",
        "question": "The indices of specified elements are collected in what?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "typetorch.int64",
        "question": "What element does PyTorch hybrid COO tensor have?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What does PyTorch hybrid COO tensor do?",
        "context": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "size",
        "question": "The indices of specified elements are collected inindicestensor of what?",
        "context": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "arbitrary integer or floating point number element type",
        "question": "What are the tensor values collected with?",
        "context": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M + K",
        "question": "What is the name of the dimensional tensor to denote a N-dimensional hybrid sparse tensor?",
        "context": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse,dense_dims",
        "question": "The corresponding tensor values are collected invaluestensor of size(sparse_dims,nse)and",
        "context": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "inindicestensor",
        "question": "What are the indices of specified elements collected?",
        "context": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M and K",
        "question": "What are the numbers of sparse and dense dimensions?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "size(nse,dense_dims)",
        "question": "What are tensor values collected invaluestensor of?",
        "context": "the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nse,dense_dims",
        "question": "What are the corresponding tensor values collected invaluestensor of size?",
        "context": "the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "entry [3, 4] at location (0, 2)",
        "question": "What is the name of the entry in a 2 + 1-dimensional tensor?",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "ifsis a sparse COO tensor",
        "question": "What would we write to create a 2 + 1-dimensional tensor?",
        "context": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "ifsis a sparse COO tensor",
        "question": "What would we write if we want to create a sparse tensor?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M + K",
        "question": "What is the number of sparse and dense dimensions?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "entry [3, 4] at location (0, 2)",
        "question": "What is the location of the 2 + 1-dimensional tensor?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "ifsis a sparse COO tensor",
        "question": "What would we write to create a 2 + 1-dimensional tensor with the entry [3, 4 at location (0, 2), entry [",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "s.indices()",
        "question": "What is the name of the function that stores sparse indices?",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where is the entry [5, 6] in a 2 + 1-dimensional tensor?",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What is the COO tensor?",
        "context": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "ifsis a sparse COO tensor",
        "question": "What do we have the following invariants?",
        "context": "In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What is the meaning of strided tensors in a sparse COO tensor?",
        "context": "In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "s.indices()",
        "question": "What is the name of the function where sparse indices are stored explicitly?",
        "context": "M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse dimensions",
        "question": "Dense dimensions always follow what?",
        "context": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided",
        "question": "What are values stored as strided tensors?",
        "context": "M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "K-dimensional tensors",
        "question": "What are the values of a hybrid tensor?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "s.values().layout==torch.strided- values are stored as what?",
        "context": "s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "What are s.values().layout==torch.strided- values stored as?",
        "context": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "Dense dimensions always follow what dimensions?",
        "context": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dense",
        "question": "What is not supported when mixing sparse and dense dimensions?",
        "context": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse dimensions",
        "question": "What do dense dimensions always follow?",
        "context": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch sparse COO tensor format",
        "question": "What permits uncoalescedsparse tensors?",
        "context": "Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D",
        "question": "What is an uncoalesced tensor?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "duplicate coordinates",
        "question": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors where there",
        "context": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D",
        "question": "What type of uncoalesced tensor is created when multiple values are specified for the same index1?",
        "context": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the sum of all duplicate value entries",
        "question": "What is the value at an index in a PyTorch sparse COO tensor?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "summation",
        "question": "What does the coalescing process use to accumulate multi-valued elements into a single value?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the coalescing process",
        "question": "What process will accumulate the multi-valued elements into a single value using summation?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What does torch.Tensor.is_coalesced() return true?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical",
        "question": "In what order are the indices sorted?",
        "context": "s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What does torch.is_coalesced()returnsTrue?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical",
        "question": "In what order are the indices of specified tensor elements sorted?",
        "context": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or uncoalesced sparse tensor",
        "question": "Most operations will work identically given what two types of tensors?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or uncoalesced sparse tensor",
        "question": "Most operations will work identically given what?",
        "context": "In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or not",
        "question": "What is a sparse tensor?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced tensors",
        "question": "Some operations can be implemented more efficiently on uncoalesced tensors, and some on coalesced tensors",
        "context": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "unique",
        "question": "Are the indices of specified tensor elements unique or unique?",
        "context": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensor",
        "question": "What tensor is coalesced?",
        "context": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced tensors",
        "question": "Some operations can be implemented more efficiently on what?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch",
        "question": "Which indices are sorted in lexicographical order?",
        "context": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensor",
        "question": "What type of tensor is coalesced?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.Tensor.is_coalesced()",
        "question": "What returns True?",
        "context": "torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensor",
        "question": "What is coalesced?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "concatenating the indices and values tensors",
        "question": "How is addition of sparse COO tensors implemented?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced tensors",
        "question": "Some operations can be implemented more efficiently on uncoalesced what?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or not",
        "question": "What should you not care about a sparse tensor being?",
        "context": "Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse COO tensors",
        "question": "What is implemented by simply concatenating the indices and values tensors?",
        "context": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "concatenating the indices and values tensors",
        "question": "How is the addition of sparse COO tensors implemented?",
        "context": "For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical ordering",
        "question": "What can be advantageous for implementing algorithms that involve many element selection operations?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesce",
        "question": "What should you do with sparse tensors to prevent them from growing too large?",
        "context": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "example",
        "question": "What is an example of lexicographical ordering of indices?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "atorch.Tensorinstance",
        "question": "What is a sparse COO tensor?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dense",
        "question": "What dimension can be acquired using methodstorch.Tensor.sparse_dim() andtorch.Tens",
        "context": "The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse COO tensor",
        "question": "What type of tensor can be acquired using methodstorch.Tensor.indices() andtorch.Tens",
        "context": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "uncoalesced",
        "question": "For acquiring the COO format data of an what type of tensor, usetorch.Tensor._values",
        "context": "The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "when the tensor instance is coalesced",
        "question": "When can one acquire the COO format data?",
        "context": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced copy",
        "question": "What can one construct of a sparse COO tensor using thetorch.Tensor.coalesce()",
        "context": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "thetorch.Tensor.coalesce()method",
        "question": "What method can be used to construct a coalesced copy of a sparse COO tensor?",
        "context": "but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Slicing",
        "question": "What is supported only for dense dimensions?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "In PyTorch, the fill value of a sparse tensor is assumed to be what?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "there exists operations that may interpret the fill value differently",
        "question": "Is the fill value of a sparse tensor assumed to be zero in general?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "softmax",
        "question": "What computes the softmax with the assumption that the fill value is negative infinity?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dense dimensions",
        "question": "Slicing of a sparse COO tensor is supported only for what dimensions?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "In PyTorch, the fill value of a sparse tensor cannot be specified explicitly and is assumed to be what?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "operations that may interpret the fill value differently",
        "question": "What are there operations that may interpret the fill value differently?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "negative infinity",
        "question": "What is the fill value of a sparse tensor assumed to be?",
        "context": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "The CSR (Compressed Sparse Row) sparse tensor format",
        "question": "What implements the CSR format for storage of 2 dimensional tensors?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "MKL and MAGMA backends",
        "question": "What two backends are used in sparse matrix-vector multiplication?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CUDA",
        "question": "What does not exist as of now?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensors",
        "question": "A CSR sparse tensor consists of three what?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Compressed Sparse Row",
        "question": "What does CSR stand for?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "better use of storage and much faster computation operations",
        "question": "What is the primary advantage over the COO format?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "three",
        "question": "How many 1-D tensors does a CSR sparse tensor consist of?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Thecrow_indicestensor",
        "question": "What consists of compressed row indices?",
        "context": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor of sizesize[0]+1",
        "question": "What is the CSR sparse tensor?",
        "context": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "number of non-zeros",
        "question": "What is the last element of a CSR sparse tensor?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "where the given row starts",
        "question": "What does the index invaluesandcol_indices depend on?",
        "context": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "number of elements in a given row",
        "question": "Each successive number in the tensor subtracted by the number before it denotes what?",
        "context": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CSR sparse tensor",
        "question": "What consists of three 1-D tensors?",
        "context": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sizesize[0]+1",
        "question": "What is the size of a CSR sparse tensor?",
        "context": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "where the given row starts",
        "question": "What does the tensor encode the index invaluesandcol_indicesdepending on?",
        "context": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "compressed row indices",
        "question": "What does Thecrow_indicestensor consist of?",
        "context": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor of sizesize[0]+1",
        "question": "What is Thecrow_indicestensor?",
        "context": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the number of non-zeros",
        "question": "What is the last element of Thecrow_indicestensor?",
        "context": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "where the given row starts",
        "question": "Thecrow_indicestensor encodes the index invaluesandcol_indicesdepending on what?",
        "context": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column indices of each value",
        "question": "What does Thecol_indicestensor contain?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor of sizennz",
        "question": "What is Thecol_indicestensor?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column indices",
        "question": "What does thecol_indicestensor contain?",
        "context": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Thevaluestensor",
        "question": "What contains the values of the CSR tensor?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor of sizennz",
        "question": "What is the value of the CSR tensor?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "element type eithertorch.int64(default) ortorch.int32",
        "question": "What should the index tensorscrow_indicesandcol_indices have?",
        "context": "Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "usetorch.int32",
        "question": "What is the default linking of pytorch being with MKL LP64?",
        "context": "Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "32 bit integer indexing",
        "question": "What does MKL LP64 use?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Thecol_indicestensor",
        "question": "What contains the column indices of each value?",
        "context": "Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor",
        "question": "Thecol_indicestensor contains the column indices of each value. This is a what?",
        "context": "Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "eithertorch.int64(default) ortorch.int32",
        "question": "What should the index tensorscrow_indicesandcol_indices have element type?",
        "context": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "MKL",
        "question": "If you want to use matrix operations, usetorch.int32. This is as a result of the default linking of py",
        "context": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D",
        "question": "Thevaluestensor contains the values of the CSR tensor of sizennz.",
        "context": "Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "usetorch.int32",
        "question": "What should be used if you want to use MKL-enabled matrix operations?",
        "context": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor",
        "question": "What is the CSR tensor?",
        "context": "Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "MKL",
        "question": "If you want to use matrix operations, usetorch.int32.",
        "context": "The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "thetorch._sparse_csr_tensor()method",
        "question": "How can Sparse CSR matrices be directly constructed?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "row and column indices and values tensors",
        "question": "What must the user supply separately?",
        "context": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Thesizeargument",
        "question": "What is optional and will be deduced from thecrow_indicesandcol_indicesif it is not present?",
        "context": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "usetensor._to_sparse_csr()",
        "question": "What is the simplest way of constructing a sparse CSR tensor from a strided or sparse CO",
        "context": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "missing values",
        "question": "Any zeros in the (strided) tensor will be interpreted as what in the sparse tensor?",
        "context": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "thetorch._sparse_csr_tensor()method",
        "question": "What method can be used to construct sparse CSR matrices?",
        "context": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Thesizeargument",
        "question": "What is optional and will be deduced from thecrow_indicesandcol_indices if it is not present?",
        "context": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "missing values",
        "question": "Any zeros in the strided tensor will be interpreted as what in the sparse tensor?",
        "context": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "thetensor.matmul()method",
        "question": "What is the only math operation supported on CSR tensors?",
        "context": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CSR tensors",
        "question": "The sparse matrix-vector multiplication is currently the only math operation supported on what?",
        "context": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "math operation",
        "question": "The sparse matrix-vector multiplication is currently the only what operation supported on CSR tensors?",
        "context": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "thetorch._sparse_csr_tensor()method",
        "question": "What method can be used to construct Sparse CSR matrices?",
        "context": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "thecrow_indicesandcol_indices",
        "question": "Thesizeargument is optional and will be deduced from what if it is not present?",
        "context": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Sparse CSR matrices",
        "question": "What can be directly constructed by using thetorch._sparse_csr_tensor()method?",
        "context": "Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Linear Algebra operations",
        "question": "The following table summarizes supported what on sparse matrices?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "HereT",
        "question": "What denotes a tensor with a given layout?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix",
        "question": "What does M[layout]denote?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "float or 0-D PyTorch tensor",
        "question": "What type of scalar does fdenote?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Sparse grad?",
        "question": "What is an example of a PyTorch operation?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@V[strided]->V[strided]",
        "question": "What is the layout signature torch.mv() no?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "supported Linear Algebra operations",
        "question": "The following table summarizes what on sparse matrices where the operands layouts may vary?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor",
        "question": "HereT[layout]denotes what with a given layout?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D PyTorch tensor",
        "question": "What is a vector?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "float or 0-D PyTorch tensor",
        "question": "What is a scalar?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Sparse grad?",
        "question": "What is the name of the PyTorch operation?",
        "context": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no M[sparse_coo]@V[strided]->V[strided]",
        "question": "What is the layout signature torch.mv()?",
        "context": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. HereT[layout]denotes a tensor with a given layout. Similarly,M[layout]denotes a matrix (2-D PyTorch tensor), andV[layout]denotes a vector (1-D PyTorch tensor). In addition,fdenotes a\nscalar (float or 0-D PyTorch tensor),*is element-wise\nmultiplication, and@is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is the default value for M[sparse_coo]@V[strided]->V[strided",
        "context": "no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch operation Sparse grad",
        "question": "What is the name of the PyTorch operation Sparse grad?",
        "context": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Layout signature",
        "question": "What is a PyTorch operation Sparse grad?",
        "context": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Sparse",
        "question": "What is the name of the grad?",
        "context": "Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Layout signature",
        "question": "What is a Sparse grad?",
        "context": "Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@V[strided]->V[strided] torch",
        "question": "What does mv() no?",
        "context": "Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Layout signature",
        "question": "What does torch.mv() no M[sparse_coo]@V[strided]->V[stri",
        "context": "Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is the value of the M[sparse_coo]@V[strided]->V[strided",
        "context": "torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "M[sparse_coo]@V[strided]->V[strided] torch.mv()",
        "context": "torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is M[sparse_coo]@V[strided]->V[strided] torch?",
        "context": "no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@V[strided]->V[strided] torch",
        "question": "What does M[sparse_coo]@V[strided]->V[strided] torch do?",
        "context": "M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What does torch.mv() do?",
        "context": "torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is the value of torch.mv()?",
        "context": "torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]@V[strided]->V[strided] torch",
        "question": "What is the name of the command that does not exist?",
        "context": "no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]@V[strided]->V[strided] torch",
        "question": "What is the name of the torch that is not sparse_csr?",
        "context": "no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]->M[strided] torch",
        "question": "What is the name of the command?",
        "context": "M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]@V[strided]->V[strided] torch",
        "question": "What does M[sparse_csr]@V[strided]->V[strided] torch do",
        "context": "M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]->M[strided] torch",
        "question": "What does torch.matmul() no?",
        "context": "torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]@M[strided]->M[strided] torch",
        "question": "What does matmul() not do?",
        "context": "torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]->M[strided] torch",
        "question": "What is the name of the M[sparse_coo]@M[strided]->M[strided",
        "context": "no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]@M[strided]->M[strided] torch",
        "question": "What does matmul() no?",
        "context": "M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What is the default value of M[sparse_csr]@M[strided]->M[stride",
        "context": "torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "Is M[sparse_csr]@M[strided]->M[strided] torch?",
        "context": "no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]@M[strided]->M[strided] torch",
        "question": "What does M[sparse_csr]@M[strided]->M[strided] torch?",
        "context": "no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[strided]->M[strided] torch",
        "question": "Where is M[sparse_csr] located?",
        "context": "M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_csr]@M[strided]->M[strided] torch",
        "question": "What does M[sparse_csr]@M[strided]->M[strided] torch do",
        "context": "M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]->M[strided] torch",
        "question": "What does torch.mm() not do?",
        "context": "torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What is the name of the component that is not present at the start of a mission?",
        "context": "no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]->M[strided] torch",
        "question": "What is the name of the sparse_coo at M[strided]->M[strided] torch",
        "context": "no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]->M[strided] torch",
        "question": "What does M[sparse_coo]@M[strided]->M[strided] torch do?",
        "context": "M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]->M[sparse_coo] torch",
        "question": "What does smm() no?",
        "context": "torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What is the name of the command that is used to send a message to a target?",
        "context": "yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What is the name of the component that is at the end of a mission?",
        "context": "M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]->M[sparse_coo] torch",
        "question": "What does torch.smm() no?",
        "context": "torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What does M[sparse_coo]@M[strided]->M[sparse_coo]",
        "context": "no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What does hspmm() no M[sparse_coo]@M[strided]->M[h",
        "context": "M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "no",
        "question": "What does M[sparse_coo]@M[strided]->M[hybridsparse",
        "context": "no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]",
        "question": "What does torch.hspmm() no?",
        "context": "torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]",
        "question": "What is the name of the component that is at the end of a stride?",
        "context": "M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]",
        "question": "What does ->M[hybridsparse_coo] torch.bmm() no T[sparse_",
        "context": "M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "SVD",
        "question": "What does svd_lowrank() use?",
        "context": "f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "singular value decomposition",
        "question": "What does SVD stand for?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "no",
        "question": "What does f*M[sparse_coo]+f*(M[sparse_coo]@M",
        "context": "no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided matrix arguments",
        "question": "All PyTorch operations, excepttorch.smm(), support backward with respect to what?",
        "context": "M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What does the PyTorch operation do to support backward with respect to strided matrix arguments?",
        "context": "M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "if the PyTorch operation supports backward with respect to sparse matrix argument",
        "question": "What does the \u201cSparse grad?\u201d column indicate?",
        "context": "no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided matrix",
        "question": "All PyTorch operations, excepttorch.smm(), support backward with respect to what argument?",
        "context": "no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What do all PyTorch operations, excepttorch.smm(), support backward with respect to strided matrix arguments?",
        "context": "no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "if the PyTorch operation supports backward with respect to sparse matrix argument",
        "question": "What does the \"Sparse grad?\" column indicate?",
        "context": "f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse storage layout",
        "question": "What does the Tensor use?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.dense_dim",
        "question": "What returns the number of dense dimensions in asparse tensorself?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_dim",
        "question": "What returns the number of sparse dimensions in asparse tensorself?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse copy of the tensor",
        "question": "What does Tensor.to_sparse return?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.to_sparse",
        "question": "What returns a sparse copy of the tensor?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "The following Tensor methods are related to what tensors?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.dense_dim",
        "question": "What Returns the number of dense dimensions in asparse tensorself?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse dimensions",
        "question": "What does Tensor.sparse_dim return in asparse tensorself?",
        "context": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "newsparse tensor",
        "question": "What type of tensor does Tensor.sparse_mask return?",
        "context": "Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse storage layout",
        "question": "What does Tensor.is_sparse IsTrueif the Tensor uses?",
        "context": "Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "asparse tensorself",
        "question": "What does Tensor.sparse_dim return the number of sparse dimensions in?",
        "context": "Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "compressed row storage format",
        "question": "Tensor._to_sparse_csr Convert a tensor to what format?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "False",
        "question": "IsTrue if the Tensor uses sparse storage layout?",
        "context": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "newsparse tensor",
        "question": "What does Tensor.sparse_mask return?",
        "context": "Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "compressed row storage format",
        "question": "What format does Tensor._to_sparse_csr convert a tensor to?",
        "context": "Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse storage layout",
        "question": "IsTrue if the Tensor uses what?",
        "context": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "False",
        "question": "IsTrueif the Tensor uses sparse storage layout?",
        "context": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor._to_sparse_csr",
        "question": "What Converts a tensor to compressed row storage format?",
        "context": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.indices",
        "question": "What does a tensor use to convert a tensor to compressed row storage format?",
        "context": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.indices",
        "question": "What returns the indices tensor of asparse COO tensor?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What return the number of sparse dimensions in asparse tensorself?",
        "context": "Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "newsparse tensor",
        "question": "What tensor with values from a strided tensorselffiltered by the indices of the sparse ten",
        "context": "Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor._to_sparse_csr",
        "question": "What Convert a tensor to compressed row storage format?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "asparse tensorself",
        "question": "Return the number of dense dimensions in what?",
        "context": "Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse dimensions",
        "question": "Return the number of what in asparse tensorself?",
        "context": "Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.values",
        "question": "What returns the values tensor of asparse COO tensor?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_dim",
        "question": "What Returns the number of sparse dimensions in asparse tensorself?",
        "context": "Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "newsparse tensor",
        "question": "What tensor with values from a strided tensorself filtered by the indices of the sparse ",
        "context": "Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.indices",
        "question": "What Returns the indices tensor of asparse COO tensor?",
        "context": "Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.values",
        "question": "What Returns the values tensor of asparse COO tensor?",
        "context": "Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "asparse tensorself",
        "question": "Return the number of sparse dimensions in what?",
        "context": "Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.coalesce",
        "question": "What Tensor method is specific to sparse COO tensors?",
        "context": "Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "newsparse tensor",
        "question": "What returns a strided tensorselffiltered by the indices of the sparse tensormask?",
        "context": "Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.coalesce",
        "question": "What returns a coalesced copy ofselfifselfis anuncoalesced tensor?",
        "context": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.to_sparse",
        "question": "What Returns a sparse copy of the tensor?",
        "context": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced copy",
        "question": "What type of tensor does Tensor.coalesce return?",
        "context": "Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Resizesselfsparse tensorto",
        "question": "What does Tensor.sparse_resize_ do?",
        "context": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize",
        "question": "What _ Resizes selfsparse tensorto the desired size and the number of sparse and dense dimensions?",
        "context": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What type of copy of the tensor is returned?",
        "context": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions",
        "question": "What does Tensor.sparse_resize do?",
        "context": "Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse copy",
        "question": "What type of tensor does Tensor._to_sparse_csr return?",
        "context": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What is the name of the Tensor method that returns the desired size and the number of sparse and dense dimensions?",
        "context": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize",
        "question": "What _ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What is the name of the tensor method that resizes itselfsparse tensorto?",
        "context": "Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "compressed row storage format",
        "question": "What format can a tensor be converted to?",
        "context": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "selfifselfis",
        "question": "What is returned as a coalesced copy of an uncoalesced tensor?",
        "context": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What is another name for sparse tensorto?",
        "context": "Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What returns the desired size and the number of sparse and dense dimensions?",
        "context": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "selfifselfis",
        "question": "What is anuncoalesced tensor?",
        "context": "Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "values",
        "question": "What is the tensor of asparse COO tensor?",
        "context": "Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Return the values tensor",
        "question": "What do asparse COO tensor methods return?",
        "context": "Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What removes all specified elements from asparse tensorselfand resizes itselfto the desired size and the number of spar",
        "context": "Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the desired size and the number of sparse and dense dimensions",
        "question": "What does tensor.sparse_resize_and_clear_ Resizesselfto?",
        "context": "Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Removes all specified elements",
        "question": "What does Tensor.sparse_resize_and_clear_ do?",
        "context": "Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What removes all specified elements from asparse tensorselfand resizesselfto the desired size and number of spars",
        "context": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced",
        "question": "What type of copy of self does Tensor.coalesce return?",
        "context": "Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced",
        "question": "What returns true if selfis asparse COO tensorthat is coalesced?",
        "context": "Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided",
        "question": "What type of copy of self does Tensor.to_dense create?",
        "context": "Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced copy ofselfifselfis anuncoalesced tensor",
        "question": "What does Tensor.coalesce return?",
        "context": "Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize",
        "question": "What resizes selfsparse tensorto the desired size and the number of sparse and dense dimensions?",
        "context": "Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Trueifselfis",
        "question": "What does Tensor.is_coalesced return?",
        "context": "Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided copy ofself",
        "question": "What does Tensor.to_dense create?",
        "context": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced",
        "question": "What type of copy of self is returned if selfis anuncoalesced tensor?",
        "context": "Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Trueifselfis asparse COO tensorthat",
        "question": "What is returned if selfis anuncoalesced tensor?",
        "context": "Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "current random seed of the current GPU",
        "question": "Returns what?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of spar",
        "context": "Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize",
        "question": "What resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions?",
        "context": "Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Trueifselfis",
        "question": "What returns asparse COO tensorthat is coalesced?",
        "context": "Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.crow_indices",
        "question": "What method is specific tosparse CSR tensors?",
        "context": "Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Resizesselfsparse tensorto",
        "question": "What does it do to the desired size and the number of sparse and dense dimensions?",
        "context": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the desired size and the number of sparse and dense dimensions",
        "question": "What does Tensor.sparse_resize_and_clear_ resizesselfto?",
        "context": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced",
        "question": "What is the name of the method that returns true if selfis asparse COO tensorthat is coalesced?",
        "context": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "specific tosparse CSR tensors",
        "question": "What are the following methods?",
        "context": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the desired size and the number of sparse and dense dimensions",
        "question": "What does tensor.sparse_resize_and_clear_ resize selfto?",
        "context": "Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor containing the compressed row indices of theselftensor",
        "question": "What is returned when selfis a sparse CSR tensor of layoutsparse_csr?",
        "context": "Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor containing the compressed row indices of theselftensor",
        "question": "What does tensor.crow_indices return when selfis a sparse CSR tensor of layoutspar",
        "context": "Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column indices of theselftensor",
        "question": "What does the tensor return when selfis a sparse CSR tensor of layoutsparse_cs",
        "context": "Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Trueifselfis",
        "question": "What is returned when a COO tensorthat is coalesced?",
        "context": "Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse CSR tensor",
        "question": "When selfis a sparse CSR tensor of layoutsparse_csr, what is it?",
        "context": "ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced",
        "question": "ReturnsTrueifselfis asparse COO tensorthat is what?",
        "context": "ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.to_dense",
        "question": "What creates a strided copy of self?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.crow_indices",
        "question": "What returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR",
        "context": "Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column",
        "question": "What indices of theselftensor is returned when selfis a sparse CSR tensor of layoutspars",
        "context": "Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO tensors",
        "question": "The following Tensor methods support sparse what?",
        "context": "The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse COO tensors",
        "question": "What do the following Tensor methods support?",
        "context": "Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided",
        "question": "What type of copy of self is created?",
        "context": "Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column indices of theselftensor",
        "question": "What does the tensor return whenselfis a sparse CSR tensor of layoutsparse_cs",
        "context": "Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided copy",
        "question": "Creates what type of self?",
        "context": "Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column indices of theselftensor",
        "question": "What does the Tensor.col_indices return whenselfis a sparse CSR tensor of layoutspars",
        "context": "Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column",
        "question": "What type of indices does theselftensor return when selfis a sparse CSR tensor of layoutspar",
        "context": "The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the tensor containing the compressed row indices of theselftensor",
        "question": "What does Tensor.crow_indices return when selfis a sparse CSR tensor of layoutspars",
        "context": "Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column",
        "question": "What indices does theselftensor return when selfis a sparse CSR tensor of layoutsparse",
        "context": "Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor containing the column indices of theselftensor",
        "question": "What does Tensor.col_indices return whenselfis a sparse CSR tensor of layoutsparse",
        "context": "Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor methods",
        "question": "What methods support sparse COO tensors?",
        "context": "Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the tensor",
        "question": "What returns the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layouts",
        "context": "Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "layoutsparse_csr",
        "question": "Selfis a sparse CSR tensor of what?",
        "context": "Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "theselftensor",
        "question": "Whenselfis a sparse CSR tensor of layoutsparse_csr?",
        "context": "Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the tensor containing the column indices",
        "question": "What does the selftensor return when selfis a sparse CSR tensor of layoutsparse_cs",
        "context": "Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO(rdinate) format",
        "question": "What format is the asparse tensor in?",
        "context": "Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CSR",
        "question": "In what format is the asparse tensor constructed?",
        "context": "  Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.   Performs a matrix multiplication of asparse COO matrixmat1and a strided matrixmat2.   Performs a matrix multiplication of the sparse matrixinputwith the dense matrixmat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sum of each row of the sparse tensorinputin the given dimensionsdim",
        "question": "What does sparse.sum return?",
        "context": "  Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.   Performs a matrix multiplication of asparse COO matrixmat1and a strided matrixmat2.   Performs a matrix multiplication of the sparse matrixinputwith the dense matrixmat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "backward",
        "question": "What does the sparse.addmm function support for sparse matrixmat1?",
        "context": "  Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.   Performs a matrix multiplication of asparse COO matrixmat1and a strided matrixmat2.   Performs a matrix multiplication of the sparse matrixinputwith the dense matrixmat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse.mm",
        "question": "What is the name of the function that supports backward for sparse matrixmat1?",
        "context": "Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "givencrow_indicesandcol_indices",
        "question": "Where are specified values in the asparse tensor?",
        "context": "Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "backward",
        "question": "The function sparse.addmm supports which direction for sparse matrixmat1?",
        "context": "Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix multiplication",
        "question": "What does sparse.mm perform of the sparse matrixmat1 and the (sparse or strided) matrixmat2",
        "context": "  Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.   Performs a matrix multiplication of asparse COO matrixmat1and a strided matrixmat2.   Performs a matrix multiplication of the sparse matrixinputwith the dense matrixmat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What returns the sum of each row of the sparse tensorinputin the given dimensionsdim?",
        "context": "sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "astorch.addmm()",
        "question": "What does sparse.addmm do in the forward?",
        "context": "Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix multiplication",
        "question": "What does sparse.mm perform?",
        "context": "Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Matrix",
        "question": "What multiplies a sparse tensormat1with a dense tensormat2?",
        "context": "  Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices.   Constructs asparse tensor in CSR (Compressed Sparse Row)with specified values at the givencrow_indicesandcol_indices. sparse.sum Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.   Performs a matrix multiplication of asparse COO matrixmat1and a strided matrixmat2.   Performs a matrix multiplication of the sparse matrixinputwith the dense matrixmat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sum of each row of the sparse tensorinputin the given dimensionsdim",
        "question": "What is the return value of the sparse tensorinput?",
        "context": "Returns the sum of each row of the sparse tensorinputin the given dimensionsdim. sparse.addmm This function does exact same thing astorch.addmm()in the forward, except that it supports backward for sparse matrixmat1. sparse.mm Performs a matrix multiplication of the sparse matrixmat1and the (sparse or strided) matrixmat2.   Matrix multiplies a sparse tensormat1with a dense tensormat2, then adds the sparse tensorinputto the result.   ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensors",
        "question": "What do the followingtorchfunctions support?",
        "context": "The followingtorchfunctions support sparse tensors: cat()dstack()empty()empty_like()hstack()index_select()is_complex()is_floating_point()is_nonzero()is_same_size()is_signed()is_tensor()lobpcg()mm()native_norm()pca_lowrank()select()stack()svd_lowrank()unsqueeze()vstack()zeros()zeros_like() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "totorch.full",
        "question": "What is the equivalent of totorch.full_like?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "input(Tensor)",
        "question": "What determines the size of the output tensor?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "fill_value",
        "question": "What is the number to fill the output tensor with?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "dtype",
        "question": "What is the desired data type of returned Tensor?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "ifNone",
        "question": "What defaults to the dtype ofinput?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "a tensor",
        "question": "Returns what with the same size as inputfilled withfill_value.torch.full_like(input,fill_value)",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "the desired data type of returned Tensor",
        "question": "What is dtype(torch.dtype, optional)?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "defaults to the dtype ofinput",
        "question": "What does ifNone do?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "layout",
        "question": "What is the desired layout of returned tensor?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "ifNone",
        "question": "What defaults to the layout of input?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "device(torch.device, optional)",
        "question": "What is the desired device of returned tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "ifNone",
        "question": "What defaults to the device ofinput?",
        "context": "layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "ifNone",
        "question": "What defaults to the dtype of input?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "defaults to the device ofinput",
        "question": "What does ifNone default to?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "ifNone",
        "question": "What defaults to the layout ofinput?",
        "context": "layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "the device ofinput",
        "question": "Default: ifNone, defaults to what?",
        "context": "fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "autograd",
        "question": "What should record operations on the returned tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "False",
        "question": "What is the default value for autograd to record operations on the returned tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "ifNone",
        "question": "What defaults to the device of input?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "False",
        "question": "What is the default setting for autograd to record operations on the returned tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "memory_format",
        "question": "What is the desired memory format of returned Tensor?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "Default:torch.preserve_format",
        "question": "What is the default memory format of returned Tensor?",
        "context": "layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "False",
        "question": "If autograd should record operations on the returned tensor, what is the default?",
        "context": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Default:torch.preserve_format",
        "question": "What is the default memory format of returned tensor?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "Short-time Fourier transform",
        "question": "What is STFT?",
        "context": "Short-time Fourier transform (STFT). Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1.8.0",
        "question": "What is the latest version of STFT?",
        "context": "Short-time Fourier transform (STFT). Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "pytorch release",
        "question": "When will return_complex=Trueas be used?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "thattorch.view_as_real()",
        "question": "What can be used to recover a real tensor with an extra last dimension for real and imaginary components?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1.8.0",
        "question": "What version of pytorch does return_complex need to be given explicitly for real inputs?",
        "context": "Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex=Trueas",
        "question": "What function will only return complex tensors?",
        "context": "Short-time Fourier transform (STFT). Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "to recover a real tensor with an extra last dimension",
        "question": "What cantorch.view_as_real() be used for?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1.8.0",
        "question": "What version of pytorch does return_complex always have to be given explicitly for real inputs?",
        "context": "Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return complex tensors",
        "question": "What does return_complex=Trueas do in a future pytorch release?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1.8.0",
        "question": "In what version of Python must return_complex always be given explicitly for real inputs?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "thattorch.view_as_real()",
        "question": "What function can be used to recover a real tensor with an extra last dimension for real and imaginary components?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1.8.0",
        "question": "From what version of pytorch did return_complex always be given explicitly for real inputs?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Fourier transform",
        "question": "What does the STFT compute of short overlapping windows of the input?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "frequency components of the signal",
        "question": "What does the Fourier transform of short overlapping windows of the input give?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "thelibrosastft function",
        "question": "The interface of the STFT is modeled after what?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the optional batch dimension",
        "question": "Ignoring what, the STFT computes the following expression?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "real tensor",
        "question": "What can Torch.view_as_real() be used to recover?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "frequency components",
        "question": "The Fourier transform of short overlapping windows of the input gives what of the signal as they change over time?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the following expression",
        "question": "Ignoring the optional batch dimension, this method computes what?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "The STFT",
        "question": "What computes the Fourier transform of short overlapping windows of the input?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "wheremmmis the index of the sliding window",
        "question": "What does this method compute?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "Input must be either a 1-D time sequence or a 2-D batch of time sequences?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Ifhop_lengthisNone",
        "question": "What is treated as equal tofloor(n_fft/4)?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the index of the sliding window",
        "question": "What is wheremmmis?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "Input must be either a what?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Ifhop_lengthisNone",
        "question": "What is the default value for a time sequence?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "tofloor(n_fft/4)",
        "question": "Ifhop_lengthisNone(default), it is treated as equal what?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ton_fft",
        "question": "Ifwin_lengthisNone(default), it is treated as equal what?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "What type of input must be used?",
        "context": "inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "floor(n_fft/4)",
        "question": "Ifhop_lengthisNone(default), it is treated as equal to what?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Ifwin_lengthisNone",
        "question": "What is treated as equal ton_fft?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D tensor of sizewin_length",
        "question": "What can a window be?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "if having111everywhere in the window",
        "question": "IfwindowisNone(default), what is it treated as?",
        "context": "Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "lengthn_fft",
        "question": "Ifwin_lengthn_ffttextwin_lengthn_fft,windowwill be padded on",
        "context": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "What must input be?",
        "context": "inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "having111everywhere in the window",
        "question": "IfwindowisNone(default), it is treated as if what?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "lengthn_fft",
        "question": "What is the window padded on both sides to before being applied?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Ifhop_lengthisNone",
        "question": "What is the default value for a window to be treated as equal tofloor(n_fft/4)?",
        "context": "Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D tensor",
        "question": "What can a window be of sizewin_length?",
        "context": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "equal ton_fft",
        "question": "Ifwin_lengthisNone(default), it is treated as what?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D",
        "question": "What is the tensor of sizewin_length?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "equal ton_fft",
        "question": "Ifwin_lengthisNone(default), what is it treated as?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "lengthn_fft",
        "question": "Ifwin_lengthisNone(default), windowwill be padded on both sides to what before being applied?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D",
        "question": "What tensor of sizewin_length can a window be?",
        "context": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfwindowisNone",
        "question": "What is treated as if having111everywhere in the window?",
        "context": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "lengthn_fft",
        "question": "Ifwin_lengthn_ffttextwin_lengthn_fftwin_lengthn_ff",
        "context": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfwindowisNone",
        "question": "What is the default value of a window?",
        "context": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfcenterisTrue",
        "question": "Input will be padded on both sides so that thettt-th frame is centered at what?",
        "context": "IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "pad_mode",
        "question": "What determines the padding method used oninputwhencenterisTrue?",
        "context": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Seetorch.nn.functional.pad()",
        "question": "What is the name of the padding method used oninputwhencenterisTrue?",
        "context": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "reflect",
        "question": "What is the default value of the padding method used oninputwhencenterisTrue?",
        "context": "Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Seetorch.nn.functional.pad()",
        "question": "What is the name of the method that determines the padding method used oninputwhencenterisTrue?",
        "context": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "\"reflect\"",
        "question": "What is the default value of pad_mode?",
        "context": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "reflect",
        "question": "What is the default padding method used oninputwhencenterisTrue?",
        "context": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfonesidedisTrue",
        "question": "What is the default for real input?",
        "context": "IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "if the input or window tensors are complex",
        "question": "What makes onesidedoutput not possible?",
        "context": "IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "onesidedoutput",
        "question": "What is not possible if the input or window tensors are complex?",
        "context": "IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "normalized STFT results",
        "question": "What does IfnormalizedisTrue return?",
        "context": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ainput.dim()+1dimensional complex tensor",
        "question": "What is the return value of Ifreturn_complexisTrue?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ainput.dim()+2dimensional real tensor",
        "question": "What is the output of IfFalse?",
        "context": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfnormalizedisTrue",
        "question": "What function returns the normalized STFT results?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ainput.dim()+1dimensional complex tensor",
        "question": "Ifreturn_complexisTrue(default if input is complex) the return is what?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the last dimension",
        "question": "What represents the real and imaginary components?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ainput.dim()+1dimensional complex tensor",
        "question": "If return_complexisTrue, what is the return?",
        "context": "Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Ifreturn_complexisTrue",
        "question": "What returns ainput.dim()+1dimensional complex tensor?",
        "context": "Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfFalse",
        "question": "What is the default return value for ainput.dim()+2dimensional real tensor?",
        "context": "Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ainput.dim()+1dimensional complex tensor",
        "question": "If return_complexisTrue(default if input is complex) what is the return?",
        "context": "Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Ifreturn_complexisTrue",
        "question": "What is the default if input is complex?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "a complex tensor of size",
        "question": "What does return if return_complexis true?",
        "context": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the number of frequencies where STFT is applied",
        "question": "What is NNN?",
        "context": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "version 0.4.1",
        "question": "When did this function change signature?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Calling with the previous signature",
        "question": "What may cause error or return incorrect result?",
        "context": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "a real tensor of size",
        "question": "Returns either a complex tensor of size(NT)(* times N times T ",
        "context": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the total number of frames",
        "question": "What does TTT represent?",
        "context": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "incorrect result",
        "question": "Calling with the previous signature may cause error or return what?",
        "context": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "0.4.1",
        "question": "What version of the function changed signature?",
        "context": "Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Calling with the previous signature may cause error or return incorrect result",
        "question": "What is a warning when calling with the previous signature?",
        "context": "Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Fourier transform hop_length",
        "question": "What is the size of the distance between neighboring sliding window frames?",
        "context": "n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "win_length",
        "question": "What is the size of window frame and STFT filter?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ton_fft",
        "question": "Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) ",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "error or return incorrect result",
        "question": "Calling with the previous signature may cause what?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the distance between neighboring sliding window frames",
        "question": "What is hop_length?",
        "context": "input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Calling with the previous signature may cause error or return incorrect result",
        "question": "What can happen if a function is called with a previous signature?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "optional window function",
        "question": "What is window(Tensor,optional)?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "all111s",
        "question": "Default:None(treated as window of what number) \u2013 the optional window function?",
        "context": "n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "window(Tensor,optional)",
        "question": "What is the optional window function?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "n_fft",
        "question": "What is the size of Fourier transform hop_length?",
        "context": "n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "hop_length",
        "question": "What is the distance between neighboring sliding window frames?",
        "context": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Default:None",
        "question": "What is the default window of all111s?",
        "context": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "True",
        "question": "What is the default value of the window function?",
        "context": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "padinputon",
        "question": "What is used to center thetttt-th frame?",
        "context": "n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "whencenterisTrue",
        "question": "Default:True pad_mode(string,optional) \u2013 controls the padding method used what?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "\"reflect\"",
        "question": "What is the default name of the padding method used whencenterisTrue?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Default:None",
        "question": "What is the default window function of all111s?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "whencenterisTrue",
        "question": "What controls the padding method used?",
        "context": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Default:\"reflect\"",
        "question": "What is the default whencenterisTrue?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Default:None",
        "question": "What is the default window function?",
        "context": "window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "whencenterisTrue",
        "question": "What is the padding method used?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Default:\"reflect\" normalized",
        "question": "What controls whether to return the normalized STFT results?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "optional",
        "question": "Window(Tensor) is what type of window function?",
        "context": "window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "normalized STFT results",
        "question": "Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return what?",
        "context": "center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "whether to padinputon both sides",
        "question": "What is center(bool,optional)?",
        "context": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "whencenterisTrue",
        "question": "What is the default pad_mode?",
        "context": "center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "pad_mode",
        "question": "What controls the padding method used whencenterisTrue?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Default:False onesided",
        "question": "What is the default to return the normalized STFT results?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "realinputandwindow",
        "question": "Default:True for what?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex",
        "question": "What is a tensor with an extra last dimension for the real and imaginary components?",
        "context": "pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "half",
        "question": "Default:False onesided(bool,optional) \u2013 controls whether to return how many results to avoid redundancy for real",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Truefor realinputandwindow",
        "question": "What is the default value for the return of half of results?",
        "context": "normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex",
        "question": "What returns a complex tensor or a real tensor with an extra last dimension for the real and imaginary components?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "shape",
        "question": "A tensor containing the STFT result with what described above Tensor?",
        "context": "normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "redundancy",
        "question": "What does false onesided avoid for real inputs?",
        "context": "normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Default:Truefor realinputandwindow,Falseotherwise",
        "question": "What controls whether to return half of results to avoid redundancy for real inputs?",
        "context": "normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex",
        "question": "What is the name of the tensor that returns a complex tensor?",
        "context": "normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "containing the STFT result with shape described above Tensor",
        "question": "What is a tensor?",
        "context": "normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "NumPy\u2019s gradient function",
        "question": "What is this function analogous to?",
        "context": "This function is analogous to NumPy\u2019s gradient function. {input}\u2013 spacing(scalar,list of scalar,list of Tensor,optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at(the) \u2013 dim(int,list of python:int,optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order(int,optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "answer": "the dimension or dimensions to approximate the gradient over",
        "question": "What is dim(int,list of python:int,optional)?",
        "context": "This function is analogous to NumPy\u2019s gradient function. {input}\u2013 spacing(scalar,list of scalar,list of Tensor,optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at(the) \u2013 dim(int,list of python:int,optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order(int,optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "answer": "implicitly or explicitly represents coordinates",
        "question": "What does spacing(scalar,list of scalar,list of Tensor,optional) represent?",
        "context": "This function is analogous to NumPy\u2019s gradient function. {input}\u2013 spacing(scalar,list of scalar,list of Tensor,optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at(the) \u2013 dim(int,list of python:int,optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order(int,optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "answer": "unsupported",
        "question": "What is edge_order?",
        "context": "This function is analogous to NumPy\u2019s gradient function. {input}\u2013 spacing(scalar,list of scalar,list of Tensor,optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at(the) \u2013 dim(int,list of python:int,optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order(int,optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "answer": "Example",
        "question": "What is the default value of edge_order?",
        "context": "This function is analogous to NumPy\u2019s gradient function. {input}\u2013 spacing(scalar,list of scalar,list of Tensor,optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at(the) \u2013 dim(int,list of python:int,optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order(int,optional) \u2013 unsupported (must be equal to its default value which is 1.) Example ",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "answer": "Count the frequency of each value in an array of non-negative ints",
        "question": "What is one way to count the frequency of each value in an array of non-negative ints?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "size 1",
        "question": "What is the size of the number of bins in an array of non-negative ints?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "zeros",
        "question": "If the number of bins is at leastminlength and ifinputis empty, then the result is tensor of sizeminlength",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "positioni",
        "question": "Ifnis the value at what position, out[n]+=weights[i]ifweightsis specified elseout[n]+",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "Note",
        "question": "What is the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "Count the frequency",
        "question": "What is the frequency of each value in an array of non-negative ints?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "size 1",
        "question": "The number of bins is one larger than the largest value ininputunlessinputis empty, in which case the result is a ten",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "ifinputis empty",
        "question": "Ifminlengthis specified, the number of bins is at leastminlengthand what?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1",
        "question": "Ifnis the value at what?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "size 1",
        "question": "What is the size of the bins?",
        "context": "The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "tensor of sizeminlengthfilled with zeros",
        "question": "Ifminlengthis specified, the number of bins is at leastminlength and ifinputis empty, what is the result?",
        "context": "The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "positioni",
        "question": "Ifnis the value at what position?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "CUDA device",
        "question": "What device may produce nondeterministic gradients when given tensors?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "SeeReproducibility",
        "question": "Where can you find more information about this operation?",
        "context": "The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "Ifminlengthis specified",
        "question": "If the number of bins is at leastminlength and ifinputis empty, then the result is a tensor of size",
        "context": "The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1",
        "question": "Ifnis the value at what positioni,out[n]+=weights[i]ifweightsis specified elseout[n]",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "nondeterministic gradients",
        "question": "This operation may produce what when given tensors on a CUDA device?",
        "context": "The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "Reproducibility",
        "question": "What is the name of the operation that may produce nondeterministic gradients when given tensors on a CUDA device?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "1-d int tensor weights",
        "question": "What is input(Tensor)?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "same size",
        "question": "What should the input tensor be of?",
        "context": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "minlength(int)",
        "question": "What is the minimum number of bins?",
        "context": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "non-negative",
        "question": "What should minlength(int) be?",
        "context": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "shape",
        "question": "What is a tensor of?",
        "context": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "CUDA",
        "question": "When given tensors on what device may produce nondeterministic gradients?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "optional",
        "question": "What is the weight for each value in the input tensor?",
        "context": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "minlength",
        "question": "What is optional, minimum number of bins?",
        "context": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "ifinputis non-empty",
        "question": "What is a tensor of shapeSize([max(input)+1])?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "random number generator",
        "question": "Sets what state?",
        "context": "Sets the random number generator state. new_state(torch.ByteTensor) \u2013 The desired state ",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_rng_state.html#torch.set_rng_state"
    },
    {
        "answer": "new_state",
        "question": "What is the desired state of the random number generator?",
        "context": "Sets the random number generator state. new_state(torch.ByteTensor) \u2013 The desired state ",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_rng_state.html#torch.set_rng_state"
    },
    {
        "answer": "Alias oftorch.vstack()",
        "question": "What is the name of the file that is used by Alias?",
        "context": "Alias oftorch.vstack(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.row_stack.html#torch.row_stack"
    },
    {
        "answer": "Alias oftorch.vstack()",
        "question": "What is the name of the function that is used to create a vstack?",
        "context": "Alias oftorch.vstack(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.row_stack.html#torch.row_stack"
    },
    {
        "answer": "bymeanandstd",
        "question": "Fillsselftensor with elements samples from the normal distribution parameterized what?",
        "context": "Fillsselftensor with elements samples from the normal distribution\nparameterized bymeanandstd. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"
    },
    {
        "answer": "Fillsselftensor",
        "question": "What is filled with elements samples from the normal distribution parameterized bymeanandstd?",
        "context": "Fillsselftensor with elements samples from the normal distribution\nparameterized bymeanandstd. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"
    },
    {
        "answer": "Constructs a tensor withdata",
        "question": "What does torch.tensor() do?",
        "context": "Constructs a tensor withdata. Warning torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "torch.tensor()",
        "question": "What always copiesdata?",
        "context": "torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "NumPyndarray",
        "question": "What does torch.tensor() use if you want to avoid a copy of data?",
        "context": "Constructs a tensor withdata. Warning torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "Tensordata",
        "question": "What does torch.tensor()always copiesdata?",
        "context": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "a tensor",
        "question": "What does torch.tensor() construct withdata?",
        "context": "Constructs a tensor withdata. Warning torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "NumPyndarray",
        "question": "What type of tensor do you want to avoid a copy of?",
        "context": "Constructs a tensor withdata. Warning torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "NumPyndarray",
        "question": "What does torch.tensor use if you want to avoid a copy?",
        "context": "Warning torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "NumPyndarray",
        "question": "What does torch.tensor.as_tensor() use if you want to avoid a copy?",
        "context": "torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "Warning",
        "question": "What does torch.tensor do if you have a NumPyndarray and want to avoid a copy?",
        "context": "torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "change itsrequires_gradflag, userequires_grad_()ordetach()",
        "question": "What do you need to do to avoid a copy of a Tensordata?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "NumPyndarray",
        "question": "What does torch.as_tensor() use if you want to avoid a copy?",
        "context": "torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "Warning",
        "question": "What does torch.tensor.as_tensor() do?",
        "context": "torch.tensor()always copiesdata. If you have a Tensordataand want to avoid a copy, usetorch.Tensor.requires_grad_()ortorch.Tensor.detach().\nIf you have a NumPyndarrayand want to avoid a copy, usetorch.as_tensor(). Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "leaf variable",
        "question": "What does Torch.tensor() construct when data is a tensorx?",
        "context": "When data is a tensorx,torch.tensor()reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Thereforetorch.tensor(x)is equivalent tox.clone().detach()andtorch.tensor(x,requires_grad=True)is equivalent tox.clone().detach().requires_grad_(True).\nThe equivalents usingclone()anddetach()are recommended. data(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "tox.clone().detach()",
        "question": "What is the equivalent totorch.tensor(x)?",
        "context": "When data is a tensorx,torch.tensor()reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Thereforetorch.tensor(x)is equivalent tox.clone().detach()andtorch.tensor(x,requires_grad=True)is equivalent tox.clone().detach().requires_grad_(True).\nThe equivalents usingclone()anddetach()are recommended. data(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "usingclone()anddetach()",
        "question": "What are the equivalents of clone() and detach()?",
        "context": "When data is a tensorx,torch.tensor()reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Thereforetorch.tensor(x)is equivalent tox.clone().detach()andtorch.tensor(x,requires_grad=True)is equivalent tox.clone().detach().requires_grad_(True).\nThe equivalents usingclone()anddetach()are recommended. data(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "indices",
        "question": "What is the initial data for the tensor?",
        "context": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to atorch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "scalar",
        "question": "What can be a list, tuple, NumPyndarray, NumPyndarray, and other types",
        "context": "When data is a tensorx,torch.tensor()reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Thereforetorch.tensor(x)is equivalent tox.clone().detach()andtorch.tensor(x,requires_grad=True)is equivalent tox.clone().detach().requires_grad_(True).\nThe equivalents usingclone()anddetach()are recommended. data(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "dtype",
        "question": "What is the desired data type of returned tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "ifNone",
        "question": "What infers data type fromdata?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, infers data type fromdata. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "ifNone",
        "question": "What default uses the current device for the default tensor type?",
        "context": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "ifNone",
        "question": "What default infers data type fromdata?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, infers data type fromdata. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "current device",
        "question": "What does ifNone use for the default tensor type?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "pin_memory",
        "question": "What is the name of the pinned memory that is allocated to the returned tensor?",
        "context": "requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. pin_memory(bool,optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default:False. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "CPU tensors",
        "question": "What does pin_memory only work for?",
        "context": "requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. pin_memory(bool,optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default:False. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "answer": "ifNone",
        "question": "What is the default value of the returned tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "column coordinates",
        "question": "What does the second row of the arowbycolmatrix contain?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "rows and then columns",
        "question": "Indices are ordered based on what?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "the elements on and above the diagonal",
        "question": "What is the upper triangular part of the matrix defined as?",
        "context": "The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "2-by-N Tensor",
        "question": "Returns the indices of the upper triangular part of arowbycolmatrix in what format?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "the elements on and above the diagonal",
        "question": "What is the upper triangular part of arowbycolmatrix defined as?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "CUDA",
        "question": "When running on what platform must row*col be less than259259259 to prevent overflow during calculation?",
        "context": "When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "row(int)",
        "question": "What is the name of the number of rows in the 2-D matrix?",
        "context": "row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "col(int)",
        "question": "What is the name of the number of columns in the 2-D matrix?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "offset(int)",
        "question": "What is the diagonal offset from the main diagonal?",
        "context": "Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Default",
        "question": "What is the default value for offset(int) if not provided?",
        "context": "Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Default",
        "question": "What is ifNone,torch.long?",
        "context": "row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "row(int)",
        "question": "What is the number of rows in the 2-D matrix?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "offset(int)",
        "question": "What is the name of the diagonal offset from the main diagonal?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "if not provided",
        "question": "What is the default value of offset(int)?",
        "context": "offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "if not provided, 0. dtype",
        "question": "What is the default data type of returned tensor?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Default",
        "question": "What is the default if not provided?",
        "context": "row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "ifNone",
        "question": "What uses the current device for the default tensor type?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "col(int)",
        "question": "What is the number of columns in the 2-D matrix?",
        "context": "col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Default",
        "question": "If not provided, what is the default?",
        "context": "col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Default",
        "question": "What is the default value of offset(int) if not provided?",
        "context": "offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "currently only supporttorch.strided",
        "question": "What is the layout of a tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Example:",
        "question": "What is an example of a tensor layout?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "dividend and divisor",
        "question": "What may contain both integer and floating point numbers?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "The remainder",
        "question": "What has the same sign as the divisorother?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisorother. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "common shape,type promotion, and integer and float inputs",
        "question": "Supportsbroadcasting to what?",
        "context": "Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "Complex inputs",
        "question": "What inputs are not supported?",
        "context": "Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "not mathematically possible",
        "question": "In some cases, it is what to satisfy the definition of a modulo operation with complex numbers?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library functionfmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "Seetorch.fmod()",
        "question": "What is used for how division by zero is handled?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisorother. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "Tensor",
        "question": "What is the input for the dividend other(TensororScalar)?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisorother. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "Seetorch.fmod()",
        "question": "What is the name of the function that handles division by zero?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library functionfmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "the output tensor",
        "question": "Input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(T",
        "context": "Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "Complex inputs",
        "question": "What is not supported?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library functionfmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "it is not mathematically possible to satisfy the definition of a modulo operation with complex numbers",
        "question": "Why are complex inputs not supported?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library functionfmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "torch.fmod()",
        "question": "What computes the element-wise remainder of division equivalently to the C library functionfmod()?",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library functionfmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "the output tensor",
        "question": "The input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(T",
        "context": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSeetorch.fmod()for how division by zero is handled. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library functionfmod(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "answer": "a random number on all GPUs",
        "question": "Sets the seed for generating random numbers to what?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "64",
        "question": "What is the bit number used to seed the RNG?",
        "context": "Sets the seed for generating random numbers to a non-deterministic\nrandom number. Returns a 64 bit number used to seed the RNG. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.seed.html#torch.seed"
    },
    {
        "answer": "Alias fortorch.div()",
        "question": "What is the name of the file?",
        "context": "Alias fortorch.div(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide"
    },
    {
        "answer": "Alias fortorch.div()",
        "question": "What is another name for fortorch.div?",
        "context": "Alias fortorch.div(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide"
    },
    {
        "answer": "intraop parallelism",
        "question": "What is the number of threads used for on a CPU?",
        "context": "Sets the number of threads used for intraop parallelism on CPU. Warning To ensure that the correct number of threads is used, set_num_threads\nmust be called before running eager, JIT or autograd code. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_num_threads.html#torch.set_num_threads"
    },
    {
        "answer": "set_num_threads",
        "question": "What must be called before running eager, JIT or autograd code?",
        "context": "Sets the number of threads used for intraop parallelism on CPU. Warning To ensure that the correct number of threads is used, set_num_threads\nmust be called before running eager, JIT or autograd code. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_num_threads.html#torch.set_num_threads"
    },
    {
        "answer": "the number of threads",
        "question": "What does set set for intraop parallelism on CPU?",
        "context": "Sets the number of threads used for intraop parallelism on CPU. Warning To ensure that the correct number of threads is used, set_num_threads\nmust be called before running eager, JIT or autograd code. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_num_threads.html#torch.set_num_threads"
    },
    {
        "answer": "2-D tensor with ones on the diagonal and zeros elsewhere",
        "question": "What type of tensor is returned?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "n(int)",
        "question": "What is the name of the number of rows in a 2-D tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "ifNone",
        "question": "What uses a global default?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "layout",
        "question": "What is the desired layout of returned Tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "Default:torch.strided",
        "question": "What is the default for the layout of the returned Tensor?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "zeros",
        "question": "Returns a 2-D tensor with what else?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "n",
        "question": "What is the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "Default:torch.strided",
        "question": "What is the default layout of returned tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "the number of rows m(int,optional)",
        "question": "What does n(int) mean?",
        "context": "n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "Default:torch.strided",
        "question": "What is the default layout of returned Tensor?",
        "context": "layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "m",
        "question": "What is the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor?",
        "context": "m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "output tensor",
        "question": "What is the default beingn out(Tensor,optional)?",
        "context": "m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "Default:torch.strided",
        "question": "What is the default layout of the returned Tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) - what does out(Tensor,optional) contain?",
        "context": "out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "Default",
        "question": "What is the default type of returned tensor?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "2-D tensor",
        "question": "What type of tensor has ones on the diagonal and zeros elsewhere?",
        "context": "device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "Alias fortorch.special.erfinv()",
        "question": "What is the name of Alias fortorch.special.erfinv?",
        "context": "Alias fortorch.special.erfinv(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv"
    },
    {
        "answer": "Alias fortorch.special.erfinv()",
        "question": "What is another name for fortorch.special.erfinv()?",
        "context": "Alias fortorch.special.erfinv(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv"
    },
    {
        "answer": "Alias fortorch.special.expm1",
        "question": "What is the name of the site?",
        "context": "Alias fortorch.special.expm1(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1"
    },
    {
        "answer": "Alias fortorch.special.expm1()",
        "question": "What does Alias fortorch.special.expm1() do?",
        "context": "Alias fortorch.special.expm1(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device",
        "question": "What does this function do?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "zeroth order modified Bessel function",
        "question": "What is I_0?",
        "context": "Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Let I_0 be the zeroth order modified Bessel function of the first kind",
        "question": "What does seetorch.i0() do?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Callingtorch.kaiser_window",
        "question": "What is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Theperiodicargument",
        "question": "What is intended as a helpful shorthand to produce a periodic window as input to functions liketorch.stft()?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Note",
        "question": "What does theperiodicargument do to produce a periodic window as input to functions liketorch.stft()?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Callingtorch.kaiser_window",
        "question": "What is equivalent to callingtorch.kaiser_window?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Ifwindow_lengthis one",
        "question": "If the returned window is a single element tensor containing a one, what is the result?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "length",
        "question": "What is the length of the window?",
        "context": "window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "callingtorch.kaiser_window",
        "question": "What is equivalent to callingtorch.kaiser_window(L,B,periodic=True)?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Theperiodicargument",
        "question": "What is intended as a useful shorthand to produce a periodic window as input to functions liketorch.stft()?",
        "context": "Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "one",
        "question": "If window_length is what, then the returned window is a single element tensor containing a one?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "length of the window",
        "question": "What is window_length(int)?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "a single element tensor containing a one",
        "question": "What is the returned window ifwindow_lengthis one?",
        "context": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "length of the window",
        "question": "What does window_length(int) mean?",
        "context": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "periodic",
        "question": "What is the window suitable for use in spectral analysis?",
        "context": "Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "filter design",
        "question": "If False, returns a symmetric window suitable for use in what?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "shape parameter",
        "question": "What does beta(float,optional) provide for the window?",
        "context": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "length of the window",
        "question": "What does window_length(int) represent?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "periodic",
        "question": "If True, returns a periodic window suitable for use in spectral analysis. If False, returns a symmetric window suitable for use",
        "context": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "symmetric",
        "question": "If False, returns a window suitable for use in filter design.",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "shape parameter",
        "question": "Beta(float,optional) \u2013 what parameter for the window?",
        "context": "periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "a single element tensor containing a one",
        "question": "What is the returned window if window_lengthis one?",
        "context": "Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "a single element tensor containing a one",
        "question": "Ifwindow_lengthis one, then the returned window is what?",
        "context": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "If True",
        "question": "What returns a periodic window suitable for use in spectral analysis?",
        "context": "periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "If False",
        "question": "What returns a symmetric window suitable for use in filter design?",
        "context": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "the desired data type of returned tensor",
        "question": "dtype(torch.dtype, optional) \u2013 what?",
        "context": "window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "ifNone",
        "question": "What is the global default?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "layout",
        "question": "What is the desired layout of returned window tensor?",
        "context": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Onlytorch.strided",
        "question": "What is supported for dense layout?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "symmetric",
        "question": "If False, returns a window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window.",
        "context": "Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Onlytorch.strided",
        "question": "What is the term for a dense layout?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Onlytorch.strided",
        "question": "What is supported for a dense layout?",
        "context": "beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "shape parameter",
        "question": "What parameter does beta(float,optional) provide for the window?",
        "context": "beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "ifNone",
        "question": "What is the default value of the window tensor?",
        "context": "beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Onlytorch.strided",
        "question": "What type of layout is supported?",
        "context": "Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Default",
        "question": "What type of tensor does ifNone use?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "Common linear algebra operations",
        "question": "What is a vector or matrix norm?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "matrix norm",
        "question": "What is the determinant of a square matrix?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "vector norm",
        "question": "What is a common linear algebra operation?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "determinant of a square matrix",
        "question": "Computes the sign and natural logarithm of the absolute value of what?",
        "context": "Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the sign and natural logarithm",
        "question": "What is the absolute value of the determinant of a square matrix?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the condition number of a matrix with respect to a matrix norm",
        "question": "What does it compute?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "numerical rank",
        "question": "What does the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix compute?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Cholesky",
        "question": "Who decomposes a complex Hermitian or real symmetric positive-definite matrix?",
        "context": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Common linear algebra operations",
        "question": "What operations are used to compute a vector or matrix norm?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "error function ofinput",
        "question": "Computes what?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "condition number",
        "question": "Computes what of a matrix with respect to a matrix norm?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "singular values",
        "question": "Computes what of a matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Cholesky",
        "question": "What decomposition of a complex Hermitian or real symmetric positive-definite matrix is computed?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "a vector or matrix norm",
        "question": "What type of norm is computed?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "vector norm",
        "question": "What is a matrix norm?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "determinant of a square matrix",
        "question": "What does Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the condition number of a matrix with respect to a matrix norm",
        "question": "What does the computation of the numerical rank of a matrix consist of?",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "QR",
        "question": "What decomposition of a matrix is computed?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "determinant",
        "question": "Computes the absolute value of what of a square matrix?",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the QR decomposition of a matrix",
        "question": "What does Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "context": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "matrix norm",
        "question": "Computes a vector norm.",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "determinant of a square matrix",
        "question": "Computes the absolute value of what?",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "matrix norm",
        "question": "Computes the condition number of a matrix with respect to what?",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue",
        "question": "What is the decomposition of a square matrix if it exists?",
        "context": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "determinant",
        "question": "Computes the sign and natural logarithm of the absolute value of what of a square matrix?",
        "context": "Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues",
        "question": "What does the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix if it exists?",
        "context": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the sign and natural logarithm",
        "question": "Computes what of the absolute value of the determinant of a square matrix?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue",
        "question": "What decomposition of a square matrix if it exists?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues",
        "question": "Computes the eigenvalue decomposition of a square matrix if it exists. Computes what of a square matrix?",
        "context": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix",
        "question": "What does the Cholesky decomposition of a square matrix if it exists?",
        "context": "Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix",
        "question": "What does the Cholesky decomposition of a complex Hermitian or real symmetric matrix compute?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "if it exists",
        "question": "What is the eigenvalue decomposition of a square matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes the eigenvalues of a square matrix",
        "question": "Computes the eigenvalue decomposition of a square matrix if it exists.",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue",
        "question": "Computes the decomposition of a complex Hermitian or real symmetric matrix?",
        "context": "Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a square matrix",
        "question": "What does the Cholesky decomposition of a complex Hermitian or real symmetric matrix compute if it exists?",
        "context": "Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues",
        "question": "Computes what of a square matrix?",
        "context": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix",
        "question": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.",
        "context": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues",
        "question": "Computes what of a complex Hermitian or real symmetric matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Cholesky",
        "question": "Computes the decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a square matrix",
        "question": "What does it compute if it exists?",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues of a square matrix",
        "question": "What does the eigenvalue decomposition of a complex Hermitian or real symmetric matrix compute?",
        "context": "  Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix",
        "question": "What is the eigenvalue decomposition of a complex Hermitian or real symmetric matrix?",
        "context": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues of a complex Hermitian or real symmetric matrix",
        "question": "What does the SVD of a matrix compute?",
        "context": "  Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "if it exists",
        "question": "Computes the eigenvalue decomposition of a square matrix?",
        "context": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "a square system of linear equations with a unique solution",
        "question": "What is the solution of?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes the eigenvalues of a square matrix",
        "question": "What does Computes the eigenvalue decomposition of a square matrix if it exists?",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "singular values of a matrix",
        "question": "Computes the singular value decomposition (SVD) of a matrix.",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "unique solution",
        "question": "Computes the solution of a square system of linear equations with what?",
        "context": "  Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition of a complex Hermitian or real symmetric matrix",
        "question": "What does the SVD of a matrix do?",
        "context": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "a square system of linear equations with a unique solution",
        "question": "What solves the least squares problem of a system of linear equations?",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "a system of linear equations",
        "question": "Computes a solution to the least squares problem of what?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the least squares problem",
        "question": "Computes a solution to what problem of a system of linear equations?",
        "context": "  Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "What is the name of a square matrix if it exists?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "Computes what of a square matrix if it exists?",
        "context": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "pseudoinverse",
        "question": "What does Moore-Penrose inverse stand for?",
        "context": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Moore-Penrose",
        "question": "What is the pseudoinverse of a matrix?",
        "context": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "matrix norm",
        "question": "Computes a what?",
        "context": "  Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "a square system of linear equations with a unique solution",
        "question": "Computes the solution of what?",
        "context": "  Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "Computes the what of a square matrix if it exists?",
        "context": "  Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes then-th power of a square matrix for an integern",
        "question": "What is the function that computes the then-th power of a square matrix for an integern?",
        "context": "  Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "multiplies two or more matrices",
        "question": "How does reordering the multiplications work?",
        "context": "  Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "firstncolumns",
        "question": "Computes what of a product of Householder matrices?",
        "context": "  Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "then-th power",
        "question": "Computes what power of a square matrix for an integern?",
        "context": "  Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Efficiently multiplies two or more matrices",
        "question": "How does this function work?",
        "context": "  Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "oftorch.tensordot()",
        "question": "Computes the multiplicative inverse of what?",
        "context": "  Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "the solutionXto the systemtorch.tensordot(A, X) = B",
        "question": "What is the result of the multiplicative inverse of oftorch.tensordot()?",
        "context": "  Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "multiplicative",
        "question": "Computes the inverse oftorch.tensordot()?",
        "context": "  Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "systemtorch.tensordot",
        "question": "Computes the solutionXto what?",
        "context": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Cholesky",
        "question": "What is the name of the decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix if it is in",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Cholesky",
        "question": "Computes what decomposition of a complex Hermitian or real symmetric positive-definite matrix?",
        "context": "  Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "Computes what of a square matrix if it is invertible?",
        "context": "Common linear algebra operations.   Computes a vector or matrix norm.   Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the inverse of a square matrix if it is invertible. ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "asparse tensor",
        "question": "What type of tensor is constructed in COO(rdinate) format?",
        "context": "Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices. Note This function returns anuncoalesced tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "anuncoalesced tensor",
        "question": "What does this function return?",
        "context": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to atorch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "COO(rdinate)",
        "question": "Constructs asparse tensor in what format?",
        "context": "Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices. Note This function returns anuncoalesced tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "specified values at the givenindices",
        "question": "Constructs asparse tensor in COO(rdinate) format with what?",
        "context": "Constructs asparse tensor in COO(rdinate) formatwith specified values at the givenindices. Note This function returns anuncoalesced tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "tuple, NumPyndarray, scalar, and other types",
        "question": "Can be a list, NumPyndarray, scalar, and what other types?",
        "context": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to atorch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "atorch.LongTensor",
        "question": "What will the indices be cast to internally?",
        "context": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to atorch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "the number of non-zero values",
        "question": "What is the second dimension of the indices?",
        "context": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to atorch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "a list, tuple, NumPyndarray, scalar, and other types",
        "question": "indices(array_like) \u2013 Initial data for the tensor. Can be what?",
        "context": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to atorch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "atorch.LongTensorinternally",
        "question": "Where will the indices be cast?",
        "context": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to atorch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "the coordinates of the non-zero values in the matrix",
        "question": "What are the indices?",
        "context": "This function returns anuncoalesced tensor. indices(array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. Will be cast to atorch.LongTensorinternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "values",
        "question": "What are initial values for the tensor?",
        "context": "values(array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. size(list, tuple, ortorch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "a list, tuple, NumPyndarray, scalar",
        "question": "What are some types of values?",
        "context": "values(array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPyndarray, scalar, and other types. size(list, tuple, ortorch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "size",
        "question": "What is the size of the sparse tensor?",
        "context": "size(list, tuple, ortorch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "the minimum size big enough to hold all non-zero elements",
        "question": "What is the size of the sparse tensor inferred as if not provided?",
        "context": "size(list, tuple, ortorch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "None",
        "question": "Default: if what, infers data type fromvalues?",
        "context": "size(list, tuple, ortorch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "if None",
        "question": "What infers data type fromvalues?",
        "context": "size(list, tuple, ortorch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "None",
        "question": "Default: if what, infers data type fromvalues. device(torch.device, optional) \u2013 the desired",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "current device",
        "question": "Default: if None, uses what for the default tensor type?",
        "context": "dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type fromvalues. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "answer": "Computes the element-wise logical OR",
        "question": "What does compute the element-wise logical OR of the given input tensors do?",
        "context": "Computes the element-wise logical OR of the given input tensors. Zeros are treated asFalseand nonzeros are\ntreated asTrue. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the tensor to compute OR with out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "answer": "True",
        "question": "Nonzeros are treated as what?",
        "context": "Computes the element-wise logical OR of the given input tensors. Zeros are treated asFalseand nonzeros are\ntreated asTrue. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the tensor to compute OR with out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "answer": "Example",
        "question": "What is the tensor to compute OR with out(Tensor,optional) \u2013 the output tensor?",
        "context": "Computes the element-wise logical OR of the given input tensors. Zeros are treated asFalseand nonzeros are\ntreated asTrue. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the tensor to compute OR with out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "answer": "LU factorization of A",
        "question": "What is the solution to the system of linear equations represented byAX=BAX = BAX=Band?",
        "context": "This function returns the solution to the system of linear\nequations represented byAX=BAX = BAX=Band the LU factorization of\nA, in order as a namedtuplesolution, LU. LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "LUcontainsLandUfactors",
        "question": "What function returns the solution to the system of linear equations represented byAX=BAX = BAX=Band?",
        "context": "This function returns the solution to the system of linear\nequations represented byAX=BAX = BAX=Band the LU factorization of\nA, in order as a namedtuplesolution, LU. LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "2D inputs",
        "question": "What can torch.solve(B, A) take in?",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "batched outputssolution, LU",
        "question": "What does torch.solve(B, A) return if the inputs are batches?",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "real-valued and complex-valued inputs",
        "question": "What types of inputs does torch.solve(B, A)support?",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Warning",
        "question": "What does torch.solve(B, A) do?",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "the system of linear equations represented byAX=BAX = BAX=Band the LU factorization of A",
        "question": "What does this function return the solution to?",
        "context": "This function returns the solution to the system of linear\nequations represented byAX=BAX = BAX=Band the LU factorization of\nA, in order as a namedtuplesolution, LU. LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "LUcontainsLandUfactors",
        "question": "What does LUcontainsLandUfactors do for LU factorization ofA?",
        "context": "This function returns the solution to the system of linear\nequations represented byAX=BAX = BAX=Band the LU factorization of\nA, in order as a namedtuplesolution, LU. LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "LU factorization ofA",
        "question": "What is LUcontainsLandUfactors for?",
        "context": "LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "real-valued and complex-valued",
        "question": "What types of inputs does torch.solve support?",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "LUcontainsLandUfactors",
        "question": "What does LUcontainsLandUfactors for LU factorization ofA?",
        "context": "LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Warning",
        "question": "What does LUcontainsLandUfactors for?",
        "context": "LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "batched outputssolution",
        "question": "What does torch.solve return if the inputs are batches?",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Warning",
        "question": "What does torch.solve(B, A)notify when inputs are batches of 2D matrices?",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "real-valued and complex-valued inputs",
        "question": "What types of inputs does torch.solve() support?",
        "context": "Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "torch.solve()",
        "question": "What is deprecated in favor oftorch.linalg.solve()?",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "seetorch.lu()",
        "question": "What does torch.linalg.solve() use to get the LU factorization?",
        "context": "torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Note",
        "question": "What should be replaced with X=torch.solve(B,A)?",
        "context": "torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Note",
        "question": "What should be replaced with X=torch.solve(B,A).solution?",
        "context": "X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "A.contiguous().transpose(-1, -2)",
        "question": "What is the name of the strides that will be transposed?",
        "context": "X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "zero or more batch dimensions",
        "question": "What does input(Tensor) represent?",
        "context": "X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "zero or more batch dimensions",
        "question": "A(Tensor) \u2013 input square matrix of size(,m,m)(*, m, m)(",
        "context": "X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "GPUs",
        "question": "What do CUDA tensor types use for computation?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "useis_available()",
        "question": "What is used to determine if your system supports CUDA?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA semantics",
        "question": "What has more details about working with CUDA?",
        "context": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager StreamContext",
        "question": "What selects a given stream?",
        "context": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Checks if peer access between two devices is possible",
        "question": "What does the Context-manager do?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cublasHandle_t pointer",
        "question": "What is returned to current cuBLAS handle?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "peer access between two devices",
        "question": "Checks if what is possible?",
        "context": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current cuBLAS handle",
        "question": "What does cublasHandle_t pointer to?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cublasHandle_t pointer",
        "question": "What does cublasHandle_t pointer return to current cuBLAS handle?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the currently selectedStreamfor a given device",
        "question": "What does the cublasHandle_t pointer to current cuBLAS handle return?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "defaultStreamfor a given device",
        "question": "What is the defaultStream for a given device?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selectedStreamfor a given device",
        "question": "What is returned when the index of a currently selected device is returned?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory occupied by tensors in bytes",
        "question": "What is returned for a given device?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Checks if peer access between two devices is possible",
        "question": "What does CUDA semantics do?",
        "context": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the current GPU memory managed by the caching allocator in bytes for a given device",
        "question": "What does this return?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager",
        "question": "What changes the selected device?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "GPUs",
        "question": "Returns the number of what?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current cuBLAS handle",
        "question": "Returns cublasHandle_t pointer to what?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selectedStreamfor a given device",
        "question": "Returns the what?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "defaultStreamfor a given device",
        "question": "Returns the currently selectedStreamfor a given device.",
        "context": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "number of GPUs available",
        "question": "What does the Context-manager return?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the currently selectedStreamfor a given device",
        "question": "What does cublasHandle_t return?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "defaultStreamfor a given device",
        "question": "What does cublasHandle_t pointer to return?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager",
        "question": "What changes the current device to that of given object?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "defaultStream",
        "question": "What stream does cublasHandle_t return for a given device?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager",
        "question": "Who changes the selected device?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "number of GPUs available",
        "question": "What does the Context-manager that changes the selected device return?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA architectures",
        "question": "What does this library return a list of?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selectedStreamfor a given device",
        "question": "What does the index of a currently selected device return?",
        "context": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA architectures",
        "question": "Returns list of what library was compiled for?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a human-readable printout of the running processes and their GPU memory use for a given device",
        "question": "What does return?",
        "context": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selectedStreamfor a given device",
        "question": "Returns the index of a currently selected device. Returns the what?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "defaultStreamfor a given device",
        "question": "Returns the currently selectedStreamfor a given device. Returns what?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "What does this library return?",
        "context": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What is returned when a library is compiled?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "index",
        "question": "What is the name of a currently selected device?",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the currently selectedStreamfor a given device",
        "question": "What does the currentStream for a given device return?",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What is the name of a device?",
        "context": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the name of a device",
        "question": "What does the cuda capability of a device return?",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "What do you get from a device?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selected device",
        "question": "Returns the index of what?",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selectedStreamfor a given device",
        "question": "Returns the index of a currently selected device.",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "number of GPUs available",
        "question": "What does Context-manager return?",
        "context": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "Gets what of a device?",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selectedStreamfor a given device",
        "question": "What is returned?",
        "context": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "What does PyTorch get from a device?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the name of a device",
        "question": "What does PyTorch get?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "What does PyTorch's CUDA state return?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "PyTorch",
        "question": "What library initializes its CUDA state?",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda capability",
        "question": "What is the name of a device. Gets the name of a device. Gets the properties of a device.",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC",
        "question": "What gencode flags does this library return?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the properties of a device",
        "question": "What does PyTorch do?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "What does PyTorch return?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Force",
        "question": "What collects GPU memory after it has been released by CUDA IPC?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA architectures",
        "question": "What is the list this library was compiled for?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "Returns what flags this library was compiled with?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Initialize PyTorch\u2019s CUDA state",
        "question": "What does Force collect GPU memory after it has been released by CUDA IPC?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "PyTorch",
        "question": "What library initializes CUDA state?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "bool",
        "question": "What indicates if CUDA is currently available?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "bool",
        "question": "Returns what indicating if CUDA is currently available?",
        "context": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "What did PyTorch's CUDA state return?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the name of a device",
        "question": "What does Gets the cuda capability of a device?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA architectures",
        "question": "What does this library list?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Initialize PyTorch\u2019s CUDA state",
        "question": "What does the Force collect GPU memory after it has been released by CUDA IPC?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA architectures",
        "question": "Returns list of what architectures this library was compiled for?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current device",
        "question": "Sets what if PyTorch\u2019s CUDA state has been initialized?",
        "context": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Initialize",
        "question": "What does PyTorch do with its CUDA state?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the current device",
        "question": "What is a wrapper API to set the stream?",
        "context": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "wrapper API",
        "question": "What type of API is used to set the stream?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda capability",
        "question": "What does a device get?",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What does the bool indicating if PyTorch\u2019s CUDA state has been initialized return?",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "random number generator state of all devices",
        "question": "Sets what?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the current device",
        "question": "What is a wrapper API to set the current stream?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "wrapper API",
        "question": "What is this to set the stream?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper",
        "question": "What is the name of the API that selects a given stream?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "set the stream",
        "question": "This is a wrapper API to what?",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager StreamContext",
        "question": "Wrapper around what that selects a given stream?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "ByteTensor",
        "question": "Returns the random number generator state of the specified GPU as a what?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a list of ByteTensor",
        "question": "Returns what representation of the random number states of all devices?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does the ByteTensor do?",
        "context": "Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all devices",
        "question": "Sets the random number generator state of what?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU",
        "question": "Sets the seed for generating random numbers for what?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all GPUs",
        "question": "Sets the seed for generating random numbers on what?",
        "context": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU",
        "question": "Sets the seed for generating random numbers to a random number for what?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "ByteTensor",
        "question": "What represents the random number states of all devices?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the random number generator state of all devices",
        "question": "What does the ByteTensor set?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU",
        "question": "What is the seed for generating random numbers for?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all GPUs",
        "question": "The seed for generating random numbers is set on what?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a random number for the current GPU",
        "question": "What is the seed for generating random numbers set to?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all GPUs",
        "question": "The seed for generating random numbers is set to a random number on what?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "ByteTensor",
        "question": "Returns a list of what representing the random number states of all devices?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "generating random numbers",
        "question": "Sets the seed for what on all GPUs?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a random number",
        "question": "Sets the seed for generating random numbers to what on all GPUs?",
        "context": "Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "random number generator state",
        "question": "Sets what state of the specified GPU?",
        "context": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all GPUs",
        "question": "Sets the seed for generating random numbers to a random number on what?",
        "context": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current random seed of the current GPU",
        "question": "What does Sets the seed for generating random numbers return?",
        "context": "  Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast Broadcasts",
        "question": "What sends a tensor to specified GPU devices?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast_coalesced Broadcasts",
        "question": "What is a sequence of tensors to the specified GPUs?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sums",
        "question": "What does comm.reduce_add tensors from multiple GPUs do?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.scatter",
        "question": "What Scatters tensor across multiple GPUs?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.gather",
        "question": "What gathers tensors from multiple GPU devices?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast",
        "question": "What Broadcasts a tensor to specified GPU devices?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast_coalesced",
        "question": "What Broadcasts a sequence tensors to the specified GPUs?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.reduce_add",
        "question": "What Sums tensors from multiple GPUs?",
        "context": "comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a CUDA event",
        "question": "Wrapper around what?",
        "context": "  Wrapper around a CUDA stream.   Wrapper around a CUDA event. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA stream",
        "question": "What is Wrapper around a CUDA event?",
        "context": "  Wrapper around a CUDA stream.   Wrapper around a CUDA event. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA event",
        "question": "What is Wrapper around a CUDA stream?",
        "context": "  Wrapper around a CUDA stream.   Wrapper around a CUDA event. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all unoccupied cached memory",
        "question": "Releases what currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-s",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA",
        "question": "What type of memory allocator statistics does this return a dictionary of?",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Releases all unoccupied cached memory currently held by the caching allocator",
        "question": "What does it do to all unoccupied cached memory currently held by the caching allocator?",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a human-readable printout",
        "question": "Returns what of the running processes and their GPU memory use for a given device?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator statistics",
        "question": "Returns a dictionary of what for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current memory allocator statistics",
        "question": "Returns a human-readable printout of what for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a dictionary of CUDA memory allocator statistics",
        "question": "What does this return for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "snapshot",
        "question": "Returns a what of the CUDA memory allocator state across all devices?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "tensors in bytes",
        "question": "What is the current GPU memory occupied by for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a human-readable printout",
        "question": "What is returned of the running processes and their GPU memory use for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU memory occupied by tensors in bytes",
        "question": "What does Returns a snapshot of the CUDA memory allocator state across all devices?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a dictionary of CUDA memory allocator statistics",
        "question": "What does return for a given device?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a human-readable printout",
        "question": "What is the current memory allocator statistics for a given device?",
        "context": "Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "snapshot",
        "question": "What is the CUDA memory allocator state across all devices?",
        "context": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory occupied by tensors in bytes",
        "question": "What does Returns the current GPU memory occupied by tensors in bytes for a given device?",
        "context": "Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device",
        "question": "What does this do?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator state",
        "question": "Returns a snapshot of what across all devices?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point",
        "question": "What does it do in tracking maximum GPU memory occupied by tensors for a given device?",
        "context": "Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA",
        "question": "What is the name of the memory allocator state across all devices?",
        "context": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator state",
        "question": "Returns a snapshot of what state across all devices?",
        "context": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory occupied by tensors in bytes",
        "question": "Returns the current GPU memory occupied by tensors in bytes for a given device.",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors",
        "question": "What does this function do for a given device?",
        "context": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU memory managed by the caching allocator in bytes",
        "question": "Returns the current GPU memory managed by the caching allocator in bytes for a given device.",
        "context": "Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device",
        "question": "What does it do to the starting point in tracking maximum GPU memory occupied by tensors for a given device?",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "caching allocator in bytes",
        "question": "Returns the current GPU memory managed by what for a given device?",
        "context": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory",
        "question": "What is occupied by tensors in bytes for a given device?",
        "context": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU memory managed by the caching allocator in bytes",
        "question": "What does Returns the starting point in tracking maximum GPU memory occupied by tensors for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the caching allocator in bytes",
        "question": "Returns the maximum GPU memory managed by what for a given device?",
        "context": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction",
        "question": "What does a process do for a process?",
        "context": "Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "tensors",
        "question": "What is the maximum GPU memory occupied by?",
        "context": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction for a process",
        "question": "What does seememory_reserved do?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Deprecated",
        "question": "What is seememory_reserved()?",
        "context": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "seemax_memory_reserved()",
        "question": "What function is deprecated?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "tensors",
        "question": "Returns the maximum GPU memory occupied by what in bytes for a given device?",
        "context": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors",
        "question": "What does it do for a given device?",
        "context": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "memory fraction",
        "question": "Set what for a process?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "seememory_reserved()",
        "question": "Set memory fraction for a process. Deprecated; what?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "seemax_memory_reserved()",
        "question": "What is the name of the function that returns the maximum GPU memory occupied by tensors in bytes for a given device?",
        "context": "Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Deprecated",
        "question": "What is seememory_reserved?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "tensors",
        "question": "Resets the starting point in tracking maximum GPU memory occupied by what for a given device?",
        "context": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "defaultStream",
        "question": "Returns what for a given device?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "seemax_memory_reserved()",
        "question": "What is deprecated?",
        "context": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction",
        "question": "What does seememory_reserved do for a process?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA",
        "question": "Which memory allocator resets the \"peak\" stats?",
        "context": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "maximum GPU memory managed by the caching allocator",
        "question": "What is returned in bytes for a given device?",
        "context": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction",
        "question": "What does this function do for a process?",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "seememory_reserved()",
        "question": "What is the name of the function that sets memory fraction for a process?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "seemax_memory_reserved()",
        "question": "What is the name of the function that returns the maximum GPU memory managed by the caching allocator for a given device?",
        "context": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "starting point",
        "question": "What is reset in tracking maximum GPU memory managed by the caching allocator for a given device?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator",
        "question": "What does the \u201cpeak\u201d stats track?",
        "context": "  Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "instantaneous",
        "question": "nvtx.mark Describe an event that occurred at some point.",
        "context": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "nvtx.range_push",
        "question": "What pushes a range onto a stack of nested range spans?",
        "context": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "nvtx.range_pop",
        "question": "What pops a range off of a stack of nested range spans?",
        "context": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "an instantaneous event",
        "question": "What does nvtx.mark describe?",
        "context": "nvtx.mark Describe an instantaneous event that occurred at some point. nvtx.range_push Pushes a range onto a stack of nested range span. nvtx.range_pop Pops a range off of a stack of nested range spans. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "The torch.special module",
        "question": "What is modeled after SciPy'sspecialmodule?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "SciPy",
        "question": "What is the torch.special module modeled after?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "BETA",
        "question": "What version of PyTorch is the torch.special module in?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "New functions are still being added",
        "question": "What is the status of the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the documentation of each function for details",
        "question": "Where can you find more information about the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "elementwise",
        "question": "Computes the entropy oninput, what is it?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) - what does out(Tensor,optional) represent?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "error function ofinput",
        "question": "What does the torch.special module compute?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the error function of input defined as?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "New functions",
        "question": "What is still being added to the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "documentation",
        "question": "What is the name of each function in the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) \u2013 what?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "BETA",
        "question": "What version of PyTorch is this module in?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "PyTorch releases",
        "question": "Some functions may change in future what?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "documentation",
        "question": "What is the name of each function in the PyTorch module?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "entropy oninput",
        "question": "Computes what elementwise?",
        "context": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "What is out(Tensor,optional) defined as?",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Example:",
        "question": "What is an example of a PyTorch module?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the error function ofinput",
        "question": "What is the function that computes the error function of input?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "complementary error function ofinput",
        "question": "What is the error function of input?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function ofinput",
        "question": "What is the complementary error function of input?",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor)",
        "question": "What is the name of the input tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the inverse error function ofinput",
        "question": "What is the inverse error function of input?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the complementary error function defined as?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "What error function is defined in the range(1,1)(-1,1)(1,1)?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "What is defined in the range(1,1)(-1, 1)(1,1)?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "expit",
        "question": "What is another name for the logistic sigmoid function?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "What error function is defined in the range(1,1)(-1, 1)(1,1)?",
        "context": "Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor)",
        "question": "What is the input tensor defined as?",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the logistic sigmoid function",
        "question": "What is the expit also known as?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "Computes what error function of input?",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "expit",
        "question": "What is the logistic sigmoid function?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "1",
        "question": "Computes the exponential of the elements minus what ofinput?",
        "context": "Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Note",
        "question": "What is the name of the function that computes the exponential of the elements minus 1 ofinput?",
        "context": "Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponential",
        "question": "Computes what of the elements minus 1 ofinput?",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Note",
        "question": "What does the exponential of the elements minus 1 ofinput do?",
        "context": "Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) is what?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Note",
        "question": "What is the name of the function that computes the exponential of the elements minus 1 of input?",
        "context": "Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "What function is defined in the range(1,1)(-1, 1)(1,1)?",
        "context": "Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "Computes what function of input?",
        "context": "Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exp(x) - 1",
        "question": "What function provides greater precision than the exponential of the elements minus 1 ofinput?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "base two",
        "question": "What is the base of the exponential function of input?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the exponential of the elements minus 1 ofinput",
        "question": "What function provides greater precision than exp(x) - 1 for small values of x?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "greater precision",
        "question": "What does the exponential of the elements minus 1 ofinput provide?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input tensor",
        "question": "What is the input(Tensor)?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exp(x) - 1",
        "question": "The exponential of the elements minus 1 ofinput provides greater precision than what for small values of x?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "What is out(Tensor,optional)?",
        "context": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "base two exponential function ofinput",
        "question": "What is the function that computes the exponential of the elements minus 1 ofinput?",
        "context": "Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponential",
        "question": "Computes what of the elements minus 1 of input?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the base two exponential function ofinput",
        "question": "What does this function compute?",
        "context": "Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the logistic sigmoid function",
        "question": "Computes the expit also known as what?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exp(x) - 1",
        "question": "This function provides greater precision than what for small values of x?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "logarithm",
        "question": "Computes the natural what of the absolute value of the gamma function oninput?",
        "context": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "greater precision",
        "question": "What does this function provide than exp(x) - 1 for small values of x?",
        "context": "Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the natural logarithm of the absolute value of the gamma function oninput",
        "question": "What does Compute?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What type of function is computed for each element of input?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "first kind",
        "question": "Computes the exponentially scaled zeroth order modified Bessel function of what kind?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the base two exponential function ofinput",
        "question": "What is an example of a function that computes the base two exponential function of input?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "natural logarithm",
        "question": "What is the absolute value of the gamma function oninput?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) - what is the output tensor?",
        "context": "This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "zeroth",
        "question": "What order modified Bessel function is computed for each element of input?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "Out(Tensor,optional) - what?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "first kind",
        "question": "What is the exponentially scaled zeroth order modified Bessel function of?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the base two exponential function ofinput",
        "question": "What is the base two exponential function of input?",
        "context": "Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What is computed for each element of input?",
        "context": "Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the natural logarithm of the absolute value of the gamma function oninput",
        "question": "What is the result of the natural logarithm of the absolute value of the gamma function oninput?",
        "context": "Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "eps",
        "question": "Input is clamped to what when eps is not None?",
        "context": "Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "NaN",
        "question": "When eps is None andinput 0 orinput> 1, the function will yield what?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "eps",
        "question": "Inputis clamped to what when eps is not None?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "float",
        "question": "What is the epsilon for input clamp bound?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "What does out(Tensor,optional) return?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computesinput*log1p(other)with the following cases",
        "question": "What is an example of a function that computes input*log1p?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "SciPy",
        "question": "What is'sscipy.special.xlog1py' similar to?",
        "context": "Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computesinput*log1p(other)with the following cases",
        "question": "What is an example of an output tensor?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Vandermonde",
        "question": "What is the name of the matrix that generates?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "elementwise powers",
        "question": "What are the columns of the output matrix?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "True",
        "question": "If increasing is true, the order of the columns is reversedx0,x1,...,x(N1),x(N2)",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Alexandre-Theophile Vandermonde",
        "question": "Who is a Vandermonde matrix named for?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "1-D",
        "question": "What type of input tensor is x(Tensor)?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Vandermonde matrix",
        "question": "What is generated by the output matrix?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "elementwise",
        "question": "What are the powers of the input vectorx(N1),x(N2),...,x0x(N-1)",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "reversed",
        "question": "If increasing is True, the order of the columns of the output matrix is what?",
        "context": "The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Alexandre-Theophile Vandermonde",
        "question": "For whom is a Vandermonde matrix named?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "1-D input tensor",
        "question": "What is x(Tensor)?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "True",
        "question": "If increasing is true, the order of the columns is reversedx0,x1,...,x(N1)x0, x",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Alexandre-Theophile Vandermonde",
        "question": "Who is a matrix with a geometric progression in each row named for?",
        "context": "The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Alexandre-Theophile Vandermonde",
        "question": "What is the name of the matrix with a geometric progression in each row?",
        "context": "The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Number of columns in the output",
        "question": "What does N(int,optional) mean?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "N",
        "question": "If what is not specified, a square array is returned?",
        "context": "x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Order of the powers of the columns",
        "question": "What is increasing(bool,optional)?",
        "context": "increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "True",
        "question": "If what, the powers increase from left to right?",
        "context": "N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Number of columns in the output",
        "question": "What is N(int,optional)?",
        "context": "N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "a square array",
        "question": "What is returned if N is not specified?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "increasing",
        "question": "What is the order of the powers of columns?",
        "context": "N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "the powers increase from left to right",
        "question": "What happens if True?",
        "context": "x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "N=len(x)",
        "question": "What is the square array returned if N is not specified?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "the powers increase from left to right",
        "question": "If True, what happens to the powers of columns?",
        "context": "N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "True",
        "question": "If increasing is true, the columns arex0,x1,...,x(N-1)x0,x1,...,x(N-",
        "context": "increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Vandermonde matrix",
        "question": "What is the name of the matrix where the powers increase from left to right?",
        "context": "increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "False",
        "question": "If increasing is what, the first column isx(N1)x(N-1)x(N1)?",
        "context": "increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Tensor",
        "question": "What is an example of a Vandermonde matrix?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "fault-tolerant and elastic",
        "question": "What does PyTorch make?",
        "context": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "Usage API Advanced Plugins",
        "question": "What makes distributed PyTorch fault-tolerant?",
        "context": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "fault-tolerant",
        "question": "Is distributed PyTorch fault-tolerant or elastic?",
        "context": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "elastic",
        "question": "Makes distributed PyTorch fault-tolerant and what else?",
        "context": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "Usage API Advanced Plugins",
        "question": "What makes distributed PyTorch fault-tolerant and elastic?",
        "context": "Makes distributed PyTorch fault-tolerant and elastic. Usage API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "Usage",
        "question": "What is the most important aspect of a computer?",
        "context": "Usage ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "Usage",
        "question": "What is the term for what?",
        "context": "Usage ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "API Advanced Plugins",
        "question": "What kind of plugins are available?",
        "context": "API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "API Advanced Plugins",
        "question": "What is the name of what?",
        "context": "API Advanced Plugins ",
        "source": "https://pytorch.org/docs/stable/distributed.elastic.html"
    },
    {
        "answer": "Decomposesinputinto mantissa and exponent tensors",
        "question": "What does it do to a mantissa and exponent tensor?",
        "context": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "answer": "the open interval",
        "question": "What is the range of mantissa?",
        "context": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "answer": "float inputs",
        "question": "What type of inputs does mantissa support?",
        "context": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "answer": "output tensors",
        "question": "What is the input tensor out(tuple,optional)?",
        "context": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "answer": "Decomposes",
        "question": "What does inputinto mantissa and exponent tensors do?",
        "context": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "answer": "float inputs",
        "question": "What type of inputs does the mantissa support?",
        "context": "Decomposesinputinto mantissa and exponent tensors\nsuch thatinput=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input(Tensor) \u2013 the input tensor out(tuple,optional) \u2013 the output tensors Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "answer": "the scalarvalue",
        "question": "What does oftensor1bytensor2 multiply the result by?",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "Warning",
        "question": "What is the name of the function that performs the element-wise division of oftensor1bytensor2?",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "Warning",
        "question": "What does the element-wise division of oftensor1bytensor2 do?",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "oftensor1bytensor2",
        "question": "What is the element-wise division?",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "Warning Integer",
        "question": "What division with addcdiv is no longer supported?",
        "context": "Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "torch.trunc",
        "question": "What is the historic addcdiv behavior implemented as?",
        "context": "Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "dtypes",
        "question": "The future addcdiv behavior is the same for all what?",
        "context": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "addcdiv",
        "question": "What is no longer supported for integer division?",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes ofinput,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "float inputs",
        "question": "What inputs can be implemented as (input + value * tensor1 / tensor2)?",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes ofinput,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "all dtypes",
        "question": "What does the future addcdiv behavior apply to?",
        "context": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "torch.trunc",
        "question": "The historic addcdiv behavior can be implemented as (input + value * what?",
        "context": "Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "shapes",
        "question": "The input,tensor1, andtensor2must bebroadcastable.",
        "context": "The shapes ofinput,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "real number",
        "question": "For inputs of typeFloatTensororDoubleTensor,valuemust be a what?",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes ofinput,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "input(Tensor)",
        "question": "What is the tensor to be added tensor1(Tensor)?",
        "context": "The shapes ofinput,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "aFuturetype",
        "question": "What encapsulates an asynchronous execution and a set of utility functions to simplify operations on Futureobjects?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "theDistributed RPC Framework",
        "question": "What is theFuturetype primarily used by?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "atorch._C.Future",
        "question": "What wrapper encapsulates an asynchronous execution of a callable?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "add callback functions and set results",
        "question": "What does the package expose a set of APIs for?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Warning GPU support",
        "question": "What is a beta feature, subject to changes?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "theDistributed RPC Framework",
        "question": "What is the Futuretype primarily used by?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "atorch._C.Future",
        "question": "What encapsulates an asynchronous execution of a callable?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "APIs",
        "question": "What does atorch._C.Future expose to add callback functions and set results?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Warning GPU support",
        "question": "What is a beta feature?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "add callback functions and set results",
        "question": "What does the wrapper expose a set of APIs for?",
        "context": "Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "APIs",
        "question": "What does atorch._C.Future expose?",
        "context": "Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Append the given callback function to thisFuture",
        "question": "What will be run when theFutureis completed?",
        "context": "Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Multiple callbacks",
        "question": "What can be added to the sameFuture, but the order in which they will be executed cannot be guaranteed?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "one argument",
        "question": "How many arguments must a callback take?",
        "context": "Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thevalue()method",
        "question": "What can the callback function use to get the value?",
        "context": "Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inline",
        "question": "If thisFutureis already completed, the given callback will be run what?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "Append the given callback function to what?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the order in which they will be executed",
        "question": "What cannot be guaranteed when multiple callbacks are added to the sameFuture?",
        "context": "Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "The callback must take one argument, which is the reference to what?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "GPU",
        "question": "What GPU support is a beta feature?",
        "context": "GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "beta",
        "question": "What version of GPU support is subject to changes?",
        "context": "GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "What will the callback function be run when theFutureis completed?",
        "context": "Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "immediately inline",
        "question": "If thisFutureis already completed, the given callback will be run what way?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "asthen()",
        "question": "What method behaves in the same way with respect to GPU tensors?",
        "context": "With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "GPU tensors",
        "question": "What does this method behave in the same way asthen()?",
        "context": "With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aCallable",
        "question": "What is the reference to the Future?",
        "context": "callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What does aCallable that takes in one argument, is the reference to this Future?",
        "context": "callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "asthen()",
        "question": "What method behaves in the same way as GPU tensors?",
        "context": "With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "callback(Future)",
        "question": "What is aCallable that takes in one argument, is the reference to this Future?",
        "context": "callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What does callback(Future) take in one argument, is the reference to this Future?",
        "context": "With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "this Future",
        "question": "What is the reference to?",
        "context": "is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "error handling",
        "question": "What must be carefully taken care of if the callback function throws?",
        "context": "callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the user is responsible for handling completion/waiting on those futures independently",
        "question": "What is the user responsible for if the callback later completes additional futures?",
        "context": "A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completion/waiting",
        "question": "What is the user responsible for handling if the callback later completes additional futures?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "callback function throws",
        "question": "What must be carefully taken care of if the callback function throws an error?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completion/waiting",
        "question": "What is the user responsible for handling on futures that are not marked as completed with an error?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "ReturnTrueif thisFutureis done",
        "question": "What does Future.done() return if it has a result or an exception?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "AFutureis done",
        "question": "What if it has a result or an exception?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "asynchronous kernels",
        "question": "What is populating the tensors that reside on GPUs?",
        "context": "With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "AFuture",
        "question": "What is done if it has a result or an exception?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device",
        "question": "If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven what?",
        "context": "With respect to GPU tensors, this method behaves in the same way asthen(). callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device",
        "question": "If the value contains tensors that reside on GPUs,Future.done() will returnTrueeven what?",
        "context": "If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "What will mark thisFutureas completed with an error and trigger all attached callbacks?",
        "context": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inline",
        "question": "When calling wait()/value() on thisFuture, the exception set here will be raised what way?",
        "context": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aFuturecannot be marked completed twice",
        "question": "What is the exception for thisFuture?",
        "context": "result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Set the result for thisFuture",
        "question": "What will mark thisFutureas completed and trigger all attached callbacks?",
        "context": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aFuturecannot be marked completed twice",
        "question": "What does thisFuture not mark completed twice?",
        "context": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "twice",
        "question": "How many times can aFuture not be marked completed?",
        "context": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aFuturecannot be marked completed twice",
        "question": "Is aFuture able to be marked twice?",
        "context": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "result(object)",
        "question": "What is the result object of thisFuture?",
        "context": "result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "What does a callback return that reside on a GPU?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "re-synchronize them with the original streams",
        "question": "What must one do if one wants to change streams?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aCallable",
        "question": "What takes thisFutureas the only argument?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "callback",
        "question": "What is aCallable that takes thisFutureas the only argument?",
        "context": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "return value",
        "question": "A newFutureobject that holds what will be marked as completed when the givencallbackfinishes?",
        "context": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What does aCallable that takes thisFutureas the only argument do?",
        "context": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "What argument does callback(Callable) take as the only argument?",
        "context": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completed",
        "question": "A newFutureobject that holds the return value of thecallback will be marked as what when the givencallbackfinishes?",
        "context": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What do callback(Callable) and callback(Callable) take?",
        "context": "callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "when the givencallbackfinishes",
        "question": "When will a newFutureobject that holds the return value of thecallbackand be marked as completed?",
        "context": "A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Note",
        "question": "What do you need to do to mark a newFutureobject as completed when the givencallbackfinishes?",
        "context": "A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the future returned bythenwill be marked appropriately with the encountered error",
        "question": "What happens if the callback function throws?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the future returned bythenwill be marked appropriately",
        "question": "What happens if a callback function throws an error?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "an already-completed future",
        "question": "What is the value of?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "after a call towait()has completed, or inside a callback function passed tothen()",
        "question": "When should this method be called?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuturemay not yet hold a value and callingvalue()could fail",
        "question": "What happens in other cases?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If the value contains what that reside on GPUs, this method willnotperform any additional synchronization?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "bythen()",
        "question": "Where is the synchronization of callbacks already taken care of?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Obtain the value of an already-completed future",
        "question": "What is the purpose of this method?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuturemay not yet hold a value",
        "question": "What could cause callingvalue() to fail?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "synchronization",
        "question": "If the value contains tensors that reside on GPUs, this method will not perform what?",
        "context": "Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "bythen()",
        "question": "What is already being taken care of within callbacks?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuturemay not yet hold a value and callingvalue()could fail",
        "question": "What happens when callingvalue() fails?",
        "context": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "bythen()",
        "question": "How is synchronization taken care of within callbacks?",
        "context": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inside a callback function passed tothen()",
        "question": "Where should this method be called after a call towait() has completed?",
        "context": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "synchronization",
        "question": "If the value contains tensors that reside on GPUs, this method will not perform any additional what?",
        "context": "If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "separately",
        "question": "If the value contains tensors that reside on GPUs, then this method willnot perform any additional synchronization. This should be done",
        "context": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "What does the value contain that reside on GPUs?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "bythen()",
        "question": "What is the name of the callback for which towait() is already taken care of?",
        "context": "If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "What is the value held by?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisvalue()method",
        "question": "What method will throw an error if the function creating the value has thrown an error?",
        "context": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Block",
        "question": "What should be done until the value of thisFuture is ready?",
        "context": "If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "towait()",
        "question": "What is the name of the method that performs the synchronization of tensors that reside on GPUs?",
        "context": "If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "until the value of thisFutureis ready",
        "question": "When does thisvalue() method block?",
        "context": "If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisvalue()method",
        "question": "What method throws an error if the function creating the value has thrown an error?",
        "context": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisvalue()method",
        "question": "What method throws an error if the function creating the value throws an error?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Block",
        "question": "How long until the value of thisFutureis ready?",
        "context": "Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "until the value of thisFutureis ready",
        "question": "How long does it take to block the value of thisFuture?",
        "context": "Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "combined",
        "question": "Collects the providedFutureobjects into what kind of Future?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "futures(list)",
        "question": "What is a list of Futureobjects?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Futures",
        "question": "Returns aFutureobject to a list of the passed in what?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "What holds the value held by?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Collects the providedFutureobjects into a single combinedFuture",
        "question": "What does thiswaitmethod do when all of the sub-futures are completed?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aFutureobject",
        "question": "Returns what to a list of the passed in Futures?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "combinedFuture",
        "question": "Collects the providedFutureobjects into a single what?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aFutureobject",
        "question": "What does futures(list) return to a list of the passed in Futures?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Waits for all provided futures to be complete",
        "question": "What does the method do that returns the list of completed values?",
        "context": "Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "exit early",
        "question": "What happens if a future encounters an error?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a list ofFutureobjects",
        "question": "What is futures(list)?",
        "context": "futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "when all of the sub-futures are completed",
        "question": "When is a combinedFuture completed?",
        "context": "Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Waits for all provided futures to be complete",
        "question": "What does futures(list) do?",
        "context": "futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "exit early",
        "question": "If any futures encounter an error, the method will do what?",
        "context": "Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "a list ofFutureobject",
        "question": "What does futures(list) return?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completedFutureresults",
        "question": "What is a list of in futures?",
        "context": "futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "anyFuturethrows",
        "question": "The method will throw an error ifwaiton what?",
        "context": "futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "both for integer and floating point numbers",
        "question": "What can the dividend and divisor contain?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "dividendinput",
        "question": "The remainder has the same sign as what?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "Supportsbroadcasting to a common shape,type promotion, and integer and float inputs",
        "question": "What does the dividend and divisor support?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "zero",
        "question": "When the divisor is what, returnsNaN for floating point dtypes on both CPU and GPU?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "input(Tensor)",
        "question": "What is the dividend other(TensororScalar)?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "raisesRuntimeError",
        "question": "What happens when the divisor is zero on CPU?",
        "context": "Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "Tensor",
        "question": "What is the dividend other(TensororScalar) \u2013 the divisor?",
        "context": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividendinput. Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "the output tensor",
        "question": "What is the divisor out(Tensor,optional) a part of?",
        "context": "Supportsbroadcasting to a common shape,type promotion, and integer and float inputs. Note When the divisor is zero, returnsNaNfor floating point dtypes\non both CPU and GPU; raisesRuntimeErrorfor integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input(Tensor) \u2013 the dividend other(TensororScalar) \u2013 the divisor out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "answer": "input-other",
        "question": "What is the p-norm of?",
        "context": "Returns the p-norm of (input-other) The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the Right-hand-side input tensor p(float,optional) \u2013 the norm to be computed Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist"
    },
    {
        "answer": "the norm to be computed",
        "question": "What does p(float,optional) return?",
        "context": "Returns the p-norm of (input-other) The shapes ofinputandothermust bebroadcastable. input(Tensor) \u2013 the input tensor. other(Tensor) \u2013 the Right-hand-side input tensor p(float,optional) \u2013 the norm to be computed Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist"
    },
    {
        "answer": "Alias fortorch.mul",
        "question": "What is another name for Alias fortorch.mul?",
        "context": "Alias fortorch.mul(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply"
    },
    {
        "answer": "Alias fortorch.mul()",
        "question": "What does Alias fortorch.mul() do?",
        "context": "Alias fortorch.mul(). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply"
    },
    {
        "answer": "1",
        "question": "What is the first argument of the matrix-matrix product?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "a batched matrix multiply",
        "question": "What is returned if both arguments are at least 1-dimensional and at least one argument is N-dimensional?",
        "context": "If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1",
        "question": "If the first argument is 2-dimensional and the second argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the batched",
        "context": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If the second argument is what, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after?",
        "context": "If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "non-matrix",
        "question": "What type of dimensions are broadcastable?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "batch",
        "question": "What is another term for non-matrix dimensions?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1",
        "question": "What dimension is the first argument in a batched matrix multiply?",
        "context": "If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "broadcastable",
        "question": "What must the non-matrix dimensions be?",
        "context": "If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "final two dimensions",
        "question": "Inputs are valid for broadcasting even though what are different?",
        "context": "Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "multi-dimensional matrix",
        "question": "Atorch.Tensoris is a what?",
        "context": "Atorch.Tensoris a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "LongTensor Boolean torch",
        "question": "What type of torch does Torch define?",
        "context": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dtype",
        "question": "What is the data type of CPU tensor GPU tensor 32-bit floating point torch?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "dtype CPU tensor GPU tensor",
        "question": "What is the name of the 32-bit floating point torch?",
        "context": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "float32ortorch",
        "question": "What is the name of the float torch?",
        "context": "GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "32",
        "question": "How many bits of floating point torch does float32ortorch.float torch.FloatTensor torch.cuda.",
        "context": "32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit floating point",
        "question": "What is 1 torch?",
        "context": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "float16ortorch.half torch",
        "question": "What is a torch called?",
        "context": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "1 sign, 5 exponent, and 10 significand bits",
        "question": "What does binary16 use?",
        "context": "128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "precision is important at the expense of range",
        "question": "When is binary16 useful?",
        "context": "torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Brain Floating Point",
        "question": "What is binary16 sometimes referred to as?",
        "context": "32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "range is important",
        "question": "When is Brain Floating Point useful?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "binary16",
        "question": "ByteTensor is sometimes referred to as what?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is the quantized 4-bit integer stored as?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "EmbeddingBag operator",
        "question": "What is Brain Floating Point only supported in?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is float32 quantized 4-bit integer stored as?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "EmbeddingBag operator",
        "question": "What operator supports Brain Floating Point?",
        "context": "32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "default tensor type",
        "question": "What is torch.Tensoris an alias for?",
        "context": "32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "thetorch.tensor()",
        "question": "How can a tensor be constructed from a Pythonlistor sequence?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "userequires_grad_()ordetach()",
        "question": "What is used to avoid a copy of a tensordata?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "automatic differentiation",
        "question": "What does thattorch.autogradrecords operations do on a tensor?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "indexing",
        "question": "What is Indexing, Slicing, Joining, Mutating Ops?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "multi-dimensional,stridedview",
        "question": "What type of storage does the tensor class provide?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "underscore",
        "question": "What suffix mark methods which mutate a tensor?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What is the term for a tensor'storch.deviceand/ortorch.dtype?",
        "context": "For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Warning",
        "question": "What is a tensor'storch.deviceand/ortorch.dtype?",
        "context": "A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor",
        "question": "What are there a few main ways to create?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "*tensor creation ops",
        "question": "What is one way to create a tensor with specific size?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "withfill_value",
        "question": "What does Tensor.new_full return a Tensor of sizesizefilled?",
        "context": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, usetorch.tensor(). To create a tensor with specific size, usetorch.*tensor creation\nops (seeCreation Ops). To create a tensor with the same size (and similar types) as another tensor,\nusetorch.*_liketensor creation ops\n(seeCreation Ops). To create a tensor with similar type but different size as another tensor,\nusetensor.new_*creation ops. Is this Tensor with its dimensions reversed. Ifnis the number of dimensions inx,x.Tis equivalent tox.permute(n-1,n-2,...,0). Tensor.new_tensor Returns a new Tensor withdataas the tensor data. Tensor.new_full Returns a Tensor of sizesizefilled withfill_value. Tensor.new_empty Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Add a scalar or tensor toselftensor",
        "question": "What is added to a scalar or tensor toselftensor?",
        "context": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ Fills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). Tensor.bfloat16 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addcdiv",
        "question": "What does Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor",
        "context": "IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor?",
        "context": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "real values of theselftensor",
        "question": "What does Tensor.ndim Alias fordim() Tensor.real Return a new tensor containing?",
        "context": "Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Add a scalar or tensor toselftensor",
        "question": "What is added to a scalar or tensor?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What Seetorch.addbmm() Tensor?",
        "context": "Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "imaginary values",
        "question": "What does Tensor.imag return?",
        "context": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Add a scalar or tensor toselftensor",
        "question": "What does add do to a scalar or tensor?",
        "context": "Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor",
        "question": "What is the name of the scalar or tensor toselftensor?",
        "context": "Returns a Tensor of sizesizefilled with uninitialized data. Tensor.new_ones Returns a Tensor of sizesizefilled with1. Tensor.new_zeros Returns a Tensor of sizesizefilled with0. Tensor.is_cuda IsTrueif the Tensor is stored on the GPU,Falseotherwise. Tensor.is_quantized IsTrueif the Tensor is quantized,Falseotherwise. Tensor.is_meta IsTrueif the Tensor is a meta tensor,Falseotherwise. Tensor.device Is thetorch.devicewhere this Tensor is. Tensor.grad This attribute isNoneby default and becomes a Tensor the first time a call tobackward()computes gradients forself. Tensor.ndim Alias fordim() Tensor.real Returns a new tensor containing real values of theselftensor. Tensor.imag Returns a new tensor containing imaginary values of theselftensor. Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Add a scalar or tensor toselftensor",
        "question": "What does add a scalar or tensor toselftensor do?",
        "context": "In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What does Seetorch.addbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.",
        "context": "Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "scalar or tensor toselftensor",
        "question": "What do you add to a tensor?",
        "context": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "the value returned bycallable",
        "question": "What is replaced by each element in the tensor?",
        "context": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "PyTorch",
        "question": "What represent a multi-dimensional array containing elements of a single data type?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "array elements",
        "question": "What are stored contiguously in memory?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse arrays",
        "question": "What is another name for multi-dimensional arrays where the contiguous memory storage of array elements turns out to be suboptimal?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Sparse arrays",
        "question": "What has a property of having a vast portion of elements being equal to zero?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO, CSR/CSC, LIL",
        "question": "What are some sparse storage formats?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "specific operations on the arrays",
        "question": "What are sparse storage formats optimized for?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What is a sparse array format?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "specified elements",
        "question": "What do we use when talking about storing only non-zero elements of a sparse array?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "fill value",
        "question": "What term is used to denote the unspecified elements of a sparse array?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensor",
        "question": "What is at leastproduct(tensorshape>)*sizeofelementtypeinbytes>?",
        "context": "The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "10 000 x 10 000",
        "question": "What is the size of a strided tensor?",
        "context": "The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "200 fold",
        "question": "How much memory saving does a sparse COO tensor have from using the COO storage format?",
        "context": "The memory consumption of a sparse COO tensor is at least(ndim*8+<sizeofelementtypeinbytes>)*nsebytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at leastproduct(<tensorshape>)*<sizeofelementtypeinbytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "at least(2*8+4)*100000=2000000bytes",
        "question": "What is the memory consumption of a 10 000 x 10 000 tensor with 100 000 non-zero 32-bit floating point numbers",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero",
        "question": "What is the default value of the fill value in a sparse tensor?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "transpose",
        "question": "If you want to write your indices this way, you should do what before passing them to the sparse constructor?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "index tuples",
        "question": "What is the inputinot a list of?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "multi-dimensional tensor",
        "question": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing thevaluestensor to",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where is the entry [5, 6] in a sparse COO tensor?",
        "context": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "write",
        "question": "What would we do to create a sparse COO tensor?",
        "context": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where is the entry [5, 6] in a dimensional tensor?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "write",
        "question": "What would we do to create a 2 + 1-dimensional tensor?",
        "context": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "inindicestensor",
        "question": "The indices of specified elements are collected what?",
        "context": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where do we want to create a 2 + 1-dimensional tensor?",
        "context": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "entry 3",
        "question": "What is the location of a sparse tensor?",
        "context": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least(2*8+4)*100000=2000000bytes when using COO tensor\nlayout and10000*10000*4=400000000bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "What are the values of a sparse tensor stored as?",
        "context": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "What are the values of a hybrid sparse tensor stored as?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "What are the values of a hybrid tensor stored as?",
        "context": "In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "strided tensors",
        "question": "What are values stored as?",
        "context": "s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "duplicate coordinates",
        "question": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors, where",
        "context": "In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D",
        "question": "What is the uncoalesced tensor?",
        "context": "Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "multi-valued elements",
        "question": "What will the coalescing process accumulate into a single value using summation?",
        "context": "M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "multiple values",
        "question": "What leads to an 1-D uncoalesced tensor?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "most operations will work identically",
        "question": "What happens when a sparse tensor is coalesced or uncoalesced?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "duplicate coordinates",
        "question": "What can be found in the indices of PyTorch sparse COO tensors?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the indices of specified tensor elements are unique",
        "question": "What are the properties of a sparse tensor?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "to prevent them from growing too large",
        "question": "Why should you coalesce sparse tensors?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical",
        "question": "In what order are indices sorted?",
        "context": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "torch.Tensor.is_coalesced()",
        "question": "What returns true?",
        "context": "torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical ordering",
        "question": "What can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse",
        "question": "What type of COO tensor is atorch.Tensorinstance?",
        "context": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "multiplying all the uncoalesced values with the scalar",
        "question": "How can the scalar multiplication on an uncoalesced sparse tensor be implemented?",
        "context": "but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a square root",
        "question": "What nonlinear operation cannot be implemented by applying the operation to uncoalesced data?",
        "context": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "additive nature of uncoalesced data",
        "question": "When working with uncoalesced sparse COO tensors, one must take into account what?",
        "context": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the terms of a sum",
        "question": "What are the values of the same indices?",
        "context": "but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "dense dimensions",
        "question": "Slicing (with positive step) of a sparse COO tensor is supported only for what?",
        "context": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Indexing",
        "question": "What is supported for both sparse and dense dimensions?",
        "context": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "multiplying all the uncoalesced values with the scalar",
        "question": "How could the scalar multiplication on an uncoalesced sparse tensor be implemented?",
        "context": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CSR",
        "question": "What format implements the CSR format for storage of 2 dimensional tensors?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "where the given row starts",
        "question": "The CSR sparse tensor encodes the index invaluesandcol_indicesdepending on what?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sizesize[0]+1",
        "question": "Thecrow_indicestensor is a 1-D tensor of what size?",
        "context": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "MKL-enabled matrix operations",
        "question": "What do you want to use if you want to usetorch.int32?",
        "context": "A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "MKL LP64",
        "question": "What is the default linking of pytorch with?",
        "context": "Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "compressed row indices",
        "question": "Thecrow_indicestensor consists of what?",
        "context": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "MKL",
        "question": "If you want to use what -enabled matrix operations, usetorch.int32.",
        "context": "Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "MKL-enabled matrix operations",
        "question": "What does usetorch.int32 do?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "The user must supply the row and column indices and values tensors separately",
        "question": "What does the user have to do to construct a sparse CSR matrices?",
        "context": "Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Thesizeargument",
        "question": "What is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zeros",
        "question": "What will be interpreted as missing values in the sparse tensor?",
        "context": "Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "MKL LP64",
        "question": "What is the default linking of pytorch?",
        "context": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "thetensor.matmul()method",
        "question": "What is currently the only math operation supported on CSR tensors?",
        "context": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "currently the only math operation supported on CSR tensors",
        "question": "What is the sparse matrix-vector multiplication?",
        "context": "Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch",
        "question": "PyTorch, operation Sparse grad?",
        "context": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo]@V[strided]->V[strided] torch.mv() no M[sparse_csr]@V[strided]->V[strided] torch.matmul() no M[sparse_coo]@M[strided]->M[strided] torch.matmul() no M[sparse_csr]@M[strided]->M[strided] torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]->M[strided]",
        "question": "What does torch.mm() no?",
        "context": "torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "All PyTorch operations, excepttorch.smm(), support",
        "question": "Which PyTorch operations support backward with respect to sparse matrix argument?",
        "context": "torch.mm() no M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M[sparse_coo]@M[strided]->M[strided]",
        "question": "What does torch.sparse.mm() do?",
        "context": "M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "All PyTorch operations, excepttorch.smm(), support backward with respect to",
        "question": "All PyTorch operations, excepttorch.smm(), support backward with respect to sparse matrix argument. <sep>",
        "context": "M[sparse_coo]@M[strided]->M[strided] torch.sparse.mm() yes M[sparse_coo]@M[strided]->M[strided] torch.smm() no M[sparse_coo]@M[strided]->M[sparse_coo] torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "SVD",
        "question": "What does M[sparse_coo] stand for?",
        "context": "torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch does not support matrix multiplication",
        "question": "What does PyTorch not support with the layout signatureM[strided]@M[sparse_coo]?",
        "context": "torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "matrix relationD@S==(S.t()@D.t()).t()",
        "question": "Applications can still compute this using what?",
        "context": "torch.hspmm() no M[sparse_coo]@M[strided]->M[hybridsparse_coo] torch.bmm() no T[sparse_coo]@T[strided]->T[strided] torch.addmm() no f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sparse.addmm() yes f*M[strided]+f*(M[sparse_coo]@M[strided])->M[strided] torch.sspaddmm() no f*M[sparse_coo]+f*(M[sparse_coo]@M[strided])->M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo])->M[strided],M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo])->M[strided],M[strided],M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo])->M[strided],M[strided],M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcepttorch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signatureM[strided]@M[sparse_coo]. However,\napplications can still compute this using the matrix relationD@S==(S.t()@D.t()).t(). ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse dimensions",
        "question": "What does Tensor.sparse_dim Return in asparse tensorself?",
        "context": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse copy of the tensor",
        "question": "What does Tensor.to_sparse Return?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What is the name of the tensor method that resizes itselfsparse tensorto the desired size and the number",
        "context": "IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What removes all specified elements from asparse tensorself?",
        "context": "Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.coalesce",
        "question": "What Returns a coalesced copy ofselfifselfis anuncoalesced tensor?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Trueifselfis",
        "question": "What does Tensor.is_coalesced return if asparse COO tensorthat is coalesced?",
        "context": "Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.to_dense",
        "question": "What Returns trueifselfis asparse COO tensorthat is coalesced?",
        "context": "Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CSR",
        "question": "The following methods are specific tosparse what tensors?",
        "context": "Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.crow_indices",
        "question": "What method is specific to sparse CSR tensors?",
        "context": "Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.col_indices",
        "question": "What returns the indices of theselftensor whenselfis a sparse CSR tensor of layoutspars",
        "context": "Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Convert a tensor to compressed row storage format",
        "question": "What is one way to convert a tensor to a compressed row storage format?",
        "context": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "indices",
        "question": "Return what tensor of asparse COO tensor?",
        "context": "Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "column indices",
        "question": "What does the tensor containing return when selfis a sparse CSR tensor of layoutsparse_",
        "context": "Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.crow_indices",
        "question": "What Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse C",
        "context": "Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "tensor",
        "question": "Return the values of what of asparse COO tensor?",
        "context": "Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Trueifselfis",
        "question": "What is returned when a sparse COO tensorthat is coalesced?",
        "context": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse CSR tensor",
        "question": "When selfis a sparse CSR tensor of layoutsparse_csr, what type of tens",
        "context": "Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a tensor with the same size asinputfilled withfill_value",
        "question": "What returns a tensor with the same size as inputfilled withfill_value?",
        "context": "Returns a tensor with the same size asinputfilled withfill_value.torch.full_like(input,fill_value)is equivalent totorch.full(input.size(),fill_value,dtype=input.dtype,layout=input.layout,device=input.device). input(Tensor) \u2013 the size ofinputwill determine size of the output tensor. fill_value\u2013 the number to fill the output tensor with. dtype(torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: ifNone, defaults to the dtype ofinput. layout(torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: ifNone, defaults to the layout ofinput. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, defaults to the device ofinput. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. memory_format(torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default:torch.preserve_format. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "answer": "return_complex",
        "question": "What must always be given explicitly for real inputs?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "real tensor",
        "question": "What cantorch.view_as_real() be used to recover?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "The STFT computes the Fourier transform",
        "question": "What does STFT compute of short overlapping windows of the input?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "thelibrosastft function",
        "question": "What is the interface of the STFT modeled after?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the index of the sliding window",
        "question": "What does wheremmmis?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return complex tensors",
        "question": "In a future pytorch release, this function will only what?",
        "context": "Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return complex tensors",
        "question": "In a future pytorch release, this function will only do what?",
        "context": "From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "The input must be either a what?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "wheremmmis the index of the sliding window",
        "question": "What is the index of the sliding window?",
        "context": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "Inputmust be either a what?",
        "context": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Ifhop_lengthisNone",
        "question": "What is the default value for a sliding window?",
        "context": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "lengthn_fft",
        "question": "Ifwin_lengthisNone(default), what will the window be padded on both sides to before being applied?",
        "context": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfcenterisTrue",
        "question": "What is the default for input to be padded on both sides?",
        "context": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "thettt-th frame",
        "question": "What begins at timethop_lengtht times texthop_lengththop_length?",
        "context": "wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Ifhop_lengthisNone",
        "question": "What is the default value for n_fft/4?",
        "context": "inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfcenterisTrue",
        "question": "Inputwill be padded on both sides so that thettt-th frame is centered at what?",
        "context": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Seetorch.nn.functional.pad()",
        "question": "What is the name of all available options?",
        "context": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "reflect",
        "question": "What is the default value for input when centerisTrue?",
        "context": "inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Ifhop_lengthisNone",
        "question": "What is the default value that is treated as equal tofloor(n_fft/4)?",
        "context": "Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D tensor of sizewin_length",
        "question": "Windowcan be a what?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "lengthn_fft",
        "question": "Ifwin_lengthisNone(default),windowwill be padded on both sides to what before being applied?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "equal ton_fft",
        "question": "Ifwin_lengthisNone(default) is treated as what?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfwindowisNone",
        "question": "What is the default value for a window?",
        "context": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "reflect",
        "question": "What is the default setting for windows?",
        "context": "windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfnormalizedisTrue",
        "question": "What returns the normalized STFT results?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ainput.dim()+2dimensional real tensor",
        "question": "If true, the output is what?",
        "context": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the number of frequencies where STFT is applied",
        "question": "What is the optional batch size of input?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "input(Tensor)",
        "question": "What is the input tensor n_fft(int) \u2013 size of Fourier transform?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the total number of frames used",
        "question": "What is TTT?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "all111s",
        "question": "Default:None(treated as window of what number) \u2013 the optional window function.",
        "context": "Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "input(Tensor)",
        "question": "What is the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int",
        "context": "Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Default:None",
        "question": "What is the default value for window of all111s?",
        "context": "n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "normalized",
        "question": "What type of STFT results does \"reflect\" return?",
        "context": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "redundancy",
        "question": "What does false onesided(bool,optional) avoid for real inputs?",
        "context": "n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex",
        "question": "What is the name of the function that returns a complex tensor?",
        "context": "hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "return_complex",
        "question": "What is the name of the tensor function that returns a complex tensor?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "tensor",
        "question": "What containing the STFT result with shape described above?",
        "context": "win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Ifminlengthis specified",
        "question": "If the number of bins is at leastminlength, then the result is a tensor of sizeminlengthfilled with zeros.",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "optional",
        "question": "Input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 what?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "same size",
        "question": "What should the input tensor weights(Tensor) be of?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "minlength(int)",
        "question": "What is the optional, minimum number of bins?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "non-negative",
        "question": "Minlength(int) \u2013 optional, minimum number of bins. Should be what?",
        "context": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value ininputunlessinputis empty, in which case the result is a\ntensor of size 0. Ifminlengthis specified, the number of bins is at leastminlengthand ifinputis empty, then the result is tensor of sizeminlengthfilled with zeros. Ifnis the value at positioni,out[n]+=weights[i]ifweightsis specified elseout[n]+=1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. SeeReproducibilityfor more information. input(Tensor) \u2013 1-d int tensor weights(Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength(int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shapeSize([max(input)+1])ifinputis non-empty, elseSize(0) output (Tensor) Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "answer": "CUDA",
        "question": "When running on what platform,row*colmust be less than259259259 to prevent overflow during calculation?",
        "context": "Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "currently only supporttorch.strided",
        "question": "What is layout(torch.layout, optional)?",
        "context": "Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Example:",
        "question": "What is an example of a CUDA tensor type?",
        "context": "Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "zeros",
        "question": "Returns a 2-D tensor with ones on the diagonal and what else?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "ifNone",
        "question": "What is the default for a global default?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "Default:torch.strided",
        "question": "What is the default setting for a 2-D tensor with ones on the diagonal and zeros elsewhere?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "False",
        "question": "What is the default value for autograd?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "zeros",
        "question": "A 2-D tensor with ones on the diagonal and what else?",
        "context": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n(int) \u2013 the number of rows m(int,optional) \u2013 the number of columns with default beingn out(Tensor,optional) \u2013 the output tensor. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault:torch.strided. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad(bool,optional) \u2013 If autograd should record operations on the\nreturned tensor. Default:False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "answer": "shape parameterbeta",
        "question": "Computes the Kaiser window with what?",
        "context": "Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "callingtorch.kaiser_window(L+1,B,periodic=False)",
        "question": "What is the equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)?",
        "context": "Computes the Kaiser window with window lengthwindow_lengthand shape parameterbeta. Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "one",
        "question": "Ifwindow_lengthis what, then the returned window is a single element tensor containing a one?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "beta",
        "question": "What is the shape parameter for the window?",
        "context": "Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). layout(torch.layout, optional) \u2013 the desired layout of returned window tensor. Onlytorch.strided(dense layout) is supported. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "zeroth order",
        "question": "Let I_0 be the modified Bessel function of the first kind?",
        "context": "Let I_0 be the zeroth order modified Bessel function of the first kind (seetorch.i0()) andN=L-1ifperiodicis False andLifperiodicis True,\nwhereLis thewindow_length. This function computes: Callingtorch.kaiser_window(L,B,periodic=True)is equivalent to callingtorch.kaiser_window(L+1,B,periodic=False)[:-1]).\nTheperiodicargument is intended as a helpful shorthand\nto produce a periodic window as input to functions liketorch.stft(). Note Ifwindow_lengthis one, then the returned window is a single element tensor containing a one. window_length(int) \u2013 length of the window. periodic(bool,optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta(float,optional) \u2013 shape parameter for the window. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone, uses a global default (seetorch.set_default_tensor_type()). ",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "answer": "eigenvalues",
        "question": "Computes the eigenvalue decomposition of a square matrix if it exists. Computes the eigenvalue decom",
        "context": "Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalues of a square matrix",
        "question": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.",
        "context": "Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "singular values of a matrix",
        "question": "Computes the singular value decomposition (SVD) of a matrix. Computes the solution of a square system of linear equation",
        "context": "Computes a vector norm.   Computes a matrix norm.   Computes the determinant of a square matrix.   Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "eigenvalue decomposition",
        "question": "Computes the inverse of a square matrix if it exists. Computes the eigenvalues of a square matrix.",
        "context": "Computes the sign and natural logarithm of the absolute value of the determinant of a square matrix.   Computes the condition number of a matrix with respect to a matrix norm.   Computes the numerical rank of a matrix.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   Computes the QR decomposition of a matrix.   Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix",
        "question": "What does Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix?",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes the singular values of a matrix",
        "question": "What is the singular value decomposition of a matrix?",
        "context": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "multiplies two or more matrices",
        "question": "What does reordering the multiplications do?",
        "context": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "inverse",
        "question": "Computes the eigenvalue decomposition of a square matrix if it exists. Computes the pseudoinverse (Moore-",
        "context": "Computes the eigenvalue decomposition of a square matrix if it exists.   Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "if it exists",
        "question": "Computes the inverse of a square matrix what?",
        "context": "Computes the eigenvalues of a square matrix.   Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix",
        "question": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix?",
        "context": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes then-th power of a square matrix for an integern.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the firstncolumns of a product of Householder matrices.   Computes the multiplicative inverse oftorch.tensordot().   Computes the solutionXto the systemtorch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.   ",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "answer": "seetorch.lu()",
        "question": "What is used to get the LU factorization?",
        "context": "LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Note Irrespective of the original strides",
        "question": "What should X=torch.solve(B,A)solution be replaced with?",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "A.contiguous().transpose(-1, -2)",
        "question": "What is the name of the strides that are used to transpose the returned matrices?",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "zero or more batch dimensions",
        "question": "What is input matrixBBBof size(,m,k)(*, m, k)(,m,k)",
        "context": "LUcontainsLandUfactors for LU factorization ofA. torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "seetorch.lu()",
        "question": "To get the LU factorization of the input, what may be used withtorch.lu_solve() andtorch.",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "zero or more batch dimensions",
        "question": "What does the input matrixBBBof size(,m,k)(*, m, k)(,m,k",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "A(Tensor)",
        "question": "What is an input square matrix of size(,m,m)(*, m, m)(,m,m)",
        "context": "torch.solve(B, A)can take in 2D inputsB, Aor inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputssolution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve()is deprecated in favor oftorch.linalg.solve()and will be removed in a future PyTorch release.torch.linalg.solve()has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization seetorch.lu(),\nwhich may be used withtorch.lu_solve()andtorch.lu_unpack(). X=torch.solve(B,A).solutionshould be replaced with Note Irrespective of the original strides, the returned matricessolutionandLUwill be transposed, i.e. with strides likeB.contiguous().transpose(-1, -2).stride()andA.contiguous().transpose(-1, -2).stride()respectively. input(Tensor) \u2013 input matrixBBBof size(\u2217,m,k)(*, m, k)(\u2217,m,k), where\u2217*\u2217is zero or more batch dimensions. A(Tensor) \u2013 input square matrix of size(\u2217,m,m)(*, m, m)(\u2217,m,m), where\u2217*\u2217is zero or more batch dimensions. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What is returned when a library is compiled for CUDA?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the properties of a device",
        "question": "What does Gets the properties of a device?",
        "context": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "peer access between two devices",
        "question": "Checks if what is possible. Returns cublasHandle_t pointer to current cuBLAS handle. Returns the index",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "Returns a bool indicating if PyTorch\u2019s CUDA state has been initialized.",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "name",
        "question": "Gets the cuda capability of a device. Gets the properties of a device.",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "a bool",
        "question": "What indicating if CUDA is currently available?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current device",
        "question": "Sets what if PyTorch's CUDA state has been initialized?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selectedStreamfor a given device",
        "question": "Returns what stream for a given device?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "set the stream",
        "question": "Sets the current device. This is a wrapper API to what?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "StreamContext",
        "question": "What is the wrapper around that selects a given stream?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What is returned when a device is selected?",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What does Returns a bool indicating if PyTorch\u2019s CUDA state has been initialized?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "What happens when a CUDA device is selected?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What is returned when a device is compiled?",
        "context": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "set the stream",
        "question": "Sets the current stream.This is a wrapper API to what?",
        "context": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "What does the wrapper API do?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "ByteTensor",
        "question": "What is the random number generator state of the specified GPU?",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the number of GPUs available",
        "question": "What does Return?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What is returned when a bool indicating if CUDA is currently available?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does Set the random number generator state of the specified GPU do?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What is the cuda capability of a device?",
        "context": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What does Returns a bool indicating if CUDA is currently available?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "What happens when all kernels in all streams on a CUDA device complete?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does Sets the random number generator state of the specified GPU do?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "random number generator state",
        "question": "Sets what of all devices?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What does the bool indicating if CUDA is currently available return?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of all devices",
        "question": "What does Sets the seed for generating random numbers for the current GPU do?",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers for the current GPU",
        "question": "What does Sets the seed for generating random numbers for the current GPU?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers on all GPUs",
        "question": "What does Sets the seed for generating random numbers on all GPUs?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does it do when a ByteTensor returns a list of ByteTensors representing the random number",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "generating random numbers",
        "question": "Sets the seed for what to a random number on all GPUs?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current random seed of the current GPU",
        "question": "What does Sets the seed for generating random numbers on all GPUs return?",
        "context": "  Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "tensors",
        "question": "Returns the current GPU memory occupied by what in bytes for a given device?",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU memory managed by the caching allocator in bytes",
        "question": "Returns the maximum GPU memory managed by the caching allocator for a given device.",
        "context": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors",
        "question": "Returns the starting point in tracking maximum GPU memory occupied by tensors for a given device.",
        "context": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current GPU memory managed by the caching allocator in bytes",
        "question": "Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.",
        "context": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the maximum GPU memory managed by the caching allocator in bytes for a given device",
        "question": "What does Returns the current GPU memory managed by the caching allocator in bytes for a given device?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Set memory fraction",
        "question": "What does seememory_reserved() do?",
        "context": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Resets the starting point in tracking maximum GPU memory occupied by tensors",
        "question": "Returns the starting point in tracking maximum GPU memory occupied by tensors in bytes for a given device.",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "seemax_memory_reserved()",
        "question": "What is the name of the function that returns the memory fraction for a process?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "\u201cpeak\u201d stats",
        "question": "Resets what stats tracked by the CUDA memory allocator?",
        "context": "  Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "BETA",
        "question": "In what version of PyTorch is the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "New functions",
        "question": "What are still being added to the torch.special module?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "documentation",
        "question": "What is the name of each function that may change in future PyTorch releases?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the inverse error function ofinput",
        "question": "What does the torch.special module do?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the complementary error function ofinput defined as?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the inverse error function ofinput",
        "question": "What is the name of the function that computes the inverse error function ofinput?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the exponential of the elements minus 1 ofinput",
        "question": "What is an example of a function that provides greater precision than exp(x) - 1 for small values of x?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "inverse error function",
        "question": "Computes what error function ofinput?",
        "context": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the complementary error function of input defined as?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the inverse error function defined as?",
        "context": "Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "the exponentially scaled zeroth order modified Bessel function",
        "question": "Computes what for each element of input?",
        "context": "Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Generates a Vandermonde matrix",
        "question": "What does a Vandermonde matrix do?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Alexandre-Theophile Vandermonde",
        "question": "What is a Vandermonde matrix named for?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "N(int,optional)",
        "question": "What is the number of columns in the output?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "increasing(bool,optional)",
        "question": "What is the name of the order of the powers of the columns in a Vandermonde matrix?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "the powers increase from left to right",
        "question": "If increasing is True, what happens to the powers of the columns in a Vandermonde matrix?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "Vandermonde matrix",
        "question": "What is the name of the matrix with a geometric progression in each row named for Alexandre-Theophile Vandermonde?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "False",
        "question": "If increasing is True, the columns of the Vandermonde matrix are reversed if what?",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "True",
        "question": "If increasing is False, the columns arex0,x1,...,x(N1)x0, x1, ",
        "context": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vectorx(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversedx0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x(Tensor) \u2013 1-D input tensor. N(int,optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned(N=len(x))(N = len(x))(N=len(x)). increasing(bool,optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column isx(N\u22121)x^{(N-1)}x(N\u22121),\nthe secondx(N\u22122)x^{(N-2)}x(N\u22122)and so forth. If increasing is True, the columns\narex0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "answer": "oftensor1bytensor2",
        "question": "What is the element-wise division performed by addcdiv?",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes ofinput,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "future addcdiv behavior",
        "question": "What is just the latter implementation of addcdiv?",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes ofinput,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "shapes",
        "question": "What of input,tensor1, andtensor2must bebroadcastable?",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes ofinput,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "input(Tensor)",
        "question": "What is the tensor to be added tensor1(Tensor) \u2013 the numerator tensor",
        "context": "Performs the element-wise division oftensor1bytensor2,\nmultiply the result by the scalarvalueand add it toinput. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes ofinput,tensor1, andtensor2must bebroadcastable. For inputs of typeFloatTensororDoubleTensor,valuemust be\na real number, otherwise an integer. input(Tensor) \u2013 the tensor to be added tensor1(Tensor) \u2013 the numerator tensor tensor2(Tensor) \u2013 the denominator tensor value(Number,optional) \u2013 multiplier fortensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "answer": "aFuturetype",
        "question": "What encapsulates an asynchronous execution and a set of utility functions to simplify operations onFutureobjects?",
        "context": "This package provides aFuturetype that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\nonFutureobjects. Currently, theFuturetype is primarily used by theDistributed RPC Framework. Wrapper around atorch._C.Futurewhich encapsulates an asynchronous\nexecution of a callable, e.g.rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "ReturnTrueif thisFutureis done",
        "question": "What does thisFuture return if it has a result or an exception?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If the value contains what that reside on GPUs, Future.done()will returnTrueeven if the asynchronous kernels that are",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "ReturnTrue",
        "question": "What does Future.done() return if thisFutureis done?",
        "context": "is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "inline",
        "question": "When calling wait()/value() on thisFuture, the exception set here will be raised what?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aFuturecannot be marked completed twice",
        "question": "What does the exception for thisFuture mean?",
        "context": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If the result contains what that reside on GPUs, this method can be called even if the asynchronous kernels that are populating those ",
        "context": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "immediately after launching those kernels",
        "question": "When is it safe to call this method?",
        "context": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "This method will record events on all the relevant current streams and use them to ensure proper scheduling for all the consumers of what?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aFuturecannot be marked completed twice",
        "question": "What does thisFuture not do?",
        "context": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "current ones",
        "question": "What are the streams on which asynchronous kernels were enqueued set as when this method is called?",
        "context": "result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "This method will use the events to ensure proper scheduling for all the consumers of what?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Append",
        "question": "What does the callback function do to thisFuture?",
        "context": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "chaining:fut.then(cb1).then(cb2)",
        "question": "What can be used to enforce a certain order?",
        "context": "result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "chaining:fut.then(cb1).then(cb2)",
        "question": "What is a way to enforce a certain order?",
        "context": "Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thevalue()method",
        "question": "What method can the callback function use to get the value?",
        "context": "Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If theFuture's value contains what that reside on GPUs, the callback might be invoked while the async kernels",
        "context": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "current",
        "question": "The callback will be invoked with dedicated streams set as what?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "after the kernels complete",
        "question": "When will any operation performed by the callback on tensors be scheduled on the device?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "switch streams",
        "question": "What does the callback not do?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "non-blocking behavior ofwait()",
        "question": "What is similar to the non-blocking behavior ofwait()?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "What does callback(Callable) take as the only argument?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "when the givencallbackfinishes",
        "question": "When will a newFutureobject that holds the return value of thecallback be marked as completed?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Waits for all provided futures to be complete",
        "question": "What does this method do?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures(list) \u2013 a list ofFutureobject. A list of the completedFutureresults. This\nmethod will throw an error ifwaiton anyFuturethrows. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Obtain the value of an already-completed future",
        "question": "What is a method that can be called after a call towait() has been completed?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "the future returned bythen",
        "question": "What will be marked appropriately with the encountered error if the callback function throws?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "separately",
        "question": "If the value contains tensors that reside on GPUs, this method willnotperform any additional synchronization. This should be done",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "completion/waiting",
        "question": "What is the user responsible for handling if a callback later completes additional futures?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "What may not yet hold a value and callingvalue()could fail?",
        "context": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "until the value of thisFutureis ready",
        "question": "How long should thisFuture be blocked?",
        "context": "This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. If the value contains tensors that reside on GPUs, then this method willnotperform any additional synchronization. This should be done\nbeforehand, separately, through a call towait()(except within\ncallbacks, for which it\u2019s already being taken care of bythen()). The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisvalue()method",
        "question": "What method will throw an error if the function (callback or RPC) creating the value has thrown an error?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "until the value of thisFutureis ready",
        "question": "When does block occur?",
        "context": "Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "If the value contains what that reside on GPUs, an additional synchronization is performed with the kernels (executing on the device) which may be",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()will insert the necessary instructions in the current streams",
        "question": "What does non-blocking do to ensure that further operations enqueued on those streams will be properly scheduled after the async",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "No further synchronization is required",
        "question": "What is required when accessing and using the values?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thiswaitmethod",
        "question": "What will throw an error if the function (callback or RPC) creating the value has thrown an error?",
        "context": "The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thisvalue()method will\nalso throw an error. Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "synchronization",
        "question": "What is performed if the value contains tensors that reside on GPUs?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()will insert the necessary instructions in the current streams",
        "question": "What happens if the value contains tensors that reside on GPUs?",
        "context": "Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "No further synchronization",
        "question": "What is required when accessing and using the values, as long as one doesn\u2019t change streams?",
        "context": "Block until the value of thisFutureis ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thiswaitmethod",
        "question": "If the function (callback or RPC) creating the value has thrown an error, what will also throw an error?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "wait()will insert the necessary instructions in the current streams",
        "question": "What does non-blocking mean?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "when all of the sub-futures are completed",
        "question": "When is the combinedFuture completed?",
        "context": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means thatwait()will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done,wait()will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by thisFuture. If the function (callback or RPC)\ncreating the value has thrown an error, thiswaitmethod will\nalso throw an error. Collects the providedFutureobjects into a single\ncombinedFuturethat is completed when all of the\nsub-futures are completed. futures(list) \u2013 a list ofFutureobjects. Returns aFutureobject to a list of the passed\nin Futures. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "Matrix product of two tensors",
        "question": "What product is returned if both tensors are 1-dimensional?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If both tensors are what dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional,",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix-matrix",
        "question": "If both arguments are 2-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, what product is returned?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "matrix multiply",
        "question": "If the first argument is 1-dimensional and the second argument is 2-dimensional, a 1 is prepended to its dimension for the purpose of what?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the prepended dimension is removed",
        "question": "After the matrix multiply, what happens to the prepended dimension?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "batched matrix multiply",
        "question": "If both arguments are at least 1-dimensional and at least one argument is N-dimensional (where N > 2), what is returned?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If the first argument is what dimension, a 1 is prepended to its dimension for the purpose of the batched matrix multiply and removed after?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If the second argument is what dimension, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "broadcastable",
        "question": "The non-matrix dimensions are Broadcasted and therefore must be what?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the broadcasting logic",
        "question": "What only looks at the batch dimensions when determining if the inputs are broadcastable, and not the matrix dimensions?",
        "context": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "If both tensors are what, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "1-dimensional",
        "question": "The matrix-matrix product is returned if the first argument is what dimension?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "batched matrix multiply",
        "question": "If both arguments are at least 1-dimensional and at least one argument is N-dimensional, what is returned?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "the broadcasting logic",
        "question": "What only looks at the batch dimensions when determining if the inputs are broadcastable?",
        "context": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions arebroadcasted(and thus\nmust be broadcastable).  For example, ifinputis a(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n)tensor andotheris a(k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)tensor,outwill be a(j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n)tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, ifinputis a(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m)tensor andotheris a(k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different.outwill be a(j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p)tensor. This operator supportsTensorFloat32. Note The 1-dimensional dot product version of this function does not support anoutparameter. input(Tensor) \u2013 the first tensor to be multiplied other(Tensor) \u2013 the second tensor to be multiplied out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "answer": "Torch",
        "question": "What defines 10 tensor types with CPU and GPU variants?",
        "context": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Data type dtype CPU tensor GPU tensor",
        "question": "What is the name of the data type dtype CPU tensor GPU tensor?",
        "context": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "CPU tensor",
        "question": "What is a GPU tensor?",
        "context": "CPU tensor GPU tensor 32-bit floating point torch.float32ortorch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "64-bit floating point",
        "question": "What type of torch?",
        "context": "64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "precision",
        "question": "Useful when what is important at the expense of range?",
        "context": "torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Sometimes referred",
        "question": "What is sometimes referred to as binary16?",
        "context": "64-bit floating point torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Brain Float",
        "question": "What is Brain Float sometimes referred to as?",
        "context": "torch.float64ortorch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16",
        "question": "How many bits floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.",
        "context": "torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Brain Floating Point",
        "question": "What is another term for the use of 1 sign, 8 exponent, and 7 significand bits?",
        "context": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Use",
        "question": "Brain Floating Point uses 1 sign, 8 exponent, and 7 significand bits. Useful when precision is important at the expense of range",
        "context": "torch.cuda.DoubleTensor 16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16",
        "question": "How many bits are floating point1?",
        "context": "16-bit floating point1 torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "precision is important at the expense of range",
        "question": "When precision is important at the expense of range?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "range is important",
        "question": "Useful when precision is important at the expense of range.",
        "context": "torch.float16ortorch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Brain Floating Point",
        "question": "What is another name for a sign, 8 exponent, and 7 significand bits?",
        "context": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "binary16",
        "question": "What is the term for ByteTensor?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "range is important",
        "question": "Useful when precision is important at the expense of range, since it has the same number of exponent bits asfloat32 quantized 4-bit integer is",
        "context": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "precision",
        "question": "What is important at the expense of range?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Currently it",
        "question": "What is the current state of the torch?",
        "context": "torch.cuda.HalfTensor 16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is a Brain Floating Point stored as?",
        "context": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "EmbeddingBag operator",
        "question": "Where is EmbeddingBag only supported?",
        "context": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is a float32 quantized 4-bit integer stored as?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "EmbeddingBag operator",
        "question": "Currently it\u2019s only supported in what?",
        "context": "16-bit floating point2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "torch.Tensoris",
        "question": "What is an EmbeddingBag operator?",
        "context": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is a quantized 4-bit integer stored as?",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "EmbeddingBag operator",
        "question": "Currently it\u2019s only supported in what operator?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Brain Floating Point",
        "question": "What is the term for a sign, 8 exponent, and 7 significand bits?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Pythonlistor",
        "question": "A tensor can be constructed from what?",
        "context": "torch.complex64 128-bit complex torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Pythonlistor sequence",
        "question": "What can a tensor be constructed from?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit integer",
        "question": "What is unsigned?",
        "context": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit signed integer",
        "question": "What is stored as a quantized 4-bit integer?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "EmbeddingBag operator",
        "question": "In what operator is Brain Floating Point only supported?",
        "context": "torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Pythonlistor",
        "question": "A tensor can be constructed from what sequence?",
        "context": "torch.complex128ortorch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tens",
        "question": "What type of Tensor?",
        "context": "torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "binary16",
        "question": "What is another name for ByteTensor?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensordata",
        "question": "What do you have if you want a tensor to be constructed from a Pythonlistor sequence?",
        "context": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "itsrequires_gradflag, userequires_grad_()ordetach",
        "question": "If you have a Tensordataand just want to change what?",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "8-bit",
        "question": "What type of integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16",
        "context": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "16-bit",
        "question": "What type of integer (signed) is torch?",
        "context": "16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "userequires_grad_()ordetach()",
        "question": "What is used to avoid a copy of a Tensordata?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "usetorch.as_tensor()",
        "question": "What is a way to avoid a copy of a numpy array?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "specific data type",
        "question": "A tensor of what can be a tensor of?",
        "context": "torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Pythonlistor",
        "question": "What sequence can a tensor be constructed from?",
        "context": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16ortorch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "usetorch.as_tensor()",
        "question": "If you have a numpy array and want to avoid a copy, what is the best way to avoid a copy?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "atorch.dtype",
        "question": "How can a tensor of specific data type be constructed?",
        "context": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32ortorch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64ortorch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned)3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits asfloat32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensoris an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "usetorch.as_tensor()",
        "question": "What is used to avoid a copy of a numpy array?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "specific data type",
        "question": "A tensor of what can be constructed by passing atorch.dtypeand/or atorch.device to a construct",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "tensor views",
        "question": "What are Tensor Views?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "thetorch.dtype,torch.device, andtorch.layoutattributes",
        "question": "What are some examples of atorch.Tensor Attributes?",
        "context": "A tensor can be constructed from a Pythonlistor sequence using thetorch.tensor()constructor: Warning torch.tensor()always copiesdata. If you have a Tensordataand just want to change itsrequires_gradflag, userequires_grad_()ordetach()to avoid a copy.\nIf you have a numpy array and want to avoid a copy, usetorch.as_tensor(). A tensor of specific data type can be constructed by passing atorch.dtypeand/or atorch.deviceto a\nconstructor or tensor creation op: For more information about building Tensors, seeCreation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Usetorch.Tensor.item()to get a Python number from a tensor containing a\nsingle value: For more information about indexing, seeIndexing, Slicing, Joining, Mutating Ops A tensor can be created withrequires_grad=Trueso thattorch.autogradrecords operations on them for automatic differentiation. Each tensor has an associatedtorch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional,stridedview of a storage and defines numeric operations on it. Note For more information on tensor views, seeTensor Views. Note For more information on thetorch.dtype,torch.device, andtorch.layoutattributes of atorch.Tensor, seeTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example,torch.FloatTensor.abs_()computes the absolute value\nin-place and returns the modified tensor, whiletorch.FloatTensor.abs()computes the result in a new tensor. Note To change an existing tensor\u2019storch.deviceand/ortorch.dtype, consider usingto()method on the tensor. Warning Current implementation oftorch.Tensorintroduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure. ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.addbmm",
        "question": "What is the In-place version of add()?",
        "context": "Tensor.abs Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Tensor.argmax",
        "question": "What does Tensor.argmax stand for?",
        "context": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ Fills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). Tensor.bfloat16 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Add a scalar or tensor toselftensor",
        "question": "What does Add a scalar or tensor toselftensor do?",
        "context": "Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What does addbmm stand for?",
        "context": "Seetorch.abs() Tensor.abs_ In-place version ofabs() Tensor.absolute Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "addbmm",
        "question": "What is the name of the element that adds a scalar or tensor toselftensor?",
        "context": "Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ Fills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). Tensor.bfloat16 ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "Alias forabs() Tensor.absolute",
        "question": "What does Alias forabs() Tensor.absolute stand for?",
        "context": "Alias forabs() Tensor.absolute_ In-place version ofabsolute()Alias forabs_() Tensor.acos Seetorch.acos() Tensor.acos_ In-place version ofacos() Tensor.arccos Seetorch.arccos() Tensor.arccos_ In-place version ofarccos() Tensor.add Add a scalar or tensor toselftensor. Tensor.add_ In-place version ofadd() Tensor.addbmm Seetorch.addbmm() Tensor.addbmm_ In-place version ofaddbmm() Tensor.addcdiv Seetorch.addcdiv() Tensor.addcdiv_ In-place version ofaddcdiv() Tensor.addcmul Seetorch.addcmul() Tensor.addcmul_ In-place version ofaddcmul() Tensor.addmm Seetorch.addmm() Tensor.addmm_ In-place version ofaddmm() Tensor.sspaddmm Seetorch.sspaddmm() Tensor.addmv Seetorch.addmv() Tensor.addmv_ In-place version ofaddmv() Tensor.addr Seetorch.addr() Tensor.addr_ In-place version ofaddr() Tensor.allclose Seetorch.allclose() Tensor.amax Seetorch.amax() Tensor.amin Seetorch.amin() Tensor.angle Seetorch.angle() Tensor.apply_ Applies the functioncallableto each element in the tensor, replacing each element with the value returned bycallable. Tensor.argmax Seetorch.argmax() Tensor.argmin Seetorch.argmin() Tensor.argsort Seetorch.argsort() Tensor.asin Seetorch.asin() Tensor.asin_ In-place version ofasin() Tensor.arcsin Seetorch.arcsin() Tensor.arcsin_ In-place version ofarcsin() Tensor.as_strided Seetorch.as_strided() Tensor.atan Seetorch.atan() Tensor.atan_ In-place version ofatan() Tensor.arctan Seetorch.arctan() Tensor.arctan_ In-place version ofarctan() Tensor.atan2 Seetorch.atan2() Tensor.atan2_ In-place version ofatan2() Tensor.all Seetorch.all() Tensor.any Seetorch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm Seetorch.baddbmm() Tensor.baddbmm_ In-place version ofbaddbmm() Tensor.bernoulli Returns a result tensor where eachresult[i]\\texttt{result[i]}result[i]is independently sampled fromBernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]). Tensor.bernoulli_ Fills each location ofselfwith an independent sample fromBernoulli(p)\\text{Bernoulli}(\\texttt{p})Bernoulli(p). ",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "answer": "A sparse COO tensor",
        "question": "What can be constructed by providing the two tensors of indices and values, as well as the size of the sparse ",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1, 0",
        "question": "What is the value of entry 4 in a sparse tensor?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "zero by default",
        "question": "What is the fill value of a sparse tensor?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(2 + 1)-dimensional",
        "question": "What type of tensor would we want to create?",
        "context": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a functiontorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1, 0",
        "question": "Suppose we want to define a sparse tensor with the entry 4 at location?",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M and K",
        "question": "What are the numbers of sparse and dense dimensions, respectively?",
        "context": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(2 + 1)",
        "question": "Suppose we want to create a what -dimensional tensor with the entry [3, 4] at location (0, 2), entry [",
        "context": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the inputiis NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthevaluestensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "Where is the entry [5, 6] in the tensor?",
        "context": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M+K==len(s.shape)==s.ndim",
        "question": "What dimensionality of a tensor is the sum of the number of sparse and dense dimensions?",
        "context": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch sparse COO tensor format",
        "question": "What permitsuncoalescedsparse tensors?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "For example, one",
        "question": "What is an example of a sparse COO tensor format that permitsuncoalescedsparse tens",
        "context": "the indices of specified elements are collected inindicestensor of size(sparse_dims,nse)and with element typetorch.int64, the corresponding (tensor) values are collected invaluestensor of size(nse,dense_dims)and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(2 + 1)",
        "question": "What is a dimensional tensor?",
        "context": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Dense dimensions",
        "question": "What always follow sparse dimensions?",
        "context": "Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch sparse COO tensor",
        "question": "What format permitsuncoalescedsparse tensors?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(2 + 1)",
        "question": "What is the definition of a dimensional tensor?",
        "context": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "(1, 0",
        "question": "What is the location of a 2 + 1)-dimensional tensor?",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "M+K==len(s.shape)==s.ndim",
        "question": "What is the sum of the number of sparse and dense dimensions in a tensor?",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch",
        "question": "What sparse COO tensor format permitsuncoalescedsparse tensors?",
        "context": "In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note For the most part",
        "question": "Suppose we want to create a 2 + 1-dimensional tensor with the entry [3, 4] at location (0, 2), entry",
        "context": "Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, ifsis a sparse COO tensor andM=s.sparse_dim(),K=s.dense_dim(), then we have the following\ninvariants: M+K==len(s.shape)==s.ndim- dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape==(M,nse)- sparse indices are stored\nexplicitly, s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D",
        "question": "What kind of uncoalesced tensor can one specify multiple values,3and4, for the same index1 that leads to?",
        "context": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or not",
        "question": "What do most operations work identically given a coalesced or uncoalesced sparse tensor?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensor",
        "question": "What is coalesced or not?",
        "context": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse COO tensors",
        "question": "Addition of what is implemented by simply concatenating the indices and values tensors?",
        "context": "s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexico",
        "question": "What is the lexico of sparse tensors?",
        "context": "s.values().shape==(nse,)+s.shape[M:M+K]- the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse COO tens",
        "question": "What is an example of a sparse COO tensor?",
        "context": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical",
        "question": "What type of ordering of indices can be advantageous for implementing algorithms that involve many element selection operations?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or uncoalesced sparse tensor",
        "question": "Most operations will work identically given what two types of sparse tensors?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "slicing or matrix products",
        "question": "The lexicographical ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as what?",
        "context": "the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO format data",
        "question": "What can be acquired using methodstorch.Tensor.indices() andtorch.Tensor.indices()?",
        "context": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "a sparse tensor",
        "question": "What is the output oftorch.Tensor.coalesce()method?",
        "context": "In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "coalesced or uncoalesced sparse tensor",
        "question": "Most operations will work identically given a what?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO format data",
        "question": "What can be acquired using methodstorch.Tensor.indices() andtorch.Tensor.values()?",
        "context": "Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO format data",
        "question": "What can one acquire only when a sparse COO tensor is acquired?",
        "context": "In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse tensor",
        "question": "Which tensor is coalesced or not?",
        "context": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO format data",
        "question": "What can be acquired using methodstorch.Tensor.indices() and usetorch.Tensor.values()",
        "context": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "usetorch",
        "question": "What is used to acquire the COO format data of an uncoalesced tensor?",
        "context": "the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Note",
        "question": "What does torch.Tensor.is_coalesced()returnsTrue?",
        "context": "s.values().layout==torch.strided- values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO",
        "question": "What format data can one acquire only when the tensor instance is coalesced?",
        "context": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "COO format data",
        "question": "What can one acquire when the tensor instance is coalesced?",
        "context": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "PyTorch providestorch.Tensorto",
        "question": "What represents a multi-dimensional array containing elements of a single data type?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sparse arrays",
        "question": "What class of multi-dimensional arrays have a property of having a vast portion of elements being equal to zero?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "not strict",
        "question": "What is the usage of adjective \"non-zero\"?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "fill value",
        "question": "What term is used to denote unspecified elements in sparse arrays?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "when the size and sparsity levels of arrays are high",
        "question": "When can using a sparse storage format for storing sparse arrays be advantageous?",
        "context": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "beta",
        "question": "What beta is the PyTorch API of sparse tensors in?",
        "context": "PyTorch providestorch.Tensorto represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical",
        "question": "What ordering of indices can be advantageous for implementing algorithms that involve many element selection operations?",
        "context": "For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "lexicographical",
        "question": "What ordering of indices can be advantageous for implementing algorithms that involve many element selection operations, such as slicing or matrix products?",
        "context": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nonline",
        "question": "What type of nonlinear tensor can be implemented by multiplying all the uncoalesced values with the scalar because",
        "context": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "the additive nature of uncoalesced",
        "question": "What must one take into account when working with uncoalesced sparse COO tensors?",
        "context": "Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "nonlinear operation",
        "question": "What cannot be implemented by applying the operation to uncoalesced data becausesqrt(a+b)==sqrt",
        "context": "Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is atorch.Tensorinstance and to distinguish it from theTensorinstances that use\nsome other layout, on can usetorch.Tensor.is_sparseortorch.Tensor.layoutproperties: The number of sparse and dense dimensions can be acquired using\nmethodstorch.Tensor.sparse_dim()andtorch.Tensor.dense_dim(), respectively. For instance: Ifsis a sparse COO tensor then its COO format data can be\nacquired using methodstorch.Tensor.indices()andtorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, usetorch.Tensor._values()andtorch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthetorch.Tensor.coalesce()method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar becausec*(a+b)==c*a+c*bholds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data becausesqrt(a+b)==sqrt(a)+sqrt(b)does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance,torch.sparse.softmax()computes the softmax with the\nassumption that the fill value is negative infinity. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "duplicate coordinates",
        "question": "PyTorch sparse COO tensor format permits what in the indices?",
        "context": "PyTorch sparse COO tensor format permitsuncoalescedsparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,3and4, for the same index1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output oftorch.Tensor.coalesce()method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced()returnsTrue. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g.,torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "CUDA",
        "question": "What type of support does not exist as of now?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor of sizesize[0]+1",
        "question": "Thecrow_indicestensor consists of compressed row indices. This is what?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "where the given row starts",
        "question": "What does the CSR sparse tensor encode the index invaluesandcol_indicesdepending on?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Each successive number in the tensor",
        "question": "What is subtracted by the number before it denotes the number of elements in a given row?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "sizennz",
        "question": "Thecol_indicestensor contains the column indices of each value. This is a 1-D tensor of what?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "1-D tensor of sizennz",
        "question": "Thevaluestensor contains the values of the CSR tensor. This is a what?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "row and column indices and values tensors separately",
        "question": "What must the user supply?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "simplest way of constructing a sparse CSR",
        "question": "What is the simplest way of constructing a sparse CSR matrices?",
        "context": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors:crow_indices,col_indicesandvalues: Thecrow_indicestensor consists of compressed row indices. This is a 1-D tensor\nof sizesize[0]+1. The last element is the number of non-zeros. This tensor\nencodes the index invaluesandcol_indicesdepending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. Thecol_indicestensor contains the column indices of each value. This is a 1-D\ntensor of sizennz. Thevaluestensor  contains the values of the CSR tensor. This is a 1-D tensor\nof sizennz. Note The index tensorscrow_indicesandcol_indicesshould have element type eithertorch.int64(default) ortorch.int32. If you want to use MKL-enabled matrix\noperations, usetorch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using thetorch._sparse_csr_tensor()method. The user must supply the row and column indices and values tensors separately.\nThesizeargument is optional and will be deduced from the thecrow_indicesandcol_indicesif it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to usetensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with thetensor.matmul()method. This is currently the only math operation\nsupported on CSR tensors. ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.is_sparse",
        "question": "What is true if the Tensor uses sparse storage layout?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.dense_dim",
        "question": "What Return the number of dense dimensions in asparse tensorself?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_dim",
        "question": "What Return the number of sparse dimensions in asparse tensorself?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "newsparse tensor",
        "question": "What is returned by the sparse tensormask?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.indices",
        "question": "What Return the indices tensor of asparse COO tensor?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.values",
        "question": "What Return the values tensor of asparse COO tensor?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.sparse_resize_and_clear",
        "question": "What _ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Trueifselfis asparse COO tensorthat is coalesced",
        "question": "Tensor.is_coalesced Returns what?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.to_dense",
        "question": "What creates a strided copy ofself?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "Tensor.crow_indices",
        "question": "What Returns the tensor containing the compressed row indices?",
        "context": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse IsTrueif the Tensor uses sparse storage layout,Falseotherwise. Tensor.dense_dim Return the number of dense dimensions in asparse tensorself. Tensor.sparse_dim Return the number of sparse dimensions in asparse tensorself. Tensor.sparse_mask Returns a newsparse tensorwith values from a strided tensorselffiltered by the indices of the sparse tensormask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of asparse COO tensor. Tensor.values Return the values tensor of asparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy ofselfifselfis anuncoalesced tensor. Tensor.sparse_resize_ Resizesselfsparse tensorto the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from asparse tensorselfand resizesselfto the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced ReturnsTrueifselfis asparse COO tensorthat is coalesced,Falseotherwise. Tensor.to_dense Creates a strided copy ofself. The following methods are specific tosparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. Tensor.col_indices Returns the tensor containing the column indices of theselftensor whenselfis a sparse CSR tensor of layoutsparse_csr. The following Tensor methods support sparse COO tensors: add()add_()addmm()addmm_()any()asin()asin_()arcsin()arcsin_()bmm()clone()deg2rad()deg2rad_()detach()detach_()dim()div()div_()floor_divide()floor_divide_()get_device()index_select()isnan()log1p()log1p_()mm()mul()mul_()mv()narrow_copy()neg()neg_()negative()negative_()numel()rad2deg()rad2deg_()resize_as_()size()pow()sqrt()square()smm()sspaddmm()sub()sub_()t()t_()transpose()transpose_()zero_() ",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "answer": "real and imaginary components",
        "question": "For what components can a real tensor have an extra last dimension?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Fourier transform",
        "question": "The STFT computes what of short overlapping windows of the input?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "wheremmmis the index of the sliding window",
        "question": "What expression does the STFT compute?",
        "context": "Warning From version 1.8.0,return_complexmust always be given\nexplicitly for real inputs andreturn_complex=Falsehas been\ndeprecated. Strongly preferreturn_complex=Trueas in a future\npytorch release, this function will only return complex tensors. Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "1-D time sequence or a 2-D batch of time sequences",
        "question": "inputmust be either a what?",
        "context": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "lengthn_fft",
        "question": "Windowwill be padded on both sides to what before being applied?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfcenterisTrue",
        "question": "What is the default setting that allows input to be padded on both sides so that thettt-th frame is centered?",
        "context": "Note thattorch.view_as_real()can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after thelibrosastft function. Ignoring the optional batch dimension, this method computes the following\nexpression: wheremmmis the index of the sliding window, and\u03c9\\omega\u03c9is\nthe frequency0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fftforonesided=False,\nor0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1foronesided=True. inputmust be either a 1-D time sequence or a 2-D batch of time\nsequences. Ifhop_lengthisNone(default), it is treated as equal tofloor(n_fft/4). Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "IfcenterisTrue",
        "question": "Inputwill be padded on both sides so that thettt-th frame is centered at timethop_lengtht ",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "all available options",
        "question": "Seetorch.nn.functional.pad() for what?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "\"reflect\"",
        "question": "What is the default setting for the padding method used oninputwhencenterisTrue?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "real input",
        "question": "For what type of input is IfonesidedisTrue default?",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "if the input or window tensors are complex",
        "question": "If the input or window tensors are complex, thenonesidedoutputisTrue(default for real input), only values for",
        "context": "Ifwin_lengthisNone(default), it is treated as equal ton_fft. windowcan be a 1-D tensor of sizewin_length, e.g., fromtorch.hann_window(). IfwindowisNone(default), it is\ntreated as if having111everywhere in the window. Ifwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft,windowwill be padded on\nboth sides to lengthn_fftbefore being applied. IfcenterisTrue(default),inputwill be padded on\nboth sides so that thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, thettt-th frame\nbegins at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ainput.dim()+1dimensional complex tensor",
        "question": "Ifreturn_complexisTrue(default if input is complex), the return is what?",
        "context": "pad_modedetermines the padding method used oninputwhencenterisTrue. Seetorch.nn.functional.pad()for\nall available options. Default is\"reflect\". IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "ainput.dim()+2dimensional real tensor",
        "question": "What is the output of ainput.dim()+2dimensional real tensor?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "the optional batch size ofinput",
        "question": "Where*is what?",
        "context": "IfonesidedisTrue(default for real input), only values for\u03c9\\omega\u03c9in[0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1]are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e.,X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, thenonesidedoutput is not possible. IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "version 0.4.1",
        "question": "At what version did the function change signature?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Fourier transform",
        "question": "input(Tensor) \u2013 the input tensor n_fft(int) \u2013 the input ten",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "center(bool,optional)",
        "question": "What is the option to padinputon both sides so that thettt-th frame is centered at timethop_lengtht",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "Default",
        "question": "IfnormalizedisTrue(default isFalse) returns the normalized STFT results?",
        "context": "IfnormalizedisTrue(default isFalse), the function\nreturns the normalized STFT results, i.e., multiplied by(frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. Ifreturn_complexisTrue(default if input is complex), the\nreturn is ainput.dim()+1dimensional complex tensor. IfFalse,\nthe output is ainput.dim()+2dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size(\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T)ifreturn_complexis true, or a real tensor of size(\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where\u2217*\u2217is the optional batch size ofinput,NNNis the number of frequencies where STFT is applied\nandTTTis the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input(Tensor) \u2013 the input tensor n_fft(int) \u2013 size of Fourier transform hop_length(int,optional) \u2013 the distance between neighboring sliding window\nframes. Default:None(treated as equal tofloor(n_fft/4)) win_length(int,optional) \u2013 the size of window frame and STFT filter.\nDefault:None(treated as equal ton_fft) window(Tensor,optional) \u2013 the optional window function.\nDefault:None(treated as window of all111s) center(bool,optional) \u2013 whether to padinputon both sides so\nthat thettt-th frame is centered at timet\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault:True pad_mode(string,optional) \u2013 controls the padding method used whencenterisTrue. Default:\"reflect\" normalized(bool,optional) \u2013 controls whether to return the normalized STFT results\nDefault:False onesided(bool,optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault:Truefor realinputandwindow,Falseotherwise. return_complex(bool,optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor ",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "answer": "2-by-N Tensor",
        "question": "Returns the indices of the upper triangular part of arowbycolmatrix in what?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "elements on and above the diagonal",
        "question": "The upper triangular part of the matrix is defined as what?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "argumentoffset",
        "question": "What controls which diagonal to consider?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Ifoffset= 0, all elements on and above the main diagonal are retained",
        "question": "Ifoffset=0, all elements on and above the main diagonal are retained?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "A positive value",
        "question": "What excludes just as many diagonals above the main diagonal?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "main diagonal",
        "question": "The set of indices(i,i)lbrace (i, i) rbrace(i,",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "to prevent overflow during calculation",
        "question": "Why does row*colmust be less than259259259?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "ifNone,torch.long",
        "question": "What are the default data types of returned tensors?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "layout",
        "question": "What is currently only supporttorch supported?",
        "context": "Returns the indices of the upper triangular part of arowbycolmatrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argumentoffsetcontrols which diagonal to consider. Ifoffset= 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)}fori\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]whered1,d2d_{1}, d_{2}d1\u200b,d2\u200bare the dimensions of the matrix. Note When running on CUDA,row*colmust be less than2592^{59}259to\nprevent overflow during calculation. row(int) \u2013 number of rows in the 2-D matrix. col(int) \u2013 number of columns in the 2-D matrix. offset(int) \u2013 diagonal offset from the main diagonal.\nDefault: if not provided, 0. dtype(torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: ifNone,torch.long. device(torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: ifNone, uses the current device for the default tensor type\n(seetorch.set_default_tensor_type()).devicewill be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. layout(torch.layout, optional) \u2013 currently only supporttorch.strided. Example: ",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "answer": "Gets the cuda capability of a device",
        "question": "What does this library do?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current device",
        "question": "What does Sets the current stream. This is a wrapper API to set the stream?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers on all GPUs",
        "question": "What does Sets the seed for generating random numbers on all GPUs do?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers to a random number for the current GPU",
        "question": "What does Sets the seed for generating random numbers to a random number for the current GPU?",
        "context": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "Gets the cuda capability of a device. Gets the name of a device. Gets what of a device?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "whether PyTorch\u2019s CUDA state has been initialized",
        "question": "What does a bool indicating if CUDA is currently available return?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "What happens when all kernels in all streams on a CUDA device are selected?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers to a random number for the current GPU",
        "question": "What does Sets the seed for generating random numbers to a random number on all GPUs?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current random seed of the current GPU",
        "question": "What does comm.broadcast return?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast",
        "question": "What is the name of the command that returns the current random seed of the current GPU?",
        "context": "It is lazily initialized, so you can always import it, and useis_available()to determine if your system supports CUDA. CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "defaultStream",
        "question": "Returns the currently selectedStreamfor a given device. Returns what for a given device?",
        "context": "Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the name of a device",
        "question": "Gets the cuda capability of a device. Gets the name of a device. Gets the properties of a device. Return",
        "context": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "Gets the properties of a device. Gets the cuda capability of a device. Gets the name of a device. Get",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Initialize PyTorch\u2019s CUDA state",
        "question": "Force collects GPU memory after it has been released by CUDA IPC. Returns whether PyTorch\u2019s CUDA state has",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "wrapper API to set the stream",
        "question": "Sets the current stream.This is a what?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around the Context-manager StreamContext",
        "question": "What wrapper wraps around the Context-manager StreamContext that selects a given stream?",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "Waits for all kernels in all streams on a CUDA device to complete?",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "Sets the random number generator state of the specified GPU. Sets the seed for generating random numbers for the current GPU. Sets the seed for",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the seed for generating random numbers on all GPUs",
        "question": "Sets the seed for generating random numbers on all GPUs. Sets the seed for generating random numbers on all GPUs. Returns the",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast Broadcasts a tensor",
        "question": "What does comm.broadcast broadcast to specified GPU devices?",
        "context": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPU",
        "question": "comm.broadcast Broadcasts a tensor to specified GPU devices?",
        "context": "CUDA semanticshas more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Context-manager",
        "question": "Which entity selects a given stream?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "the number of GPUs available",
        "question": "What does Context-manager that changes the selected device return?",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda",
        "question": "Gets the capability of a device. Gets the name of a device. Gets the properties of a device. Gets the capability",
        "context": "Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the name of a device",
        "question": "What does Gets the name of a device?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "random number generator state",
        "question": "Sets what of the specified GPU. Sets the random number generator state of all devices. Sets the seed for generating random numbers for the current",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "current random seed of the current GPU",
        "question": "Sets the seed for generating random numbers on all GPUs. Sets the seed for generating random numbers on all GPUs. Sets the",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.reduce_add Sums tens",
        "question": "What does comm.reduce_add Sums tens tens?",
        "context": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "list CUDA architectures",
        "question": "Returns what this library was compiled for. Gets the cuda capability of a device. Gets the name of a device.",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "Returns what flags this library was compiled with. Initialize PyTorch\u2019s CUDA state. Force collects GPU memory after",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.s",
        "question": "comm.reduce_add Sums tensors from multiple GPUs?",
        "context": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "list CUDA architectures",
        "question": "Returns what this library was compiled for?",
        "context": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "currently selected device",
        "question": "Returns the index of a what?",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda",
        "question": "Gets the capability of a device. Gets the name of a device. Gets the properties of a device. Gets the properties",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "name",
        "question": "Gets the cuda capability of a device. Gets the what of a device?",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties",
        "question": "Gets what of a device. Returns NVCC gencode flags this library was compiled with. Initialize PyTorch",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Initialize PyTorch\u2019s CUDA state",
        "question": "What does Force collects GPU memory after it has been released by CUDA IPC?",
        "context": "Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "generating random numbers",
        "question": "Sets the seed for generating random numbers on all GPUs. Sets the seed for what?",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "comm.gather",
        "question": "What Gathers tensors from multiple GPUs?",
        "context": "Returns the index of a currently selected device.   Returns the currently selectedStreamfor a given device.   Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda",
        "question": "Gets the capability of a device. Gets the name of a device. Gets the properties of a device.",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Waits for all kernels in all streams on a CUDA device to complete",
        "question": "What happens to all kernels in all streams on a CUDA device to complete?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA stream",
        "question": "Wrapper around a CUDA stream <sep>",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA event",
        "question": "Wrapper around what event?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Gets the name of a device",
        "question": "Gets the name of a device. Gets the properties of a device. Returns NVCC gencode flags this library was",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA event",
        "question": "Releases all unoccupied cached memory currently held by CUDA IPC. Releases all unoccupied cached memory currently held by CUDA",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Releases all unoccupied cached memory",
        "question": "What release releases all unoccupied cached memory currently held by CUDA?",
        "context": "Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "generating random numbers",
        "question": "Sets the seed for generating random numbers on all GPUs. Sets the seed for generating random numbers to a random number on all GPU",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Releases all unoccupied cached memory",
        "question": "What releases all unoccupied cached memory currently held by the caching allocator?",
        "context": "Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does Sets the random number generator state of the specified GPU?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "cuda capability",
        "question": "Gets what of a device. Gets the name of a device. Gets the properties of a device. Returns NVCC",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA stream",
        "question": "Wrapper around a CUDA event. Releases all unoccupied cached memory currently held by the caching allocator. Releases",
        "context": "Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "all unoccupied cached memory",
        "question": "Releases what currently held by the caching allocator?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "human-readable",
        "question": "Returns a what?",
        "context": "Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "unoccupied cached memory",
        "question": "Releases all what currently held by the caching allocator?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Sets the random number generator state of the specified GPU",
        "question": "What does it do?",
        "context": "Returns the defaultStreamfor a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Wrapper around a CUDA stream",
        "question": "Wrapper around a CUDA event. Releases all unoccupied cached memory currently held by the caching allocator so that those",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "properties of a device",
        "question": "Gets what?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "NVCC gencode flags",
        "question": "Returns what flags this library was compiled with. Initialize PyTorch\u2019s CUDA state.",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator statistics",
        "question": "Returns a dictionary of what?",
        "context": "  Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.   Wrapper around a CUDA event.   Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "Releases all unoccupied cached memory currently held by the caching allocator",
        "question": "What does it do to the unoccupied cached memory currently held by the caching allocator?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "seemax_memory_reserved()",
        "question": "What is the name of the function that releases all unoccupied cached memory currently held by the caching allocator?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "CUDA memory allocator",
        "question": "Resets the \u201cpeak\u201d stats tracked by what?",
        "context": "  Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; seememory_reserved().   Deprecated; seemax_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator. ",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "answer": "BETA",
        "question": "What is the current state of this module?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "documentation of each function",
        "question": "See what for details. Computes the entropy oninput(as defined below), elementwise.",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "Computes the error function ofinput. The error function is defined as follows: what is the input tensor?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "output tensor",
        "question": "Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input ten",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the complementary error function ofinput",
        "question": "What is the name of the function that Computes the error function ofinput?",
        "context": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "Computes the complementary error function ofinput. The complementary error function is defined as follows: what is the input tensor?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the inverse error function ofinput",
        "question": "What is the name of the function that Computes the inverse error function ofinput?",
        "context": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the exponential of the elements minus 1 ofinput",
        "question": "What is an example of a Computes the exponential of the elements minus 1 ofinput?",
        "context": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor)",
        "question": "Computes the natural logarithm of the absolute value of the gamma function oninput?",
        "context": "The torch.special module, modeled after SciPy\u2019sspecialmodule. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "out(Tensor,optional)",
        "question": "Input(Tensor) \u2013 the input tensor. Out(Tensor,optional) \u2013 the output",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "input(Tensor) \u2013 the input tensor",
        "question": "What is the error function defined as?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponential",
        "question": "Computes the elements minus 1 ofinput. Note This function provides greater precision than exp(x) - 1 for small values of ",
        "context": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "natural logarithm",
        "question": "Computes the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tens",
        "context": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "What is the first kind?",
        "context": "Computes the entropy oninput(as defined below), elementwise. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the complementary error function ofinput",
        "question": "What is an example of a function that computes the error function ofinput?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the inverse error function ofinput",
        "question": "What is an example of an error function that is defined in the range(1,1)(-1, 1)(1,1)?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "expit",
        "question": "What is also known as the logistic sigmoid function?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the natural logarithm of the absolute value of the gamma function oninput",
        "question": "What is an example of a computation?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "Computes the first kind of what function for each element ofinput?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the natural logarithm of the absolute value of the gamma function oninput",
        "question": "What is an example?",
        "context": "out(Tensor,optional) \u2013 the output tensor. Computes the error function ofinput. The error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "exponentially scaled zeroth order modified Bessel function",
        "question": "Computes the first kind for each element ofinput. input(Tensor) \u2013 the input tensor. out(",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "Computes the inverse error function ofinput",
        "question": "What is an example of a function that computes the inverse error function ofinput?",
        "context": "input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the complementary error function ofinput.\nThe complementary error function is defined as follows: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the inverse error function ofinput.\nThe inverse error function is defined in the range(\u22121,1)(-1, 1)(\u22121,1)as: input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nofinput. Note This function provides greater precision than exp(x) - 1 for small values of x. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the base two exponential function ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function oninput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element ofinput. input(Tensor) \u2013 the input tensor. out(Tensor,optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements ofinput.inputis clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None andinput< 0 orinput> 1, the function will yields NaN. input(Tensor) \u2013 the input tensor. eps(float,optional) \u2013 the epsilon for input clamp bound. Default:None out(Tensor,optional) \u2013 the output tensor. Example: Computesinput*log1p(other)with the following cases. Similar to SciPy\u2019sscipy.special.xlog1py. input(NumberorTensor) \u2013 Multiplier other(NumberorTensor) \u2013 Argument Note At least one ofinputorothermust be a tensor. out(Tensor,optional) \u2013 the output tensor. Example: ",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "answer": "tensors",
        "question": "If the value contains what that reside on GPUs, Future.done()will returnTrueeven if the asynchronous kernels that",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aFuturecannot be marked completed twice",
        "question": "What does aFuturecannot be marked twice?",
        "context": "callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "change streams in between",
        "question": "What is safe to call this method immediately after launching kernels without any additional synchronization?",
        "context": "callback(Future) \u2013 aCallablethat takes in one argument, is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "This method will use events on all the relevant current streams to ensure proper scheduling for all the consumers of what?",
        "context": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFuture",
        "question": "This method will record events on all the relevant current streams and will use them to ensure proper scheduling for all the consumers of what?",
        "context": "is the reference to this Future.(which) \u2013 Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. ReturnTrueif thisFutureis done. AFutureis done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "if the asynchronous kernels that are populating those tensors haven\u2019t yet completed running on the device",
        "question": "If the value contains tensors that reside on GPUs, what will Future.done() returnTrueeven?",
        "context": "If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "aFuturecannot be marked completed twice",
        "question": "What is a limitation of thisFuture?",
        "context": "If the value contains tensors that reside on GPUs,Future.done()will returnTrueeven if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (seewait()). Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "current ones",
        "question": "What are the streams on which the kernels were enqueued set as when this method is called?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "immediately after launching those kernels",
        "question": "When is it safe to call this method if the result contains tensors that reside on GPUs?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "chaining:fut.then(cb1).then(cb2)",
        "question": "To enforce a certain order, what can be added to the sameFuture, but the order in which they will be executed cannot be guaranteed?",
        "context": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "one argument",
        "question": "How many arguments must the callback take?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thisFutureis already completed",
        "question": "When will the given callback be run immediately inline?",
        "context": "Set an exception for thisFuture, which will mark thisFutureas\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on thisFuture, the exception set here\nwill be raised inline. result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "chaining:fut.then(cb1).then(cb2)",
        "question": "To enforce a certain order, what can be used to add multiple callbacks to the sameFuture?",
        "context": "result(BaseException) \u2013 the exception for thisFuture. Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "chaining:fut.then(cb1).then(cb2)",
        "question": "To enforce a certain order in which callbacks will be executed, what can be used?",
        "context": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "thevalue()method",
        "question": "What can the callback function use to get the value of thisFuture?",
        "context": "Set the result for thisFuture, which will mark thisFutureas\ncompleted and trigger all attached callbacks. Note that aFuturecannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "current",
        "question": "The callback will be invoked with some dedicated streams set as what?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "after the kernels complete",
        "question": "When will any operation performed by the callback on these tensors be scheduled on the device?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "switch streams",
        "question": "If the callback doesn't do what, it can safely manipulate the result without any additional synchronization?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "chaining:fut.then(cb1).then(cb2)",
        "question": "To enforce a certain order, what can be added to the sameFuture?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "while the async kernels that are populating those tensors haven\u2019t yet finished executing on the device",
        "question": "If theFuture's value contains tensors that reside on GPUs, the callback might be invoked when?",
        "context": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of thisFuture. result(object) \u2013 the result object of thisFuture. Append the given callback function to thisFuture, which will be run\nwhen theFutureis completed.  Multiple callbacks can be added to\nthe sameFuture, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:fut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to thisFuture. The callback function can use thevalue()method to get the value. Note that if thisFutureis\nalready completed, the given callback will be run immediately inline. If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "tensors",
        "question": "What does the future's value contain that reside on GPUs?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "current",
        "question": "What are some dedicated streams set as?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "non-blocking behavior",
        "question": "What is the behavior of wait() similar to?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "if the callback returns a value that contains tensors that reside on a GPU",
        "question": "If the callback returns a value that contains tensors that reside on a GPU, it can do so even if the kernel",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "answer": "after a call towait()has completed, or inside a callback function passed tothen()",
        "question": "When should Obtain the value of an already-completed future be called?",
        "context": "If theFuture\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior ofwait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback(Callable) \u2013 aCallablethat takes thisFutureas\nthe only argument. A newFutureobject that holds the return value of thecallbackand will be marked as completed when the givencallbackfinishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncallingfut.wait(), or through other code in the callback, the\nfuture returned bythenwill be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call towait()has\ncompleted, or inside a callback function passed tothen(). In\nother cases thisFuturemay not yet hold a value and callingvalue()could fail. ",
        "source": "https://pytorch.org/docs/stable/futures.html"
    }
]