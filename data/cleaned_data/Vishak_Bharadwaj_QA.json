[
    {
        "X": "What does this make it possible to supply to the C++ and CUDA?",
        "Y": "different flags",
        "Z": "You can override the default behavior using TORCH_CUDA_ARCH_LIST to explicitly specify which\nCCs you want the extension to support: TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the subclass that takes care of passing the minimum required compiler flags?",
        "Y": "Note",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is True (default)?",
        "Y": "use_ninja",
        "Z": "TORCH_CUDA_ARCH_LIST=\u201d6.1 8.6\u201d python build_my_extension.py\nTORCH_CUDA_ARCH_LIST=\u201d5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX\u201d python build_my_extension.py The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What is the name of the program that attempts to build using the Ninja backend?",
        "Y": "use_ninja",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "Why does the Ninja backend use #CPUS + 2 workers to build the extension?",
        "Y": "may use up too many resources on some systems",
        "Z": "The +PTX option causes extension kernel binaries to include PTX instructions for the specified\nCC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC >=\nthe specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with\nCC >= 8.6). This improves your binary\u2019s forward compatibility. However, relying on older PTX to\nprovide forward compat by runtime-compiling for newer CCs can modestly reduce performance on\nthose newer CCs. If you know exact CC(s) of the GPUs you want to target, you\u2019re always better\noff specifying them individually. For example, if you want your extension to run on 8.0 and 8.6,\n\u201c8.0+PTX\u201d would work functionally because it includes PTX that can runtime-compile for 8.6, but\n\u201c8.0 8.6\u201d would be better. Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What type of compilation does setuptools.build_ext take care of?",
        "Y": "CUDA",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "What may the Ninja backend use to build the extension?",
        "Y": "too many resources",
        "Z": "Note that while it\u2019s possible to include all supported archs, the more archs get included the\nslower the building process will be, as it will build a separate kernel image for each arch. A custom setuptools build extension . This setuptools.build_ext subclass takes care of passing the\nminimum required compiler flags (e.g. -std=c++14) as well as mixed\nC++/CUDA compilation (and support for CUDA files in general). When using BuildExtension, it is allowed to supply a dictionary\nfor extra_compile_args (rather than the usual list) that maps from\nlanguages (cxx or nvcc) to a list of additional compiler flags to\nsupply to the compiler. This makes it possible to supply different flags to\nthe C++ and CUDA compiler during mixed compilation. use_ninja (bool): If use_ninja is True (default), then we\nattempt to build using the Ninja backend. Ninja greatly speeds up\ncompilation compared to the standard setuptools.build_ext.\nFallbacks to the standard distutils backend if Ninja is not available. Note By default, the Ninja backend uses #CPUS + 2 workers to build the\nextension. This may use up too many resources on some systems. One\ncan control the number of workers by setting the MAX_JOBS environment\nvariable to a non-negative number. Loads a PyTorch C++ extension just-in-time (JIT). To load an extension, a Ninja build file is emitted, which is used to\ncompile the given sources into a dynamic library. This library is\nsubsequently loaded into the current Python process as a module and\nreturned from this function, ready for use.",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "When enabled, operations will use deterministic algorithms when available, and if only nondeterministic algorithms are available they will throw what when called?",
        "Y": "RuntimeError",
        "Z": "Sets whether PyTorch operations must use \u201cdeterministic\u201d\nalgorithms. That is, algorithms which, given the same input, and when\nrun on the same software and hardware, always produce the same output.\nWhen enabled, operations will use deterministic algorithms when available,\nand if only nondeterministic algorithms are available they will throw a\nRuntimeError when called. The following normally-nondeterministic operations will act\ndeterministically when mode=True: torch.nn.Conv1d when called on CUDA tensor torch.nn.Conv2d when called on CUDA tensor torch.nn.Conv3d when called on CUDA tensor torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is torch.nn.ConvTranspose1d when called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ConvTranspose1d when called on CUDA tensor torch.nn.ConvTranspose2d when called on CUDA tensor torch.nn.ConvTranspose3d when called on CUDA tensor torch.bmm() when called on sparse-dense CUDA tensors torch.Tensor.__getitem__() when attempting to differentiate a CPU tensor\nand the index is a list of tensors torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When is index_add() called on?",
        "Y": "CUDA tensor torch",
        "Z": "torch.Tensor.index_put() with accumulate=False torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When input dimension is one and called on a which CUDA tensor that requires grad torch?",
        "Y": "CUDA tensor that requires grad torch",
        "Z": "torch.Tensor.index_put() with accumulate=True when called on a CPU\ntensor torch.Tensor.put_() with accumulate=True when called on a CPU\ntensor torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What requires grad torch.gather() when input dimension is one and called on?",
        "Y": "CUDA tensor",
        "Z": "torch.gather() when input dimension is one and called\non a CUDA tensor that requires grad torch.index_add() when called on CUDA tensor torch.index_select() when attempting to differentiate a CUDA tensor torch.repeat_interleave() when attempting to differentiate a CUDA tensor torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA",
        "Y": "RuntimeError",
        "Z": "torch.Tensor.index_copy() when called on a CPU or CUDA tensor The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "When mode=True, the following normally-nondeterministic operations will throw a what?",
        "Y": "RuntimeError when mode=True",
        "Z": "The following normally-nondeterministic operations will throw a\nRuntimeError when mode=True: torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the torch.nn.AvgPool3d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.AvgPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the torch.nn.AdaptiveAvgPool2d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.AdaptiveAvgPool2d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the torch.nn.AdaptiveAvgPool3d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.AdaptiveAvgPool3d when attempting to differentiate a CUDA tensor torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is a torch.nn.MaxPool3d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.MaxPool3d when attempting to differentiate a CUDA tensor torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv()",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the torch.nn.AdaptiveMaxPool2d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.AdaptiveMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm()",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is a torch.nn.FractionalMaxPool2d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.FractionalMaxPool2d when attempting to differentiate a CUDA tensor torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is a torch.nn.FractionalMaxPool3d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.FractionalMaxPool3d when attempting to differentiate a CUDA tensor torch.nn.functional.interpolate() when attempting to differentiate a CUDA tensor\nand one of the following modes is used: linear bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "For more details, see what for more details: https://docs.nvidia.com/cuda/cublas/index.",
        "Y": "CUDA documentation",
        "Z": "torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note This flag does not detect or prevent nondeterministic behavior caused\nby calling an inplace operation on a tensor with an internal memory\noverlap or by giving such a tensor as the out argument for an\noperation. In these cases, multiple writes of different data may target\na single memory location, and the order of writes is not guaranteed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is a reflectionPad1d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "bilinear bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "For more details, see what for more details?",
        "Y": "CUDA documentation",
        "Z": "torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What type of torch is used?",
        "Y": "bicubic trilinear torch",
        "Z": "bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the CUDA documentation for?",
        "Y": "See the CUDA documentation for more details",
        "Z": "bicubic trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What type of torch does a CUDA tensor torch have?",
        "Y": "trilinear torch",
        "Z": "trilinear torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the torch.nn.ReflectionPad1d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ReflectionPad1d when attempting to differentiate a CUDA tensor torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the torch.nn.ReflectionPad2d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ReflectionPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note This flag does not detect or prevent nondeterministic behavior caused\nby calling an inplace operation on a tensor with an internal memory\noverlap or by giving such a tensor as the out argument for an\noperation. In these cases, multiple writes of different data may target\na single memory location, and the order of writes is not guaranteed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is a torch.nn.ReplicationPad1d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ReplicationPad1d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note This flag does not detect or prevent nondeterministic behavior caused\nby calling an inplace operation on a tensor with an internal memory\noverlap or by giving such a tensor as the out argument for an\noperation. In these cases, multiple writes of different data may target\na single memory location, and the order of writes is not guaranteed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is torch.nn.ReplicationPad2d when attempting to differentiate?",
        "Y": "CUDA tensor torch",
        "Z": "torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note This flag does not detect or prevent nondeterministic behavior caused\nby calling an inplace operation on a tensor with an internal memory\noverlap or by giving such a tensor as the out argument for an\noperation. In these cases, multiple writes of different data may target\na single memory location, and the order of writes is not guaranteed. mode (bool) \u2013 If True, makes potentially nondeterministic\noperations switch to a deterministic algorithm or throw a runtime\nerror. If False, allows nondeterministic operations. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What documentation provides more details about nondeterministic CUDA operations?",
        "Y": "CUDA",
        "Z": "torch.nn.ReplicationPad2d when attempting to differentiate a CUDA tensor torch.nn.ReplicationPad3d when attempting to differentiate a CUDA tensor torch.nn.NLLLoss when called on a CUDA tensor torch.nn.CTCLoss when attempting to differentiate a CUDA tensor torch.nn.EmbeddingBag when attempting to differentiate a CUDA tensor when\nmode='max' torch.Tensor.scatter_add_() when called on a CUDA tensor torch.Tensor.put_() when accumulate=False torch.Tensor.put_() when accumulate=True and called on a CUDA tensor torch.histc() when called on a CUDA tensor torch.bincount() when called on a CUDA tensor torch.kthvalue() with called on a CUDA tensor torch.median() with indices output when called on a CUDA tensor torch.gather() when input dimension is larger than one\nand called on a CUDA tensor that requires grad torch.nn.functional.grid_sample() when attempting to differentiate a CUDA tensor A handful of CUDA operations are nondeterministic if the CUDA version is\n10.2 or greater, unless the environment variable CUBLAS_WORKSPACE_CONFIG=:4096:8\nor CUBLAS_WORKSPACE_CONFIG=:16:8 is set. See the CUDA documentation for more\ndetails: https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility\nIf one of these environment variable configurations is not set, a RuntimeError\nwill be raised from these operations when called with CUDA tensors: torch.mm() torch.mv() torch.bmm() Note that deterministic operations tend to have worse performance than\nnondeterministic operations. Note This flag does not detect or prevent nondeterministic behavior caused\nby calling an inplace operation on a tensor with an internal memory\noverlap or by giving such a tensor as the out argument for an\noperation. In these cases, multiple writes of different data may target\na single memory location, and the order of writes is not guaranteed. mode (bool) \u2013 If True, makes potentially nondeterministic\noperations switch to a deterministic algorithm or throw a runtime\nerror. If False, allows nondeterministic operations. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "What is the default value for the dimension along which to split a tensor?",
        "Y": "0",
        "Z": "Splits a tensor into multiple sub-tensors, all of which are views of input,\nalong dimension dim according to the indices or number of sections specified\nby indices_or_sections. This function is based on NumPy\u2019s\nnumpy.array_split(). input (Tensor) \u2013 the tensor to split indices_or_sections (Tensor, int or list or tuple of python:ints) \u2013  If indices_or_sections is an integer n or a zero dimensional long tensor\nwith value n, input is split into n sections along dimension dim.\nIf input is divisible by n along dimension dim, each\nsection will be of equal size, input.size(dim) / n. If input\nis not divisible by n, the sizes of the first int(input.size(dim) % n)\nsections will have size int(input.size(dim) / n) + 1, and the rest will\nhave size int(input.size(dim) / n). If indices_or_sections is a list or tuple of ints, or a one-dimensional long\ntensor, then input is split along dimension dim at each of the indices\nin the list, tuple or tensor. For instance, indices_or_sections=[2, 3] and dim=0\nwould result in the tensors input[:2], input[2:3], and input[3:]. If indices_or_sections is a tensor, it must be a zero-dimensional or one-dimensional\nlong tensor on the CPU. dim (int, optional) \u2013 dimension along which to split the tensor. Default: 0 Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "What does a quantized model use instead of floating point values?",
        "Y": "integers",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators. PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How much faster is hardware support for INT8 compared to FP32?",
        "Y": "2 to 4 times faster",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators. PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does PyTorch support to quantize a deep learning model?",
        "Y": "multiple approaches",
        "Z": "Warning Quantization is in beta and subject to change. Quantization refers to techniques for performing computations and storing\ntensors at lower bitwidths than floating point precision. A quantized model\nexecutes some or all of the operations on tensors with integers rather than\nfloating point values. This allows for a more compact model representation and\nthe use of high performance vectorized operations on many hardware platforms.\nPyTorch supports INT8 quantization compared to typical FP32 models allowing for\na 4x reduction in the model size and a 4x reduction in memory bandwidth\nrequirements.  Hardware support for  INT8 computations is typically 2 to 4\ntimes faster compared to FP32 compute. Quantization is primarily a technique to\nspeed up inference and only the forward pass is supported for quantized\noperators. PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "In most cases the model is trained in what?",
        "Y": "FP32",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack')",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What can quantized tensors be used to do?",
        "Y": "directly construct models",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What supports multiple approaches to quantizing a deep learning model?",
        "Y": "PyTorch",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack')",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of training does PyTorch support?",
        "Y": "quantization aware training",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack')",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does PyTorch convert the trained model into?",
        "Y": "lower precision",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack')",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the qconfig set by PyTorch?",
        "Y": "torch.quantization.get_default_qconfig",
        "Z": "PyTorch supports multiple approaches to quantizing a deep learning model. In\nmost cases the model is trained in FP32 and then the model is converted to\nINT8. In addition, PyTorch also supports quantization aware training, which\nmodels quantization errors in both the forward and backward passes using\nfake-quantization modules. Note that the entire computation is carried out in\nfloating point. At the end of quantization aware training, PyTorch provides\nconversion functions to convert the trained model into lower precision. At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack')",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What CPUs support AVX2 support?",
        "Y": "x86 CPUs",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does PyTorch not provide quantized operator implementations on?",
        "Y": "CUDA",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where should a model be moved in order to test the quantized functionality?",
        "Y": "CPU",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What must match the backend on which the model will be executed?",
        "Y": "qconfig and the engine",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What should you set if you are interested in quantizing a model to run on ARM?",
        "Y": "qconfig",
        "Z": "At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "For inference, the backend is set to qnnpack as follows torch.backends.quantized.engine = 'q",
        "Y": "qnnpack",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization?",
        "Y": "qnnpack",
        "Z": "qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are the two modes of quantization in PyTorch?",
        "Y": "Eager Mode Quantization and FX Graph Mode Quantization",
        "Z": "PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is a new automated quantization framework in PyTorch?",
        "Y": "FX Graph Mode Quantization",
        "Z": "torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "People might need to refactor the model to make it compatible with what?",
        "Y": "FX Graph Mode Quantization",
        "Z": "Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is not expected to work on arbitrary models?",
        "Y": "FX Graph Mode Quantization",
        "Z": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "To make FX Graph Mode Quantization work, users might need to be familiar with what?",
        "Y": "torch.fx",
        "Z": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, what is it called?",
        "Y": "eager mode quantization",
        "Z": "qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is FX Graph Mode Quantization Release Status beta prototype?",
        "Y": "Operator Fusion Manual Automatic",
        "Z": "qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is qconfig for quantization aware training?",
        "Y": "torch.quantization.get_default_qat_qconfig('qnnpack')",
        "Z": "for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "To make FX Graph Mode Quantization work, users might need to familiarize themselves with what?",
        "Y": "torch.fx",
        "Z": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the new automated quantization framework in PyTorch?",
        "Y": "FX Graph Mode Quantization",
        "Z": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the torch.backends.quantized.engine parameter set to match the backend?",
        "Y": "quantization aware training",
        "Z": "for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack' PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What domain library will FX Graph Mode Quantization be integrated into?",
        "Y": "torchvision",
        "Z": "PyTorch provides two different modes of quantization: Eager Mode Quantization and FX Graph Mode Quantization. Eager Mode Quantization is a beta feature. User needs to do fusion and specify where quantization and dequantization happens manually, also it only supports modules and not functionals. FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does FX Graph Mode Quantization improve upon Eager Mode Quantization?",
        "Y": "adding support for functionals and automating the quantization process",
        "Z": "FX Graph Mode Quantization is a new automated quantization framework in PyTorch, and currently it\u2019s a prototype feature. It improves upon Eager Mode Quantization by adding support for functionals and automating the quantization process, although people might need to refactor the model to make the model compatible with FX Graph Mode Quantization (symbolically traceable with torch.fx). Note that FX Graph Mode Quantization is not expected to work on arbitrary models since the model might not be symbolically traceable, we will integrate it into domain libraries like torchvision and users will be able to quantize models similar to the ones in supported domain libraries with FX Graph Mode Quantization. For arbitrary models we\u2019ll provide general guidelines, but to actually make it work, users might need to be familiar with torch.fx, especially on how to make a model symbolically traceable. New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization:",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is another type of quantization supported in Eager Mode Quantization?",
        "Y": "static quantization",
        "Z": "New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does the model execution time dominated by?",
        "Y": "loading weights from memory",
        "Z": "New users of quantization are encouraged to try out FX Graph Mode Quantization first, if it does not work, user may try to follow the guideline of using FX Graph Mode Quantization or fall back to eager mode quantization. The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of models are LSTM and dynamic quantization used for?",
        "Y": "Transformer type models with small batch size",
        "Z": "Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What type of models are LSTM and FX Graph Mode Quantization used for?",
        "Y": "Transformer type models with small batch size",
        "Z": "The following table compares the differences between Eager Mode Quantization and FX Graph Mode Quantization: Eager Mode\nQuantization FX Graph\nMode\nQuantization Release\nStatus beta prototype Operator\nFusion Manual Automatic Quant/DeQuant\nPlacement Manual Automatic Quantizing\nModules Supported Supported Quantizing\nFunctionals/Torch\nOps Manual Automatic Support for\nCustomization Limited Support Fully\nSupported Quantization Mode\nSupport Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What backend is set to qnnpack?",
        "Y": "qnnpack",
        "Z": "At lower level, PyTorch provides a way to represent quantized tensors and\nperform operations with them. They can be used to directly construct models\nthat perform all or part of the computation in lower precision. Higher-level\nAPIs are provided that incorporate typical workflows of converting FP32 model\nto lower precision with minimal accuracy loss. Today, PyTorch supports the following backends for running quantized operators efficiently: x86 CPUs with AVX2 support or higher (without AVX2 some operations have\ninefficient implementations) ARM CPUs (typically found in mobile/embedded devices) The corresponding implementation is chosen automatically based on the PyTorch build mode. Note At the moment PyTorch doesn\u2019t provide quantized operator implementations on CUDA -\nthis is the direction for future work. Move the model to CPU in order to test the\nquantized functionality. Quantization-aware training (through FakeQuantize)\nsupports both CPU and CUDA. Note When preparing a quantized model, it is necessary to ensure that qconfig\nand the engine used for quantized computations match the backend on which\nthe model will be executed. Quantization currently supports two backends:\nfbgemm (for use on x86, https://github.com/pytorch/FBGEMM) and qnnpack\n(for use on the ARM QNNPACK library https://github.com/pytorch/QNNPACK).\nFor example, if you are interested in quantizing a model to run on ARM, it\nis recommended to set the qconfig by calling: qconfig = torch.quantization.get_default_qconfig('qnnpack') for post training quantization and qconfig = torch.quantization.get_default_qat_qconfig('qnnpack') for quantization aware training. In addition, the torch.backends.quantized.engine parameter should be set to\nmatch the backend. For using qnnpack for inference, the backend is set to\nqnnpack as follows torch.backends.quantized.engine = 'qnnpack'",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the type of quantization that quantizes the weights and activations of a model?",
        "Y": "dynamic quantization",
        "Z": "Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What does static quantization fuses activations into where possible?",
        "Y": "preceding layers",
        "Z": "Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Post Training Quantization is typically used when what are important?",
        "Y": "memory bandwidth and compute savings",
        "Z": "Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is another name for static quantization?",
        "Y": "Post Training Quantization",
        "Z": "Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the type of quantization used in Post Training Quantization?",
        "Y": "static quantization",
        "Z": "There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Static quantization fuses activations where possible?",
        "Y": "preceding layers",
        "Z": "Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Post Training\nQuantization:\nStatic, Dynamic,\nWeight Only Quantiztion Aware\nTraining:\nStatic Input/Output\nModel Type torch.nn.Module torch.nn.Module\n(May need some\nrefactors to make\nthe model\ncompatible with FX\nGraph Mode\nQuantization) There are three types of quantization supported in Eager Mode Quantization: dynamic quantization (weights quantized with activations read/stored in\nfloating point and quantized for compute.) static quantization (weights quantized, activations quantized, calibration\nrequired post training) quantization aware training (weights quantized, activations quantized,\nquantization numerics modeled during training) Please see our Introduction to Quantization on Pytorch blog post\nfor a more comprehensive overview of the tradeoffs between these quantization\ntypes. This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size. Diagram: API example: To learn more about dynamic quantization please see our dynamic quantization tutorial. Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ. Diagram: API Example: To learn more about static quantization, please see the static quantization tutorial.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the name of the FX Graph Mode Post Training Static Quantization?",
        "Y": "User Guide on Using FX Graph Mode Quantization",
        "Z": "Quantization types supported by FX Graph Mode can be classified in two ways: Post Training Quantization (apply quantization after training, quantization parameters are calculated based on sample calibration data) Quantization Aware Training (simulate quantization during training so that the quantization parameters can be learned together with the model using training data) And then each of these two may include any or all of the following types: Weight Only Quantization (only weight is statically quantized) Dynamic Quantization (weight is statically quantized, activation is dynamically quantized) Static Quantization (both weight and activations are statically quantized) These two ways of classification are independent, so theoretically we can have 6 different types of quantization. The supported quantization types in FX Graph Mode Quantization are: Post Training Quantization Weight Only Quantization Dynamic Quantization Static Quantization Quantization Aware Training Static Quantization There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). API Example: Please see the following tutorials for more information about FX Graph Mode Quantization: User Guide on Using FX Graph Mode Quantization FX Graph Mode Post Training Static Quantization FX Graph Mode Post Training Dynamic Quantization",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is the benefit of per channel quantization?",
        "Y": "lesser error",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values. The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What are quantized data represented as in a Quantized Tensor?",
        "Y": "int8/uint8/int32",
        "Z": "The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Quantized Tensors allow for what?",
        "Y": "serialization of data in a quantized format",
        "Z": "PyTorch supports both per tensor and per channel asymmetric linear\nquantization. Per tensor means that all the values within the tensor are\nscaled the same way. Per channel means that for each dimension, typically\nthe channel dimension of a tensor, the values\nin the tensor are scaled and offset by a different value (effectively\nthe scale and offset become vectors). This allows for lesser error in converting tensors\nto quantized values. The mapping is performed by converting the floating point tensors using Note that, we ensure that zero in floating point is represented with no error\nafter quantization, thereby ensuring that operations like padding do not cause\nadditional quantization error. In order to do quantization in PyTorch, we need to be able to represent\nquantized data in Tensors. A Quantized Tensor allows for storing\nquantized data (represented as int8/uint8/int32) along with quantization\nparameters like scale and zero_point. Quantized Tensors allow for many\nuseful operations making quantized arithmetic easy, in addition to\nallowing for serialization of data in a quantized format.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Many operations for quantized tensors are available under the same API as full float version in torch or what?",
        "Y": "torch.nn",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "Where do we support fused versions corresponding to common fusion patterns that impact quantization?",
        "Y": "torch.nn.intrinsic.quantized",
        "Z": "Quantized Tensors support a limited subset of data manipulation methods of the\nregular full-precision tensor. For NN operators included in PyTorch, we\nrestrict support to: 8 bit weights (data_type = qint8) 8 bit activations (data_type = quint8) Note that operator implementations currently only\nsupport per channel quantization for weights of the conv and linear\noperators. Furthermore the minimum and the maximum of the input data is\nmapped linearly to the minimum and the maximum of the quantized data\ntype such that zero is represented with no quantization error. Additional data types and quantization schemes can be implemented through\nthe custom operator mechanism. Many operations for quantized tensors are available under the same API as full\nfloat version in torch or torch.nn. Quantized version of NN modules that\nperform re-quantization are available in torch.nn.quantized. Those\noperations explicitly take output quantization parameters (scale and zero_point) in\nthe operation signature. In addition, we also support fused versions corresponding to common fusion\npatterns that impact quantization at: torch.nn.intrinsic.quantized. For quantization aware training, we support modules prepared for quantization\naware training at torch.nn.qat and torch.nn.intrinsic.qat The list of supported operations is sufficient to\ncover typical CNN and RNN models",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How does quantization currently work?",
        "Y": "module by module basis",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What API takes in lists of modules to be fused?",
        "Y": "torch.quantization.fuse_modules()",
        "Z": "It is necessary to currently make some modifications to the model definition\nprior to quantization. This is because currently quantization works on a module\nby module basis. Specifically, for all quantization techniques, the user needs to: Convert any operations that require output requantization (and thus have\nadditional parameters) from functionals to module form (for example,\nusing torch.nn.ReLU instead of torch.nn.functional.relu). Specify which parts of the model need to be quantized either by assigning\n.qconfig attributes on submodules or by specifying qconfig_dict.\nFor example, setting model.conv1.qconfig = None means that the\nmodel.conv layer will not be quantized, and setting\nmodel.linear1.qconfig = custom_qconfig means that the quantization\nsettings for model.linear1 will be using custom_qconfig instead\nof the global qconfig. For static quantization techniques which quantize activations, the user needs\nto do the following in addition: Specify where activations are quantized and de-quantized. This is done using\nQuantStub and\nDeQuantStub modules. Use torch.nn.quantized.FloatFunctional to wrap tensor operations\nthat require special handling for quantization into modules. Examples\nare operations like add and cat which require special handling to\ndetermine output quantization parameters. Fuse modules: combine operations/modules into a single module to obtain\nhigher accuracy and performance. This is done using the\ntorch.quantization.fuse_modules() API, which takes in lists of modules\nto be fused. We currently support the following fusions:\n[Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu]",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What quantization does the rounding of the key nn modules simulate?",
        "Y": "INT8",
        "Z": "torch.quantization This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What is another name for GRUCell?",
        "Y": "RNNCell",
        "Z": "torch.quantization This module implements the functions you call directly to convert your\nmodel from FP32 to quantized form. For example the\nprepare() is used in post training quantization\nto prepares your model for the calibration step and\nconvert() actually converts the weights to int8\nand replaces the operations with their quantized counterparts. There are\nother helper functions for things like quantizing the input to your\nmodel and performing critical fusions like conv+relu. torch.nn.intrinsic This module implements the combined (fused) modules conv + relu which can\nthen be quantized. torch.nn.intrinsic.qat This module implements the versions of those fused operations needed for\nquantization aware training. torch.nn.intrinsic.quantized This module implements the quantized implementations of fused operations\nlike conv + relu. torch.nn.qat This module implements versions of the key nn modules Conv2d() and\nLinear() which run in FP32 but with rounding applied to simulate the\neffect of INT8 quantization. torch.nn.quantized This module implements the quantized versions of the nn layers such as\n~`torch.nn.Conv2d` and torch.nn.ReLU. torch.nn.quantized.dynamic Dynamically quantized Linear, LSTM,\nLSTMCell, GRUCell, and\nRNNCell.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "What should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and summ",
        "Y": "crashed and resumed experiments",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How often, in seconds, to flush the pending events and summaries to disk?",
        "Y": "flush_secs",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "flush_secs (int) \u2013 How often, in seconds, to flush the pending events and summaries to disk?",
        "Y": "every two minutes",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "More details on what can be found in tensorboard.summary.writer.event_file_writer.EventFile",
        "Y": "filename construction",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is an example of a suffix added to all event filenames in the log_dir directory?",
        "Y": "Add scalar data to summary",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding",
        "Y": "main_tag",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does tag (string) refer to?",
        "Y": "Data identifier values",
        "Z": "purge_step (int) \u2013 When logging crashes at step T+XT+XT+X and restarts at step TTT,\nany events whose global_step larger or equal to TTT will be\npurged and hidden from TensorBoard.\nNote that crashed and resumed experiments should have the same log_dir. max_queue (int) \u2013 Size of the queue for pending events and\nsummaries before one of the \u2018add\u2019 calls forces a flush to disk.\nDefault is ten items. flush_secs (int) \u2013 How often, in seconds, to flush the\npending events and summaries to disk. Default is every two minutes. filename_suffix (string) \u2013 Suffix added to all event filenames in\nthe log_dir directory. More details on filename construction in\ntensorboard.summary.writer.event_file_writer.EventFileWriter. Examples: Add scalar data to summary. tag (string) \u2013 Data identifier scalar_value (float or string/blobname) \u2013 Value to save global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nwith seconds after epoch of event new_style (boolean) \u2013 Whether to use new style (tensor field) or old\nstyle (simple_value field). New style could lead to faster data loading. Examples: Expected result: Adds many scalar data to summary. main_tag (string) \u2013 The parent name for the tags tag_scalar_dict (dict) \u2013 Key-value pair storing the tag and corresponding values global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add histogram to summary. tag (string) \u2013 Data identifier values (torch.Tensor, numpy.array, or string/blobname) \u2013 Values to build histogram global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is the name of the global step value to record bins?",
        "Y": "global_step",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What does Add image data to summary require?",
        "Y": "pillow package",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What is also suitable as long as corresponding dataformats argument is passed?",
        "Y": "Tensor",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What dataformats argument is passed?",
        "Y": "CHW, HWC, HW",
        "Z": "global_step (int) \u2013 Global step value to record bins (string) \u2013 One of {\u2018tensorflow\u2019,\u2019auto\u2019, \u2018fd\u2019, \u2026}. This determines how the bins are made. You can find\nother options in: https://docs.scipy.org/doc/numpy/reference/generated/numpy.histogram.html walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event Examples: Expected result: Add image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event img_tensor: Default is (3,H,W)(3, H, W)(3,H,W). You can use torchvision.utils.make_grid() to\nconvert a batch of tensor into 3xHxW format or call add_images and let us do the job.\nTensor with (1,H,W)(1, H, W)(1,H,W), (H,W)(H, W)(H,W), (H,W,3)(H, W, 3)(H,W,3) is also suitable as long as\ncorresponding dataformats argument is passed, e.g. CHW, HWC, HW. Examples: Expected result: Add batched image data to summary. Note that this requires the pillow package. tag (string) \u2013 Data identifier img_tensor (torch.Tensor, numpy.array, or string/blobname) \u2013 Image data global_step (int) \u2013 Global step value to record walltime (float) \u2013 Optional override default walltime (time.time())\nseconds after epoch of event dataformats (string) \u2013 Image data format specification of the form\nNCHW, NHWC, CHW, HWC, HW, WH, etc. img_tensor: Default is (N,3,H,W)(N, 3, H, W)(N,3,H,W). If dataformats is specified, other shape will be\naccepted. e.g. NCHW or NHWC. Examples: Expected result: Render matplotlib figure into an image and add it to summary. Note that this requires the matplotlib package. tag (string) \u2013 Data identifier figure (matplotlib.pyplot.figure) \u2013 Figure or a list of figures global_step (int) \u2013 Global step value to record close (bool) \u2013 Flag to automatically close the figure",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "What provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions?",
        "Y": "torch.autograd",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is required to declare Tensor s for which gradients should be computed?",
        "Y": "requires_grad=True keyword",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What type of Tensor types do we only support autograd for?",
        "Y": "floating point Tensor types",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is another name for computes and returns the sum of gradients of outputs with respect to the inputs?",
        "Y": "Computes and returns the sum of gradients of outputs with respect to the inputs",
        "Z": "torch.autograd provides classes and functions implementing automatic\ndifferentiation of arbitrary scalar valued functions. It requires minimal\nchanges to the existing code - you only need to declare Tensor s\nfor which gradients should be computed with the requires_grad=True keyword.\nAs of now, we only support autograd for floating point Tensor types (\nhalf, float, double and bfloat16) and complex Tensor types (cfloat, cdouble).   Computes the sum of gradients of given tensors with respect to graph leaves.   Computes and returns the sum of gradients of outputs with respect to the inputs. Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What are very unlikely to change?",
        "Y": "function signatures",
        "Z": "This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set, you can use",
        "Y": "lambda",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does f(input, constant, flag=flag) have as f(input, constant, flag=flag)?",
        "Y": "boolean flag",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Functional.jacobian Function that computes what of a given function. functional.hessian Function that computes the Hessian of",
        "Y": "Jacobian",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What are the differences between Locally disabling gradient computation?",
        "Y": "no-grad and inference mode",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What disabled gradient calculation?",
        "Y": "Context-manager",
        "Z": "Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Context-manager sets gradient calculation to what?",
        "Y": "on or off",
        "Z": "This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does a context-manager enable or disable?",
        "Y": "inference mode",
        "Z": "This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "In what state is the API in?",
        "Y": "beta",
        "Z": "This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the autograd section contain?",
        "Y": "higher level API",
        "Z": "This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Context-manager that enables or disables what?",
        "Y": "inference mode",
        "Z": "This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "For more information on the differences between no-grad and inference mode, see what?",
        "Y": "Locally disabling gradient computation",
        "Z": "This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the context-manager set gradient calculation to?",
        "Y": "on or off",
        "Z": "This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Context-manager that enables or disables what mode when a non-sparse param receives a non-spar",
        "Y": "inference mode",
        "Z": "This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "This API works with what?",
        "Y": "user-provided functions",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Sets gradient calculation to what?",
        "Y": "on or off",
        "Z": "This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs. See Locally disabling gradient computation for more information on the differences\nbetween no-grad and inference mode as well as other related mechanisms that\nmay be confused with the two.   Context-manager that disabled gradient calculation.   Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout).",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does a context-manager do when a non-sparse param receives a non-sparse gradient?",
        "Y": "enables or disables inference mode",
        "Z": "Context-manager that enables gradient calculation.   Context-manager that sets gradient calculation to on or off.   Context-manager that enables or disables inference mode When a non-sparse param receives a non-sparse gradient during\ntorch.autograd.backward() or torch.Tensor.backward()\nparam.grad is accumulated as follows. If param.grad is initially None: If param\u2019s memory is non-overlapping and dense, .grad is\ncreated with strides matching param (thus matching param\u2019s\nlayout). Otherwise, .grad is created with rowmajor-contiguous strides. If param already has a non-sparse .grad attribute: If create_graph=False, backward() accumulates into .grad\nin-place, which preserves its strides. If create_graph=True, backward() replaces .grad with a\nnew tensor .grad + new grad, which attempts (but does not guarantee)\nmatching the preexisting .grad\u2019s strides. The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts. In fact, resetting all .grads to None before each\naccumulation phase, e.g.: such that they\u2019re recreated according to 1 or 2 every time,\nis a valid alternative to model.zero_grad() or optimizer.zero_grad()\nthat may improve performance for some networks. If you need manual control over .grad\u2019s strides,\nassign param.grad = a zeroed tensor with desired strides\nbefore the first backward(), and never reset it to None.\n3 guarantees your layout is preserved as long as create_graph=False.\n4 indicates your layout is likely preserved even if create_graph=True.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If your function takes other arguments that are not Tensors or Tensors that don't have requires_grad set, you can use",
        "Y": "lambda",
        "Z": "Warning This API is in beta. Even though the function signatures are very unlikely to change, major\nimprovements to performances are planned before we consider this stable. This section contains the higher level API for the autograd that builds on the basic API above\nand allows you to compute jacobians, hessians, etc. This API works with user-provided functions that take only Tensors as input and return\nonly Tensors.\nIf your function takes other arguments that are not Tensors or Tensors that don\u2019t have requires_grad set,\nyou can use a lambda to capture them.\nFor example, for a function f that takes three inputs, a Tensor for which we want the jacobian, another\ntensor that should be considered constant and a boolean flag as f(input, constant, flag=flag)\nyou can use it as functional.jacobian(lambda x: f(x, constant, flag=flag), input). functional.jacobian Function that computes the Jacobian of a given function. functional.hessian Function that computes the Hessian of a given scalar function. functional.vjp Function that computes the dot product between a vector v and the Jacobian of the given function at the point given by the inputs. functional.jvp Function that computes the dot product between  the Jacobian of the given function at the point given by the inputs and a vector v. functional.vhp Function that computes the dot product between a vector v and the Hessian of a given scalar function at the point given by the inputs. functional.hvp Function that computes the dot product between the Hessian of a given scalar function and a vector v at the point given by the inputs.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "is thread local and is automatically propagated into the async tasks enabled",
        "Y": "profiler",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "if shapes recording is set, information about input dimensions will be collected",
        "Y": "record_shapes",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "record_shapes (bool, optional) \u2013 If shapes recording is set, information about input dimensions will be collected and further group by",
        "Y": "prof.key_averages",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "skew your profiling data",
        "Y": "shape recording",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "it is recommended to use what to validate the timing?",
        "Y": "separate runs with and without shape recording",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "most likely the skew will be what for bottom most events?",
        "Y": "negligible",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "what might be artificially increased because of the shape collection?",
        "Y": "the total self cpu time",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "FLOPS",
        "Y": "floating pointer operations per second",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "FLOPS (floating pointer operations per second) value using the operator\u2019s input shape and total time",
        "Y": "hardware performance",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "the profiler will estimate the FLOPS (floating pointer operations per second) value using the operator\u2019s input shape and total time.",
        "Y": "matrix multiplication and 2D convolution operators",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "track tensor memory allocation/deallocation",
        "Y": "profile_memory",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "record source information (file and line number) for the ops",
        "Y": "with_stack",
        "Z": "Autograd includes a profiler that lets you inspect the cost of different\noperators inside your model - both on the CPU and GPU. There are two modes\nimplemented at the moment - CPU-only using profile.\nand nvprof based (registers both CPU and GPU activity) using\nemit_nvtx. Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Context manager manages what?",
        "Y": "autograd profiler state",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Under the hood it just records events of functions being executed in what language?",
        "Y": "C++",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What can you do to report runtime of PyTorch functions?",
        "Y": "wrap any code into it",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes",
        "Y": "profiler",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If shapes recording is set, information about input dimensions will be collected. This allows one to see which dimensions have been used under the hood and further group",
        "Y": "record_shapes",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What might skew your profiling data?",
        "Y": "shape recording",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Most likely the skew will be what for bottom most events?",
        "Y": "negligible",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What might be artificially increased because of the shape collection?",
        "Y": "the total self cpu time",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If with_flops is set, the profiler will estimate the FLOPS (floating pointer operations per second) value using the operator\u2019",
        "Y": "with_flops",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "FLOPS (floating pointer operations per second) value using the operator\u2019s input shape and total time allows one to estimate what?",
        "Y": "hardware performance",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "profile_memory (bool, optional) \u2013 what?",
        "Y": "track tensor memory allocation/deallocation",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profil",
        "Y": "with_stack",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False",
        "Y": "use_kineto",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "use_cpu (bool, optional) \u2013 what does use_cpu do?",
        "Y": "profile CPU events",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the name of the profiler that is used to lower the overhead for GPU-only profiling?",
        "Y": "Example profiler",
        "Z": "Context manager that manages autograd profiler state and holds a summary of results.\nUnder the hood it just records events of functions being executed in C++ and\nexposes those events to Python. You can wrap any code into it and it will\nonly report runtime of PyTorch functions.\nNote: profiler is thread local and is automatically propagated into the async tasks enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does setting this to False do?",
        "Y": "makes this context manager a no-op",
        "Z": "enabled (bool, optional) \u2013 Setting this to False makes this context manager a no-op. use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "If with_flops is set, the profiler will estimate what value?",
        "Y": "FLOPS",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does the FLOPS value allow one to estimate?",
        "Y": "hardware performance",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is used to enable profiling with Kineto profiler?",
        "Y": "use_kineto",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Use_cpu (bool, optional) \u2013 what does use_cpu do?",
        "Y": "profile CPU events",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API?",
        "Y": "use_cuda",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is the likely skew for bottom most events?",
        "Y": "negligible",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Why might the total self cpu time be artificially increased?",
        "Y": "the shape collection",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Record source information (file and line number) for the ops. what (bool, optional) \u2013 record source information (file and line number",
        "Y": "with_stack",
        "Z": "use_cuda (bool, optional) \u2013 Enables timing of CUDA events as well using the cudaEvent API.\nAdds approximately 4us of overhead to each tensor operation. record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What allows one to see which dimensions have been used under the hood and further group by them?",
        "Y": "prof.key_averages",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Context manager that makes every autograd operation emit an NVTX range is useful when running the program under what?",
        "Y": "nvprof",
        "Z": "record_shapes (bool, optional) \u2013 If shapes recording is set, information\nabout input dimensions will be collected. This allows one to see which\ndimensions have been used under the hood and further group by them\nusing prof.key_averages(group_by_input_shape=True). Please note that\nshape recording might skew your profiling data. It is recommended to\nuse separate runs with and without shape recording to validate the timing.\nMost likely the skew will be negligible for bottom most events (in a case\nof nested function calls). But for higher level functions the total\nself cpu time might be artificially increased because of the shape\ncollection. with_flops (bool, optional) \u2013 If with_flops is set, the profiler will estimate\nthe FLOPS (floating pointer operations per second) value using the operator\u2019s input shape\nand total time. This allows one to estimate the hardware performance. Currently,\nthis option only works for the matrix multiplication and 2D convolution operators. profile_memory (bool, optional) \u2013 track tensor memory allocation/deallocation. with_stack (bool, optional) \u2013 record source information (file and line number) for the ops. use_kineto (bool, optional) \u2013 experimental, enable profiling with Kineto profiler. use_cpu (bool, optional) \u2013 profile CPU events; setting to False requires\nuse_kineto=True and can be used to lower the overhead for GPU-only profiling. Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof:",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does one have to use this context manager to annotate nvprof traces and wait for the process to exit before inspecting them",
        "Y": "CUDA profiling",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "In what language can nvprof() be used to load results for inspection?",
        "Y": "Python REPL.",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What default value does enabled=False default to?",
        "Y": "True",
        "Z": "Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does setting enabled=False make this context manager a no-op?",
        "Y": "record_shapes",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where will arguments be listed?",
        "Y": "in the order they are received by the backend op",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What order may not match the order in which arguments are received by the backend op?",
        "Y": "order in which those arguments were passed on the Python side",
        "Z": "profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What correlation can be difficult when viewing a profile created using emit_nvtx in the Nvidia Visual Profiler?",
        "Y": "Forward-backward",
        "Z": "Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is done to help correlating each backward-pass op with the corresponding forward-pass op?",
        "Y": "To ease this",
        "Z": "Example profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Averages all events. Context manager that makes every autograd operation emit an NVTX range.",
        "Y": "total_average",
        "Z": "profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Set enabled=False to what default?",
        "Y": "True",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What is an example of a forward-backward correlation?",
        "Y": "Example Forward-backward correlation",
        "Z": "profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Why does Nvidia Visual Profiler use record_shapes instead of record_shapes?",
        "Y": "ease this task",
        "Z": "profiler.profile.export_chrome_trace Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Exports an EventList as what?",
        "Y": "Chrome tracing tools file",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Average",
        "Y": "profiler.profile.self_cpu_time_total",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "This order may not match what order?",
        "Y": "order in which those arguments were passed on the Python side",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Where can you view a profile created using emit_nvtx?",
        "Y": "Nvidia Visual Profiler",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "What does emit_nvtx append to ease correlating each backward-pass op with the corresponding forward-pass o",
        "Y": "sequence number information",
        "Z": "Exports an EventList as a Chrome tracing tools file. profiler.profile.key_averages Averages all function events over their keys. profiler.profile.self_cpu_time_total Returns total time spent on CPU obtained as a sum of all self times across all the events. profiler.profile.total_average Averages all events. Context manager that makes every autograd operation emit an NVTX range. It is useful when running the program under nvprof: Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. enabled (bool, optional, default=True) \u2013 Setting enabled=False makes this context manager a no-op.\nDefault: True. record_shapes (bool, optional, default=False) \u2013 If record_shapes=True, the nvtx range wrapping\neach autograd op will append information about the sizes of Tensor arguments received\nby that op, in the following format:\n[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]\nNon-tensor arguments will be represented by [].\nArguments will be listed in the order they are received by the backend op.\nPlease note that this order may not match the order in which those arguments were passed\non the Python side.  Also note that shape recording may increase the overhead of nvtx range creation. Example Forward-backward correlation When viewing a profile created using emit_nvtx in the Nvidia Visual Profiler,\ncorrelating each backward-pass op with the corresponding forward-pass op can be difficult.\nTo ease this task, emit_nvtx appends sequence number information to the ranges it\ngenerates.",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "Bite-size, ready-to-deploy what?",
        "Y": "PyTorch code examples",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is used to visualize data and model training. Interpretability,Getting-Started,TensorBoard Finetune a pre",
        "Y": "TensorBoard",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is used to classify names?",
        "Y": "character-level RNN",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the third and final tutorial on doing \"NLP From Scratch\"?",
        "Y": "third and final",
        "Z": "Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript  Learn how PyTorch provides to go from an existing Python model to a serialized representation that can be loaded and executed purely from C++, with no dependency on Python. Production,TorchScript  Convert a model defined in PyTorch into the ONNX format and then run it with ONNX Runtime. Production  Build a simple FX pass that fuses batch norm into convolution to improve performance during inference. FX  Build a simple FX interpreter to record the runtime of op, module, and function calls and report statistics FX  Get an overview of Channels Last memory format and understand how it is used to order NCHW tensors in memory preserving dimensions. Memory-Format,Best-Practice,Frontend-APIs",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the third and final tutorial on doing?",
        "Y": "NLP From Scratch",
        "Z": "Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What library does PyTorch use to build the dataset and classify text?",
        "Y": "torchtext library",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What type of learning does PyTorch train?",
        "Y": "Reinforcement-Learning",
        "Z": "Bite-size, ready-to-deploy PyTorch code examples.  A step-by-step guide to building a complete ML workflow with PyTorch. Getting-Started  This tutorial introduces the fundamental concepts of PyTorch through self-contained examples. Getting-Started  Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a Getting-Started,TensorBoard Finetune a pre-trained Mask R-CNN",
        "Y": "Interpretability",
        "Z": "Use torch.nn to create and train a neural network. Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "After using character-level RNN to classify names, leanr how to generate names from what?",
        "Y": "languages",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the third in a series of three tutorials on doing \"NLP From Scratch\"?",
        "Y": "third and final tutorial",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "How do you deploy a PyTorch model?",
        "Y": "PyTorch model using Flask",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the term for a model that detects the image?",
        "Y": "Production",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Getting-Started,TensorBoard Finetune a pre-trained Mask R-CNN model. What is",
        "Y": "Interpretability",
        "Z": "Getting-Started  Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Train a generative adversarial network to generate new celebrities.",
        "Y": "GAN",
        "Z": "Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What are some of the features of TensorBoard?",
        "Y": "Interpretability,Getting-Started,TensorBoard Finetune a pre-trained Mask R-CNN model",
        "Z": "Learn to use TensorBoard to visualize data and model training. Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What are some examples of fine tuning a pre-trained Mask R-CNN model?",
        "Y": "Interpretability,Getting-Started,TensorBoard",
        "Z": "Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "Image/Video Train a convolutional neural network for image classification using transfer learning. Image/Video Train a generative adversarial network (",
        "Y": "Image/Video",
        "Z": "Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What does PyTorch use to deploy a model?",
        "Y": "Flask",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is an intermediate representation of a PyTorch model?",
        "Y": "Production Introduction to TorchScript",
        "Z": "Interpretability,Getting-Started,TensorBoard  Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++.",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a pre-trained Mask R-CNN model?",
        "Y": "Finetune",
        "Z": "Finetune a pre-trained Mask R-CNN model. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is the intermediate representation of a PyTorch model that can then be run in a high-performance environment such as C++?",
        "Y": "Production",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is TorchScript an intermediate representation of a PyTorch model?",
        "Y": "Production",
        "Z": "Train a convolutional neural network for image classification using transfer learning. Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What is a PyTorch model deployed using?",
        "Y": "Flask",
        "Z": "Image/Video  Apply cutting-edge, attention-based transformer models to computer vision tasks. Image/Video  Train a convolutional neural network for image classification using transfer learning. Image/Video  Train a generative adversarial network (GAN) to generate new celebrities. Image/Video  Learn how to augment your network using a visual attention mechanism. Image/Video  Learn to load and preprocess data from a simple dataset with PyTorch's torchaudio library. Audio  Learn how to correctly format an audio dataset and then train/test an audio classifier network on the dataset. Audio  Learn how to train a sequence-to-sequence model that uses the nn.Transformer module. Text  Build and train a basic character-level RNN to classify word from scratch without the use of torchtext. First in a series of three tutorials. Text  After using character-level RNN to classify names, leanr how to generate names from languages. Second in a series of three tutorials. Text  This is the third and final tutorial on doing \u201cNLP From Scratch\u201d, where we write our own classes and functions to preprocess the data to do our NLP modeling tasks. Text  Learn how to build the dataset and classify text using torchtext library. Text  Train a language translation model from scratch using Transformer. Text  Learn how to use PyTorch to train a Deep Q Learning (DQN) agent on the CartPole-v0 task from the OpenAI Gym. Reinforcement-Learning  Use PyTorch to train a Double Q-learning agent to play Mario. Reinforcement-Learning  Deploy a PyTorch model using Flask and expose a REST API for model inference using the example of a pretrained DenseNet 121 model which detects the image. Production  Introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++. Production,TorchScript",
        "source": "https://pytorch.org/tutorials"
    },
    {
        "X": "What does python -m torch.utils.bottleneck -h refer to?",
        "Y": "script.py",
        "Z": "Run it on the command line with where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "If the profiler outputs don't help, you could try looking at the result of what with nvprof?",
        "Y": "torch.autograd.profiler.emit_nvtx()",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What are any number of arguments to script.py?",
        "Y": "args",
        "Z": "where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What are the two types of autograd profiles?",
        "Y": "CPU-only-mode or CUDA-mode",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "If your script spends most of its time executing on the GPU, it makes sense to start looking for what in the output of the CUDA",
        "Y": "responsible CUDA operators",
        "Z": "where [args] are any number of arguments to script.py, or run\npython -m torch.utils.bottleneck -h for more usage instructions. Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the reality if your script spends most of its time executing on the GPU?",
        "Y": "much more complicated",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "What is the first profiler that bottleneck runs (cProfile) will include in its time reporting?",
        "Y": "CUDA startup time",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "For more complicated uses of profilers, please see https://docs.python.org/3/library/profile.html or torch.",
        "Y": "multi-GPU",
        "Z": "Warning Because your script will be profiled, please ensure that it exits in a\nfinite amount of time. Warning Due to the asynchronous nature of CUDA kernels, when running against\nCUDA code, the cProfile output and CPU-mode autograd profilers may\nnot show correct timings: the reported CPU time reports the amount of time\nused to launch the kernels but does not include the time the kernel\nspent executing on a GPU unless the operation does a synchronize.\nOps that do synchronize appear to be extremely expensive under regular\nCPU-mode profilers.\nIn these case where timings are incorrect, the CUDA-mode autograd profiler\nmay be helpful. Note To decide which (CPU-only-mode or CUDA-mode) autograd profiler output to\nlook at, you should first check if your script is CPU-bound\n(\u201cCPU total time is much greater than CUDA total time\u201d).\nIf it is CPU-bound, looking at the results of the CPU-mode autograd\nprofiler will help. If on the other hand your script spends most of its\ntime executing on the GPU, then it makes sense to start\nlooking for responsible CUDA operators in the output of the CUDA-mode\nautograd profiler. Of course the reality is much more complicated and your script might not be\nin one of those two extremes depending on the part of the model you\u2019re\nevaluating. If the profiler outputs don\u2019t help, you could try looking at\nthe result of torch.autograd.profiler.emit_nvtx() with nvprof.\nHowever, please take into account that the NVTX overhead is very high and\noften gives a heavily skewed timeline. Warning If you are profiling CUDA code, the first profiler that bottleneck runs\n(cProfile) will include the CUDA startup time (CUDA buffer allocation cost)\nin its time reporting. This should not matter if your bottlenecks result\nin code much slower than the CUDA startup time. For more complicated uses of the profilers (like in a multi-GPU case),\nplease see https://docs.python.org/3/library/profile.html\nor torch.autograd.profiler.profile() for more information.",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "If deterministic output compared to non-checkpointed passes is not required, what can be done to omit stashing and",
        "Y": "supply preserve_rng_state=False to checkpoint or checkpoint_sequential",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does the stashing logic save and restore the RNG state for?",
        "Y": "current device and the device of all cuda Tensor arguments",
        "Z": "Note Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does the checkpointed part do instead of storing all intermediate activations of the entire computation graph for computing backward?",
        "Y": "the checkpointed part does not save intermediate activations",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does the stashing logic save and restore to the run_fn?",
        "Y": "current device and the device of all cuda Tensor arguments",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does the stashing logic have no way to anticipate if the user will move Tensors to a new device within the run_f",
        "Y": "the logic has no way to anticipate if the user will move Tensors to a new device within the run_fn itself",
        "Z": "The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does checkpointing trade?",
        "Y": "compute for memory",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In the backwards pass, the saved inputs and function are retrieved and what?",
        "Y": "the forward pass is computed on function again",
        "Z": "Checkpointing is implemented by rerunning a forward-pass segment for\neach checkpointed segment during backward.  This can cause persistent\nstates like the RNG state to be advanced than they would without\ncheckpointing.  By default, checkpointing includes logic to juggle\nthe RNG state such that checkpointed passes making use of RNG\n(through dropout for example) have deterministic output as\ncompared to non-checkpointed passes.  The logic to stash and restore\nRNG states can incur a moderate performance hit depending on the runtime\nof checkpointed operations.  If deterministic output compared to\nnon-checkpointed passes is not required, supply preserve_rng_state=False\nto checkpoint or checkpoint_sequential to omit stashing and\nrestoring the RNG state during each checkpoint. The stashing logic saves and restores the RNG state for the current device\nand the device of all cuda Tensor arguments to the run_fn.\nHowever, the logic has no way to anticipate if the user will move\nTensors to a new device within the run_fn itself.  Therefore, if you move\nTensors to a new device (\u201cnew\u201d meaning not belonging to the set of\n[current device + devices of Tensor arguments]) within run_fn, deterministic\noutput compared to non-checkpointed passes is never guaranteed. Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What part of a model does not save intermediate activations?",
        "Y": "the checkpointed part",
        "Z": "Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What can the checkpointed part be applied to?",
        "Y": "any part of a model",
        "Z": "Checkpoint a model or part of the model Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In the backwards pass, what happens after the saved inputs and function are retrieved?",
        "Y": "the forward pass is computed on function again",
        "Z": "Checkpointing works by trading compute for memory. Rather than storing all\nintermediate activations of the entire computation graph for computing\nbackward, the checkpointed part does not save intermediate activations,\nand instead recomputes them in backward pass. It can be applied on any part\nof a model. Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What will not be considered as part of autograd if the output consists of nested structures consisting of Tensors?",
        "Y": "custom structures",
        "Z": "The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is not supported by Checkpointing currently only supports torch.autograd.backward() and only if its inputs argument is not passed?",
        "Y": "torch.autograd.grad()",
        "Z": "Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "If function invocation during backward does anything different than the one during forward, the checkpointed version won\u2019t be equivalent, and unfortunately it",
        "Y": "global variable",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What describes what to run in the forward pass of the model or part of the model?",
        "Y": "function",
        "Z": "Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What should function know how to handle?",
        "Y": "inputs passed as the tuple",
        "Z": "Specifically, in the forward pass, function will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. Instead, the forward pass saves the inputs tuple and the\nfunction parameter. In the backwards pass, the saved inputs and\nfunction is retrieved, and the forward pass is computed on\nfunction again, now tracking the intermediate activations, and then\nthe gradients are calculated using these activation values. The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What are examples of nested structures consisting of Tensors?",
        "Y": "custom objects, lists, dicts",
        "Z": "The output of function can contain non-Tensor values and gradient\nrecording is only performed for the Tensor values. Note that if the output\nconsists of nested structures (ex: custom objects, lists, dicts etc.)\nconsisting of Tensors, these Tensors nested in custom structures will not\nbe considered as part of autograd. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What should a function know how to handle the inputs passed as?",
        "Y": "tuple",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is an example of a function that should know how to handle the inputs passed as the tuple?",
        "Y": "LSTM",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does a function describe?",
        "Y": "what to run in the forward pass of the model or part of the model",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the name of the function that checks the outputs of each checkpointed segment?",
        "Y": "checkpoint()",
        "Z": "Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What are the functions used to run sequentially?",
        "Y": "A torch.nn.Sequential or the list of modules or functions",
        "Z": "Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially.",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In what model should the first input be used as activation and the second as hidden?",
        "Y": "LSTM",
        "Z": "Warning If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is the name of the function that checks the inputs of each checkpointed segment?",
        "Y": "checkpoint()",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a tensor that is input to functions?",
        "Y": "Number of chunks to create in the model input",
        "Z": "If function invocation during backward does anything different\nthan the one during forward, e.g., due to some global variable, the\ncheckpointed version won\u2019t be equivalent, and unfortunately it can\u2019t be\ndetected. Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What is a way to circumvent the issue of a tensor having no gradient in the model?",
        "Y": "detach the tensors outside of the checkpoint function",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "In what function should the first input be used as activation and the second as hidden?",
        "Y": "LSTM",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What function describes how checkpointing works?",
        "Y": "checkpoint()",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What do segments represent in the model input?",
        "Y": "Number of chunks",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "Output of running functions sequentially on what?",
        "Y": "*inputs",
        "Z": "Warning If checkpointed segment contains tensors detached from the computational\ngraph by detach() or torch.no_grad(), the backward pass will raise an\nerror. This is because checkpoint makes all the outputs require\ngradients which causes issues when a tensor is defined to have no\ngradient in the model. To circumvent this, detach the tensors outside of\nthe checkpoint function. function \u2013 describes what to run in the forward pass of the model or\npart of the model. It should also know how to handle the inputs\npassed as the tuple. For example, in LSTM, if user passes\n(activation, hidden), function should correctly use the\nfirst input as activation and the second input as hidden preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. args \u2013 tuple containing inputs to the function Output of running function on *args A helper function for checkpointing sequential models. Sequential models execute a list of modules/functions in order\n(sequentially). Therefore, we can divide such a model in various segments\nand checkpoint each segment. All segments except the last will run in\ntorch.no_grad() manner, i.e., not storing the intermediate\nactivations. The inputs of each checkpointed segment will be saved for\nre-running the segment in the backward pass. See checkpoint() on how checkpointing works. Warning Checkpointing currently only supports torch.autograd.backward()\nand only if its inputs argument is not passed. torch.autograd.grad()\nis not supported. functions \u2013 A torch.nn.Sequential or the list of modules or\nfunctions (comprising the model) to run sequentially. segments \u2013 Number of chunks to create in the model input \u2013 A Tensor that is input to functions preserve_rng_state (bool, optional, default=True) \u2013 Omit stashing and restoring\nthe RNG state during each checkpoint. Output of running functions sequentially on *inputs Example",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "What does it do to a matrix or batches of matrices A?",
        "Y": "Computes the LU factorization",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Where is the status of the factorization present in the return tuple?",
        "Y": "third element",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Why is the LU factorization of batches of square matrices with size less than 32 on a CUDA device?",
        "Y": "LU factorization is repeated for singular matrices",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "L, U, and P can be derived using what?",
        "Y": "torch.lu_unpack()",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default value of a tuple of tensors containing factorization (Tensor)?",
        "Y": "None",
        "Z": "Computes the LU factorization of a matrix or batches of matrices\nA. Returns a tuple containing the LU factorization and\npivots of A.  Pivoting is done if pivot is set to\nTrue. Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing).",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "The pivots returned by the function are what?",
        "Y": "1-indexed",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Why is LU factorization with pivot = False not available for CPU?",
        "Y": "not available for CPU",
        "Z": "Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "The status of the factorization is present in what element of the return tuple?",
        "Y": "third element",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Why is LU factorization repeated for singular matrices?",
        "Y": "LU factorization is repeated for singular matrices",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What default value does get_infos return an info IntTensor?",
        "Y": "True",
        "Z": "Note The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What is the default value for an optional output tuple?",
        "Y": "False out",
        "Z": "The pivots returned by the function are 1-indexed. If pivot is False,\nthen the returned pivots is a tensor filled with zeros of the appropriate size. Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "If get_infos is False, then the elements in the tuple are what?",
        "Y": "Tensor, IntTensor",
        "Z": "Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "Where is the status of the factorization present?",
        "Y": "third element of the return tuple",
        "Z": "Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What does if set to True, returns an info IntTensor?",
        "Y": "True get_infos",
        "Z": "Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What could be reconstructed by applying swap(per)?",
        "Y": "final permutation perm",
        "Z": "Note LU factorization with pivot = False is not available for CPU, and attempting\nto do so will throw an error. However, LU factorization with pivot = False is\navailable for CUDA. Note This function does not check if the factorization was successful or not if\nget_infos is True since the status of the factorization is present in the\nthird element of the return tuple. Note In the case of batches of square matrices with size less or\nequal to 32 on a CUDA device, the LU factorization is repeated\nfor singular matrices due to the bug in the MAGMA library (see\nmagma issue 13). Note L, U, and P can be derived using torch.lu_unpack(). Warning The LU factorization does have backward support,\nbut only for square inputs of full rank. A (Tensor) \u2013 the tensor to factor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivot (bool, optional) \u2013 controls whether pivoting is done. Default: True get_infos (bool, optional) \u2013 if set to True, returns an info IntTensor.\nDefault: False out (tuple, optional) \u2013 optional output tuple. If get_infos is True,\nthen the elements in the tuple are Tensor, IntTensor,\nand IntTensor. If get_infos is False, then the\nelements in the tuple are Tensor, IntTensor. Default: None  A tuple of tensors containing factorization (Tensor): the factorization of size (\u2217,m,n)(*, m, n)(\u2217,m,n) pivots (IntTensor): the pivots of size (\u2217,min(m,n))(*, \\text{min}(m, n))(\u2217,min(m,n)).\npivots stores all the intermediate transpositions of rows.\nThe final permutation perm could be reconstructed by\napplying swap(perm[i], perm[pivots[i] - 1]) for i = 0, ..., pivots.size(-1) - 1,\nwhere perm is initially the identity permutation of mmm elements\n(essentially this is what torch.lu_unpack() is doing). infos (IntTensor, optional): if get_infos is True, this is a tensor of\nsize (\u2217)(*)(\u2217) where non-zero values indicate whether factorization for the matrix or\neach minibatch has succeeded or failed (Tensor, IntTensor, IntTensor (optional)) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "What are the builtin location tags 'cpu' and 'cuda:device_id' for?",
        "Y": "CUDA tensors",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "If map_location returns a storage, it will be used as what?",
        "Y": "final deserialized object",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "If map_location is a string containing a device tag, it indicates the location where all tensors should be loaded. Otherwise",
        "Y": "torch.device object or a string containing a device tag",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "If map_location is a torch.device object or a string containing a device tag, it indicates the location where all ten",
        "Y": "dict",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "Who can register their own location tags and tagging and deserialization methods using torch.serialization.register_package()?",
        "Y": "User extensions",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is the risk of using pickle module implicitly?",
        "Y": "insecure",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "What is possible to construct?",
        "Y": "construct",
        "Z": "If map_location is a callable, it will be called once for each serialized\nstorage with two arguments: storage and location. The storage argument\nwill be the initial deserialization of the storage, residing on the CPU.\nEach serialized storage has a location tag associated with it which\nidentifies the device it was saved from, and this tag is the second\nargument passed to map_location. The builtin location tags are 'cpu'\nfor CPU tensors and 'cuda:device_id' (e.g. 'cuda:2') for CUDA tensors.\nmap_location should return either None or a storage. If\nmap_location returns a storage, it will be used as the final deserialized\nobject, already moved to the right device. Otherwise, torch.load() will\nfall back to the default behavior, as if map_location wasn\u2019t specified. If map_location is a torch.device object or a string containing\na device tag, it indicates the location where all tensors should be loaded. Otherwise, if map_location is a dict, it will be used to remap location tags\nappearing in the file (keys), to ones that specify where to put the\nstorages (values). User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). f \u2013 a file-like object (has to implement read(), readline(), tell(), and seek()),\nor a string or os.PathLike object containing a file name map_location \u2013 a function, torch.device, string or a dict specifying how to remap storage\nlocations pickle_module \u2013 module used for unpickling metadata and objects (has to\nmatch the pickle_module used to serialize file) pickle_load_args \u2013 (Python 3 only) optional keyword arguments passed over to\npickle_module.load() and pickle_module.Unpickler(), e.g.,\nerrors=.... Warning torch.load() uses pickle module implicitly, which is known to be insecure.\nIt is possible to construct malicious pickle data which will execute arbitrary code\nduring unpickling. Never load data that could have come from an untrusted\nsource, or that could have been tampered with. Only load data you trust. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "How to use torch.atanh, give an example?",
        "Y": ">>> a = torch.randn(4).uniform_(-1, 1)\n>>> a\ntensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])\n>>> torch.atanh(a)\ntensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])",
        "Z": ">>> a = torch.randn(4).uniform_(-1, 1)\n>>> a\ntensor([ -0.9385, 0.2968, -0.8591, -0.1871 ])\n>>> torch.atanh(a)\ntensor([ -1.7253, 0.3060, -1.2899, -0.1893 ])",
        "source": "https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh"
    },
    {
        "X": "How to use torch.logaddexp, give an example?",
        "Y": ">>> torch.logaddexp(torch.tensor([-1.0]), torch.tensor([-1.0, -2, -3]))\ntensor([-0.3069, -0.6867, -0.8731])\n>>> torch.logaddexp(torch.tensor([-100.0, -200, -300]), torch.tensor([-1.0, -2, -3]))\ntensor([-1., -2., -3.])\n>>> torch.logaddexp(torch.tensor([1.0, 2000, 30000]), torch.tensor([-1.0, -2, -3]))\ntensor([1.1269e+00, 2.0000e+03, 3.0000e+04])",
        "Z": "This op should be disambiguated with torch.logsumexp() which performs a\nreduction on a single tensor. >>> torch.logaddexp(torch.tensor([-1.0]), torch.tensor([-1.0, -2, -3]))\ntensor([-0.3069, -0.6867, -0.8731])\n>>> torch.logaddexp(torch.tensor([-100.0, -200, -300]), torch.tensor([-1.0, -2, -3]))\ntensor([-1., -2., -3.])\n>>> torch.logaddexp(torch.tensor([1.0, 2000, 30000]), torch.tensor([-1.0, -2, -3]))\ntensor([1.1269e+00, 2.0000e+03, 3.0000e+04])",
        "source": "https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"
    },
    {
        "X": "How to use torch.testing.assert_close, give an example?",
        "Y": ">>> # tensor to tensor comparison\n>>> expected = torch.tensor([1e0, 1e-1, 1e-2])\n>>> actual = torch.acos(torch.cos(expected))\n>>> torch.testing.assert_close(actual, expected)",
        "Z": "For max_abs_diff and max_rel_diff the type depends on the dtype of the inputs. >>> # tensor to tensor comparison\n>>> expected = torch.tensor([1e0, 1e-1, 1e-2])\n>>> actual = torch.acos(torch.cos(expected))\n>>> torch.testing.assert_close(actual, expected)",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "How  For max_abs_diff and max_rel_diff the type depends on the dtype of the inputs., give an example?",
        "Y": ">>> expected = torch.tensor([1.0, 2.0, 3.0])\n>>> actual = torch.tensor([1.0, 4.0, 5.0])\n>>> # The default mismatch message can be overwritten.\n>>> torch.testing.assert_close(actual, expected, msg=\"Argh, the tensors are not close!\")\nAssertionError: Argh, the tensors are not close!\n>>> # The error message can also created at runtime by passing a callable.\n>>> def custom_msg(actual, expected, diagnostic_info):\n...     return (\n...         f\"Argh, we found {diagnostic_info.total_mismatches} mismatches! \"\n...         f\"That is {diagnostic_info.mismatch_ratio:.1%}!\"\n...     )\n>>> torch.testing.assert_close(actual, expected, msg=custom_msg)\nAssertionError: Argh, we found 2 mismatches! That is 66.7%!",
        "Z": "For max_abs_diff and max_rel_diff the type depends on the dtype of the inputs. >>> expected = torch.tensor([1.0, 2.0, 3.0])\n>>> actual = torch.tensor([1.0, 4.0, 5.0])\n>>> # The default mismatch message can be overwritten.\n>>> torch.testing.assert_close(actual, expected, msg=\"Argh, the tensors are not close!\")\nAssertionError: Argh, the tensors are not close!\n>>> # The error message can also created at runtime by passing a callable.\n>>> def custom_msg(actual, expected, diagnostic_info):\n...     return (\n...         f\"Argh, we found {diagnostic_info.total_mismatches} mismatches! \"\n...         f\"That is {diagnostic_info.mismatch_ratio:.1%}!\"\n...     )\n>>> torch.testing.assert_close(actual, expected, msg=custom_msg)\nAssertionError: Argh, we found 2 mismatches! That is 66.7%!",
        "source": "https://pytorch.org/docs/stable/testing.html"
    },
    {
        "X": "How to use torch.diag, give an example?",
        "Y": ">>> a = torch.randn(3)\n>>> a\ntensor([ 0.5950,-0.0872, 2.3298])\n>>> torch.diag(a)\ntensor([[ 0.5950, 0.0000, 0.0000],\n        [ 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 2.3298]])\n>>> torch.diag(a, 1)\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n        [ 0.0000, 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 0.0000, 2.3298],\n        [ 0.0000, 0.0000, 0.0000, 0.0000]])",
        "Z": "Get the square matrix where the input vector is the diagonal: >>> a = torch.randn(3)\n>>> a\ntensor([ 0.5950,-0.0872, 2.3298])\n>>> torch.diag(a)\ntensor([[ 0.5950, 0.0000, 0.0000],\n        [ 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 2.3298]])\n>>> torch.diag(a, 1)\ntensor([[ 0.0000, 0.5950, 0.0000, 0.0000],\n        [ 0.0000, 0.0000,-0.0872, 0.0000],\n        [ 0.0000, 0.0000, 0.0000, 2.3298],\n        [ 0.0000, 0.0000, 0.0000, 0.0000]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "How  Get the square matrix where the input vector is the diagonal:Get the k-th diagonal of a given matrix:, give an example?",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.4264, 0.0255,-0.1064],\n        [ 0.8795,-0.2429, 0.1374],\n        [ 0.1029,-0.6482,-1.6300]])\n>>> torch.diag(a, 0)\ntensor([-0.4264,-0.2429,-1.6300])\n>>> torch.diag(a, 1)\ntensor([ 0.0255, 0.1374])",
        "Z": "Get the square matrix where the input vector is the diagonal:Get the k-th diagonal of a given matrix: >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.4264, 0.0255,-0.1064],\n        [ 0.8795,-0.2429, 0.1374],\n        [ 0.1029,-0.6482,-1.6300]])\n>>> torch.diag(a, 0)\ntensor([-0.4264,-0.2429,-1.6300])\n>>> torch.diag(a, 1)\ntensor([ 0.0255, 0.1374])",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"
    },
    {
        "X": "How to use The context managers torch.no_grad(), torch.enable_grad(), and\ntorch.set_grad_enabled() are helpful for locally disabling and enabling\ngradient computation. See Locally disabling gradient computation for more details on\ntheir usage.  These context managers are thread local, so they won\u2019t\nwork if you send work to another thread using the threading module, etc., give an example?",
        "Y": ">>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse",
        "Z": ">>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse",
        "source": "https://pytorch.org/docs/stable/torch.html#math-operations"
    },
    {
        "X": "How to use torch.overrides.get_ignored_functions, give an example?",
        "Y": ">>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()\nTrue\n>>> torch.add in torch.overrides.get_ignored_functions()\nFalse",
        "Z": ">>> torch.Tensor.as_subclass in torch.overrides.get_ignored_functions()\nTrue\n>>> torch.add in torch.overrides.get_ignored_functions()\nFalse",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How to use torch.overrides.get_testing_overrides, give an example?",
        "Y": ">>> import inspect\n>>> my_add = torch.overrides.get_testing_overrides()[torch.add]\n>>> inspect.signature(my_add)\n<Signature (input, other, out=None)>",
        "Z": ">>> import inspect\n>>> my_add = torch.overrides.get_testing_overrides()[torch.add]\n>>> inspect.signature(my_add)\n<Signature (input, other, out=None)>",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How to use torch.overrides.handle_torch_function, give an example?",
        "Y": ">>> def func(a):\n...     if type(a) is not torch.Tensor:  # This will make func dispatchable by __torch_function__\n...         return handle_torch_function(func, (a,), a)\n...     return a + 0",
        "Z": ":raises TypeError : if no implementation is found.: >>> def func(a):\n...     if type(a) is not torch.Tensor:  # This will make func dispatchable by __torch_function__\n...         return handle_torch_function(func, (a,), a)\n...     return a + 0",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How to use torch.overrides.is_tensor_like, give an example?",
        "Y": ">>> class SubTensor(torch.Tensor): ...\n>>> is_tensor_like(SubTensor([0]))\nTrue",
        "Z": "A subclass of tensor is generally a Tensor-like. >>> class SubTensor(torch.Tensor): ...\n>>> is_tensor_like(SubTensor([0]))\nTrue",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How  A subclass of tensor is generally a Tensor-like.Built-in or user types aren\u2019t usually Tensor-like., give an example?",
        "Y": ">>> is_tensor_like(6)\nFalse\n>>> is_tensor_like(None)\nFalse\n>>> class NotATensor: ...\n>>> is_tensor_like(NotATensor())\nFalse",
        "Z": "A subclass of tensor is generally a Tensor-like.Built-in or user types aren\u2019t usually Tensor-like. >>> is_tensor_like(6)\nFalse\n>>> is_tensor_like(None)\nFalse\n>>> class NotATensor: ...\n>>> is_tensor_like(NotATensor())\nFalse",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How  Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__., give an example?",
        "Y": ">>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrue",
        "Z": "Built-in or user types aren\u2019t usually Tensor-like.But, they can be made Tensor-like by implementing __torch_function__. >>> class TensorLike:\n...     def __torch_function__(self, func, types, args, kwargs):\n...         return -1\n>>> is_tensor_like(TensorLike())\nTrue",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How to use torch.overrides.is_tensor_method_or_property, give an example?",
        "Y": ">>> is_tensor_method_or_property(torch.Tensor.add)\nTrue\n>>> is_tensor_method_or_property(torch.add)\nFalse",
        "Z": "This may be needed, in particular, for the following reasons: >>> is_tensor_method_or_property(torch.Tensor.add)\nTrue\n>>> is_tensor_method_or_property(torch.add)\nFalse",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How to use torch.overrides.wrap_torch_function, give an example?",
        "Y": ">>> def dispatcher(a): # Must have the same signature as func\n...     return (a,)\n>>> @torch.overrides.wrap_torch_function(dispatcher)\n>>> def func(a): # This will make func dispatchable by __torch_function__\n...     return a + 0",
        "Z": ">>> def dispatcher(a): # Must have the same signature as func\n...     return (a,)\n>>> @torch.overrides.wrap_torch_function(dispatcher)\n>>> def func(a): # This will make func dispatchable by __torch_function__\n...     return a + 0",
        "source": "https://pytorch.org/docs/stable/torch.overrides.html"
    },
    {
        "X": "How to use torch.mvlgamma, give an example?",
        "Y": ">>> a = torch.empty(2, 3).uniform_(1, 2)\n>>> a\ntensor([[1.6835, 1.8474, 1.1929],\n        [1.0475, 1.7162, 1.4180]])\n>>> torch.mvlgamma(a, 2)\ntensor([[0.3928, 0.4007, 0.7586],\n        [1.0311, 0.3901, 0.5049]])",
        "Z": "All elements must be greater than p\u221212\\frac{p - 1}{2}2p\u22121\u200b, otherwise an error would be thrown. >>> a = torch.empty(2, 3).uniform_(1, 2)\n>>> a\ntensor([[1.6835, 1.8474, 1.1929],\n        [1.0475, 1.7162, 1.4180]])\n>>> torch.mvlgamma(a, 2)\ntensor([[0.3928, 0.4007, 0.7586],\n        [1.0311, 0.3901, 0.5049]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma"
    },
    {
        "X": "How to use torch.fake_quantize_per_channel_affine, give an example?",
        "Y": ">>> x = torch.randn(2, 2, 2)\n>>> x\ntensor([[[-0.2525, -0.0466],\n         [ 0.3491, -0.2168]],\n\n        [[-0.5906,  1.6258],\n         [ 0.6444, -0.0542]]])\n>>> scales = (torch.randn(2) + 1) * 0.05\n>>> scales\ntensor([0.0475, 0.0486])\n>>> zero_points = torch.zeros(2).to(torch.long)\n>>> zero_points\ntensor([0, 0])\n>>> torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255)\ntensor([[[0.0000, 0.0000],\n         [0.3405, 0.0000]],\n\n        [[0.0000, 1.6134],\n        [0.6323, 0.0000]]])",
        "Z": ">>> x = torch.randn(2, 2, 2)\n>>> x\ntensor([[[-0.2525, -0.0466],\n         [ 0.3491, -0.2168]],\n\n        [[-0.5906,  1.6258],\n         [ 0.6444, -0.0542]]])\n>>> scales = (torch.randn(2) + 1) * 0.05\n>>> scales\ntensor([0.0475, 0.0486])\n>>> zero_points = torch.zeros(2).to(torch.long)\n>>> zero_points\ntensor([0, 0])\n>>> torch.fake_quantize_per_channel_affine(x, scales, zero_points, 1, 0, 255)\ntensor([[[0.0000, 0.0000],\n         [0.3405, 0.0000]],\n\n        [[0.0000, 1.6134],\n        [0.6323, 0.0000]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_channel_affine.html#torch.fake_quantize_per_channel_affine"
    },
    {
        "X": "How to use torch.atleast_1d, give an example?",
        "Y": ">>> x = torch.randn(2)\n>>> x\ntensor([1.4584, 0.7583])\n>>> torch.atleast_1d(x)\ntensor([1.4584, 0.7583])\n>>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_1d(x)\ntensor([1.])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_1d((x,y))\n(tensor([0.5000]), tensor([1.]))",
        "Z": ">>> x = torch.randn(2)\n>>> x\ntensor([1.4584, 0.7583])\n>>> torch.atleast_1d(x)\ntensor([1.4584, 0.7583])\n>>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_1d(x)\ntensor([1.])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_1d((x,y))\n(tensor([0.5000]), tensor([1.]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_1d.html#torch.atleast_1d"
    },
    {
        "X": "How to use torch.heaviside, give an example?",
        "Y": ">>> input = torch.tensor([-1.5, 0, 2.0])\n>>> values = torch.tensor([0.5])\n>>> torch.heaviside(input, values)\ntensor([0.0000, 0.5000, 1.0000])\n>>> values = torch.tensor([1.2, -2.0, 3.5])\n>>> torch.heaviside(input, values)\ntensor([0., -2., 1.])",
        "Z": ">>> input = torch.tensor([-1.5, 0, 2.0])\n>>> values = torch.tensor([0.5])\n>>> torch.heaviside(input, values)\ntensor([0.0000, 0.5000, 1.0000])\n>>> values = torch.tensor([1.2, -2.0, 3.5])\n>>> torch.heaviside(input, values)\ntensor([0., -2., 1.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside"
    },
    {
        "X": "How to use torch.atleast_2d, give an example?",
        "Y": ">>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_2d(x)\ntensor([[1.]])\n>>> x = torch.randn(2,2)\n>>> x\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> torch.atleast_2d(x)\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_2d((x,y))\n(tensor([[0.5000]]), tensor([[1.]]))",
        "Z": ">>> x = torch.tensor(1.)\n>>> x\ntensor(1.)\n>>> torch.atleast_2d(x)\ntensor([[1.]])\n>>> x = torch.randn(2,2)\n>>> x\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> torch.atleast_2d(x)\ntensor([[2.2086, 2.5165],\n        [0.1757, 0.5194]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_2d((x,y))\n(tensor([[0.5000]]), tensor([[1.]]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_2d.html#torch.atleast_2d"
    },
    {
        "X": "How to use torch.ldexp, give an example?",
        "Y": ">>> torch.ldexp(torch.tensor([1.]), torch.tensor([1]))\ntensor([2.])\n>>> torch.ldexp(torch.tensor([1.0]), torch.tensor([1, 2, 3, 4]))\ntensor([ 2.,  4.,  8., 16.])",
        "Z": "Typically this function is used to construct floating point numbers by multiplying\nmantissas in input with integral powers of two created from the exponents\nin :attr:\u2019other\u2019. >>> torch.ldexp(torch.tensor([1.]), torch.tensor([1]))\ntensor([2.])\n>>> torch.ldexp(torch.tensor([1.0]), torch.tensor([1, 2, 3, 4]))\ntensor([ 2.,  4.,  8., 16.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp"
    },
    {
        "X": "How to use torch.promote_types, give an example?",
        "Y": ">>> torch.promote_types(torch.int32, torch.float32)\ntorch.float32\n>>> torch.promote_types(torch.uint8, torch.long)\ntorch.long",
        "Z": ">>> torch.promote_types(torch.int32, torch.float32)\ntorch.float32\n>>> torch.promote_types(torch.uint8, torch.long)\ntorch.long",
        "source": "https://pytorch.org/docs/stable/generated/torch.promote_types.html#torch.promote_types"
    },
    {
        "X": "How to use torch.nextafter, give an example?",
        "Y": ">>> eps = torch.finfo(torch.float32).eps\n>>> torch.nextafter(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 1.0])) == torch.tensor([eps + 1, 2 - eps])\ntensor([True, True])",
        "Z": "The shapes of input and other must be\nbroadcastable. >>> eps = torch.finfo(torch.float32).eps\n>>> torch.nextafter(torch.tensor([1.0, 2.0]), torch.tensor([2.0, 1.0])) == torch.tensor([eps + 1, 2 - eps])\ntensor([True, True])",
        "source": "https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter"
    },
    {
        "X": "How to use torch.set_default_tensor_type, give an example?",
        "Y": ">>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\ntorch.float32\n>>> torch.set_default_tensor_type(torch.DoubleTensor)\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64",
        "Z": "The default floating point tensor type is initially torch.FloatTensor. >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32\ntorch.float32\n>>> torch.set_default_tensor_type(torch.DoubleTensor)\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_default_tensor_type.html#torch.set_default_tensor_type"
    },
    {
        "X": "How to use torch.diagflat, give an example?",
        "Y": ">>> a = torch.randn(3)\n>>> a\ntensor([-0.2956, -0.9068,  0.1695])\n>>> torch.diagflat(a)\ntensor([[-0.2956,  0.0000,  0.0000],\n        [ 0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.1695]])\n>>> torch.diagflat(a, 1)\ntensor([[ 0.0000, -0.2956,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.1695],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n>>> a = torch.randn(2, 2)\n>>> a\ntensor([[ 0.2094, -0.3018],\n        [-0.1516,  1.9342]])\n>>> torch.diagflat(a)\ntensor([[ 0.2094,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.3018,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1516,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  1.9342]])",
        "Z": ">>> a = torch.randn(3)\n>>> a\ntensor([-0.2956, -0.9068,  0.1695])\n>>> torch.diagflat(a)\ntensor([[-0.2956,  0.0000,  0.0000],\n        [ 0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.1695]])\n>>> torch.diagflat(a, 1)\ntensor([[ 0.0000, -0.2956,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.9068,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.1695],\n        [ 0.0000,  0.0000,  0.0000,  0.0000]])\n\n>>> a = torch.randn(2, 2)\n>>> a\ntensor([[ 0.2094, -0.3018],\n        [-0.1516,  1.9342]])\n>>> torch.diagflat(a)\ntensor([[ 0.2094,  0.0000,  0.0000,  0.0000],\n        [ 0.0000, -0.3018,  0.0000,  0.0000],\n        [ 0.0000,  0.0000, -0.1516,  0.0000],\n        [ 0.0000,  0.0000,  0.0000,  1.9342]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat"
    },
    {
        "X": "How to use torch.lu_unpack, give an example?",
        "Y": ">>> A = torch.randn(2, 3, 3)\n>>> A_LU, pivots = A.lu()\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n>>>\n>>> # can recover A from factorization\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n\n>>> # LU factorization of a rectangular matrix:\n>>> A = torch.randn(2, 3, 2)\n>>> A_LU, pivots = A.lu()\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n>>> P\ntensor([[[1., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.]],\n\n        [[0., 0., 1.],\n         [0., 1., 0.],\n         [1., 0., 0.]]])\n>>> A_L\ntensor([[[ 1.0000,  0.0000],\n         [ 0.4763,  1.0000],\n         [ 0.3683,  0.1135]],\n\n        [[ 1.0000,  0.0000],\n         [ 0.2957,  1.0000],\n         [-0.9668, -0.3335]]])\n>>> A_U\ntensor([[[ 2.1962,  1.0881],\n         [ 0.0000, -0.8681]],\n\n        [[-1.0947,  0.3736],\n         [ 0.0000,  0.5718]]])\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n>>> torch.norm(A_ - A)\ntensor(2.9802e-08)",
        "Z": "Returns a tuple of tensors as (the P tensor (permutation matrix), the L tensor, the U tensor). >>> A = torch.randn(2, 3, 3)\n>>> A_LU, pivots = A.lu()\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n>>>\n>>> # can recover A from factorization\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n\n>>> # LU factorization of a rectangular matrix:\n>>> A = torch.randn(2, 3, 2)\n>>> A_LU, pivots = A.lu()\n>>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)\n>>> P\ntensor([[[1., 0., 0.],\n         [0., 1., 0.],\n         [0., 0., 1.]],\n\n        [[0., 0., 1.],\n         [0., 1., 0.],\n         [1., 0., 0.]]])\n>>> A_L\ntensor([[[ 1.0000,  0.0000],\n         [ 0.4763,  1.0000],\n         [ 0.3683,  0.1135]],\n\n        [[ 1.0000,  0.0000],\n         [ 0.2957,  1.0000],\n         [-0.9668, -0.3335]]])\n>>> A_U\ntensor([[[ 2.1962,  1.0881],\n         [ 0.0000, -0.8681]],\n\n        [[-1.0947,  0.3736],\n         [ 0.0000,  0.5718]]])\n>>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))\n>>> torch.norm(A_ - A)\ntensor(2.9802e-08)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_unpack.html#torch.lu_unpack"
    },
    {
        "X": "How to use torch.addmv, give an example?",
        "Y": ">>> M = torch.randn(2)\n>>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.addmv(M, mat, vec)\ntensor([-0.3768, -5.5565])",
        "Z": "For inputs of type FloatTensor or DoubleTensor, arguments beta and\nalpha must be real numbers, otherwise they should be integers >>> M = torch.randn(2)\n>>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.addmv(M, mat, vec)\ntensor([-0.3768, -5.5565])",
        "source": "https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv"
    },
    {
        "X": "How to use torch.nanmedian, give an example?",
        "Y": ">>> a = torch.tensor([1, float('nan'), 3, 2])\n>>> a.median()\ntensor(nan)\n>>> a.nanmedian()\ntensor(2.)",
        "Z": "This function is identical to torch.median() when there are no NaN values in input.\nWhen input has one or more NaN values, torch.median() will always return NaN,\nwhile this function will return the median of the non-NaN elements in input.\nIf all the elements in input are NaN it will also return NaN. >>> a = torch.tensor([1, float('nan'), 3, 2])\n>>> a.median()\ntensor(nan)\n>>> a.nanmedian()\ntensor(2.)",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian"
    },
    {
        "X": "How  This function is identical to torch.median() when there are no NaN values in a reduced row. When a reduced row has\none or more NaN values, torch.median() will always reduce it to NaN, while this function will reduce it to the\nmedian of the non-NaN elements. If all the elements in a reduced row are NaN then it will be reduced to NaN, too., give an example?",
        "Y": ">>> a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]])\n>>> a\ntensor([[2., 3., 1.],\n        [nan, 1., nan]])\n>>> a.median(0)\ntorch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))\n>>> a.nanmedian(0)\ntorch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))",
        "Z": "This function is identical to torch.median() when there are no NaN values in a reduced row. When a reduced row has\none or more NaN values, torch.median() will always reduce it to NaN, while this function will reduce it to the\nmedian of the non-NaN elements. If all the elements in a reduced row are NaN then it will be reduced to NaN, too. >>> a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]])\n>>> a\ntensor([[2., 3., 1.],\n        [nan, 1., nan]])\n>>> a.median(0)\ntorch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))\n>>> a.nanmedian(0)\ntorch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian"
    },
    {
        "X": "How to use torch.empty_strided, give an example?",
        "Y": ">>> a = torch.empty_strided((2, 3), (1, 2))\n>>> a\ntensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],\n        [0.0000e+00, 0.0000e+00, 3.0705e-41]])\n>>> a.stride()\n(1, 2)\n>>> a.size()\ntorch.Size([2, 3])",
        "Z": ">>> a = torch.empty_strided((2, 3), (1, 2))\n>>> a\ntensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],\n        [0.0000e+00, 0.0000e+00, 3.0705e-41]])\n>>> a.stride()\n(1, 2)\n>>> a.size()\ntorch.Size([2, 3])",
        "source": "https://pytorch.org/docs/stable/generated/torch.empty_strided.html#torch.empty_strided"
    },
    {
        "X": "How to use torch.isnan, give an example?",
        "Y": ">>> torch.isnan(torch.tensor([1, float('nan'), 2]))\ntensor([False, True, False])",
        "Z": ">>> torch.isnan(torch.tensor([1, float('nan'), 2]))\ntensor([False, True, False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan"
    },
    {
        "X": "How to use torch.atleast_3d, give an example?",
        "Y": ">>> x = torch.tensor(0.5)\n>>> x\ntensor(0.5000)\n>>> torch.atleast_3d(x)\ntensor([[[0.5000]]])\n>>> y = torch.randn(2,2)\n>>> y\ntensor([[-0.8079,  0.7460],\n        [-1.1647,  1.4734]])\n>>> torch.atleast_3d(y)\ntensor([[[-0.8079],\n        [ 0.7460]],\n\n        [[-1.1647],\n        [ 1.4734]]])\n>>> x = torch.randn(1,1,1)\n>>> x\ntensor([[[-1.5689]]])\n>>> torch.atleast_3d(x)\ntensor([[[-1.5689]]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_3d((x,y))\n(tensor([[[0.5000]]]), tensor([[[1.]]]))",
        "Z": ">>> x = torch.tensor(0.5)\n>>> x\ntensor(0.5000)\n>>> torch.atleast_3d(x)\ntensor([[[0.5000]]])\n>>> y = torch.randn(2,2)\n>>> y\ntensor([[-0.8079,  0.7460],\n        [-1.1647,  1.4734]])\n>>> torch.atleast_3d(y)\ntensor([[[-0.8079],\n        [ 0.7460]],\n\n        [[-1.1647],\n        [ 1.4734]]])\n>>> x = torch.randn(1,1,1)\n>>> x\ntensor([[[-1.5689]]])\n>>> torch.atleast_3d(x)\ntensor([[[-1.5689]]])\n>>> x = torch.tensor(0.5)\n>>> y = torch.tensor(1.)\n>>> torch.atleast_3d((x,y))\n(tensor([[[0.5000]]]), tensor([[[1.]]]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "X": "How to use torch.bitwise_xor, give an example?",
        "Y": ">>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-2, -2,  0], dtype=torch.int8)\n>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, False, False])",
        "Z": ">>> torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-2, -2,  0], dtype=torch.int8)\n>>> torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, False, False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor"
    },
    {
        "X": "How to use torch.sinh, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.5380, -0.8632, -0.1265,  0.9399])\n>>> torch.sinh(a)\ntensor([ 0.5644, -0.9744, -0.1268,  1.0845])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.5380, -0.8632, -0.1265,  0.9399])\n>>> torch.sinh(a)\ntensor([ 0.5644, -0.9744, -0.1268,  1.0845])",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "X": "How to use torch.roll, give an example?",
        "Y": ">>> x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)\n>>> x\ntensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])\n>>> torch.roll(x, 1, 0)\ntensor([[7, 8],\n        [1, 2],\n        [3, 4],\n        [5, 6]])\n>>> torch.roll(x, -1, 0)\ntensor([[3, 4],\n        [5, 6],\n        [7, 8],\n        [1, 2]])\n>>> torch.roll(x, shifts=(2, 1), dims=(0, 1))\ntensor([[6, 5],\n        [8, 7],\n        [2, 1],\n        [4, 3]])",
        "Z": ">>> x = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8]).view(4, 2)\n>>> x\ntensor([[1, 2],\n        [3, 4],\n        [5, 6],\n        [7, 8]])\n>>> torch.roll(x, 1, 0)\ntensor([[7, 8],\n        [1, 2],\n        [3, 4],\n        [5, 6]])\n>>> torch.roll(x, -1, 0)\ntensor([[3, 4],\n        [5, 6],\n        [7, 8],\n        [1, 2]])\n>>> torch.roll(x, shifts=(2, 1), dims=(0, 1))\ntensor([[6, 5],\n        [8, 7],\n        [2, 1],\n        [4, 3]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "X": "How to use torch.tile, give an example?",
        "Y": ">>> x = torch.tensor([1, 2, 3])\n>>> x.tile((2,))\ntensor([1, 2, 3, 1, 2, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.tile(y, (2, 2))\ntensor([[1, 2, 1, 2],\n        [3, 4, 3, 4],\n        [1, 2, 1, 2],\n        [3, 4, 3, 4]])",
        "Z": "Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). >>> x = torch.tensor([1, 2, 3])\n>>> x.tile((2,))\ntensor([1, 2, 3, 1, 2, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.tile(y, (2, 2))\ntensor([[1, 2, 1, 2],\n        [3, 4, 3, 4],\n        [1, 2, 1, 2],\n        [3, 4, 3, 4]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "How to use torch.topk, give an example?",
        "Y": ">>> x = torch.arange(1., 6.)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n>>> torch.topk(x, 3)\ntorch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))",
        "Z": "The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted >>> x = torch.arange(1., 6.)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n>>> torch.topk(x, 3)\ntorch.return_types.topk(values=tensor([5., 4., 3.]), indices=tensor([4, 3, 2]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "How to use torch.sqrt, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.sqrt(a)\ntensor([    nan,  1.0112,  0.2883,  0.6933])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.sqrt(a)\ntensor([    nan,  1.0112,  0.2883,  0.6933])",
        "source": "https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt"
    },
    {
        "X": "How to use torch.minimum, give an example?",
        "Y": ">>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.minimum(a, b)\ntensor([1, 0, -1])",
        "Z": ">>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.minimum(a, b)\ntensor([1, 0, -1])",
        "source": "https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum"
    },
    {
        "X": "How to use torch.floor_divide, give an example?",
        "Y": ">>> a = torch.tensor([4.0, 3.0])\n>>> b = torch.tensor([2.0, 2.0])\n>>> torch.floor_divide(a, b)\ntensor([2.0, 1.0])\n>>> torch.floor_divide(a, 1.4)\ntensor([2.0, 2.0])",
        "Z": "Supports broadcasting to a common shape, type promotion, and integer and float inputs. >>> a = torch.tensor([4.0, 3.0])\n>>> b = torch.tensor([2.0, 2.0])\n>>> torch.floor_divide(a, b)\ntensor([2.0, 1.0])\n>>> torch.floor_divide(a, 1.4)\ntensor([2.0, 2.0])",
        "source": "https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide"
    },
    {
        "X": "How to use torch.sgn, give an example?",
        "Y": ">>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j])\n>>> t.sgn()\ntensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])",
        "Z": ">>> t = torch.tensor([3+4j, 7-24j, 0, 1+2j])\n>>> t.sgn()\ntensor([0.6000+0.8000j, 0.2800-0.9600j, 0.0000+0.0000j, 0.4472+0.8944j])",
        "source": "https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn"
    },
    {
        "X": "How to use torch.ones_like, give an example?",
        "Y": ">>> input = torch.empty(2, 3)\n>>> torch.ones_like(input)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])",
        "Z": ">>> input = torch.empty(2, 3)\n>>> torch.ones_like(input)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones_like.html#torch.ones_like"
    },
    {
        "X": "How to use torch.no_grad, give an example?",
        "Y": ">>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n>>> z = doubler(x)\n>>> z.requires_grad\nFalse",
        "Z": "Also functions as a decorator. (Make sure to instantiate with parenthesis.) >>> x = torch.tensor([1], requires_grad=True)\n>>> with torch.no_grad():\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> @torch.no_grad()\n... def doubler(x):\n...     return x * 2\n>>> z = doubler(x)\n>>> z.requires_grad\nFalse",
        "source": "https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad"
    },
    {
        "X": "How to use torch.view_as_complex, give an example?",
        "Y": ">>> x=torch.randn(4, 2)\n>>> x\ntensor([[ 1.6116, -0.5772],\n        [-1.4606, -0.9120],\n        [ 0.0786, -1.7497],\n        [-0.6561, -1.6623]])\n>>> torch.view_as_complex(x)\ntensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])",
        "Z": ">>> x=torch.randn(4, 2)\n>>> x\ntensor([[ 1.6116, -0.5772],\n        [-1.4606, -0.9120],\n        [ 0.0786, -1.7497],\n        [-0.6561, -1.6623]])\n>>> torch.view_as_complex(x)\ntensor([(1.6116-0.5772j), (-1.4606-0.9120j), (0.0786-1.7497j), (-0.6561-1.6623j)])",
        "source": "https://pytorch.org/docs/stable/generated/torch.view_as_complex.html#torch.view_as_complex"
    },
    {
        "X": "How to use torch.swapaxes, give an example?",
        "Y": ">>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapaxes(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapaxes(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])",
        "Z": "This function is equivalent to NumPy\u2019s swapaxes function. >>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapaxes(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapaxes(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes"
    },
    {
        "X": "How to use torch.logical_xor, give an example?",
        "Y": ">>> torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([False, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_xor(a, b)\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a.double(), b.double())\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a.double(), b)\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True, False, False])",
        "Z": ">>> torch.logical_xor(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([False, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_xor(a, b)\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a.double(), b.double())\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a.double(), b)\ntensor([ True,  True, False, False])\n>>> torch.logical_xor(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True, False, False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor"
    },
    {
        "X": "How to use A floating point scalar operand has dtype torch.get_default_dtype() and an integral\nnon-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimum dtypes of an operand.  Quantized and complex types\nare not yet supported., give an example?",
        "Y": ">>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32",
        "Z": "A floating point scalar operand has dtype torch.get_default_dtype() and an integral\nnon-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimum dtypes of an operand.  Quantized and complex types\nare not yet supported. >>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How to use torch dtype, give an example?",
        "Y": "# allowed:\n>>> float_tensor *= float_tensor\n>>> float_tensor *= int_tensor\n>>> float_tensor *= uint_tensor\n>>> float_tensor *= bool_tensor\n>>> float_tensor *= double_tensor\n>>> int_tensor *= long_tensor\n>>> int_tensor *= uint_tensor\n>>> uint_tensor *= int_tensor\n\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\n>>> int_tensor *= float_tensor\n>>> bool_tensor *= int_tensor\n>>> bool_tensor *= uint_tensor\n>>> float_tensor *= complex_float_tensor",
        "Z": "# allowed:\n>>> float_tensor *= float_tensor\n>>> float_tensor *= int_tensor\n>>> float_tensor *= uint_tensor\n>>> float_tensor *= bool_tensor\n>>> float_tensor *= double_tensor\n>>> int_tensor *= long_tensor\n>>> int_tensor *= uint_tensor\n>>> uint_tensor *= int_tensor\n\n# disallowed (RuntimeError: result type can't be cast to the desired output type):\n>>> int_tensor *= float_tensor\n>>> bool_tensor *= int_tensor\n>>> bool_tensor *= uint_tensor\n>>> float_tensor *= complex_float_tensor",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How to use A torch.device can be constructed via a string or via a string and device ordinalVia a string:, give an example?",
        "Y": ">>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')",
        "Z": "A torch.device can be constructed via a string or via a string and device ordinalVia a string: >>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How to use Via a string:Via a string and device ordinal:, give an example?",
        "Y": ">>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)",
        "Z": "Via a string:Via a string and device ordinal: >>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How to use NoteThe torch.device argument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code., give an example?",
        "Y": ">>> # Example of a function that takes in a torch.device\n>>> cuda1 = torch.device('cuda:1')\n>>> torch.randn((2,3), device=cuda1)",
        "Z": "The torch.device argument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. >>> # Example of a function that takes in a torch.device\n>>> cuda1 = torch.device('cuda:1')\n>>> torch.randn((2,3), device=cuda1)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How  The torch.device argument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code., give an example?",
        "Y": ">>> # You can substitute the torch.device with a string\n>>> torch.randn((2,3), device='cuda:1')",
        "Z": "The torch.device argument in functions can generally be substituted with a string.\nThis allows for fast prototyping of code. >>> # You can substitute the torch.device with a string\n>>> torch.randn((2,3), device='cuda:1')",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc"
    },
    {
        "X": "How to use NoteFor legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors., give an example?",
        "Y": ">>> torch.device(1)\ndevice(type='cuda', index=1)",
        "Z": "For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. >>> torch.device(1)\ndevice(type='cuda', index=1)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How to use NoteMethods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent:, give an example?",
        "Y": ">>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy",
        "Z": "Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: >>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How to use torch.strided represents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associated\ntorch.Storage, which holds its data. These tensors provide\nmulti-dimensional, strided\nview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently., give an example?",
        "Y": ">>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)",
        "Z": "torch.strided represents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associated\ntorch.Storage, which holds its data. These tensors provide\nmulti-dimensional, strided\nview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype"
    },
    {
        "X": "How to use torch.median, give an example?",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 1.5219, -1.5212,  0.2202]])\n>>> torch.median(a)\ntensor(0.2202)",
        "Z": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 1.5219, -1.5212,  0.2202]])\n>>> torch.median(a)\ntensor(0.2202)",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "How  If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input., give an example?",
        "Y": ">>> a = torch.randn(4, 5)\n>>> a\ntensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],\n        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],\n        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],\n        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])\n>>> torch.median(a, 1)\ntorch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe outputs tensor having 1 fewer dimension than input. >>> a = torch.randn(4, 5)\n>>> a\ntensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],\n        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],\n        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],\n        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])\n>>> torch.median(a, 1)\ntorch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"
    },
    {
        "X": "How to use Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:, give an example?",
        "Y": "import torch\nimport torchvision\n\ndummy_input = torch.randn(10, 3, 224, 224, device='cuda')\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\n\n# Providing input and output names sets the display names for values\n# within the model's graph. Setting these does not change the semantics\n# of the graph; it is only for readability.\n#\n# The inputs to the network consist of the flat list of inputs (i.e.\n# the values you would pass to the forward() method) followed by the\n# flat list of parameters. You can partially specify names, i.e. provide\n# a list here shorter than the number of inputs to the model, and we will\n# only set that subset of names, starting from the beginning.\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\noutput_names = [ \"output1\" ]\n\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)",
        "Z": "Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx: import torch\nimport torchvision\n\ndummy_input = torch.randn(10, 3, 224, 224, device='cuda')\nmodel = torchvision.models.alexnet(pretrained=True).cuda()\n\n# Providing input and output names sets the display names for values\n# within the model's graph. Setting these does not change the semantics\n# of the graph; it is only for readability.\n#\n# The inputs to the network consist of the flat list of inputs (i.e.\n# the values you would pass to the forward() method) followed by the\n# flat list of parameters. You can partially specify names, i.e. provide\n# a list here shorter than the number of inputs to the model, and we will\n# only set that subset of names, starting from the beginning.\ninput_names = [ \"actual_input_1\" ] + [ \"learned_%d\" % i for i in range(16) ]\noutput_names = [ \"output1\" ]\n\ntorch.onnx.export(model, dummy_input, \"alexnet.onnx\", verbose=True, input_names=input_names, output_names=output_names)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Here is a simple script which exports a pretrained AlexNet as defined in\ntorchvision into ONNX.  It runs a single round of inference and then\nsaves the resulting traced model to alexnet.onnx:The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:, give an example?",
        "Y": "# These are the inputs and parameters to the network, which have taken on\n# the names we specified earlier.\ngraph(%actual_input_1 : Float(10, 3, 224, 224)\n      %learned_0 : Float(64, 3, 11, 11)\n      %learned_1 : Float(64)\n      %learned_2 : Float(192, 64, 5, 5)\n      %learned_3 : Float(192)\n      # ---- omitted for brevity ----\n      %learned_14 : Float(1000, 4096)\n      %learned_15 : Float(1000)) {\n  # Every statement consists of some output tensors (and their types),\n  # the operator to be run (with its attributes, e.g., kernels, strides,\n  # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)\n  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  # ---- omitted for brevity ----\n  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  # Dynamic means that the shape is not known. This may be because of a\n  # limitation of our implementation (which we would like to fix in a\n  # future release) or shapes which are truly dynamic.\n  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\n  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\n  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n  # ---- omitted for brevity ----\n  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\n  return (%output1);\n}",
        "Z": "The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network: # These are the inputs and parameters to the network, which have taken on\n# the names we specified earlier.\ngraph(%actual_input_1 : Float(10, 3, 224, 224)\n      %learned_0 : Float(64, 3, 11, 11)\n      %learned_1 : Float(64)\n      %learned_2 : Float(192, 64, 5, 5)\n      %learned_3 : Float(192)\n      # ---- omitted for brevity ----\n      %learned_14 : Float(1000, 4096)\n      %learned_15 : Float(1000)) {\n  # Every statement consists of some output tensors (and their types),\n  # the operator to be run (with its attributes, e.g., kernels, strides,\n  # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)\n  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]\n  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]\n  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]\n  # ---- omitted for brevity ----\n  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]\n  # Dynamic means that the shape is not known. This may be because of a\n  # limitation of our implementation (which we would like to fix in a\n  # future release) or shapes which are truly dynamic.\n  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet\n  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet\n  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet\n  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet\n  # ---- omitted for brevity ----\n  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]\n  return (%output1);\n}",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda:, give an example?",
        "Y": "conda install -c conda-forge onnx",
        "Z": "The resulting alexnet.onnx is a binary protobuf file which contains both\nthe network structure and parameters of the model you exported\n(in this case, AlexNet).  The keyword argument verbose=True causes the\nexporter to print out a human-readable representation of the network:You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda: conda install -c conda-forge onnx",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda:Then, you can run:, give an example?",
        "Y": "import onnx\n\n# Load the ONNX model\nmodel = onnx.load(\"alexnet.onnx\")\n\n# Check that the IR is well formed\nonnx.checker.check_model(model)\n\n# Print a human readable representation of the graph\nonnx.helper.printable_graph(model.graph)",
        "Z": "You can also verify the protobuf using the ONNX library.\nYou can install ONNX with conda:Then, you can run: import onnx\n\n# Load the ONNX model\nmodel = onnx.load(\"alexnet.onnx\")\n\n# Check that the IR is well formed\nonnx.checker.check_model(model)\n\n# Print a human readable representation of the graph\nonnx.helper.printable_graph(model.graph)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions.Once these are installed, you can use the backend for Caffe2:, give an example?",
        "Y": "# ...continuing from above\nimport caffe2.python.onnx.backend as backend\nimport numpy as np\n\nrep = backend.prepare(model, device=\"CUDA:0\") # or \"CPU\"\n# For the Caffe2 backend:\n#     rep.predict_net is the Caffe2 protobuf for the network\n#     rep.workspace is the Caffe2 workspace for the network\n#       (see the class caffe2.python.onnx.backend.Workspace)\noutputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\n# To run networks with more than one input, pass a tuple\n# rather than a single numpy ndarray.\nprint(outputs[0])",
        "Z": "To run the exported script with caffe2, you will need to install caffe2: If you don\u2019t have one already, Please follow the install instructions.Once these are installed, you can use the backend for Caffe2: # ...continuing from above\nimport caffe2.python.onnx.backend as backend\nimport numpy as np\n\nrep = backend.prepare(model, device=\"CUDA:0\") # or \"CPU\"\n# For the Caffe2 backend:\n#     rep.predict_net is the Caffe2 protobuf for the network\n#     rep.workspace is the Caffe2 workspace for the network\n#       (see the class caffe2.python.onnx.backend.Workspace)\noutputs = rep.run(np.random.randn(10, 3, 224, 224).astype(np.float32))\n# To run networks with more than one input, pass a tuple\n# rather than a single numpy ndarray.\nprint(outputs[0])",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions.Once these are installed, you can use the backend for ONNX Runtime:, give an example?",
        "Y": "# ...continuing from above\nimport onnxruntime as ort\n\nort_session = ort.InferenceSession('alexnet.onnx')\n\noutputs = ort_session.run(None, {'actual_input_1': np.random.randn(10, 3, 224, 224).astype(np.float32)})\n\nprint(outputs[0])",
        "Z": "You can also run the exported model with ONNX Runtime,\nyou will need to install ONNX Runtime: please follow these instructions.Once these are installed, you can use the backend for ONNX Runtime: # ...continuing from above\nimport onnxruntime as ort\n\nort_session = ort.InferenceSession('alexnet.onnx')\n\noutputs = ort_session.run(None, {'actual_input_1': np.random.randn(10, 3, 224, 224).astype(np.float32)})\n\nprint(outputs[0])",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use The ONNX exporter can be both trace-based and script-based exporter.We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example:, give an example?",
        "Y": "import torch\n\n# Trace-based only\n\nclass LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        for i in range(y):\n            x = x + i\n        return x\n\nmodel = LoopModel()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\n\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True)",
        "Z": "We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example: import torch\n\n# Trace-based only\n\nclass LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        for i in range(y):\n            x = x + i\n        return x\n\nmodel = LoopModel()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\n\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example:With trace-based exporter, we get the result ONNX graph which unrolls the for loop:, give an example?",
        "Y": "graph(%0 : Long(2, 3),\n      %1 : Long()):\n  %2 : Tensor = onnx::Constant[value={1}]()\n  %3 : Tensor = onnx::Add(%0, %2)\n  %4 : Tensor = onnx::Constant[value={2}]()\n  %5 : Tensor = onnx::Add(%3, %4)\n  %6 : Tensor = onnx::Constant[value={3}]()\n  %7 : Tensor = onnx::Add(%5, %6)\n  %8 : Tensor = onnx::Constant[value={4}]()\n  %9 : Tensor = onnx::Add(%7, %8)\n  return (%9)",
        "Z": "We allow mixing tracing and scripting. You can compose tracing and scripting to suit the particular requirements\nof a part of a model.  Checkout this example:With trace-based exporter, we get the result ONNX graph which unrolls the for loop: graph(%0 : Long(2, 3),\n      %1 : Long()):\n  %2 : Tensor = onnx::Constant[value={1}]()\n  %3 : Tensor = onnx::Add(%0, %2)\n  %4 : Tensor = onnx::Constant[value={2}]()\n  %5 : Tensor = onnx::Add(%3, %4)\n  %6 : Tensor = onnx::Constant[value={3}]()\n  %7 : Tensor = onnx::Add(%5, %6)\n  %8 : Tensor = onnx::Constant[value={4}]()\n  %9 : Tensor = onnx::Add(%7, %8)\n  return (%9)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use With trace-based exporter, we get the result ONNX graph which unrolls the for loop:To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:, give an example?",
        "Y": "# Mixing tracing and scripting\n\n@torch.jit.script\ndef loop(x, y):\n    for i in range(int(y)):\n        x = x + i\n    return x\n\nclass LoopModel2(torch.nn.Module):\n    def forward(self, x, y):\n        return loop(x, y)\n\nmodel = LoopModel2()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True,\n                  input_names=['input_data', 'loop_range'])",
        "Z": "With trace-based exporter, we get the result ONNX graph which unrolls the for loop:To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module: # Mixing tracing and scripting\n\n@torch.jit.script\ndef loop(x, y):\n    for i in range(int(y)):\n        x = x + i\n    return x\n\nclass LoopModel2(torch.nn.Module):\n    def forward(self, x, y):\n        return loop(x, y)\n\nmodel = LoopModel2()\ndummy_input = torch.ones(2, 3, dtype=torch.long)\nloop_count = torch.tensor(5, dtype=torch.long)\ntorch.onnx.export(model, (dummy_input, loop_count), 'loop.onnx', verbose=True,\n                  input_names=['input_data', 'loop_range'])",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:Now the exported ONNX graph becomes:, give an example?",
        "Y": "graph(%input_data : Long(2, 3),\n      %loop_range : Long()):\n  %2 : Long() = onnx::Constant[value={1}](), scope: LoopModel2/loop\n  %3 : Tensor = onnx::Cast[to=9](%2)\n  %4 : Long(2, 3) = onnx::Loop(%loop_range, %3, %input_data), scope: LoopModel2/loop\n    block0(%i.1 : Long(), %cond : bool, %x.6 : Long(2, 3)):\n      %8 : Long(2, 3) = onnx::Add(%x.6, %i.1), scope: LoopModel2/loop\n      %9 : Tensor = onnx::Cast[to=9](%2)\n      -> (%9, %8)\n  return (%4)",
        "Z": "To utilize script-based exporter for capturing the dynamic loop,\nwe can write the loop in script, and call it from the regular nn.Module:Now the exported ONNX graph becomes: graph(%input_data : Long(2, 3),\n      %loop_range : Long()):\n  %2 : Long() = onnx::Constant[value={1}](), scope: LoopModel2/loop\n  %3 : Tensor = onnx::Cast[to=9](%2)\n  %4 : Long(2, 3) = onnx::Loop(%loop_range, %3, %input_data), scope: LoopModel2/loop\n    block0(%i.1 : Long(), %cond : bool, %x.6 : Long(2, 3)):\n      %8 : Long(2, 3) = onnx::Add(%x.6, %i.1), scope: LoopModel2/loop\n      %9 : Tensor = onnx::Cast[to=9](%2)\n      -> (%9, %8)\n  return (%4)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Now the exported ONNX graph becomes:The dynamic control flow is captured correctly. We can verify in backends with different loop range., give an example?",
        "Y": "import caffe2.python.onnx.backend as backend\nimport numpy as np\nimport onnx\nmodel = onnx.load('loop.onnx')\n\nrep = backend.prepare(model)\noutputs = rep.run((dummy_input.numpy(), np.array(9).astype(np.int64)))\nprint(outputs[0])\n#[[37 37 37]\n# [37 37 37]]\n\n\nimport onnxruntime as ort\nort_sess = ort.InferenceSession('loop.onnx')\noutputs = ort_sess.run(None, {'input_data': dummy_input.numpy(),\n                              'loop_range': np.array(9).astype(np.int64)})\nprint(outputs)\n#[array([[37, 37, 37],\n#       [37, 37, 37]], dtype=int64)]",
        "Z": "Now the exported ONNX graph becomes:The dynamic control flow is captured correctly. We can verify in backends with different loop range. import caffe2.python.onnx.backend as backend\nimport numpy as np\nimport onnx\nmodel = onnx.load('loop.onnx')\n\nrep = backend.prepare(model)\noutputs = rep.run((dummy_input.numpy(), np.array(9).astype(np.int64)))\nprint(outputs[0])\n#[[37 37 37]\n# [37 37 37]]\n\n\nimport onnxruntime as ort\nort_sess = ort.InferenceSession('loop.onnx')\noutputs = ort_sess.run(None, {'input_data': dummy_input.numpy(),\n                              'loop_range': np.array(9).astype(np.int64)})\nprint(outputs)\n#[array([[37, 37, 37],\n#       [37, 37, 37]], dtype=int64)]",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use The dynamic control flow is captured correctly. We can verify in backends with different loop range.To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please\navoid use of torch.Tensor.item(). Torch supports implicit cast of single-element tensors to numbers.\nE.g.:, give an example?",
        "Y": "class LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        res = []\n        arr = x.split(2, 0)\n        for i in range(int(y)):\n            res += [arr[i].sum(0, False)]\n        return torch.stack(res)\n\nmodel = torch.jit.script(LoopModel())\ninputs = (torch.randn(16), torch.tensor(8))\n\nout = model(*inputs)\ntorch.onnx.export(model, inputs, 'loop_and_list.onnx', opset_version=11, example_outputs=out)",
        "Z": "The dynamic control flow is captured correctly. We can verify in backends with different loop range.To avoid exporting a variable scalar tensor as a fixed value constant as part of the ONNX model, please\navoid use of torch.Tensor.item(). Torch supports implicit cast of single-element tensors to numbers.\nE.g.: class LoopModel(torch.nn.Module):\n    def forward(self, x, y):\n        res = []\n        arr = x.split(2, 0)\n        for i in range(int(y)):\n            res += [arr[i].sum(0, False)]\n        return torch.stack(res)\n\nmodel = torch.jit.script(LoopModel())\ninputs = (torch.randn(16), torch.tensor(8))\n\nout = model(*inputs)\ntorch.onnx.export(model, inputs, 'loop_and_list.onnx', opset_version=11, example_outputs=out)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use TorchScript only supports a subset of Python types. You can find more details about type annotation\nhere.Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations., give an example?",
        "Y": "import torch\n\nclass Module(torch.nn.Module):\n    def forward(self, x, tup):\n        # type: (int, Tuple[Tensor, Tensor]) -> Tensor\n        t0, t1 = tup\n        return t0 + t1 + x",
        "Z": "Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations. import torch\n\nclass Module(torch.nn.Module):\n    def forward(self, x, tup):\n        # type: (int, Tuple[Tensor, Tensor]) -> Tensor\n        t0, t1 = tup\n        return t0 + t1 + x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations.If the type annotation is not specified, TorchScript compiler fails with the runtime error below., give an example?",
        "Y": "RuntimeError:\nTensor (inferred) cannot be used as a tuple:\n  File <filename>\n        def forward(self, x, tup):\n            t0, t1 = tup\n                     ~~~ <--- HERE\n            return t0 + t1 + x",
        "Z": "Due to optimization purposes, TorchScript only supports variables with single static types for script functions.\nBy default, each variable is assumed to be Tensor. If an argument to a ScriptModule function is not Tensor,\nits type should be specified using MyPy-style annotations.If the type annotation is not specified, TorchScript compiler fails with the runtime error below. RuntimeError:\nTensor (inferred) cannot be used as a tuple:\n  File <filename>\n        def forward(self, x, tup):\n            t0, t1 = tup\n                     ~~~ <--- HERE\n            return t0 + t1 + x",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors:, give an example?",
        "Y": "np.concatenate((x, y, z), axis=1)",
        "Z": "PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors: np.concatenate((x, y, z), axis=1)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use PyTorch models can be written using numpy manipulations, but this is not proper when we convert to the ONNX model.\nFor the trace-based exporter, tracing treats the numpy values as the constant node,\ntherefore it calculates the wrong result if we change the input.\nSo the PyTorch model need implement using torch operators.\nFor example, do not use numpy operators on numpy tensors:do not convert to numpy types:, give an example?",
        "Y": "y = x.astype(np.int)",
        "Z": "do not convert to numpy types: y = x.astype(np.int)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use do not convert to numpy types:Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e.,, give an example?",
        "Y": "class MyModule(nn.Module):\n    def __init__(self):\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.dropout(x)",
        "Z": "do not convert to numpy types:Always use torch tensors and torch operators: torch.concat, etc.\nIn addition, Dropout layer need defined in init function so that inferencing can handle it properly, i.e., class MyModule(nn.Module):\n    def __init__(self):\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, x):\n        x = self.dropout(x)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use There are two ways to handle models which consist of named parameters or keyword arguments as inputs:For example, in the model:, give an example?",
        "Y": "class Model(torch.nn.Module):\n  def forward(self, x, y=None, z=None):\n    if y is not None:\n      return x + y\n    if z is not None:\n      return x + z\n    return x\nm = Model()\nx = torch.randn(2, 3)\nz = torch.randn(2, 3)",
        "Z": "For example, in the model: class Model(torch.nn.Module):\n  def forward(self, x, y=None, z=None):\n    if y is not None:\n      return x + y\n    if z is not None:\n      return x + z\n    return x\nm = Model()\nx = torch.randn(2, 3)\nz = torch.randn(2, 3)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model, give an example?",
        "Y": "torch.onnx.export(model, (x, None, z), \u2018test.onnx\u2019)",
        "Z": "Not using a dictionary for the keyword arguments and passing all the inputs in the same order\nas required by the model torch.onnx.export(model, (x, None, z), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple., give an example?",
        "Y": "torch.onnx.export(model, (x, {'y': None, 'z': z}), \u2018test.onnx\u2019)",
        "Z": "Using a dictionary to represent the keyword arguments. This dictionary is always passed in\naddition to the non-keyword arguments and is always the last argument in the args tuple. torch.onnx.export(model, (x, {'y': None, 'z': z}), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use There are two ways of exporting the model:For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example,, give an example?",
        "Y": "torch.onnx.export(model, (x, {}), \u2018test.onnx\u2019)\nor\ntorch.onnx.export(model, (x, ), \u2018test.onnx\u2019)",
        "Z": "There are two ways of exporting the model:For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example, torch.onnx.export(model, (x, {}), \u2018test.onnx\u2019)\nor\ntorch.onnx.export(model, (x, ), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example,An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,, give an example?",
        "Y": "class Model(torch.nn.Module):\n  def forward(self, k, x):\n    ...\n    return x\nm = Model()\nk =\u202ftorch.randn(2, 3)\nx = {torch.tensor(1.):\u202ftorch.randn(2, 3)}",
        "Z": "For cases in which there are no keyword arguments, models can be exported with either an\nempty or no dictionary. For example,An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example, class Model(torch.nn.Module):\n  def forward(self, k, x):\n    ...\n    return x\nm = Model()\nk =\u202ftorch.randn(2, 3)\nx = {torch.tensor(1.):\u202ftorch.randn(2, 3)}",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this., give an example?",
        "Y": "torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)",
        "Z": "An exception to this rule are cases in which the last input is also of a dictionary type.\nIn these cases it is mandatory to have an empty dictionary as the last argument in the\nargs tuple. For example,Without the presence of the empty dictionary, the export call assumes that the\n\u2018x\u2019 input is intended to represent the optional dictionary consisting of named arguments.\nIn order to prevent this from being an issue a constraint is placed to provide an empty\ndictionary as the last input in the tuple args in such cases.\nThe new call would look like this. torch.onnx.export(model, (k, x, {}), \u2018test.onnx\u2019)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:, give an example?",
        "Y": "data = torch.randn(3, 4)\nindex = torch.tensor([1, 2])\n\n# RHS indexing is supported in ONNX opset >= 11.\nclass RHSIndexing(torch.nn.Module):\n    def forward(self, data, index):\n        return data[index]\n\nout = RHSIndexing()(data, index)\n\ntorch.onnx.export(RHSIndexing(), (data, index), 'indexing.onnx', opset_version=9)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('indexing.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: data.numpy(),\n    sess.get_inputs()[1].name: index.numpy(),\n})\n\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))",
        "Z": "This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.: data = torch.randn(3, 4)\nindex = torch.tensor([1, 2])\n\n# RHS indexing is supported in ONNX opset >= 11.\nclass RHSIndexing(torch.nn.Module):\n    def forward(self, data, index):\n        return data[index]\n\nout = RHSIndexing()(data, index)\n\ntorch.onnx.export(RHSIndexing(), (data, index), 'indexing.onnx', opset_version=9)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('indexing.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: data.numpy(),\n    sess.get_inputs()[1].name: index.numpy(),\n})\n\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use This type of indexing occurs on the RHS. Export is supported for ONNX opset version >= 9. E.g.:Below is the list of supported patterns for RHS indexing., give an example?",
        "Y": "# Scalar indices\ndata[0, 1]\n\n# Slice indices\ndata[:3]\n\n# Tensor indices\ndata[torch.tensor([[1, 2], [2, 3]])]\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])]\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])]\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])]\n\n# Ellipsis followed by tensor indexing\n# Not supported in scripting\n# i.e. torch.jit.script(model) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[..., torch.tensor([2, 1])]\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4, torch.tensor([[1], [2]])]\n\n# Boolean mask (supported for ONNX opset version >= 11)\ndata[data != 1]",
        "Z": "Below is the list of supported patterns for RHS indexing. # Scalar indices\ndata[0, 1]\n\n# Slice indices\ndata[:3]\n\n# Tensor indices\ndata[torch.tensor([[1, 2], [2, 3]])]\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])]\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])]\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])]\n\n# Ellipsis followed by tensor indexing\n# Not supported in scripting\n# i.e. torch.jit.script(model) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[..., torch.tensor([2, 1])]\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4, torch.tensor([[1], [2]])]\n\n# Boolean mask (supported for ONNX opset version >= 11)\ndata[data != 1]",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Below is the list of supported patterns for RHS indexing.And below is the list of unsupported patterns for RHS indexing., give an example?",
        "Y": "# Tensor indices that includes negative values.\ndata[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]",
        "Z": "Below is the list of supported patterns for RHS indexing.And below is the list of unsupported patterns for RHS indexing. # Tensor indices that includes negative values.\ndata[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.:, give an example?",
        "Y": "data = torch.zeros(3, 4)\nnew_data = torch.arange(4).to(torch.float32)\n\n# LHS indexing is supported in ONNX opset >= 11.\nclass LHSIndexing(torch.nn.Module):\n    def forward(self, data, new_data):\n        data[1] = new_data\n        return data\n\nout = LHSIndexing()(data, new_data)\n\ndata = torch.zeros(3, 4)\nnew_data = torch.arange(4).to(torch.float32)\ntorch.onnx.export(LHSIndexing(), (data, new_data), 'inplace_assign.onnx', opset_version=11)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('inplace_assign.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: torch.zeros(3, 4).numpy(),\n    sess.get_inputs()[1].name: new_data.numpy(),\n})\n\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))",
        "Z": "In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.: data = torch.zeros(3, 4)\nnew_data = torch.arange(4).to(torch.float32)\n\n# LHS indexing is supported in ONNX opset >= 11.\nclass LHSIndexing(torch.nn.Module):\n    def forward(self, data, new_data):\n        data[1] = new_data\n        return data\n\nout = LHSIndexing()(data, new_data)\n\ndata = torch.zeros(3, 4)\nnew_data = torch.arange(4).to(torch.float32)\ntorch.onnx.export(LHSIndexing(), (data, new_data), 'inplace_assign.onnx', opset_version=11)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('inplace_assign.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: torch.zeros(3, 4).numpy(),\n    sess.get_inputs()[1].name: new_data.numpy(),\n})\n\nassert torch.all(torch.eq(out, torch.tensor(out_ort)))",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use In code, this type of indexing occurs on the LHS.\nExport is supported for ONNX opset version >= 11. E.g.:Below is the list of supported patterns for LHS indexing., give an example?",
        "Y": "# Scalar indices\ndata[0, 1] = new_data\n\n# Slice indices\ndata[:3] = new_data\n\n# Tensor indices\n# If more than one tensor are used as indices, only consecutive 1-d tensor indices are supported.\ndata[torch.tensor([[1, 2], [2, 3]])] = new_data\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])] = new_data\n\n# Ellipsis followed by tensor indexing\n# Not supported to export in script modules\n# i.e. torch.onnx.export(torch.jit.script(model)) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[..., torch.tensor([2, 1])] = new_data\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4] += update\n\n# Boolean mask\ndata[data != 1] = new_data",
        "Z": "Below is the list of supported patterns for LHS indexing. # Scalar indices\ndata[0, 1] = new_data\n\n# Slice indices\ndata[:3] = new_data\n\n# Tensor indices\n# If more than one tensor are used as indices, only consecutive 1-d tensor indices are supported.\ndata[torch.tensor([[1, 2], [2, 3]])] = new_data\ndata[torch.tensor([2, 3]), torch.tensor([1, 2])] = new_data\n\n# Ellipsis followed by tensor indexing\n# Not supported to export in script modules\n# i.e. torch.onnx.export(torch.jit.script(model)) will fail if model contains this pattern.\n# Export is supported under tracing\n# i.e. torch.onnx.export(model)\ndata[..., torch.tensor([2, 1])] = new_data\n\n# The combination of above\ndata[2, ..., torch.tensor([2, 1, 3]), 2:4] += update\n\n# Boolean mask\ndata[data != 1] = new_data",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Below is the list of supported patterns for LHS indexing.And below is the list of unsupported patterns for LHS indexing., give an example?",
        "Y": "# Multiple tensor indices if any has rank >= 2\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data\n\n# Multiple tensor indices that are not consecutive\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data\n\n# Tensor indices that includes negative values.\ndata[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data",
        "Z": "Below is the list of supported patterns for LHS indexing.And below is the list of unsupported patterns for LHS indexing. # Multiple tensor indices if any has rank >= 2\ndata[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data\n\n# Multiple tensor indices that are not consecutive\ndata[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data\n\n# Tensor indices that includes negative values.\ndata[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use If the operator is a non-ATen operator, the symbolic function has to be\nadded in the corresponding PyTorch Function class. Please read the following\ninstructions:Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this:, give an example?",
        "Y": "def operator/symbolic(g, *inputs):\n  \"\"\"\n  Modifies Graph (e.g., using \"op\"), adding the ONNX operations representing\n  this PyTorch function, and returning a Value or tuple of Values specifying the\n  ONNX outputs whose values correspond to the original PyTorch return values\n  of the autograd Function (or None if an output is not supported by ONNX).\n\n  Args:\n    g (Graph): graph to write the ONNX representation into\n    inputs (Value...): list of values representing the variables which contain\n        the inputs for this function\n  \"\"\"\n\nclass Value(object):\n  \"\"\"Represents an intermediate tensor value computed in ONNX.\"\"\"\n  def type(self):\n    \"\"\"Returns the Type of the value.\"\"\"\n\nclass Type(object):\n  def sizes(self):\n    \"\"\"Returns a tuple of ints representing the shape of a tensor this describes.\"\"\"\n\nclass Graph(object):\n  def op(self, opname, *inputs, **attrs):\n    \"\"\"\n    Create an ONNX operator 'opname', taking 'args' as inputs\n    and attributes 'kwargs' and add it as a node to the current graph,\n    returning the value representing the single output of this\n    operator (see the `outputs` keyword argument for multi-return\n    nodes).\n\n    The set of operators and the inputs/attributes they take\n    is documented at https://github.com/onnx/onnx/blob/master/docs/Operators.md\n\n    Args:\n        opname (string): The ONNX operator name, e.g., `Abs` or `Add`.\n        args (Value...): The inputs to the operator; usually provided\n            as arguments to the `symbolic` definition.\n        kwargs: The attributes of the ONNX operator, with keys named\n            according to the following convention: `alpha_f` indicates\n            the `alpha` attribute with type `f`.  The valid type specifiers are\n            `f` (float), `i` (int), `s` (string) or `t` (Tensor).  An attribute\n            specified with type float accepts either a single float, or a\n            list of floats (e.g., you would say `dims_i` for a `dims` attribute\n            that takes a list of integers).\n        outputs (int, optional):  The number of outputs this operator returns;\n            by default an operator is assumed to return a single output.\n            If `outputs` is greater than one, this functions returns a tuple\n            of output `Value`, representing each output of the ONNX operator\n            in positional.\n    \"\"\"",
        "Z": "Symbolic functions should be implemented in Python. All of these functions interact\nwith Python methods which are implemented via C++-Python bindings,\nbut intuitively the interface they provide looks like this: def operator/symbolic(g, *inputs):\n  \"\"\"\n  Modifies Graph (e.g., using \"op\"), adding the ONNX operations representing\n  this PyTorch function, and returning a Value or tuple of Values specifying the\n  ONNX outputs whose values correspond to the original PyTorch return values\n  of the autograd Function (or None if an output is not supported by ONNX).\n\n  Args:\n    g (Graph): graph to write the ONNX representation into\n    inputs (Value...): list of values representing the variables which contain\n        the inputs for this function\n  \"\"\"\n\nclass Value(object):\n  \"\"\"Represents an intermediate tensor value computed in ONNX.\"\"\"\n  def type(self):\n    \"\"\"Returns the Type of the value.\"\"\"\n\nclass Type(object):\n  def sizes(self):\n    \"\"\"Returns a tuple of ints representing the shape of a tensor this describes.\"\"\"\n\nclass Graph(object):\n  def op(self, opname, *inputs, **attrs):\n    \"\"\"\n    Create an ONNX operator 'opname', taking 'args' as inputs\n    and attributes 'kwargs' and add it as a node to the current graph,\n    returning the value representing the single output of this\n    operator (see the `outputs` keyword argument for multi-return\n    nodes).\n\n    The set of operators and the inputs/attributes they take\n    is documented at https://github.com/onnx/onnx/blob/master/docs/Operators.md\n\n    Args:\n        opname (string): The ONNX operator name, e.g., `Abs` or `Add`.\n        args (Value...): The inputs to the operator; usually provided\n            as arguments to the `symbolic` definition.\n        kwargs: The attributes of the ONNX operator, with keys named\n            according to the following convention: `alpha_f` indicates\n            the `alpha` attribute with type `f`.  The valid type specifiers are\n            `f` (float), `i` (int), `s` (string) or `t` (Tensor).  An attribute\n            specified with type float accepts either a single float, or a\n            list of floats (e.g., you would say `dims_i` for a `dims` attribute\n            that takes a list of integers).\n        outputs (int, optional):  The number of outputs this operator returns;\n            by default an operator is assumed to return a single output.\n            If `outputs` is greater than one, this functions returns a tuple\n            of output `Value`, representing each output of the ONNX operator\n            in positional.\n    \"\"\"",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h.Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:, give an example?",
        "Y": "UserWarning: ONNX export failed on elu because torch.onnx.symbolic_opset9.elu does not exist\nRuntimeError: ONNX export failed: Couldn't export operator elu",
        "Z": "The ONNX graph C++ definition is in torch/csrc/jit/ir/ir.h.Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below: UserWarning: ONNX export failed on elu because torch.onnx.symbolic_opset9.elu does not exist\nRuntimeError: ONNX export failed: Couldn't export operator elu",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py:, give an example?",
        "Y": "def elu(g, input, alpha, inplace=False):\n    return g.op(\"Elu\", input, alpha_f=_scalar(alpha))",
        "Z": "Here is an example of handling missing symbolic function for elu operator.\nWe try to export the model and see the error message as below:The export fails because PyTorch does not support exporting elu operator.\nWe find virtual Tensor elu(const Tensor & input, Scalar alpha, bool inplace) const override;\nin VariableType.h. This means elu is an ATen operator.\nWe check the ONNX operator list,\nand confirm that Elu is standardized in ONNX.\nWe add the following lines to symbolic_opset9.py: def elu(g, input, alpha, inplace=False):\n    return g.op(\"Elu\", input, alpha_f=_scalar(alpha))",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.:, give an example?",
        "Y": "# Create custom symbolic function\nfrom torch.onnx.symbolic_helper import parse_args\n@parse_args('v', 'v', 'f', 'i')\ndef symbolic_foo_forward(g, input1, input2, attr1, attr2):\n    return g.op(\"Foo\", input1, input2, attr1_f=attr1, attr2_i=attr2)\n\n# Register custom symbolic function\nfrom torch.onnx import register_custom_op_symbolic\nregister_custom_op_symbolic('custom_ops::foo_forward', symbolic_foo_forward, 9)\n\nclass FooModel(torch.nn.Module):\n    def __init__(self, attr1, attr2):\n        super(FooModule, self).__init__()\n        self.attr1 = attr1\n        self.attr2 = attr2\n\n    def forward(self, input1, input2):\n        # Calling custom op\n        return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)\n\nmodel = FooModel(attr1, attr2)\ntorch.onnx.export(model, (dummy_input1, dummy_input2), 'model.onnx', custom_opsets={\"custom_domain\": 2})",
        "Z": "Following this tutorial Extending TorchScript with Custom C++ Operators,\nyou can create and register your own custom ops implementation in PyTorch. Here\u2019s how to export such model to ONNX.: # Create custom symbolic function\nfrom torch.onnx.symbolic_helper import parse_args\n@parse_args('v', 'v', 'f', 'i')\ndef symbolic_foo_forward(g, input1, input2, attr1, attr2):\n    return g.op(\"Foo\", input1, input2, attr1_f=attr1, attr2_i=attr2)\n\n# Register custom symbolic function\nfrom torch.onnx import register_custom_op_symbolic\nregister_custom_op_symbolic('custom_ops::foo_forward', symbolic_foo_forward, 9)\n\nclass FooModel(torch.nn.Module):\n    def __init__(self, attr1, attr2):\n        super(FooModule, self).__init__()\n        self.attr1 = attr1\n        self.attr2 = attr2\n\n    def forward(self, input1, input2):\n        # Calling custom op\n        return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)\n\nmodel = FooModel(attr1, attr2)\ntorch.onnx.export(model, (dummy_input1, dummy_input2), 'model.onnx', custom_opsets={\"custom_domain\": 2})",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode., give an example?",
        "Y": "Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:exp(%0)\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:div(%0, %3)\n    return (%4)\n\nIs exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Exp(%0)\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Div(%0, %1)\n    return (%2)",
        "Z": "This mode is used to export all operators as regular ONNX operators. This is the default operator_export_type mode. Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:exp(%0)\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten:div(%0, %3)\n    return (%4)\n\nIs exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Exp(%0)\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx:Div(%0, %1)\n    return (%2)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use This mode is used to export all operators as ATen ops, and avoid conversion to ONNX., give an example?",
        "Y": "Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::exp(%0)\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%0, %3)\n    return (%4)\n\nIs exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"exp\"](%0)\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"div\"](%0, %1)\n    return (%2)",
        "Z": "This mode is used to export all operators as ATen ops, and avoid conversion to ONNX. Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::exp(%0)\n    %4 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%0, %3)\n    return (%4)\n\nIs exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %1 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"exp\"](%0)\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::ATen[operator=\"div\"](%0, %1)\n    return (%2)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator., give an example?",
        "Y": "Example torch ir graph:\n\n  graph(%0 : Float):\n    %3 : int = prim::Constant[value=0]()\n    %4 : Float = aten::triu(%0, %3) # unsupported op\n    %5 : Float = aten::mul(%4, %0) # registered op\n    return (%5)\n\nis exported as:\n\n  graph(%0 : Float):\n    %1 : Long() = onnx::Constant[value={0}]()\n    %2 : Float = aten::ATen[operator=\"triu\"](%0, %1) # unsupported op\n    %3 : Float = onnx::Mul(%2, %0) # registered op\n    return (%3)",
        "Z": "To fallback on unsupported ATen operators in ONNX. Supported operators are exported to ONNX regularly.\nIn the following example, aten::triu is not supported in ONNX. Exporter falls back on this operator. Example torch ir graph:\n\n  graph(%0 : Float):\n    %3 : int = prim::Constant[value=0]()\n    %4 : Float = aten::triu(%0, %3) # unsupported op\n    %5 : Float = aten::mul(%4, %0) # registered op\n    return (%5)\n\nis exported as:\n\n  graph(%0 : Float):\n    %1 : Long() = onnx::Constant[value={0}]()\n    %2 : Float = aten::ATen[operator=\"triu\"](%0, %1) # unsupported op\n    %3 : Float = onnx::Mul(%2, %0) # registered op\n    return (%3)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use To export a raw ir., give an example?",
        "Y": "Example torch ir graph:\n\n  graph(%x.1 : Float(1, strides=[1])):\n    %1 : Tensor = aten::exp(%x.1)\n    %2 : Tensor = aten::div(%x.1, %1)\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\n    return (%y.1)\n\nis exported as:\n\n  graph(%x.1 : Float(1, strides=[1])):\n    %1 : Tensor = aten::exp(%x.1)\n    %2 : Tensor = aten::div(%x.1, %1)\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\n    return (%y.1)",
        "Z": "To export a raw ir. Example torch ir graph:\n\n  graph(%x.1 : Float(1, strides=[1])):\n    %1 : Tensor = aten::exp(%x.1)\n    %2 : Tensor = aten::div(%x.1, %1)\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\n    return (%y.1)\n\nis exported as:\n\n  graph(%x.1 : Float(1, strides=[1])):\n    %1 : Tensor = aten::exp(%x.1)\n    %2 : Tensor = aten::div(%x.1, %1)\n    %y.1 : Tensor[] = prim::ListConstruct(%2)\n    return (%y.1)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX.\nExported falls through and exports the operator as is, as custom op. Exporting custom operators\nenables users to register and implement the operator as part of their runtime backend., give an example?",
        "Y": "Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %6 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\n    %7 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%6, %0) # registered op\n    return (%7))\n\nis exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx::Div(%2, %0) # registered op\n    return (%3",
        "Z": "This mode can be used to export any operator (ATen or non-ATen) that is not registered and supported in ONNX.\nExported falls through and exports the operator as is, as custom op. Exporting custom operators\nenables users to register and implement the operator as part of their runtime backend. Example torch ir graph:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %6 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\n    %7 : Float(2, 3, 4, strides=[12, 4, 1]) = aten::div(%6, %0) # registered op\n    return (%7))\n\nis exported as:\n\n  graph(%0 : Float(2, 3, 4, strides=[12, 4, 1]),\n        %1 : Float(2, 3, 4, strides=[12, 4, 1])):\n    %2 : Float(2, 3, 4, strides=[12, 4, 1]) = foo_namespace::bar(%0, %1) # custom op\n    %3 : Float(2, 3, 4, strides=[12, 4, 1]) = onnx::Div(%2, %0) # registered op\n    return (%3",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api., give an example?",
        "Y": "layer_count = 4\n\nmodel = nn.LSTM(10, 20, num_layers=layer_count, bidirectional=True)\nmodel.eval()\n\nwith torch.no_grad():\n    input = torch.randn(5, 3, 10)\n    h0 = torch.randn(layer_count * 2, 3, 20)\n    c0 = torch.randn(layer_count * 2, 3, 20)\n    output, (hn, cn) = model(input, (h0, c0))\n\n    # default export\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx')\n    onnx_model = onnx.load('lstm.onnx')\n    # input shape [5, 3, 10]\n    print(onnx_model.graph.input[0])\n\n    # export with `dynamic_axes`\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx',\n                    input_names=['input', 'h0', 'c0'],\n                    output_names=['output', 'hn', 'cn'],\n                    dynamic_axes={'input': {0: 'sequence'}, 'output': {0: 'sequence'}})\n    onnx_model = onnx.load('lstm.onnx')\n    # input shape ['sequence', 3, 10]\n    print(onnx_model.graph.input[0])",
        "Z": "The tracer records the example inputs shape in the graph. In case the model should accept\ninputs of dynamic shape, you can utilize the parameter dynamic_axes in export api. layer_count = 4\n\nmodel = nn.LSTM(10, 20, num_layers=layer_count, bidirectional=True)\nmodel.eval()\n\nwith torch.no_grad():\n    input = torch.randn(5, 3, 10)\n    h0 = torch.randn(layer_count * 2, 3, 20)\n    c0 = torch.randn(layer_count * 2, 3, 20)\n    output, (hn, cn) = model(input, (h0, c0))\n\n    # default export\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx')\n    onnx_model = onnx.load('lstm.onnx')\n    # input shape [5, 3, 10]\n    print(onnx_model.graph.input[0])\n\n    # export with `dynamic_axes`\n    torch.onnx.export(model, (input, (h0, c0)), 'lstm.onnx',\n                    input_names=['input', 'h0', 'c0'],\n                    output_names=['output', 'hn', 'cn'],\n                    dynamic_axes={'input': {0: 'sequence'}, 'output': {0: 'sequence'}})\n    onnx_model = onnx.load('lstm.onnx')\n    # input shape ['sequence', 3, 10]\n    print(onnx_model.graph.input[0])",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future., give an example?",
        "Y": "class ImplicitCastType(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x):\n        # Exporter knows x is float32, will export '2' as float32 as well.\n        y = x + 2\n        # Without type propagation, exporter doesn't know the datatype of y.\n        # Thus '3' is exported as int64 by default.\n        return y + 3\n        # The following will export correctly.\n        # return y + torch.tensor([3], dtype=torch.float32)\n\nx = torch.tensor([1.0], dtype=torch.float32)\ntorch.onnx.export(ImplicitCastType(), x, 'models/implicit_cast.onnx',\n                  example_outputs=ImplicitCastType()(x))",
        "Z": "No, but the exporter will try to handle that part.  Scalars are converted to constant tensors in ONNX.\nThe exporter will try to figure out the right datatype for scalars.  However for cases that it failed\nto do so, you will need to manually provide the datatype information.  This often happens with scripted models,\nwhere the datatypes are not recorded.  We are trying to improve the datatype\npropagation in the exporter such that manual changes are not required in the future. class ImplicitCastType(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x):\n        # Exporter knows x is float32, will export '2' as float32 as well.\n        y = x + 2\n        # Without type propagation, exporter doesn't know the datatype of y.\n        # Thus '3' is exported as int64 by default.\n        return y + 3\n        # The following will export correctly.\n        # return y + torch.tensor([3], dtype=torch.float32)\n\nx = torch.tensor([1.0], dtype=torch.float32)\ntorch.onnx.export(ImplicitCastType(), x, 'models/implicit_cast.onnx',\n                  example_outputs=ImplicitCastType()(x))",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Yes, this is supported now for ONNX opset version >= 11. ONNX introduced the concept of Sequence in opset 11.\nSimilar to list, Sequence is a data type that contains arbitrary number of Tensors.\nAssociated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc.\nHowever, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace\nadd operator.\nE.g.:, give an example?",
        "Y": "class ListLoopModel(torch.nn.Module):\n    def forward(self, x):\n        res = []\n        res1 = []\n        arr = x.split(2, 0)\n        res2 = torch.zeros(3, 4, dtype=torch.long)\n        for i in range(len(arr)):\n            res += [arr[i].sum(0, False)]\n            res1 += [arr[-1 - i].sum(0, False)]\n            res2 += 1\n        return torch.stack(res), torch.stack(res1), res2\n\nmodel = torch.jit.script(ListLoopModel())\ninputs = torch.randn(16)\n\nout = model(inputs)\ntorch.onnx.export(model, (inputs, ), 'loop_and_list.onnx', opset_version=11, example_outputs=out)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('loop_and_list.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: inputs.numpy(),\n})\n\nassert [torch.allclose(o, torch.tensor(o_ort)) for o, o_ort in zip(out, out_ort)]",
        "Z": "Yes, this is supported now for ONNX opset version >= 11. ONNX introduced the concept of Sequence in opset 11.\nSimilar to list, Sequence is a data type that contains arbitrary number of Tensors.\nAssociated operators are also introduced in ONNX, such as SequenceInsert, SequenceAt, etc.\nHowever, in-place list append within loops is not exportable to ONNX. To implement this, please use inplace\nadd operator.\nE.g.: class ListLoopModel(torch.nn.Module):\n    def forward(self, x):\n        res = []\n        res1 = []\n        arr = x.split(2, 0)\n        res2 = torch.zeros(3, 4, dtype=torch.long)\n        for i in range(len(arr)):\n            res += [arr[i].sum(0, False)]\n            res1 += [arr[-1 - i].sum(0, False)]\n            res2 += 1\n        return torch.stack(res), torch.stack(res1), res2\n\nmodel = torch.jit.script(ListLoopModel())\ninputs = torch.randn(16)\n\nout = model(inputs)\ntorch.onnx.export(model, (inputs, ), 'loop_and_list.onnx', opset_version=11, example_outputs=out)\n\n# onnxruntime\nimport onnxruntime\nsess = onnxruntime.InferenceSession('loop_and_list.onnx')\nout_ort = sess.run(None, {\n    sess.get_inputs()[0].name: inputs.numpy(),\n})\n\nassert [torch.allclose(o, torch.tensor(o_ort)) for o, o_ort in zip(out, out_ort)]",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model., give an example?",
        "Y": "model = torchvision.models.mobilenet_v2(pretrained=True)\ninput = torch.randn(2, 3, 224, 224, requires_grad=True)\ntorch.onnx.export(model, (input, ), './large_model.onnx', use_external_data_format=True)",
        "Z": "use_external_data_format argument in export API enables export of models in ONNX external\ndata format. With this option enabled, the exporter stores some model parameters in external\nbinary files, rather than the ONNX file itself. These external binary files are stored in the\nsame location as the ONNX file. Argument \u2018f\u2019 must be a string specifying the location of the model. model = torchvision.models.mobilenet_v2(pretrained=True)\ninput = torch.randn(2, 3, 224, 224, requires_grad=True)\ntorch.onnx.export(model, (input, ), './large_model.onnx', use_external_data_format=True)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use torch.onnx.export, give an example?",
        "Y": "\"args = (x, y, z)\"",
        "Z": "ONLY A TUPLE OF ARGUMENTS or torch.Tensor: \"args = (x, y, z)\"",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How  A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS:, give an example?",
        "Y": "\"args = (x,\n        {\n        'y': input_y,\n        'z': input_z\n        })\"",
        "Z": "A TUPLE OF ARGUEMENTS WITH A DICTIONARY OF NAMED PARAMETERS: \"args = (x,\n        {\n        'y': input_y,\n        'z': input_z\n        })\"",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How  , give an example?",
        "Y": ">>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse",
        "Z": ">>> x = torch.zeros(1, requires_grad=True)\n>>> with torch.no_grad():\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...     y = x * 2\n>>> y.requires_grad\nFalse\n\n>>> torch.set_grad_enabled(True)  # this can also be used as a function\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse",
        "source": "https://pytorch.org/docs/stable/torch.html#reduction-ops"
    },
    {
        "X": "How  is exported as:, give an example?",
        "Y": "graph(%x.1 : Long(1, strides=[1]))::\n  %1 : Tensor = onnx::ReduceSum[keepdims=0](%x.1)\n  %y.1 : Long() = prim::ListConstruct(%1)\n  return (%y.1)",
        "Z": "is exported as: graph(%x.1 : Long(1, strides=[1]))::\n  %1 : Tensor = onnx::ReduceSum[keepdims=0](%x.1)\n  %y.1 : Long() = prim::ListConstruct(%1)\n  return (%y.1)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use Functions, give an example?",
        "Y": "shape(input_1) = ('b', 3, 'w', 'h')\nand shape(input_2) = ('b', 4)\nand shape(output)  = ('b', 'd', 5)",
        "Z": "shape(input_1) = ('b', 3, 'w', 'h')\nand shape(input_2) = ('b', 4)\nand shape(output)  = ('b', 'd', 5)",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How  ONLY INDICES:, give an example?",
        "Y": "``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':[0],\n                  'output':[0, 1]}``\nwhere automatic names will be generated for exported dynamic axes",
        "Z": "ONLY INDICES: ``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':[0],\n                  'output':[0, 1]}``\nwhere automatic names will be generated for exported dynamic axes",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How  INDICES WITH CORRESPONDING NAMES:, give an example?",
        "Y": "``dynamic_axes = {'input_1':{0:'batch',\n                             1:'width',\n                             2:'height'},\n                  'input_2':{0:'batch'},\n                  'output':{0:'batch',\n                            1:'detections'}}``\nwhere provided names will be applied to exported dynamic axes",
        "Z": "INDICES WITH CORRESPONDING NAMES: ``dynamic_axes = {'input_1':{0:'batch',\n                             1:'width',\n                             2:'height'},\n                  'input_2':{0:'batch'},\n                  'output':{0:'batch',\n                            1:'detections'}}``\nwhere provided names will be applied to exported dynamic axes",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How  MIXED MODE OF (1) and (2):, give an example?",
        "Y": "``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':{0:'batch'},\n                  'output':[0,1]}``",
        "Z": "MIXED MODE OF (1) and (2): ``dynamic_axes = {'input_1':[0, 2, 3],\n                  'input_2':{0:'batch'},\n                  'output':[0,1]}``",
        "source": "https://pytorch.org/docs/stable/onnx.html"
    },
    {
        "X": "How to use torch.tril_indices, give an example?",
        "Y": ">>> a = torch.tril_indices(3, 3)\n>>> a\ntensor([[0, 1, 1, 2, 2, 2],\n        [0, 0, 1, 0, 1, 2]])\n\n>>> a = torch.tril_indices(4, 3, -1)\n>>> a\ntensor([[1, 2, 2, 3, 3, 3],\n        [0, 0, 1, 0, 1, 2]])\n\n>>> a = torch.tril_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],\n        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])",
        "Z": "The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and below the main diagonal are\nretained. A positive value includes just as many diagonals above the main\ndiagonal, and similarly a negative value excludes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. >>> a = torch.tril_indices(3, 3)\n>>> a\ntensor([[0, 1, 1, 2, 2, 2],\n        [0, 0, 1, 0, 1, 2]])\n\n>>> a = torch.tril_indices(4, 3, -1)\n>>> a\ntensor([[1, 2, 2, 3, 3, 3],\n        [0, 0, 1, 0, 1, 2]])\n\n>>> a = torch.tril_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3],\n        [0, 1, 0, 1, 2, 0, 1, 2, 0, 1, 2]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.tril_indices.html#torch.tril_indices"
    },
    {
        "X": "How to use torch.sub, give an example?",
        "Y": ">>> a = torch.tensor((1, 2))\n>>> b = torch.tensor((0, 1))\n>>> torch.sub(a, b, alpha=2)\ntensor([1, 0])",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer, float, and complex inputs. >>> a = torch.tensor((1, 2))\n>>> b = torch.tensor((0, 1))\n>>> torch.sub(a, b, alpha=2)\ntensor([1, 0])",
        "source": "https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub"
    },
    {
        "X": "How to use torch.cuda.amp.autocast, give an example?",
        "Y": "# Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with autocast():\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()",
        "Z": "autocast should wrap only the forward pass(es) of your network, including the loss\ncomputation(s).  Backward passes under autocast are not recommended.\nBackward ops run in the same type that autocast used for corresponding forward ops. # Creates model and optimizer in default precision\nmodel = Net().cuda()\noptimizer = optim.SGD(model.parameters(), ...)\n\nfor input, target in data:\n    optimizer.zero_grad()\n\n    # Enables autocasting for the forward pass (model + loss)\n    with autocast():\n        output = model(input)\n        loss = loss_fn(output, target)\n\n    # Exits the context manager before backward()\n    loss.backward()\n    optimizer.step()",
        "source": "https://pytorch.org/docs/stable/amp.html"
    },
    {
        "X": "How to use autocast can also be used as a decorator, e.g., on the forward method of your model:, give an example?",
        "Y": "class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...",
        "Z": "See the Automatic Mixed Precision examples for usage (along with gradient scaling)\nin more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).autocast can also be used as a decorator, e.g., on the forward method of your model: class AutocastModel(nn.Module):\n    ...\n    @autocast()\n    def forward(self, input):\n        ...",
        "source": "https://pytorch.org/docs/stable/amp.html"
    },
    {
        "X": "How to use Floating-point Tensors produced in an autocast-enabled region may be float16.\nAfter returning to an autocast-disabled region, using them with floating-point\nTensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)\nproduced in the autocast region back to float32 (or other dtype if desired).\nIf a Tensor from the autocast region is already float32, the cast is a no-op,\nand incurs no additional overhead.  Example:, give an example?",
        "Y": "# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())",
        "Z": "autocast can also be used as a decorator, e.g., on the forward method of your model: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    # torch.mm is on autocast's list of ops that should run in float16.\n    # Inputs are float32, but the op runs in float16 and produces float16 output.\n    # No manual casts are required.\n    e_float16 = torch.mm(a_float32, b_float32)\n    # Also handles mixed input types\n    f_float16 = torch.mm(d_float32, e_float16)\n\n# After exiting autocast, calls f_float16.float() to use with d_float32\ng_float32 = torch.mm(d_float32, f_float16.float())",
        "source": "https://pytorch.org/docs/stable/amp.html"
    },
    {
        "X": "How to use autocast(enabled=False) subregions can be nested in autocast-enabled regions.\nLocally disabling autocast can be useful, for example, if you want to force a subregion\nto run in a particular dtype.  Disabling autocast gives you explicit control over\nthe execution type.  In the subregion, inputs from the surrounding region\nshould be cast to dtype before use:, give an example?",
        "Y": "# Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    e_float16 = torch.mm(a_float32, b_float32)\n\n    with autocast(enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)",
        "Z": "Type mismatch errors in an autocast-enabled region are a bug; if this is what you observe,\nplease file an issue.autocast(enabled=False) subregions can be nested in autocast-enabled regions.\nLocally disabling autocast can be useful, for example, if you want to force a subregion\nto run in a particular dtype.  Disabling autocast gives you explicit control over\nthe execution type.  In the subregion, inputs from the surrounding region\nshould be cast to dtype before use: # Creates some tensors in default dtype (here assumed to be float32)\na_float32 = torch.rand((8, 8), device=\"cuda\")\nb_float32 = torch.rand((8, 8), device=\"cuda\")\nc_float32 = torch.rand((8, 8), device=\"cuda\")\nd_float32 = torch.rand((8, 8), device=\"cuda\")\n\nwith autocast():\n    e_float16 = torch.mm(a_float32, b_float32)\n\n    with autocast(enabled=False):\n        # Calls e_float16.float() to ensure float32 execution\n        # (necessary because e_float16 was created in an autocasted region)\n        f_float32 = torch.mm(c_float32, e_float16.float())\n\n    # No manual casts are required when re-entering the autocast-enabled region.\n    # torch.mm again runs in float16 and produces float16 output, regardless of input types.\n    g_float16 = torch.mm(d_float32, f_float32)",
        "source": "https://pytorch.org/docs/stable/amp.html"
    },
    {
        "X": "How to use torch.cuda.amp.GradScaler.unscale_, give an example?",
        "Y": "...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()",
        "Z": "unscale_() is optional, serving cases where you need to\nmodify or inspect gradients\nbetween the backward pass(es) and step().\nIf unscale_() is not called explicitly,  gradients will be unscaled  automatically during step().Simple example, using unscale_() to enable clipping of unscaled gradients: ...\nscaler.scale(loss).backward()\nscaler.unscale_(optimizer)\ntorch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\nscaler.step(optimizer)\nscaler.update()",
        "source": "https://pytorch.org/docs/stable/amp.html"
    },
    {
        "X": "How to use torch.logdet, give an example?",
        "Y": ">>> A = torch.randn(3, 3)\n>>> torch.det(A)\ntensor(0.2611)\n>>> torch.logdet(A)\ntensor(-1.3430)\n>>> A\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> A.det()\ntensor([1.1990, 0.4099, 0.7386])\n>>> A.det().log()\ntensor([ 0.1815, -0.8917, -0.3031])",
        "Z": ">>> A = torch.randn(3, 3)\n>>> torch.det(A)\ntensor(0.2611)\n>>> torch.logdet(A)\ntensor(-1.3430)\n>>> A\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> A.det()\ntensor([1.1990, 0.4099, 0.7386])\n>>> A.det().log()\ntensor([ 0.1815, -0.8917, -0.3031])",
        "source": "https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet"
    },
    {
        "X": "How to use torch.lt, give an example?",
        "Y": ">>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, False], [True, False]])",
        "Z": "The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.lt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, False], [True, False]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt"
    },
    {
        "X": "How to use torch.quantize_per_tensor, give an example?",
        "Y": ">>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)\ntensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)\n>>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()\ntensor([ 0, 10, 20, 30], dtype=torch.uint8)",
        "Z": ">>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8)\ntensor([-1.,  0.,  1.,  2.], size=(4,), dtype=torch.quint8,\n       quantization_scheme=torch.per_tensor_affine, scale=0.1, zero_point=10)\n>>> torch.quantize_per_tensor(torch.tensor([-1.0, 0.0, 1.0, 2.0]), 0.1, 10, torch.quint8).int_repr()\ntensor([ 0, 10, 20, 30], dtype=torch.uint8)",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_tensor.html#torch.quantize_per_tensor"
    },
    {
        "X": "How to use torch.square, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.square(a)\ntensor([ 4.3077,  1.0457,  0.0069,  0.2310])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([-2.0755,  1.0226,  0.0831,  0.4806])\n>>> torch.square(a)\ntensor([ 4.3077,  1.0457,  0.0069,  0.2310])",
        "source": "https://pytorch.org/docs/stable/generated/torch.square.html#torch.square"
    },
    {
        "X": "How to use torch.take, give an example?",
        "Y": ">>> src = torch.tensor([[4, 3, 5],\n...                     [6, 7, 8]])\n>>> torch.take(src, torch.tensor([0, 2, 5]))\ntensor([ 4,  5,  8])",
        "Z": ">>> src = torch.tensor([[4, 3, 5],\n...                     [6, 7, 8]])\n>>> torch.take(src, torch.tensor([0, 2, 5]))\ntensor([ 4,  5,  8])",
        "source": "https://pytorch.org/docs/stable/generated/torch.take.html#torch.take"
    },
    {
        "X": "How to use torch.normal, give an example?",
        "Y": ">>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\ntensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n          8.0505,   8.1408,   9.0563,  10.0566])",
        "Z": "The shapes of mean and std don\u2019t need to match, but the\ntotal number of elements in each tensor need to be the same. >>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\ntensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n          8.0505,   8.1408,   9.0563,  10.0566])",
        "source": "https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal"
    },
    {
        "X": "How  Similar to the function above, but the means are shared among all drawn\nelements., give an example?",
        "Y": ">>> torch.normal(mean=0.5, std=torch.arange(1., 6.))\ntensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])",
        "Z": "Similar to the function above, but the means are shared among all drawn\nelements. >>> torch.normal(mean=0.5, std=torch.arange(1., 6.))\ntensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])",
        "source": "https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal"
    },
    {
        "X": "How  Similar to the function above, but the standard deviations are shared among\nall drawn elements., give an example?",
        "Y": ">>> torch.normal(mean=torch.arange(1., 6.))\ntensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])",
        "Z": "Similar to the function above, but the standard deviations are shared among\nall drawn elements. >>> torch.normal(mean=torch.arange(1., 6.))\ntensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])",
        "source": "https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal"
    },
    {
        "X": "How  Similar to the function above, but the means and standard deviations are shared\namong all drawn elements. The resulting tensor has size given by size., give an example?",
        "Y": ">>> torch.normal(2, 3, size=(1, 4))\ntensor([[-1.3987, -1.9544,  3.6048,  0.7909]])",
        "Z": "Similar to the function above, but the means and standard deviations are shared\namong all drawn elements. The resulting tensor has size given by size. >>> torch.normal(2, 3, size=(1, 4))\ntensor([[-1.3987, -1.9544,  3.6048,  0.7909]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.normal.html#torch.normal"
    },
    {
        "X": "How to use torch.std_mean, give an example?",
        "Y": ">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.std_mean(a, unbiased=False)\n(tensor(0.4188), tensor(-0.8509))",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. >>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.std_mean(a, unbiased=False)\n(tensor(0.4188), tensor(-0.8509))",
        "source": "https://pytorch.org/docs/stable/generated/torch.std_mean.html#torch.std_mean"
    },
    {
        "X": "How to use torch.swapdims, give an example?",
        "Y": ">>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])",
        "Z": "This function is equivalent to NumPy\u2019s swapaxes function. >>> x = torch.tensor([[[0,1],[2,3]],[[4,5],[6,7]]])\n>>> x\ntensor([[[0, 1],\n        [2, 3]],\n\n        [[4, 5],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 1)\ntensor([[[0, 1],\n        [4, 5]],\n\n        [[2, 3],\n        [6, 7]]])\n>>> torch.swapdims(x, 0, 2)\ntensor([[[0, 4],\n        [2, 6]],\n\n        [[1, 5],\n        [3, 7]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims"
    },
    {
        "X": "How to use torch.dstack, give an example?",
        "Y": ">>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.dstack((a,b))\ntensor([[[1, 4],\n         [2, 5],\n         [3, 6]]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.dstack((a,b))\ntensor([[[1, 4]],\n        [[2, 5]],\n        [[3, 6]]])",
        "Z": "This is equivalent to concatenation along the third axis after 1-D and 2-D tensors have been reshaped by torch.atleast_3d(). >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.dstack((a,b))\ntensor([[[1, 4],\n         [2, 5],\n         [3, 6]]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.dstack((a,b))\ntensor([[[1, 4]],\n        [[2, 5]],\n        [[3, 6]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.dstack.html#torch.dstack"
    },
    {
        "X": "How to use torch.special.entr, give an example?",
        "Y": ">>> a = torch.arange(-0.5, 1, 0.5)\n>>> a\ntensor([-0.5000,  0.0000,  0.5000])\n>>> torch.special.entr(a)\ntensor([  -inf, 0.0000, 0.3466])",
        "Z": ">>> a = torch.arange(-0.5, 1, 0.5)\n>>> a\ntensor([-0.5000,  0.0000,  0.5000])\n>>> torch.special.entr(a)\ntensor([  -inf, 0.0000, 0.3466])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.special.erf, give an example?",
        "Y": ">>> torch.special.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])",
        "Z": ">>> torch.special.erf(torch.tensor([0, -1., 10.]))\ntensor([ 0.0000, -0.8427,  1.0000])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.special.erfc, give an example?",
        "Y": ">>> torch.special.erfc(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 1.8427,  0.0000])",
        "Z": ">>> torch.special.erfc(torch.tensor([0, -1., 10.]))\ntensor([ 1.0000, 1.8427,  0.0000])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.special.erfinv, give an example?",
        "Y": ">>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])",
        "Z": ">>> torch.special.erfinv(torch.tensor([0, 0.5, -1.]))\ntensor([ 0.0000,  0.4769,    -inf])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.special.expit, give an example?",
        "Y": ">>> t = torch.randn(4)\n>>> t\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n>>> torch.special.expit(t)\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])",
        "Z": ">>> t = torch.randn(4)\n>>> t\ntensor([ 0.9213,  1.0887, -0.8858, -1.7683])\n>>> torch.special.expit(t)\ntensor([ 0.7153,  0.7481,  0.2920,  0.1458])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.special.expm1, give an example?",
        "Y": ">>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\ntensor([ 0.,  1.])",
        "Z": ">>> torch.special.expm1(torch.tensor([0, math.log(2.)]))\ntensor([ 0.,  1.])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.special.exp2, give an example?",
        "Y": ">>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\ntensor([ 1.,  2.,  8., 16.])",
        "Z": ">>> torch.special.exp2(torch.tensor([0, math.log2(2.), 3, 4]))\ntensor([ 1.,  2.,  8., 16.])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.special.gammaln, give an example?",
        "Y": ">>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.special.gammaln(a)\ntensor([ 0.5724,  0.0000, -0.1208])",
        "Z": ">>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.special.gammaln(a)\ntensor([ 0.5724,  0.0000, -0.1208])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.special.i0e, give an example?",
        "Y": ">>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])",
        "Z": ">>> torch.special.i0e(torch.arange(5, dtype=torch.float32))\ntensor([1.0000, 0.4658, 0.3085, 0.2430, 0.2070])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.special.logit, give an example?",
        "Y": ">>> a = torch.rand(5)\n>>> a\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\n>>> torch.special.logit(a, eps=1e-6)\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])",
        "Z": ">>> a = torch.rand(5)\n>>> a\ntensor([0.2796, 0.9331, 0.6486, 0.1523, 0.6516])\n>>> torch.special.logit(a, eps=1e-6)\ntensor([-0.9466,  2.6352,  0.6131, -1.7169,  0.6261])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.special.xlog1py, give an example?",
        "Y": ">>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlog1py(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlog1py(x, y)\ntensor([1.3863, 2.1972, 2.0794])\n>>> torch.special.xlog1py(x, 4)\ntensor([1.6094, 3.2189, 4.8283])\n>>> torch.special.xlog1py(2, y)\ntensor([2.7726, 2.1972, 1.3863])",
        "Z": "Similar to SciPy\u2019s scipy.special.xlog1py. >>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlog1py(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlog1py(x, y)\ntensor([1.3863, 2.1972, 2.0794])\n>>> torch.special.xlog1py(x, 4)\ntensor([1.6094, 3.2189, 4.8283])\n>>> torch.special.xlog1py(2, y)\ntensor([2.7726, 2.1972, 1.3863])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.erfinv"
    },
    {
        "X": "How to use torch.isneginf, give an example?",
        "Y": ">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\n>>> torch.isneginf(a)\ntensor([ True, False, False])",
        "Z": ">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\n>>> torch.isneginf(a)\ntensor([ True, False, False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf"
    },
    {
        "X": "How to use torch.tensordot, give an example?",
        "Y": ">>> a = torch.arange(60.).reshape(3, 4, 5)\n>>> b = torch.arange(24.).reshape(4, 3, 2)\n>>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\ntensor([[4400., 4730.],\n        [4532., 4874.],\n        [4664., 5018.],\n        [4796., 5162.],\n        [4928., 5306.]])\n\n>>> a = torch.randn(3, 4, 5, device='cuda')\n>>> b = torch.randn(4, 5, 6, device='cuda')\n>>> c = torch.tensordot(a, b, dims=2).cpu()\ntensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n\n>>> a = torch.randn(3, 5, 4, 6)\n>>> b = torch.randn(6, 4, 5, 3)\n>>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\ntensor([[  7.7193,  -2.4867, -10.3204],\n        [  1.5513, -14.4737,  -6.5113],\n        [ -0.2850,   4.2573,  -3.5997]])",
        "Z": "When called with dims of the list form, the given dimensions will be contracted\nin place of the last ddd of a and the first ddd of bbb. The sizes\nin these dimensions must match, but tensordot() will deal with broadcasted\ndimensions. >>> a = torch.arange(60.).reshape(3, 4, 5)\n>>> b = torch.arange(24.).reshape(4, 3, 2)\n>>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))\ntensor([[4400., 4730.],\n        [4532., 4874.],\n        [4664., 5018.],\n        [4796., 5162.],\n        [4928., 5306.]])\n\n>>> a = torch.randn(3, 4, 5, device='cuda')\n>>> b = torch.randn(4, 5, 6, device='cuda')\n>>> c = torch.tensordot(a, b, dims=2).cpu()\ntensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],\n        [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],\n        [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])\n\n>>> a = torch.randn(3, 5, 4, 6)\n>>> b = torch.randn(6, 4, 5, 3)\n>>> torch.tensordot(a, b, dims=([2, 1, 3], [1, 2, 0]))\ntensor([[  7.7193,  -2.4867, -10.3204],\n        [  1.5513, -14.4737,  -6.5113],\n        [ -0.2850,   4.2573,  -3.5997]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensordot.html#torch.tensordot"
    },
    {
        "X": "How to use torch.clamp, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\n>>> torch.clamp(a, min=-0.5, max=0.5)\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\n\n>>> min = torch.linspace(-1, 1, steps=4)\n>>> torch.clamp(a, min=min)\ntensor([-1.0000,  0.1734,  0.3333,  1.0000])",
        "Z": "If min is None, there is no lower bound.\nOr, if max is None there is no upper bound. >>> a = torch.randn(4)\n>>> a\ntensor([-1.7120,  0.1734, -0.0478, -0.0922])\n>>> torch.clamp(a, min=-0.5, max=0.5)\ntensor([-0.5000,  0.1734, -0.0478, -0.0922])\n\n>>> min = torch.linspace(-1, 1, steps=4)\n>>> torch.clamp(a, min=min)\ntensor([-1.0000,  0.1734,  0.3333,  1.0000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp"
    },
    {
        "X": "How to use torch.renorm, give an example?",
        "Y": ">>> x = torch.ones(3, 3)\n>>> x[1].fill_(2)\ntensor([ 2.,  2.,  2.])\n>>> x[2].fill_(3)\ntensor([ 3.,  3.,  3.])\n>>> x\ntensor([[ 1.,  1.,  1.],\n        [ 2.,  2.,  2.],\n        [ 3.,  3.,  3.]])\n>>> torch.renorm(x, 1, 0, 5)\ntensor([[ 1.0000,  1.0000,  1.0000],\n        [ 1.6667,  1.6667,  1.6667],\n        [ 1.6667,  1.6667,  1.6667]])",
        "Z": ">>> x = torch.ones(3, 3)\n>>> x[1].fill_(2)\ntensor([ 2.,  2.,  2.])\n>>> x[2].fill_(3)\ntensor([ 3.,  3.,  3.])\n>>> x\ntensor([[ 1.,  1.,  1.],\n        [ 2.,  2.,  2.],\n        [ 3.,  3.,  3.]])\n>>> torch.renorm(x, 1, 0, 5)\ntensor([[ 1.0000,  1.0000,  1.0000],\n        [ 1.6667,  1.6667,  1.6667],\n        [ 1.6667,  1.6667,  1.6667]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm"
    },
    {
        "X": "How to use torch.logcumsumexp, give an example?",
        "Y": ">>> a = torch.randn(10)\n>>> torch.logcumsumexp(a, dim=0)\ntensor([-0.42296738, -0.04462666,  0.86278635,  0.94622083,  1.05277811,\n         1.39202815,  1.83525007,  1.84492621,  2.06084887,  2.06844475]))",
        "Z": "For summation index jjj given by dim and other indices iii, the result is >>> a = torch.randn(10)\n>>> torch.logcumsumexp(a, dim=0)\ntensor([-0.42296738, -0.04462666,  0.86278635,  0.94622083,  1.05277811,\n         1.39202815,  1.83525007,  1.84492621,  2.06084887,  2.06844475]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp"
    },
    {
        "X": "How to use torch.cat, give an example?",
        "Y": ">>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 0)\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 1)\ntensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n         -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n         -0.5790,  0.1497]])",
        "Z": "torch.cat() can be best understood via examples. >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 0)\ntensor([[ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497],\n        [ 0.6580, -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497]])\n>>> torch.cat((x, x, x), 1)\ntensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n         -1.0969, -0.4614],\n        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n         -0.5790,  0.1497]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.cat.html#torch.cat"
    },
    {
        "X": "How to use torch.cdist, give an example?",
        "Y": ">>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n>>> a\ntensor([[ 0.9041,  0.0196],\n        [-0.3108, -2.4423],\n        [-0.4821,  1.0590]])\n>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\n>>> b\ntensor([[-2.1763, -0.4713],\n        [-0.6986,  1.3702]])\n>>> torch.cdist(a, b, p=2)\ntensor([[3.1193, 2.0959],\n        [2.7138, 3.8322],\n        [2.2830, 0.3791]])",
        "Z": "This function is equivalent to scipy.spatial.distance.cdist(input,\u2019minkowski\u2019, p=p)\nif p\u2208(0,\u221e)p \\in (0, \\infty)p\u2208(0,\u221e). When p=0p = 0p=0 it is equivalent to\nscipy.spatial.distance.cdist(input, \u2018hamming\u2019) * M. When p=\u221ep = \\inftyp=\u221e, the closest\nscipy function is scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max()). >>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])\n>>> a\ntensor([[ 0.9041,  0.0196],\n        [-0.3108, -2.4423],\n        [-0.4821,  1.0590]])\n>>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])\n>>> b\ntensor([[-2.1763, -0.4713],\n        [-0.6986,  1.3702]])\n>>> torch.cdist(a, b, p=2)\ntensor([[3.1193, 2.0959],\n        [2.7138, 3.8322],\n        [2.2830, 0.3791]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.cdist.html#torch.cdist"
    },
    {
        "X": "How to use torch.index_select, give an example?",
        "Y": ">>> x = torch.randn(3, 4)\n>>> x\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-0.4664,  0.2647, -0.1228, -1.1068],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n>>> indices = torch.tensor([0, 2])\n>>> torch.index_select(x, 0, indices)\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n>>> torch.index_select(x, 1, indices)\ntensor([[ 0.1427, -0.5414],\n        [-0.4664, -0.1228],\n        [-1.1734,  0.7230]])",
        "Z": "The returned tensor has the same number of dimensions as the original tensor\n(input).  The dimth dimension has the same size as the length\nof index; other dimensions have the same size as in the original tensor. >>> x = torch.randn(3, 4)\n>>> x\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-0.4664,  0.2647, -0.1228, -1.1068],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n>>> indices = torch.tensor([0, 2])\n>>> torch.index_select(x, 0, indices)\ntensor([[ 0.1427,  0.0231, -0.5414, -1.0009],\n        [-1.1734, -0.6571,  0.7230, -0.6004]])\n>>> torch.index_select(x, 1, indices)\ntensor([[ 0.1427, -0.5414],\n        [-0.4664, -0.1228],\n        [-1.1734,  0.7230]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select"
    },
    {
        "X": "How  A floating point scalar operand has dtype torch.get_default_dtype() and an integral\nnon-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimum dtypes of an operand.  Quantized and complex types\nare not yet supported., give an example?",
        "Y": ">>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32",
        "Z": "A floating point scalar operand has dtype torch.get_default_dtype() and an integral\nnon-boolean scalar operand has dtype torch.int64. Unlike numpy, we do not inspect\nvalues when determining the minimum dtypes of an operand.  Quantized and complex types\nare not yet supported. >>> float_tensor = torch.ones(1, dtype=torch.float)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> complex_float_tensor = torch.ones(1, dtype=torch.complex64)\n>>> complex_double_tensor = torch.ones(1, dtype=torch.complex128)\n>>> int_tensor = torch.ones(1, dtype=torch.int)\n>>> long_tensor = torch.ones(1, dtype=torch.long)\n>>> uint_tensor = torch.ones(1, dtype=torch.uint8)\n>>> double_tensor = torch.ones(1, dtype=torch.double)\n>>> bool_tensor = torch.ones(1, dtype=torch.bool)\n# zero-dim tensors\n>>> long_zerodim = torch.tensor(1, dtype=torch.long)\n>>> int_zerodim = torch.tensor(1, dtype=torch.int)\n\n>>> torch.add(5, 5).dtype\ntorch.int64\n# 5 is an int64, but does not have higher category than int_tensor so is not considered.\n>>> (int_tensor + 5).dtype\ntorch.int32\n>>> (int_tensor + long_zerodim).dtype\ntorch.int32\n>>> (long_tensor + int_tensor).dtype\ntorch.int64\n>>> (bool_tensor + long_tensor).dtype\ntorch.int64\n>>> (bool_tensor + uint_tensor).dtype\ntorch.uint8\n>>> (float_tensor + double_tensor).dtype\ntorch.float64\n>>> (complex_float_tensor + complex_double_tensor).dtype\ntorch.complex128\n>>> (bool_tensor + int_tensor).dtype\ntorch.int32\n# Since long is a different kind than float, result dtype only needs to be large enough\n# to hold the float.\n>>> torch.add(long_tensor, float_tensor).dtype\ntorch.float32",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc"
    },
    {
        "X": "How  A torch.device can be constructed via a string or via a string and device ordinalVia a string:, give an example?",
        "Y": ">>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')",
        "Z": "A torch.device can be constructed via a string or via a string and device ordinalVia a string: >>> torch.device('cuda:0')\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu')\ndevice(type='cpu')\n\n>>> torch.device('cuda')  # current cuda device\ndevice(type='cuda')",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc"
    },
    {
        "X": "How  Via a string:Via a string and device ordinal:, give an example?",
        "Y": ">>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)",
        "Z": "Via a string:Via a string and device ordinal: >>> torch.device('cuda', 0)\ndevice(type='cuda', index=0)\n\n>>> torch.device('cpu', 0)\ndevice(type='cpu', index=0)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc"
    },
    {
        "X": "How  For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors., give an example?",
        "Y": ">>> torch.device(1)\ndevice(type='cuda', index=1)",
        "Z": "For legacy reasons, a device can be constructed via a single device ordinal, which is treated\nas a cuda device.  This matches Tensor.get_device(), which returns an ordinal for cuda\ntensors and is not supported for cpu tensors. >>> torch.device(1)\ndevice(type='cuda', index=1)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc"
    },
    {
        "X": "How  Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent:, give an example?",
        "Y": ">>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy",
        "Z": "Methods which take a device will generally accept a (properly formatted) string\nor (legacy) integer device ordinal, i.e. the following are all equivalent: >>> torch.randn((2,3), device=torch.device('cuda:1'))\n>>> torch.randn((2,3), device='cuda:1')\n>>> torch.randn((2,3), device=1)  # legacy",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc"
    },
    {
        "X": "How  torch.strided represents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associated\ntorch.Storage, which holds its data. These tensors provide\nmulti-dimensional, strided\nview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently., give an example?",
        "Y": ">>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)",
        "Z": "torch.strided represents dense Tensors and is the memory layout that\nis most commonly used. Each strided tensor has an associated\ntorch.Storage, which holds its data. These tensors provide\nmulti-dimensional, strided\nview of a storage. Strides are a list of integers: the k-th stride\nrepresents the jump in the memory necessary to go from one element to the\nnext one in the k-th dimension of the Tensor. This concept makes it possible\nto perform many tensor operations efficiently. >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n>>> x.stride()\n(5, 1)\n\n>>> x.t().stride()\n(1, 5)",
        "source": "https://pytorch.org/docs/stable/tensor_attributes.html#type-promotion-doc"
    },
    {
        "X": "How to use torch.cholesky_solve, give an example?",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[ 0.7747, -1.9549,  1.3086],\n        [-1.9549,  6.7546, -5.4114],\n        [ 1.3086, -5.4114,  4.8733]])\n>>> b = torch.randn(3, 2)\n>>> b\ntensor([[-0.6355,  0.9891],\n        [ 0.1974,  1.4706],\n        [-0.4115, -0.6225]])\n>>> torch.cholesky_solve(b, u)\ntensor([[ -8.1625,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n>>> torch.mm(a.inverse(), b)\ntensor([[ -8.1626,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])",
        "Z": "Supports real-valued and complex-valued inputs.\nFor the complex-valued inputs the transpose operator above is the conjugate transpose. >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[ 0.7747, -1.9549,  1.3086],\n        [-1.9549,  6.7546, -5.4114],\n        [ 1.3086, -5.4114,  4.8733]])\n>>> b = torch.randn(3, 2)\n>>> b\ntensor([[-0.6355,  0.9891],\n        [ 0.1974,  1.4706],\n        [-0.4115, -0.6225]])\n>>> torch.cholesky_solve(b, u)\ntensor([[ -8.1625,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])\n>>> torch.mm(a.inverse(), b)\ntensor([[ -8.1626,  19.6097],\n        [ -5.8398,  14.2387],\n        [ -4.3771,  10.4173]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve"
    },
    {
        "X": "How to use torch.argmin, give an example?",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.1139,  0.2254, -0.1381,  0.3687],\n        [ 1.0100, -1.1975, -0.0102, -0.4732],\n        [-0.9240,  0.1207, -0.7506, -1.0213],\n        [ 1.7809, -1.2960,  0.9384,  0.1438]])\n>>> torch.argmin(a)\ntensor(13)\n>>> torch.argmin(a, dim=1)\ntensor([ 2,  1,  3,  1])\n>>> torch.argmin(a, dim=1, keepdim=True)\ntensor([[2],\n        [1],\n        [3],\n        [1]])",
        "Z": "This is the second value returned by torch.min(). See its\ndocumentation for the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.1139,  0.2254, -0.1381,  0.3687],\n        [ 1.0100, -1.1975, -0.0102, -0.4732],\n        [-0.9240,  0.1207, -0.7506, -1.0213],\n        [ 1.7809, -1.2960,  0.9384,  0.1438]])\n>>> torch.argmin(a)\ntensor(13)\n>>> torch.argmin(a, dim=1)\ntensor([ 2,  1,  3,  1])\n>>> torch.argmin(a, dim=1, keepdim=True)\ntensor([[2],\n        [1],\n        [3],\n        [1]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin"
    },
    {
        "X": "How to use where \u03b8\\theta\u03b8 are the parameters, \u03b1\\alpha\u03b1 is the learning rate,\nrrr is the reward and p(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s))p(a\u2223\u03c0\u03b8(s)) is the probability of\ntaking action aaa in state sss given policy \u03c0\u03b8\\pi^\\theta\u03c0\u03b8.In practice we would sample an action from the output of a network, apply this\naction in an environment, and then use log_prob to construct an equivalent\nloss function. Note that we use a negative because optimizers use gradient\ndescent, whilst the rule above assumes gradient ascent. With a categorical\npolicy, the code for implementing REINFORCE would be as follows:, give an example?",
        "Y": "probs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward()",
        "Z": "where \u03b8\\theta\u03b8 are the parameters, \u03b1\\alpha\u03b1 is the learning rate,\nrrr is the reward and p(a\u2223\u03c0\u03b8(s))p(a|\\pi^\\theta(s))p(a\u2223\u03c0\u03b8(s)) is the probability of\ntaking action aaa in state sss given policy \u03c0\u03b8\\pi^\\theta\u03c0\u03b8.In practice we would sample an action from the output of a network, apply this\naction in an environment, and then use log_prob to construct an equivalent\nloss function. Note that we use a negative because optimizers use gradient\ndescent, whilst the rule above assumes gradient ascent. With a categorical\npolicy, the code for implementing REINFORCE would be as follows: probs = policy_network(state)\n# Note that this is equivalent to what used to be called multinomial\nm = Categorical(probs)\naction = m.sample()\nnext_state, reward = env.step(action)\nloss = -m.log_prob(action) * reward\nloss.backward()",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use The other way to implement these stochastic/policy gradients would be to use the\nreparameterization trick from the\nrsample() method, where the\nparameterized random variable can be constructed via a parameterized\ndeterministic function of a parameter-free random variable. The reparameterized\nsample therefore becomes differentiable. The code for implementing the pathwise\nderivative would be as follows:, give an example?",
        "Y": "params = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward()",
        "Z": "The other way to implement these stochastic/policy gradients would be to use the\nreparameterization trick from the\nrsample() method, where the\nparameterized random variable can be constructed via a parameterized\ndeterministic function of a parameter-free random variable. The reparameterized\nsample therefore becomes differentiable. The code for implementing the pathwise\nderivative would be as follows: params = policy_network(state)\nm = Normal(*params)\n# Any distribution with .has_rsample == True could work based on the application\naction = m.rsample()\nnext_state, reward = env.step(action)  # Assuming that reward is differentiable\nloss = -reward\nloss.backward()",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.bernoulli.Bernoulli, give an example?",
        "Y": ">>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])",
        "Z": "Samples are binary (0 or 1). They take the value 1 with probability p\nand 0 with probability 1 - p. >>> m = Bernoulli(torch.tensor([0.3]))\n>>> m.sample()  # 30% chance 1; 70% chance 0\ntensor([ 0.])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.beta.Beta, give an example?",
        "Y": ">>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])",
        "Z": "Beta distribution parameterized by concentration1 and concentration0. >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))\n>>> m.sample()  # Beta distributed with concentration concentration1 and concentration0\ntensor([ 0.1046])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.binomial.Binomial, give an example?",
        "Y": ">>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])",
        "Z": "Creates a Binomial distribution parameterized by total_count and\neither probs or logits (but not both). total_count must be\nbroadcastable with probs/logits. >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))\n>>> x = m.sample()\ntensor([   0.,   22.,   71.,  100.])\n\n>>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))\n>>> x = m.sample()\ntensor([[ 4.,  5.],\n        [ 7.,  6.]])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.categorical.Categorical, give an example?",
        "Y": ">>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)",
        "Z": "See also: torch.multinomial() >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor(3)",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.cauchy.Cauchy, give an example?",
        "Y": ">>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])",
        "Z": "Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of\nindependent normally distributed random variables with means 0 follows a\nCauchy distribution. >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1\ntensor([ 2.3214])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.chi2.Chi2, give an example?",
        "Y": ">>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])",
        "Z": "Creates a Chi2 distribution parameterized by shape parameter df.\nThis is exactly equivalent to Gamma(alpha=0.5*df, beta=0.5) >>> m = Chi2(torch.tensor([1.0]))\n>>> m.sample()  # Chi2 distributed with shape df=1\ntensor([ 0.1046])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.continuous_bernoulli.ContinuousBernoulli, give an example?",
        "Y": ">>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])",
        "Z": "The distribution is supported in [0, 1] and parameterized by \u2018probs\u2019 (in\n(0,1)) or \u2018logits\u2019 (real-valued). Note that, unlike the Bernoulli, \u2018probs\u2019\ndoes not correspond to a probability and \u2018logits\u2019 does not correspond to\nlog-odds, but the same names are used due to the similarity with the\nBernoulli. See [1] for more details. >>> m = ContinuousBernoulli(torch.tensor([0.3]))\n>>> m.sample()\ntensor([ 0.2538])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.dirichlet.Dirichlet, give an example?",
        "Y": ">>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentrarion concentration\ntensor([ 0.1046,  0.8954])",
        "Z": "Creates a Dirichlet distribution parameterized by concentration concentration. >>> m = Dirichlet(torch.tensor([0.5, 0.5]))\n>>> m.sample()  # Dirichlet distributed with concentrarion concentration\ntensor([ 0.1046,  0.8954])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.exponential.Exponential, give an example?",
        "Y": ">>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])",
        "Z": "Creates a Exponential distribution parameterized by rate. >>> m = Exponential(torch.tensor([1.0]))\n>>> m.sample()  # Exponential distributed with rate=1\ntensor([ 0.1046])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.fishersnedecor.FisherSnedecor, give an example?",
        "Y": ">>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])",
        "Z": "Creates a Fisher-Snedecor distribution parameterized by df1 and df2. >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2\ntensor([ 0.2453])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.gamma.Gamma, give an example?",
        "Y": ">>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])",
        "Z": "Creates a Gamma distribution parameterized by shape concentration and rate. >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # Gamma distributed with concentration=1 and rate=1\ntensor([ 0.1046])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.geometric.Geometric, give an example?",
        "Y": ">>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])",
        "Z": "Samples are non-negative integers [0, inf\u2061\\infinf). >>> m = Geometric(torch.tensor([0.3]))\n>>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0\ntensor([ 2.])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.gumbel.Gumbel, give an example?",
        "Y": ">>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])",
        "Z": "Samples from a Gumbel Distribution. >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))\n>>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2\ntensor([ 1.0124])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use Creates a half-Cauchy distribution parameterized by scale where:, give an example?",
        "Y": "X ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)",
        "Z": "Creates a half-Cauchy distribution parameterized by scale where: X ~ Cauchy(0, scale)\nY = |X| ~ HalfCauchy(scale)",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.half_cauchy.HalfCauchy, give an example?",
        "Y": ">>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])",
        "Z": "Creates a half-Cauchy distribution parameterized by scale where: >>> m = HalfCauchy(torch.tensor([1.0]))\n>>> m.sample()  # half-cauchy distributed with scale=1\ntensor([ 2.3214])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use Creates a half-normal distribution parameterized by scale where:, give an example?",
        "Y": "X ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)",
        "Z": "Creates a half-normal distribution parameterized by scale where: X ~ Normal(0, scale)\nY = |X| ~ HalfNormal(scale)",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.half_normal.HalfNormal, give an example?",
        "Y": ">>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])",
        "Z": "Creates a half-normal distribution parameterized by scale where: >>> m = HalfNormal(torch.tensor([1.0]))\n>>> m.sample()  # half-normal distributed with scale=1\ntensor([ 0.1046])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use This is mainly useful for changing the shape of the result of\nlog_prob(). For example to create a diagonal Normal distribution with\nthe same shape as a Multivariate Normal distribution (so they are\ninterchangeable), you can:, give an example?",
        "Y": ">>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size((3,)), torch.Size(())]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size(()), torch.Size((3,))]",
        "Z": "Reinterprets some of the batch dims of a distribution as event dims.This is mainly useful for changing the shape of the result of\nlog_prob(). For example to create a diagonal Normal distribution with\nthe same shape as a Multivariate Normal distribution (so they are\ninterchangeable), you can: >>> loc = torch.zeros(3)\n>>> scale = torch.ones(3)\n>>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))\n>>> [mvn.batch_shape, mvn.event_shape]\n[torch.Size(()), torch.Size((3,))]\n>>> normal = Normal(loc, scale)\n>>> [normal.batch_shape, normal.event_shape]\n[torch.Size((3,)), torch.Size(())]\n>>> diagn = Independent(normal, 1)\n>>> [diagn.batch_shape, diagn.event_shape]\n[torch.Size(()), torch.Size((3,))]",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.kumaraswamy.Kumaraswamy, give an example?",
        "Y": ">>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])",
        "Z": "Samples from a Kumaraswamy distribution. >>> m = Kumaraswamy(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1\ntensor([ 0.1729])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.lkj_cholesky.LKJCholesky, give an example?",
        "Y": ">>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])",
        "Z": "LKJ distribution for lower Cholesky factor of correlation matrices.\nThe distribution is controlled by concentration parameter \u03b7\\eta\u03b7\nto make the probability of the correlation matrix MMM generated from\na Cholesky factor propotional to det\u2061(M)\u03b7\u22121\\det(M)^{\\eta - 1}det(M)\u03b7\u22121. Because of that,\nwhen concentration == 1, we have a uniform distribution over Cholesky\nfactors of correlation matrices. Note that this distribution samples the\nCholesky factor of correlation matrices and not the correlation matrices\nthemselves and thereby differs slightly from the derivations in [1] for\nthe LKJCorr distribution. For sampling, this uses the Onion method from\n[1] Section 3. >>> l = LKJCholesky(3, 0.5)\n>>> l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix\ntensor([[ 1.0000,  0.0000,  0.0000],\n        [ 0.3516,  0.9361,  0.0000],\n        [-0.1899,  0.4748,  0.8593]])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.laplace.Laplace, give an example?",
        "Y": ">>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])",
        "Z": "Creates a Laplace distribution parameterized by loc and scale. >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # Laplace distributed with loc=0, scale=1\ntensor([ 0.1046])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use Creates a log-normal distribution parameterized by\nloc and scale where:, give an example?",
        "Y": "X ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)",
        "Z": "Creates a log-normal distribution parameterized by\nloc and scale where: X ~ Normal(loc, scale)\nY = exp(X) ~ LogNormal(loc, scale)",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.log_normal.LogNormal, give an example?",
        "Y": ">>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])",
        "Z": "Creates a log-normal distribution parameterized by\nloc and scale where: >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # log-normal distributed with mean=0 and stddev=1\ntensor([ 0.1046])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use Creates a multivariate normal distribution with covariance matrix having a low-rank form\nparameterized by cov_factor and cov_diag:, give an example?",
        "Y": "covariance_matrix = cov_factor @ cov_factor.T + cov_diag",
        "Z": "Creates a multivariate normal distribution with covariance matrix having a low-rank form\nparameterized by cov_factor and cov_diag: covariance_matrix = cov_factor @ cov_factor.T + cov_diag",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal, give an example?",
        "Y": ">>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])",
        "Z": "Creates a multivariate normal distribution with covariance matrix having a low-rank form\nparameterized by cov_factor and cov_diag: >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`\ntensor([-0.2102, -0.5429])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use The computation for determinant and inverse of covariance matrix is avoided when\ncov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and\nmatrix determinant lemma.\nThanks to these formulas, we just need to compute the determinant and inverse of\nthe small size \u201ccapacitance\u201d matrix:, give an example?",
        "Y": "capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor",
        "Z": "The computation for determinant and inverse of covariance matrix is avoided when\ncov_factor.shape[1] << cov_factor.shape[0] thanks to Woodbury matrix identity and\nmatrix determinant lemma.\nThanks to these formulas, we just need to compute the determinant and inverse of\nthe small size \u201ccapacitance\u201d matrix: capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.mixture_same_family.MixtureSameFamily, give an example?",
        "Y": "# Construct Gaussian Mixture Model in 1D consisting of 5 equally\n# weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct Gaussian Mixture Modle in 2D consisting of 5 equally\n# weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n             torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct a batch of 3 Gaussian Mixture Models in 2D each\n# consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n            torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)",
        "Z": "The MixtureSameFamily distribution implements a (batch of) mixture\ndistribution where all component are from different parameterizations of\nthe same distribution type. It is parameterized by a Categorical\n\u201cselecting distribution\u201d (over k component) and a component\ndistribution, i.e., a Distribution with a rightmost batch shape\n(equal to [k]) which indexes each (batch of) component. # Construct Gaussian Mixture Model in 1D consisting of 5 equally\n# weighted normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Normal(torch.randn(5,), torch.rand(5,))\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct Gaussian Mixture Modle in 2D consisting of 5 equally\n# weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.ones(5,))\n>>> comp = D.Independent(D.Normal(\n             torch.randn(5,2), torch.rand(5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)\n\n# Construct a batch of 3 Gaussian Mixture Models in 2D each\n# consisting of 5 random weighted bivariate normal distributions\n>>> mix = D.Categorical(torch.rand(3,5))\n>>> comp = D.Independent(D.Normal(\n            torch.randn(3,5,2), torch.rand(3,5,2)), 1)\n>>> gmm = MixtureSameFamily(mix, comp)",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.multinomial.Multinomial, give an example?",
        "Y": ">>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])",
        "Z": "Note that total_count need not be specified if only log_prob() is\ncalled (see example below) >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))\n>>> x = m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 21.,  24.,  30.,  25.])\n\n>>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)\ntensor([-4.1338])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.multivariate_normal.MultivariateNormal, give an example?",
        "Y": ">>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])",
        "Z": "The multivariate normal distribution can be parameterized either\nin terms of a positive definite covariance matrix \u03a3\\mathbf{\\Sigma}\u03a3\nor a positive definite precision matrix \u03a3\u22121\\mathbf{\\Sigma}^{-1}\u03a3\u22121\nor a lower-triangular matrix L\\mathbf{L}L with positive-valued\ndiagonal entries, such that\n\u03a3=LL\u22a4\\mathbf{\\Sigma} = \\mathbf{L}\\mathbf{L}^\\top\u03a3=LL\u22a4. This triangular matrix\ncan be obtained via e.g. Cholesky decomposition of the covariance. >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))\n>>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`\ntensor([-0.2102, -0.5429])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.normal.Normal, give an example?",
        "Y": ">>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])",
        "Z": "Creates a normal (also called Gaussian) distribution parameterized by\nloc and scale. >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n>>> m.sample()  # normally distributed with loc=0 and scale=1\ntensor([ 0.1046])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.one_hot_categorical.OneHotCategorical, give an example?",
        "Y": ">>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])",
        "Z": "See also: torch.distributions.Categorical() for specifications of\nprobs and logits. >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n>>> m.sample()  # equal probability of 0, 1, 2, 3\ntensor([ 0.,  0.,  0.,  1.])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.pareto.Pareto, give an example?",
        "Y": ">>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])",
        "Z": "Samples from a Pareto Type 1 distribution. >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1\ntensor([ 1.5623])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.poisson.Poisson, give an example?",
        "Y": ">>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])",
        "Z": "Samples are nonnegative integers, with a pmf given by >>> m = Poisson(torch.tensor([4]))\n>>> m.sample()\ntensor([ 3.])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.relaxed_bernoulli.RelaxedBernoulli, give an example?",
        "Y": ">>> m = RelaxedBernoulli(torch.tensor([2.2]),\n                         torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])",
        "Z": "Creates a RelaxedBernoulli distribution, parametrized by\ntemperature, and either probs or logits\n(but not both). This is a relaxed version of the Bernoulli distribution,\nso the values are in (0, 1), and has reparametrizable samples. >>> m = RelaxedBernoulli(torch.tensor([2.2]),\n                         torch.tensor([0.1, 0.2, 0.3, 0.99]))\n>>> m.sample()\ntensor([ 0.2951,  0.3442,  0.8918,  0.9021])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.relaxed_categorical.RelaxedOneHotCategorical, give an example?",
        "Y": ">>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])",
        "Z": "Creates a RelaxedOneHotCategorical distribution parametrized by\ntemperature, and either probs or logits.\nThis is a relaxed version of the OneHotCategorical distribution, so\nits samples are on simplex, and are reparametrizable. >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),\n                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))\n>>> m.sample()\ntensor([ 0.1294,  0.2324,  0.3859,  0.2523])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.studentT.StudentT, give an example?",
        "Y": ">>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])",
        "Z": "Creates a Student\u2019s t-distribution parameterized by degree of\nfreedom df, mean loc and scale scale. >>> m = StudentT(torch.tensor([2.0]))\n>>> m.sample()  # Student's t-distributed with degrees of freedom=2\ntensor([ 0.1046])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use Extension of the Distribution class, which applies a sequence of Transforms\nto a base distribution.  Let f be the composition of transforms applied:, give an example?",
        "Y": "X ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|",
        "Z": "Extension of the Distribution class, which applies a sequence of Transforms\nto a base distribution.  Let f be the composition of transforms applied: X ~ BaseDistribution\nY = f(X) ~ TransformedDistribution(BaseDistribution, f)\nlog p(Y) = log p(X) + log |det (dX/dY)|",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use An example for the usage of TransformedDistribution would be:, give an example?",
        "Y": "# Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)",
        "Z": "Note that the .event_shape of a TransformedDistribution is the\nmaximum shape of its base distribution and its transforms, since transforms\ncan introduce correlations among events.An example for the usage of TransformedDistribution would be: # Building a Logistic Distribution\n# X ~ Uniform(0, 1)\n# f = a + b * logit(X)\n# Y ~ f(X) ~ Logistic(a, b)\nbase_distribution = Uniform(0, 1)\ntransforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]\nlogistic = TransformedDistribution(base_distribution, transforms)",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.uniform.Uniform, give an example?",
        "Y": ">>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])",
        "Z": "Generates uniformly distributed random samples from the half-open interval\n[low, high). >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))\n>>> m.sample()  # uniformly distributed in the range [0.0, 5.0)\ntensor([ 2.3418])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use VonMises, give an example?",
        "Y": ">>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample() # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])",
        "Z": ">>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample() # von Mises distributed with loc=1 and concentration=1\ntensor([1.9777])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.weibull.Weibull, give an example?",
        "Y": ">>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])",
        "Z": "Samples from a two-parameter Weibull distribution. >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))\n>>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1\ntensor([ 0.4784])",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.kl.register_kl, give an example?",
        "Y": "@register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here",
        "Z": "Decorator to register a pairwise function with kl_divergence().\nUsage: @register_kl(Normal, Normal)\ndef kl_normal_normal(p, q):\n    # insert implementation here",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How  Lookup returns the most specific (type,type) match ordered by subclass. If\nthe match is ambiguous, a RuntimeWarning is raised. For example to\nresolve the ambiguous situation:, give an example?",
        "Y": "@register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...",
        "Z": "Lookup returns the most specific (type,type) match ordered by subclass. If\nthe match is ambiguous, a RuntimeWarning is raised. For example to\nresolve the ambiguous situation: @register_kl(BaseP, DerivedQ)\ndef kl_version1(p, q): ...\n@register_kl(DerivedP, BaseQ)\ndef kl_version2(p, q): ...",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How  Lookup returns the most specific (type,type) match ordered by subclass. If\nthe match is ambiguous, a RuntimeWarning is raised. For example to\nresolve the ambiguous situation:you should register a third most-specific implementation, e.g.:, give an example?",
        "Y": "register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.",
        "Z": "Lookup returns the most specific (type,type) match ordered by subclass. If\nthe match is ambiguous, a RuntimeWarning is raised. For example to\nresolve the ambiguous situation:you should register a third most-specific implementation, e.g.: register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use Caching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values\nsince the autograd graph may be reversed. For example while the following\nworks with or without caching:, give an example?",
        "Y": "y = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.",
        "Z": "Caching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values\nsince the autograd graph may be reversed. For example while the following\nworks with or without caching: y = t(x)\nt.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use However the following will error when caching due to dependency reversal:, give an example?",
        "Y": "y = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x",
        "Z": "Caching is useful for transforms whose inverses are either expensive or\nnumerically unstable. Note that care must be taken with memoized values\nsince the autograd graph may be reversed. For example while the following\nworks with or without caching:However the following will error when caching due to dependency reversal: y = t(x)\nz = t.inv(y)\ngrad(z.sum(), [y])  # error because z is x",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use PyTorch provides two global ConstraintRegistry objects that link\nConstraint objects to\nTransform objects. These objects both\ninput constraints and return transforms, but they have different guarantees on\nbijectivity.The transform_to() registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s .arg_constraints dict. These transforms often\noverparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam:, give an example?",
        "Y": "loc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints['scale'])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum()",
        "Z": "The transform_to() registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s .arg_constraints dict. These transforms often\noverparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam: loc = torch.zeros(100, requires_grad=True)\nunconstrained = torch.zeros(100, requires_grad=True)\nscale = transform_to(Normal.arg_constraints['scale'])(unconstrained)\nloss = -Normal(loc, scale).log_prob(data).sum()",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use The transform_to() registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s .arg_constraints dict. These transforms often\noverparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam:The biject_to() registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained .support are\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.:, give an example?",
        "Y": "dist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum()",
        "Z": "The transform_to() registry is useful for performing unconstrained\noptimization on constrained parameters of probability distributions, which are\nindicated by each distribution\u2019s .arg_constraints dict. These transforms often\noverparameterize a space in order to avoid rotation; they are thus more\nsuitable for coordinate-wise optimization algorithms like Adam:The biject_to() registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained .support are\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.: dist = Exponential(rate)\nunconstrained = torch.zeros(100, requires_grad=True)\nsample = biject_to(dist.support)(unconstrained)\npotential_energy = -dist.log_prob(sample).sum()",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use The biject_to() registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained .support are\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.:The biject_to and transform_to objects can be extended by user-defined\nconstraints and transforms using their .register() method either as a\nfunction on singleton constraints:, give an example?",
        "Y": "transform_to.register(my_constraint, my_transform)",
        "Z": "The biject_to() registry is useful for Hamiltonian Monte Carlo, where\nsamples from a probability distribution with constrained .support are\npropagated in an unconstrained space, and algorithms are typically rotation\ninvariant.:The biject_to and transform_to objects can be extended by user-defined\nconstraints and transforms using their .register() method either as a\nfunction on singleton constraints: transform_to.register(my_constraint, my_transform)",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use The biject_to and transform_to objects can be extended by user-defined\nconstraints and transforms using their .register() method either as a\nfunction on singleton constraints:or as a decorator on parameterized constraints:, give an example?",
        "Y": "@transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2)",
        "Z": "The biject_to and transform_to objects can be extended by user-defined\nconstraints and transforms using their .register() method either as a\nfunction on singleton constraints:or as a decorator on parameterized constraints: @transform_to.register(MyConstraintClass)\ndef my_factory(constraint):\n    assert isinstance(constraint, MyConstraintClass)\n    return MyTransform(constraint.param1, constraint.param2)",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.distributions.constraint_registry.ConstraintRegistry.register, give an example?",
        "Y": "@my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)",
        "Z": "Registers a Constraint\nsubclass in this registry. Usage: @my_registry.register(MyConstraintClass)\ndef construct_transform(constraint):\n    assert isinstance(constraint, MyConstraint)\n    return MyTransform(constraint.arg_constraints)",
        "source": "https://pytorch.org/docs/stable/distributions.html"
    },
    {
        "X": "How to use torch.quasirandom.SobolEngine, give an example?",
        "Y": ">>> soboleng = torch.quasirandom.SobolEngine(dimension=5)\n>>> soboleng.draw(3)\ntensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],\n        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])",
        "Z": "References >>> soboleng = torch.quasirandom.SobolEngine(dimension=5)\n>>> soboleng.draw(3)\ntensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],\n        [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],\n        [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.quasirandom.SobolEngine.html#torch.quasirandom.SobolEngine"
    },
    {
        "X": "How to use torch.unique_consecutive, give an example?",
        "Y": ">>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\n>>> output = torch.unique_consecutive(x)\n>>> output\ntensor([1, 2, 3, 1, 2])\n\n>>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> inverse_indices\ntensor([0, 0, 1, 1, 2, 3, 3, 4])\n\n>>> output, counts = torch.unique_consecutive(x, return_counts=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> counts\ntensor([2, 2, 1, 2, 1])",
        "Z": ">>> x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])\n>>> output = torch.unique_consecutive(x)\n>>> output\ntensor([1, 2, 3, 1, 2])\n\n>>> output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> inverse_indices\ntensor([0, 0, 1, 1, 2, 3, 3, 4])\n\n>>> output, counts = torch.unique_consecutive(x, return_counts=True)\n>>> output\ntensor([1, 2, 3, 1, 2])\n>>> counts\ntensor([2, 2, 1, 2, 1])",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique_consecutive.html#torch.unique_consecutive"
    },
    {
        "X": "How to use torch.signbit, give an example?",
        "Y": ">>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> torch.signbit(a)\ntensor([ False, True,  False,  False])",
        "Z": ">>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> torch.signbit(a)\ntensor([ False, True,  False,  False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit"
    },
    {
        "X": "How to use torch.isinf, give an example?",
        "Y": ">>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])",
        "Z": ">>> torch.isinf(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([False,  True,  False,  True,  False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf"
    },
    {
        "X": "How to use torch.log1p, give an example?",
        "Y": ">>> a = torch.randn(5)\n>>> a\ntensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\n>>> torch.log1p(a)\ntensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])",
        "Z": ">>> a = torch.randn(5)\n>>> a\ntensor([-1.0090, -0.9923,  1.0249, -0.5372,  0.2492])\n>>> torch.log1p(a)\ntensor([    nan, -4.8653,  0.7055, -0.7705,  0.2225])",
        "source": "https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p"
    },
    {
        "X": "How to use torch.randperm, give an example?",
        "Y": ">>> torch.randperm(4)\ntensor([2, 1, 0, 3])",
        "Z": ">>> torch.randperm(4)\ntensor([2, 1, 0, 3])",
        "source": "https://pytorch.org/docs/stable/generated/torch.randperm.html#torch.randperm"
    },
    {
        "X": "How to use torch.diff, give an example?",
        "Y": ">>> a = torch.tensor([1, 3, 2])\n>>> torch.diff(a)\ntensor([ 2, -1])\n>>> b = torch.tensor([4, 5])\n>>> torch.diff(a, append=b)\ntensor([ 2, -1,  2,  1])\n>>> c = torch.tensor([[1, 2, 3], [3, 4, 5]])\n>>> torch.diff(c, dim=0)\ntensor([[2, 2, 2]])\n>>> torch.diff(c, dim=1)\ntensor([[1, 1],\n        [1, 1]])",
        "Z": "The first-order differences are given by out[i] = input[i + 1] - input[i]. Higher-order\ndifferences are calculated by using torch.diff() recursively. >>> a = torch.tensor([1, 3, 2])\n>>> torch.diff(a)\ntensor([ 2, -1])\n>>> b = torch.tensor([4, 5])\n>>> torch.diff(a, append=b)\ntensor([ 2, -1,  2,  1])\n>>> c = torch.tensor([[1, 2, 3], [3, 4, 5]])\n>>> torch.diff(c, dim=0)\ntensor([[2, 2, 2]])\n>>> torch.diff(c, dim=1)\ntensor([[1, 1],\n        [1, 1]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff"
    },
    {
        "X": "How to use torch.bernoulli, give an example?",
        "Y": ">>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n>>> a\ntensor([[ 0.1737,  0.0950,  0.3609],\n        [ 0.7148,  0.0289,  0.2676],\n        [ 0.9456,  0.8937,  0.7202]])\n>>> torch.bernoulli(a)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 1.,  1.,  1.]])\n\n>>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n>>> torch.bernoulli(a)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n>>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n>>> torch.bernoulli(a)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])",
        "Z": "out can have integral dtype, but input must have floating\npoint dtype. >>> a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]\n>>> a\ntensor([[ 0.1737,  0.0950,  0.3609],\n        [ 0.7148,  0.0289,  0.2676],\n        [ 0.9456,  0.8937,  0.7202]])\n>>> torch.bernoulli(a)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 1.,  1.,  1.]])\n\n>>> a = torch.ones(3, 3) # probability of drawing \"1\" is 1\n>>> torch.bernoulli(a)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n>>> a = torch.zeros(3, 3) # probability of drawing \"1\" is 0\n>>> torch.bernoulli(a)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.bernoulli.html#torch.bernoulli"
    },
    {
        "X": "How to use torch.amin, give an example?",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.6451, -0.4866,  0.2987, -1.3312],\n        [-0.5744,  1.2980,  1.8397, -0.2713],\n        [ 0.9128,  0.9214, -1.7268, -0.2995],\n        [ 0.9023,  0.4853,  0.9075, -1.6165]])\n>>> torch.amin(a, 1)\ntensor([-1.3312, -0.5744, -1.7268, -1.6165])",
        "Z": "If keepdim is True, the output tensors are of the same size as\ninput except in the dimension(s) dim where they are of size 1.\nOtherwise, dim`s are squeezed (see :func:`torch.squeeze), resulting in\nthe output tensors having fewer dimensions than input. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.6451, -0.4866,  0.2987, -1.3312],\n        [-0.5744,  1.2980,  1.8397, -0.2713],\n        [ 0.9128,  0.9214, -1.7268, -0.2995],\n        [ 0.9023,  0.4853,  0.9075, -1.6165]])\n>>> torch.amin(a, 1)\ntensor([-1.3312, -0.5744, -1.7268, -1.6165])",
        "source": "https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin"
    },
    {
        "X": "How to use torch.equal, give an example?",
        "Y": ">>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))\nTrue",
        "Z": ">>> torch.equal(torch.tensor([1, 2]), torch.tensor([1, 2]))\nTrue",
        "source": "https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal"
    },
    {
        "X": "How to use torch.logical_not, give an example?",
        "Y": ">>> torch.logical_not(torch.tensor([True, False]))\ntensor([False,  True])\n>>> torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))\ntensor([ True, False, False])\n>>> torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))\ntensor([ True, False, False])\n>>> torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))\ntensor([1, 0, 0], dtype=torch.int16)",
        "Z": ">>> torch.logical_not(torch.tensor([True, False]))\ntensor([False,  True])\n>>> torch.logical_not(torch.tensor([0, 1, -10], dtype=torch.int8))\ntensor([ True, False, False])\n>>> torch.logical_not(torch.tensor([0., 1.5, -10.], dtype=torch.double))\ntensor([ True, False, False])\n>>> torch.logical_not(torch.tensor([0., 1., -10.], dtype=torch.double), out=torch.empty(3, dtype=torch.int16))\ntensor([1, 0, 0], dtype=torch.int16)",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not"
    },
    {
        "X": "How to use torch.acos, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.3348, -0.5889,  0.2005, -0.1584])\n>>> torch.acos(a)\ntensor([ 1.2294,  2.2004,  1.3690,  1.7298])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.3348, -0.5889,  0.2005, -0.1584])\n>>> torch.acos(a)\ntensor([ 1.2294,  2.2004,  1.3690,  1.7298])",
        "source": "https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos"
    },
    {
        "X": "How to use torch.abs, give an example?",
        "Y": ">>> torch.abs(torch.tensor([-1, -2, 3]))\ntensor([ 1,  2,  3])",
        "Z": ">>> torch.abs(torch.tensor([-1, -2, 3]))\ntensor([ 1,  2,  3])",
        "source": "https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs"
    },
    {
        "X": "How to use torch.from_numpy, give an example?",
        "Y": ">>> a = numpy.array([1, 2, 3])\n>>> t = torch.from_numpy(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])",
        "Z": "It currently accepts ndarray with dtypes of numpy.float64,\nnumpy.float32, numpy.float16, numpy.complex64, numpy.complex128,\nnumpy.int64, numpy.int32, numpy.int16, numpy.int8, numpy.uint8,\nand numpy.bool. >>> a = numpy.array([1, 2, 3])\n>>> t = torch.from_numpy(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])",
        "source": "https://pytorch.org/docs/stable/generated/torch.from_numpy.html#torch.from_numpy"
    },
    {
        "X": "How to use torch.ravel, give an example?",
        "Y": ">>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.ravel(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])",
        "Z": ">>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.ravel(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])",
        "source": "https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel"
    },
    {
        "X": "How to use torch.sinc, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.2252, -0.2948,  1.0267, -1.1566])\n>>> torch.sinc(a)\ntensor([ 0.9186,  0.8631, -0.0259, -0.1300])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.2252, -0.2948,  1.0267, -1.1566])\n>>> torch.sinc(a)\ntensor([ 0.9186,  0.8631, -0.0259, -0.1300])",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc"
    },
    {
        "X": "How to use String to distinguish measurements with identical label and\nsub_label. The principal use of description is to signal to\nCompare the columns of data. For instance one might set it\nbased on the input size  to create a table of the form:, give an example?",
        "Y": "| n=1 | n=4 | ...\n                        ------------- ...\nReLU(x + 1): (float)    | ... | ... | ...\nReLU(x + 1): (int)      | ... | ... | ...",
        "Z": "String to distinguish measurements with identical label and\nsub_label. The principal use of description is to signal to\nCompare the columns of data. For instance one might set it\nbased on the input size  to create a table of the form:                         | n=1 | n=4 | ...\n                        ------------- ...\nReLU(x + 1): (float)    | ... | ... | ...\nReLU(x + 1): (int)      | ... | ... | ...",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How to use torch.utils.benchmark.Timer.blocked_autorange, give an example?",
        "Y": "`setup`\n\ntotal_time = 0\nwhile total_time < min_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    total_time += (timer() - start)",
        "Z": "At a high level, blocked_autorange executes the following pseudo-code: `setup`\n\ntotal_time = 0\nwhile total_time < min_run_time\n    start = timer()\n    for _ in range(block_size):\n        `stmt`\n    total_time += (timer() - start)",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How to use torch.utils.benchmark.CallgrindStats.as_standardized, give an example?",
        "Y": "23234231 /tmp/first_build_dir/thing.c:foo(...)\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\n  ...\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\n  ...\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\n-23234231 /tmp/second_build_dir/thing.c:foo(...)",
        "Z": "When comparing two different sets of instruction counts, on stumbling\nblock can be path prefixes. Callgrind includes the full filepath\nwhen reporting a function (as it should). However, this can cause\nissues when diffing profiles. If a key component such as Python\nor PyTorch was built in separate locations in the two profiles, which\ncan result in something resembling: 23234231 /tmp/first_build_dir/thing.c:foo(...)\n 9823794 /tmp/first_build_dir/thing.c:bar(...)\n  ...\n   53453 .../aten/src/Aten/...:function_that_actually_changed(...)\n  ...\n -9823794 /tmp/second_build_dir/thing.c:bar(...)\n-23234231 /tmp/second_build_dir/thing.c:foo(...)",
        "source": "https://pytorch.org/docs/stable/benchmark_utils.html"
    },
    {
        "X": "How to use torch.save, give an example?",
        "Y": ">>> # Save to file\n>>> x = torch.tensor([0, 1, 2, 3, 4])\n>>> torch.save(x, 'tensor.pt')\n>>> # Save to io.BytesIO buffer\n>>> buffer = io.BytesIO()\n>>> torch.save(x, buffer)",
        "Z": "See also: Saving and loading tensors >>> # Save to file\n>>> x = torch.tensor([0, 1, 2, 3, 4])\n>>> torch.save(x, 'tensor.pt')\n>>> # Save to io.BytesIO buffer\n>>> buffer = io.BytesIO()\n>>> torch.save(x, buffer)",
        "source": "https://pytorch.org/docs/stable/generated/torch.save.html#torch.save"
    },
    {
        "X": "How to use torch.set_grad_enabled, give an example?",
        "Y": ">>> x = torch.tensor([1], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> torch.set_grad_enabled(True)\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse",
        "Z": "This context manager is thread local; it will not affect computation\nin other threads. >>> x = torch.tensor([1], requires_grad=True)\n>>> is_train = False\n>>> with torch.set_grad_enabled(is_train):\n...   y = x * 2\n>>> y.requires_grad\nFalse\n>>> torch.set_grad_enabled(True)\n>>> y = x * 2\n>>> y.requires_grad\nTrue\n>>> torch.set_grad_enabled(False)\n>>> y = x * 2\n>>> y.requires_grad\nFalse",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_grad_enabled.html#torch.set_grad_enabled"
    },
    {
        "X": "How to use torch.combinations, give an example?",
        "Y": ">>> a = [1, 2, 3]\n>>> list(itertools.combinations(a, r=2))\n[(1, 2), (1, 3), (2, 3)]\n>>> list(itertools.combinations(a, r=3))\n[(1, 2, 3)]\n>>> list(itertools.combinations_with_replacement(a, r=2))\n[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n>>> tensor_a = torch.tensor(a)\n>>> torch.combinations(tensor_a)\ntensor([[1, 2],\n        [1, 3],\n        [2, 3]])\n>>> torch.combinations(tensor_a, r=3)\ntensor([[1, 2, 3]])\n>>> torch.combinations(tensor_a, with_replacement=True)\ntensor([[1, 1],\n        [1, 2],\n        [1, 3],\n        [2, 2],\n        [2, 3],\n        [3, 3]])",
        "Z": ">>> a = [1, 2, 3]\n>>> list(itertools.combinations(a, r=2))\n[(1, 2), (1, 3), (2, 3)]\n>>> list(itertools.combinations(a, r=3))\n[(1, 2, 3)]\n>>> list(itertools.combinations_with_replacement(a, r=2))\n[(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)]\n>>> tensor_a = torch.tensor(a)\n>>> torch.combinations(tensor_a)\ntensor([[1, 2],\n        [1, 3],\n        [2, 3]])\n>>> torch.combinations(tensor_a, r=3)\ntensor([[1, 2, 3]])\n>>> torch.combinations(tensor_a, with_replacement=True)\ntensor([[1, 1],\n        [1, 2],\n        [1, 3],\n        [2, 2],\n        [2, 3],\n        [3, 3]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.combinations.html#torch.combinations"
    },
    {
        "X": "How to use torch.mv, give an example?",
        "Y": ">>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.mv(mat, vec)\ntensor([ 1.0404, -0.6361])",
        "Z": "If input is a (n\u00d7m)(n \\times m)(n\u00d7m) tensor, vec is a 1-D tensor of\nsize mmm, out will be 1-D of size nnn. >>> mat = torch.randn(2, 3)\n>>> vec = torch.randn(3)\n>>> torch.mv(mat, vec)\ntensor([ 1.0404, -0.6361])",
        "source": "https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv"
    },
    {
        "X": "How to use torch.amax, give an example?",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.8177,  1.4878, -0.2491,  0.9130],\n        [-0.7158,  1.1775,  2.0992,  0.4817],\n        [-0.0053,  0.0164, -1.3738, -0.0507],\n        [ 1.9700,  1.1106, -1.0318, -1.0816]])\n>>> torch.amax(a, 1)\ntensor([1.4878, 2.0992, 0.0164, 1.9700])",
        "Z": "If keepdim is ``True`, the output tensors are of the same size\nas input except in the dimension(s) dim where they are of size 1.\nOtherwise, dim`s are squeezed (see :func:`torch.squeeze), resulting\nin the output tensors having fewer dimension than input. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.8177,  1.4878, -0.2491,  0.9130],\n        [-0.7158,  1.1775,  2.0992,  0.4817],\n        [-0.0053,  0.0164, -1.3738, -0.0507],\n        [ 1.9700,  1.1106, -1.0318, -1.0816]])\n>>> torch.amax(a, 1)\ntensor([1.4878, 2.0992, 0.0164, 1.9700])",
        "source": "https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax"
    },
    {
        "X": "How to use torch.numel, give an example?",
        "Y": ">>> a = torch.randn(1, 2, 3, 4, 5)\n>>> torch.numel(a)\n120\n>>> a = torch.zeros(4,4)\n>>> torch.numel(a)\n16",
        "Z": ">>> a = torch.randn(1, 2, 3, 4, 5)\n>>> torch.numel(a)\n120\n>>> a = torch.zeros(4,4)\n>>> torch.numel(a)\n16",
        "source": "https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel"
    },
    {
        "X": "How to use torch.logspace, give an example?",
        "Y": ">>> torch.logspace(start=-10, end=10, steps=5)\ntensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])\n>>> torch.logspace(start=0.1, end=1.0, steps=5)\ntensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])\n>>> torch.logspace(start=0.1, end=1.0, steps=1)\ntensor([1.2589])\n>>> torch.logspace(start=2, end=2, steps=1, base=2)\ntensor([4.0])",
        "Z": ">>> torch.logspace(start=-10, end=10, steps=5)\ntensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])\n>>> torch.logspace(start=0.1, end=1.0, steps=5)\ntensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])\n>>> torch.logspace(start=0.1, end=1.0, steps=1)\ntensor([1.2589])\n>>> torch.logspace(start=2, end=2, steps=1, base=2)\ntensor([4.0])",
        "source": "https://pytorch.org/docs/stable/generated/torch.logspace.html#torch.logspace"
    },
    {
        "X": "How to use torch.acosh, give an example?",
        "Y": ">>> a = torch.randn(4).uniform_(1, 2)\n>>> a\ntensor([ 1.3192, 1.9915, 1.9674, 1.7151 ])\n>>> torch.acosh(a)\ntensor([ 0.7791, 1.3120, 1.2979, 1.1341 ])",
        "Z": ">>> a = torch.randn(4).uniform_(1, 2)\n>>> a\ntensor([ 1.3192, 1.9915, 1.9674, 1.7151 ])\n>>> torch.acosh(a)\ntensor([ 0.7791, 1.3120, 1.2979, 1.1341 ])",
        "source": "https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh"
    },
    {
        "X": "How to use The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:, give an example?",
        "Y": "$ unzip my_package.pt && tree my_package\nmy_package\n\u251c\u2500\u2500 .data\n\u2502   \u251c\u2500\u2500 94304870911616.storage\n\u2502   \u251c\u2500\u2500 94304900784016.storage\n\u2502   \u251c\u2500\u2500 extern_modules\n\u2502   \u2514\u2500\u2500 version\n\u251c\u2500\u2500 models\n\u2502   \u2514\u2500\u2500 model_1.pkl\n\u2514\u2500\u2500 torchvision\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py\n        \u2514\u2500\u2500 utils.py\n~ cd my_package && cat torchvision/models/resnet.py\n...",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: $ unzip my_package.pt && tree my_package\nmy_package\n\u251c\u2500\u2500 .data\n\u2502   \u251c\u2500\u2500 94304870911616.storage\n\u2502   \u251c\u2500\u2500 94304900784016.storage\n\u2502   \u251c\u2500\u2500 extern_modules\n\u2502   \u2514\u2500\u2500 version\n\u251c\u2500\u2500 models\n\u2502   \u2514\u2500\u2500 model_1.pkl\n\u2514\u2500\u2500 torchvision\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py\n        \u2514\u2500\u2500 utils.py\n~ cd my_package && cat torchvision/models/resnet.py\n...",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How  The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files:, give an example?",
        "Y": "# add this to your .vimrc to treat `*.pt` files as zip files\nau BufReadCmd *.pt call zip#Browse(expand(\"<amatch>\"))\n\n~ vi my_package.pt",
        "Z": "The container format for a torch.package is ZIP, so any tools that work with standard ZIP files should\nwork for exploring the contents. Some common ways to interact with ZIP files: # add this to your .vimrc to treat `*.pt` files as zip files\nau BufReadCmd *.pt call zip#Browse(expand(\"<amatch>\"))\n\n~ vi my_package.pt",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use PackageImporter and PackageExporter provide a file_structure() method, which will return a printable\nand queryable Folder object. The Folder object is a simple directory structure that you can use to explore the\ncurrent contents of a torch.package.The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-style include and exclude filtering arguments., give an example?",
        "Y": "with PackageExporter('my_package.pt', verbose=False) as pe:\n    pe.save_pickle('models', 'model_1.pkl', mod)\n    # can limit printed items with include/exclude args\n    print(pe.file_structure(include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storages\"))\n\nimporter = PackageImporter('my_package.pt')\nprint(importer.file_structure()) # will print out all files",
        "Z": "The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-style include and exclude filtering arguments. with PackageExporter('my_package.pt', verbose=False) as pe:\n    pe.save_pickle('models', 'model_1.pkl', mod)\n    # can limit printed items with include/exclude args\n    print(pe.file_structure(include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storages\"))\n\nimporter = PackageImporter('my_package.pt')\nprint(importer.file_structure()) # will print out all files",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-style include and exclude filtering arguments.Output:, give an example?",
        "Y": "# filtered with glob pattern:\n#    include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storages\"\n\u2500\u2500\u2500 my_package.pt\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 model_1.pkl\n    \u2514\u2500\u2500 torchvision\n        \u2514\u2500\u2500 models\n            \u2514\u2500\u2500 utils.py\n\n# all files\n\u2500\u2500\u2500 my_package.pt\n    \u251c\u2500\u2500 .data\n    \u2502   \u251c\u2500\u2500 94304870911616.storage\n    \u2502   \u251c\u2500\u2500 94304900784016.storage\n    \u2502   \u251c\u2500\u2500 extern_modules\n    \u2502   \u2514\u2500\u2500 version\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 model_1.pkl\n    \u2514\u2500\u2500 torchvision\n        \u2514\u2500\u2500 models\n            \u251c\u2500\u2500 resnet.py\n            \u2514\u2500\u2500 utils.py",
        "Z": "The Folder object itself is directly printable and will print out a file tree representation. To filter what is returned,\nuse the glob-style include and exclude filtering arguments.Output: # filtered with glob pattern:\n#    include=[\"**/utils.py\", \"**/*.pkl\"], exclude=\"**/*.storages\"\n\u2500\u2500\u2500 my_package.pt\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 model_1.pkl\n    \u2514\u2500\u2500 torchvision\n        \u2514\u2500\u2500 models\n            \u2514\u2500\u2500 utils.py\n\n# all files\n\u2500\u2500\u2500 my_package.pt\n    \u251c\u2500\u2500 .data\n    \u2502   \u251c\u2500\u2500 94304870911616.storage\n    \u2502   \u251c\u2500\u2500 94304900784016.storage\n    \u2502   \u251c\u2500\u2500 extern_modules\n    \u2502   \u2514\u2500\u2500 version\n    \u251c\u2500\u2500 models\n    \u2502   \u2514\u2500\u2500 model_1.pkl\n    \u2514\u2500\u2500 torchvision\n        \u2514\u2500\u2500 models\n            \u251c\u2500\u2500 resnet.py\n            \u2514\u2500\u2500 utils.py",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use Output:You can also query Folder objects with the has_file() method., give an example?",
        "Y": "exporter_file_structure = exporter.file_structure()\nfound: bool = exporter_file_structure.has_file(\"package_a/subpackage.py\")",
        "Z": "Output:You can also query Folder objects with the has_file() method. exporter_file_structure = exporter.file_structure()\nfound: bool = exporter_file_structure.has_file(\"package_a/subpackage.py\")",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use PackageExporter exposes three methods, save_pickle, save_text and save_binary that allow you to save\nPython objects, text, and binary data to a package., give an example?",
        "Y": "with torch.PackageExporter(\"package.pt\") as exporter:\n    # Pickles the object and saves to `my_resources/tens.pkl` in the archive.\n    exporter.save_pickle(\"my_resources\", \"tensor.pkl\", torch.randn(4))\n    exporter.save_text(\"config_stuff\", \"words.txt\", \"a sample string\")\n    exporter.save_binary(\"raw_data\", \"binary\", my_bytes)",
        "Z": "PackageExporter exposes three methods, save_pickle, save_text and save_binary that allow you to save\nPython objects, text, and binary data to a package. with torch.PackageExporter(\"package.pt\") as exporter:\n    # Pickles the object and saves to `my_resources/tens.pkl` in the archive.\n    exporter.save_pickle(\"my_resources\", \"tensor.pkl\", torch.randn(4))\n    exporter.save_text(\"config_stuff\", \"words.txt\", \"a sample string\")\n    exporter.save_binary(\"raw_data\", \"binary\", my_bytes)",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use PackageExporter exposes three methods, save_pickle, save_text and save_binary that allow you to save\nPython objects, text, and binary data to a package.PackageImporter exposes complementary methods named load_pickle, load_text and load_binary that allow you to load\nPython objects, text and binary data from a package., give an example?",
        "Y": "importer = torch.PackageImporter(\"package.pt\")\nmy_tensor = importer.load_pickle(\"my_resources\", \"tensor.pkl\")\ntext = importer.load_text(\"config_stuff\", \"words.txt\")\nbinary = importer.load_binary(\"raw_data\", \"binary\")",
        "Z": "PackageImporter exposes complementary methods named load_pickle, load_text and load_binary that allow you to load\nPython objects, text and binary data from a package. importer = torch.PackageImporter(\"package.pt\")\nmy_tensor = importer.load_pickle(\"my_resources\", \"tensor.pkl\")\ntext = importer.load_text(\"config_stuff\", \"words.txt\")\nbinary = importer.load_binary(\"raw_data\", \"binary\")",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use torch.package allows for the customization of how classes are packaged. This behavior is accessed through defining the method\n__reduce_package__ on a class and by defining a corresponding de-packaging function. This is similar to defining __reduce__ for\nPython\u2019s normal pickling process.Steps:, give an example?",
        "Y": "# foo.py [Example of customizing how class Foo is packaged]\nfrom torch.package import PackageExporter, PackageImporter\nimport time\n\n\nclass Foo:\n    def __init__(self, my_string: str):\n        super().__init__()\n        self.my_string = my_string\n        self.time_imported = 0\n        self.time_exported = 0\n\n    def __reduce_package__(self, exporter: PackageExporter):\n        \"\"\"\n        Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when\n        saving an instance of this object. This method should do the work to save this\n        object inside of the ``torch.package`` archive.\n\n        Returns function w/ arguments to load the object from a\n        ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.\n        \"\"\"\n\n        # use this pattern to ensure no naming conflicts with normal dependencies,\n        # anything saved under this module name shouldn't conflict with other\n        # items in the package\n        generated_module_name = f\"foo-generated._{exporter.get_unique_id()}\"\n        exporter.save_text(\n            generated_module_name,\n            \"foo.txt\",\n            self.my_string + \", with exporter modification!\",\n        )\n        time_exported = time.clock_gettime(1)\n\n        # returns de-packaging function w/ arguments to invoke with\n        return (unpackage_foo, (generated_module_name, time_exported,))\n\n\ndef unpackage_foo(\n    importer: PackageImporter, generated_module_name: str, time_exported: float\n) -> Foo:\n    \"\"\"\n    Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function\n    when depickling a Foo object.\n    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.\n    \"\"\"\n    time_imported = time.clock_gettime(1)\n    foo = Foo(importer.load_text(generated_module_name, \"foo.txt\"))\n    foo.time_imported = time_imported\n    foo.time_exported = time_exported\n    return foo",
        "Z": "Steps: # foo.py [Example of customizing how class Foo is packaged]\nfrom torch.package import PackageExporter, PackageImporter\nimport time\n\n\nclass Foo:\n    def __init__(self, my_string: str):\n        super().__init__()\n        self.my_string = my_string\n        self.time_imported = 0\n        self.time_exported = 0\n\n    def __reduce_package__(self, exporter: PackageExporter):\n        \"\"\"\n        Called by ``torch.package.PackageExporter``'s Pickler's ``persistent_id`` when\n        saving an instance of this object. This method should do the work to save this\n        object inside of the ``torch.package`` archive.\n\n        Returns function w/ arguments to load the object from a\n        ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function.\n        \"\"\"\n\n        # use this pattern to ensure no naming conflicts with normal dependencies,\n        # anything saved under this module name shouldn't conflict with other\n        # items in the package\n        generated_module_name = f\"foo-generated._{exporter.get_unique_id()}\"\n        exporter.save_text(\n            generated_module_name,\n            \"foo.txt\",\n            self.my_string + \", with exporter modification!\",\n        )\n        time_exported = time.clock_gettime(1)\n\n        # returns de-packaging function w/ arguments to invoke with\n        return (unpackage_foo, (generated_module_name, time_exported,))\n\n\ndef unpackage_foo(\n    importer: PackageImporter, generated_module_name: str, time_exported: float\n) -> Foo:\n    \"\"\"\n    Called by ``torch.package.PackageImporter``'s Pickler's ``persistent_load`` function\n    when depickling a Foo object.\n    Performs work of loading and returning a Foo instance from a ``torch.package`` archive.\n    \"\"\"\n    time_imported = time.clock_gettime(1)\n    foo = Foo(importer.load_text(generated_module_name, \"foo.txt\"))\n    foo.time_imported = time_imported\n    foo.time_exported = time_exported\n    return foo",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How  Steps:, give an example?",
        "Y": "# output of running above script\n\u2500\u2500\u2500 foo_package\n    \u251c\u2500\u2500 foo-generated\n    \u2502   \u251c\u2500\u2500 _0\n    \u2502   \u2502   \u2514\u2500\u2500 foo.txt\n    \u2502   \u2514\u2500\u2500 _1\n    \u2502       \u2514\u2500\u2500 foo.txt\n    \u251c\u2500\u2500 foo_collection\n    \u2502   \u251c\u2500\u2500 foo1.pkl\n    \u2502   \u2514\u2500\u2500 foo2.pkl\n    \u2514\u2500\u2500 foo.py\n\nfoo_1 string: 'foo_1 initial string, with reduction modification!'\nfoo_1 export time: 9857706.650140837\nfoo_1 import time: 9857706.652698385",
        "Z": "Steps: # output of running above script\n\u2500\u2500\u2500 foo_package\n    \u251c\u2500\u2500 foo-generated\n    \u2502   \u251c\u2500\u2500 _0\n    \u2502   \u2502   \u2514\u2500\u2500 foo.txt\n    \u2502   \u2514\u2500\u2500 _1\n    \u2502       \u2514\u2500\u2500 foo.txt\n    \u251c\u2500\u2500 foo_collection\n    \u2502   \u251c\u2500\u2500 foo1.pkl\n    \u2502   \u2514\u2500\u2500 foo2.pkl\n    \u2514\u2500\u2500 foo.py\n\nfoo_1 string: 'foo_1 initial string, with reduction modification!'\nfoo_1 export time: 9857706.650140837\nfoo_1 import time: 9857706.652698385",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use A PackageImporter will add the attribute __torch_package__ to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not., give an example?",
        "Y": "# In foo/bar.py:\n\nif \"__torch_package__\" in dir():  # true if the code is being loaded from a package\n    def is_in_package():\n        return True\n\n    UserException = Exception\nelse:\n    def is_in_package():\n        return False\n\n    UserException = UnpackageableException",
        "Z": "A PackageImporter will add the attribute __torch_package__ to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not. # In foo/bar.py:\n\nif \"__torch_package__\" in dir():  # true if the code is being loaded from a package\n    def is_in_package():\n        return True\n\n    UserException = Exception\nelse:\n    def is_in_package():\n        return False\n\n    UserException = UnpackageableException",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use A PackageImporter will add the attribute __torch_package__ to every module that it initializes. Your code can check for the\npresence of this attribute to determine whether it is executing in a packaged context or not.Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a\ntorch.package., give an example?",
        "Y": "from foo.bar import is_in_package\n\nprint(is_in_package())  # False\n\nloaded_module = PackageImporter(my_pacakge).import_module(\"foo.bar\")\nloaded_module.is_in_package()  # True",
        "Z": "Now, the code will behave differently depending on whether it\u2019s imported normally through your Python environment or imported from a\ntorch.package. from foo.bar import is_in_package\n\nprint(is_in_package())  # False\n\nloaded_module = PackageImporter(my_pacakge).import_module(\"foo.bar\")\nloaded_module.is_in_package()  # True",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use PackageExporter offers a save_source_string() method that allows one to save arbitrary Python source code to a module of your choosing., give an example?",
        "Y": "with PackageExporter(f) as exporter:\n    # Save the my_module.foo available in your current Python environment.\n    exporter.save_module(\"my_module.foo\")\n\n    # This saves the provided string to my_module/foo.py in the package archive.\n    # It will override the my_module.foo that was previously saved.\n    exporter.save_source_string(\"my_module.foo\", textwrap.dedent(\n        \"\"\"\\\n        def my_function():\n            print('hello world')\n        \"\"\"\n    ))\n\n    # If you want to treat my_module.bar as a package\n    # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)\n    # pass is_package=True,\n    exporter.save_source_string(\"my_module.bar\",\n                                \"def foo(): print('hello')\\n\",\n                                is_package=True)\n\nimporter = PackageImporter(f)\nimporter.import_module(\"my_module.foo\").my_function()  # prints 'hello world'",
        "Z": "PackageExporter offers a save_source_string() method that allows one to save arbitrary Python source code to a module of your choosing. with PackageExporter(f) as exporter:\n    # Save the my_module.foo available in your current Python environment.\n    exporter.save_module(\"my_module.foo\")\n\n    # This saves the provided string to my_module/foo.py in the package archive.\n    # It will override the my_module.foo that was previously saved.\n    exporter.save_source_string(\"my_module.foo\", textwrap.dedent(\n        \"\"\"\\\n        def my_function():\n            print('hello world')\n        \"\"\"\n    ))\n\n    # If you want to treat my_module.bar as a package\n    # (e.g. save to `my_module/bar/__init__.py` instead of `my_module/bar.py)\n    # pass is_package=True,\n    exporter.save_source_string(\"my_module.bar\",\n                                \"def foo(): print('hello')\\n\",\n                                is_package=True)\n\nimporter = PackageImporter(f)\nimporter.import_module(\"my_module.foo\").my_function()  # prints 'hello world'",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use PackageImporter implements the\nimportlib.resources\nAPI for accessing resources from inside a package., give an example?",
        "Y": "with PackageExporter(f) as exporter:\n    # saves text to one/a.txt in the archive\n    exporter.save_text(\"my_resource\", \"a.txt\", \"hello world!\")\n    # saves the tensor to my_pickle/obj.pkl\n    exporter.save_pickle(\"my_pickle\", \"obj.pkl\", torch.ones(2, 2))\n\n    # see below for module contents\n    exporter.save_module(\"foo\")\n    exporter.save_module(\"bar\")",
        "Z": "PackageImporter implements the\nimportlib.resources\nAPI for accessing resources from inside a package. with PackageExporter(f) as exporter:\n    # saves text to one/a.txt in the archive\n    exporter.save_text(\"my_resource\", \"a.txt\", \"hello world!\")\n    # saves the tensor to my_pickle/obj.pkl\n    exporter.save_pickle(\"my_pickle\", \"obj.pkl\", torch.ones(2, 2))\n\n    # see below for module contents\n    exporter.save_module(\"foo\")\n    exporter.save_module(\"bar\")",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use PackageImporter implements the\nimportlib.resources\nAPI for accessing resources from inside a package.The importlib.resources API allows access to resources from within packaged code., give an example?",
        "Y": "# foo.py:\nimport importlib.resources\nimport my_resource\n\n# returns \"hello world!\"\ndef get_my_resource():\n    return importlib.resources.read_text(my_resource, \"a.txt\")",
        "Z": "The importlib.resources API allows access to resources from within packaged code. # foo.py:\nimport importlib.resources\nimport my_resource\n\n# returns \"hello world!\"\ndef get_my_resource():\n    return importlib.resources.read_text(my_resource, \"a.txt\")",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use The importlib.resources API allows access to resources from within packaged code.Using importlib.resources is the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent PackageImporter instance itself from within\npackaged code., give an example?",
        "Y": "# bar.py:\nimport torch_package_importer # this is the PackageImporter that imported this module.\n\n# Prints \"hello world!\", equivalient to importlib.resources.read_text\ndef get_my_resource():\n    return torch_package_importer.load_text(\"my_resource\", \"a.txt\")\n\n# You also do things that the importlib.resources API does not support, like loading\n# a pickled object from the package.\ndef get_my_pickle():\n    return torch_package_importer.load_pickle(\"my_pickle\", \"obj.pkl\")",
        "Z": "The importlib.resources API allows access to resources from within packaged code.Using importlib.resources is the recommended way to access package contents from within packaged code, since it complies\nwith the Python standard. However, it is also possible to access the parent PackageImporter instance itself from within\npackaged code. # bar.py:\nimport torch_package_importer # this is the PackageImporter that imported this module.\n\n# Prints \"hello world!\", equivalient to importlib.resources.read_text\ndef get_my_resource():\n    return torch_package_importer.load_text(\"my_resource\", \"a.txt\")\n\n# You also do things that the importlib.resources API does not support, like loading\n# a pickled object from the package.\ndef get_my_pickle():\n    return torch_package_importer.load_pickle(\"my_pickle\", \"obj.pkl\")",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use To tell if an object\u2019s code is from a torch.package, use the torch.package.is_from_package() function.\nNote: if an object is from a package but its definition is from a module marked extern or from stdlib,\nthis check will return False., give an example?",
        "Y": "importer = PackageImporter(f)\nmod = importer.import_module('foo')\nobj = importer.load_pickle('model', 'model.pkl')\ntxt = importer.load_text('text', 'my_test.txt')\n\nassert is_from_package(mod)\nassert is_from_package(obj)\nassert not is_from_package(txt) # str is from stdlib, so this will return False",
        "Z": "To tell if an object\u2019s code is from a torch.package, use the torch.package.is_from_package() function.\nNote: if an object is from a package but its definition is from a module marked extern or from stdlib,\nthis check will return False. importer = PackageImporter(f)\nmod = importer.import_module('foo')\nobj = importer.load_pickle('model', 'model.pkl')\ntxt = importer.load_text('text', 'my_test.txt')\n\nassert is_from_package(mod)\nassert is_from_package(obj)\nassert not is_from_package(txt) # str is from stdlib, so this will return False",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use To re-export an object that was previously imported by a PackageImporter, you must make the new PackageExporter\naware of the original PackageImporter so that it can find source code for your object\u2019s dependencies., give an example?",
        "Y": "importer = PackageImporter(f)\nobj = importer.load_pickle(\"model\", \"model.pkl\")\n\n# re-export obj in a new package\nwith PackageExporter(f2, importer=(importer, sys_importer)) as exporter:\n    exporter.save_pickle(\"model\", \"model.pkl\", obj)",
        "Z": "To re-export an object that was previously imported by a PackageImporter, you must make the new PackageExporter\naware of the original PackageImporter so that it can find source code for your object\u2019s dependencies. importer = PackageImporter(f)\nobj = importer.load_pickle(\"model\", \"model.pkl\")\n\n# re-export obj in a new package\nwith PackageExporter(f2, importer=(importer, sys_importer)) as exporter:\n    exporter.save_pickle(\"model\", \"model.pkl\", obj)",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use To package a TorchScript model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving TorchScript objects that are attributes or submodules is supported as well with no extra work., give an example?",
        "Y": "# save TorchScript just like any other object\nwith PackageExporter(file_name, verbose=True) as e:\n    e.save_pickle(\"res\", \"script_model.pkl\", scripted_model)\n    e.save_pickle(\"res\", \"mixed_model.pkl\", python_model_with_scripted_submodule)\n# load as normal\nimporter = PackageImporter(file_name)\nloaded_script = importer.load_pickle(\"res\", \"script_model.pkl\")\nloaded_mixed = importer.load_pickle(\"res\", \"mixed_model.pkl\"",
        "Z": "To package a TorchScript model, use the same save_pickle and load_pickle APIs as you would with any other object.\nSaving TorchScript objects that are attributes or submodules is supported as well with no extra work. # save TorchScript just like any other object\nwith PackageExporter(file_name, verbose=True) as e:\n    e.save_pickle(\"res\", \"script_model.pkl\", scripted_model)\n    e.save_pickle(\"res\", \"mixed_model.pkl\", python_model_with_scripted_submodule)\n# load as normal\nimporter = PackageImporter(file_name)\nloaded_script = importer.load_pickle(\"res\", \"script_model.pkl\")\nloaded_mixed = importer.load_pickle(\"res\", \"mixed_model.pkl\"",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use A torch.package file is a ZIP archive which conventionally uses the .pt extension. Inside the ZIP archive, there are two kinds of files:As an example, this is what a fully packaged ResNet model from torchvision looks like:, give an example?",
        "Y": "resnet\n\u251c\u2500\u2500 .data  # All framework-specific data is stored here.\n\u2502   \u2502      # It's named to avoid conflicts with user-serialized code.\n\u2502   \u251c\u2500\u2500 94286146172688.storage  # tensor data\n\u2502   \u251c\u2500\u2500 94286146172784.storage\n\u2502   \u251c\u2500\u2500 extern_modules  # text file with names of extern modules (e.g. 'torch')\n\u2502   \u251c\u2500\u2500 version         # version metadata\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 model  # the pickled model\n\u2502   \u2514\u2500\u2500 model.pkl\n\u2514\u2500\u2500 torchvision  # all code dependencies are captured as source files\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py\n        \u2514\u2500\u2500 utils.py",
        "Z": "As an example, this is what a fully packaged ResNet model from torchvision looks like: resnet\n\u251c\u2500\u2500 .data  # All framework-specific data is stored here.\n\u2502   \u2502      # It's named to avoid conflicts with user-serialized code.\n\u2502   \u251c\u2500\u2500 94286146172688.storage  # tensor data\n\u2502   \u251c\u2500\u2500 94286146172784.storage\n\u2502   \u251c\u2500\u2500 extern_modules  # text file with names of extern modules (e.g. 'torch')\n\u2502   \u251c\u2500\u2500 version         # version metadata\n\u2502   \u251c\u2500\u2500 ...\n\u251c\u2500\u2500 model  # the pickled model\n\u2502   \u2514\u2500\u2500 model.pkl\n\u2514\u2500\u2500 torchvision  # all code dependencies are captured as source files\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py\n        \u2514\u2500\u2500 utils.py",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use The .data/ directory is owned by torch.package, and its contents are considered to be a private implementation detail.\nThe torch.package format makes no guarantees about the contents of .data/, but any changes made will be backward compatible\n(that is, newer version of PyTorch will always be able to load older torch.packages).Currently, the .data/ directory contains the following items:, give an example?",
        "Y": ".data\n\u251c\u2500\u2500 94286146172688.storage\n\u251c\u2500\u2500 94286146172784.storage\n\u251c\u2500\u2500 extern_modules\n\u251c\u2500\u2500 version\n\u251c\u2500\u2500 ...",
        "Z": "Currently, the .data/ directory contains the following items: .data\n\u251c\u2500\u2500 94286146172688.storage\n\u251c\u2500\u2500 94286146172784.storage\n\u251c\u2500\u2500 extern_modules\n\u251c\u2500\u2500 version\n\u251c\u2500\u2500 ...",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use All other files in the archive were put there by a user. The layout is identical to a Python\nregular package. For a deeper dive in how Python packaging works,\nplease consult this essay (it\u2019s slightly out of date, so double-check implementation details\nwith the Python reference documentation)., give an example?",
        "Y": "<package root>\n\u251c\u2500\u2500 model  # the pickled model\n\u2502   \u2514\u2500\u2500 model.pkl\n\u251c\u2500\u2500 another_package\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 foo.txt         # a resource file , see importlib.resources\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 torchvision\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py   # torchvision.models.resnet\n        \u2514\u2500\u2500 utils.py    # torchvision.models.utils",
        "Z": "All other files in the archive were put there by a user. The layout is identical to a Python\nregular package. For a deeper dive in how Python packaging works,\nplease consult this essay (it\u2019s slightly out of date, so double-check implementation details\nwith the Python reference documentation). <package root>\n\u251c\u2500\u2500 model  # the pickled model\n\u2502   \u2514\u2500\u2500 model.pkl\n\u251c\u2500\u2500 another_package\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 foo.txt         # a resource file , see importlib.resources\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 torchvision\n    \u2514\u2500\u2500 models\n        \u251c\u2500\u2500 resnet.py   # torchvision.models.resnet\n        \u2514\u2500\u2500 utils.py    # torchvision.models.utils",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use When you issue a save_pickle(obj, ...) call, PackageExporter will pickle the object normally. Then, it uses the\npickletools standard library module to parse the pickle bytecode.In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like:, give an example?",
        "Y": "GLOBAL 'torchvision.models.resnet Resnet`",
        "Z": "In a pickle, an object is saved along with a GLOBAL opcode that describes where to find the implementation of the object\u2019s type, like: GLOBAL 'torchvision.models.resnet Resnet`",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s what torch.package uses.Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like \"foo.**\"). You associate a pattern\nwith an action using methods on PackageImporter, e.g., give an example?",
        "Y": "my_exporter.intern(\"torchvision.**\")\nmy_exporter.extern(\"numpy\")",
        "Z": "Note that actions are only defined on entire Python modules. There is no way to package \u201cjust\u201d a function or class from module and leave the rest out.\nThis is by design. Python does not offer clean boundaries between objects defined in a module. The only defined unit of dependency organization is a\nmodule, so that\u2019s what torch.package uses.Actions are applied to modules using patterns. Patterns can either be module names (\"foo.bar\") or globs (like \"foo.**\"). You associate a pattern\nwith an action using methods on PackageImporter, e.g. my_exporter.intern(\"torchvision.**\")\nmy_exporter.extern(\"numpy\")",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use When specifying actions, you can pass multiple patterns, e.g., give an example?",
        "Y": "exporter.intern([\"torchvision.models.**\", \"torchvision.utils.**\"])",
        "Z": "When specifying actions, you can pass multiple patterns, e.g. exporter.intern([\"torchvision.models.**\", \"torchvision.utils.**\"])",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use A module will match against this action if it matches any of the patterns.You can also specify patterns to exlcude, e.g., give an example?",
        "Y": "exporter.mock(\"**\", exclude=[\"torchvision.**\"])",
        "Z": "A module will match against this action if it matches any of the patterns.You can also specify patterns to exlcude, e.g. exporter.mock(\"**\", exclude=[\"torchvision.**\"])",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use Any class that you import from a PackageImporter will be a version of the class specific to that importer. For example:, give an example?",
        "Y": "from foo import MyClass\n\nmy_class_instance = MyClass()\n\nwith PackageExporter(f) as exporter:\n    exporter.save_module(\"foo\")\n\nimporter = PackageImporter(f)\nimported_MyClass = importer.import_module(\"foo\").MyClass\n\nassert isinstance(my_class_instance, MyClass)  # works\nassert isinstance(my_class_instance, imported_MyClass)  # ERROR!",
        "Z": "Any class that you import from a PackageImporter will be a version of the class specific to that importer. For example: from foo import MyClass\n\nmy_class_instance = MyClass()\n\nwith PackageExporter(f) as exporter:\n    exporter.save_module(\"foo\")\n\nimporter = PackageImporter(f)\nimported_MyClass = importer.import_module(\"foo\").MyClass\n\nassert isinstance(my_class_instance, MyClass)  # works\nassert isinstance(my_class_instance, imported_MyClass)  # ERROR!",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use In this example, MyClass and import_MyClass are not the same type. In this specific example, MyClass and import_MyClass have exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation where import_MyClass is coming from an\nolder package with an entirely different implementation of MyClass \u2014 in that case, it\u2019s unsafe to consider them the same class.Under the hood, each importer has a prefix that allows it to uniquely identify classes:, give an example?",
        "Y": "print(MyClass.__name__)  # prints \"foo.MyClass\"\nprint(imported_MyClass.__name__)  # prints <torch_package_0>.foo.MyClass",
        "Z": "In this example, MyClass and import_MyClass are not the same type. In this specific example, MyClass and import_MyClass have exactly the\nsame implementation, so you might thing it\u2019s okay to consider them the same class. But consider the situation where import_MyClass is coming from an\nolder package with an entirely different implementation of MyClass \u2014 in that case, it\u2019s unsafe to consider them the same class.Under the hood, each importer has a prefix that allows it to uniquely identify classes: print(MyClass.__name__)  # prints \"foo.MyClass\"\nprint(imported_MyClass.__name__)  # prints <torch_package_0>.foo.MyClass",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use torch.package.PackageExporter.close, give an example?",
        "Y": "with PackageExporter(\"file.zip\") as e:\n    ...",
        "Z": "Write the package to the filesystem. Any calls after close() are now invalid.\nIt is preferable to use resource guard syntax instead: with PackageExporter(\"file.zip\") as e:\n    ...",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use torch.package.PackageExporter.register_extern_hook, give an example?",
        "Y": "hook(exporter: PackageExporter, module_name: str) -> None",
        "Z": "The hook will be called each time a module matches against an extern() pattern.\nIt should have the following signature: hook(exporter: PackageExporter, module_name: str) -> None",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use torch.package.PackageExporter.register_intern_hook, give an example?",
        "Y": "hook(exporter: PackageExporter, module_name: str) -> None",
        "Z": "The hook will be called each time a module matches against an intern() pattern.\nIt should have the following signature: hook(exporter: PackageExporter, module_name: str) -> None",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use torch.package.PackageExporter.register_mock_hook, give an example?",
        "Y": "hook(exporter: PackageExporter, module_name: str) -> None",
        "Z": "The hook will be called each time a module matches against a mock() pattern.\nIt should have the following signature: hook(exporter: PackageExporter, module_name: str) -> None",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use torch.package.PackageImporter.id, give an example?",
        "Y": "<torch_package_0>",
        "Z": "Returns internal identifier that torch.package uses to distinguish PackageImporter instances.\nLooks like: <torch_package_0>",
        "source": "https://pytorch.org/docs/stable/package.html"
    },
    {
        "X": "How to use torch.cumsum, give an example?",
        "Y": ">>> a = torch.randn(10)\n>>> a\ntensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\n         0.1850, -1.1571, -0.4243])\n>>> torch.cumsum(a, dim=0)\ntensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\n        -1.8209, -2.9780, -3.4022])",
        "Z": "For example, if input is a vector of size N, the result will also be\na vector of size N, with elements. >>> a = torch.randn(10)\n>>> a\ntensor([-0.8286, -0.4890,  0.5155,  0.8443,  0.1865, -0.1752, -2.0595,\n         0.1850, -1.1571, -0.4243])\n>>> torch.cumsum(a, dim=0)\ntensor([-0.8286, -1.3175, -0.8020,  0.0423,  0.2289,  0.0537, -2.0058,\n        -1.8209, -2.9780, -3.4022])",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum"
    },
    {
        "X": "How to use torch.linalg.matrix_power, give an example?",
        "Y": "matrix_power(torch.linalg.solve(A, B), n) == matrix_power(A, -n)  @ B",
        "Z": "Consider using torch.linalg.solve() if possible for multiplying a matrix on the left by\na negative power as, if n> 0: matrix_power(torch.linalg.solve(A, B), n) == matrix_power(A, -n)  @ B",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "How  If n= 0, it returns the identity matrix (or batch) of the same shape\nas A. If n is negative, it returns the inverse of each matrix\n(if invertible) raised to the power of abs(n)., give an example?",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.2270,  0.6663, -1.3515],\n        [-0.9838, -0.4002, -1.9313],\n        [-0.7886, -0.0450,  0.0528]])\n>>> torch.linalg.matrix_power(a, 0)\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n>>> torch.linalg.matrix_power(a, 3)\ntensor([[ 1.0756,  0.4980,  0.0100],\n        [-1.6617,  1.4994, -1.9980],\n        [-0.4509,  0.2731,  0.8001]])\n>>> torch.linalg.matrix_power(a.expand(2, -1, -1), -2)\ntensor([[[ 0.2640,  0.4571, -0.5511],\n        [-1.0163,  0.3491, -1.5292],\n        [-0.4899,  0.0822,  0.2773]],\n        [[ 0.2640,  0.4571, -0.5511],\n        [-1.0163,  0.3491, -1.5292],\n        [-0.4899,  0.0822,  0.2773]]])",
        "Z": "If n= 0, it returns the identity matrix (or batch) of the same shape\nas A. If n is negative, it returns the inverse of each matrix\n(if invertible) raised to the power of abs(n). >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-0.2270,  0.6663, -1.3515],\n        [-0.9838, -0.4002, -1.9313],\n        [-0.7886, -0.0450,  0.0528]])\n>>> torch.linalg.matrix_power(a, 0)\ntensor([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n>>> torch.linalg.matrix_power(a, 3)\ntensor([[ 1.0756,  0.4980,  0.0100],\n        [-1.6617,  1.4994, -1.9980],\n        [-0.4509,  0.2731,  0.8001]])\n>>> torch.linalg.matrix_power(a.expand(2, -1, -1), -2)\ntensor([[[ 0.2640,  0.4571, -0.5511],\n        [-1.0163,  0.3491, -1.5292],\n        [-0.4899,  0.0822,  0.2773]],\n        [[ 0.2640,  0.4571, -0.5511],\n        [-1.0163,  0.3491, -1.5292],\n        [-0.4899,  0.0822,  0.2773]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"
    },
    {
        "X": "How to use torch.profiler.profile, give an example?",
        "Y": "with torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ]\n) as p:\n    code_to_profile()\nprint(p.key_averages().table(\n    sort_by=\"self_cuda_time_total\", row_limit=-1))",
        "Z": "with torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ]\n) as p:\n    code_to_profile()\nprint(p.key_averages().table(\n    sort_by=\"self_cuda_time_total\", row_limit=-1))",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "How to use Using the profiler\u2019s schedule, on_trace_ready and step functions:, give an example?",
        "Y": "# Non-default profiler schedule allows user to turn profiler on and off\n# on different iterations of the training loop;\n# trace_handler is called every time a new trace becomes available\ndef trace_handler(prof):\n    print(prof.key_averages().table(\n        sort_by=\"self_cuda_time_total\", row_limit=-1))\n    # prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ],\n\n    # In this example with wait=1, warmup=1, active=2,\n    # profiler will skip the first step/iteration,\n    # start warming up on the second, record\n    # the third and the forth iterations,\n    # after which the trace will become available\n    # and on_trace_ready (when set) is called;\n    # the cycle repeats starting with the next step\n\n    schedule=torch.profiler.schedule(\n        wait=1,\n        warmup=1,\n        active=2),\n    on_trace_ready=trace_handler\n    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n    # used when outputting for tensorboard\n    ) as p:\n        for iter in range(N):\n            code_iteration_to_profile(iter)\n            # send a signal to the profiler that the next iteration has started\n            p.step()",
        "Z": "Using the profiler\u2019s schedule, on_trace_ready and step functions: # Non-default profiler schedule allows user to turn profiler on and off\n# on different iterations of the training loop;\n# trace_handler is called every time a new trace becomes available\ndef trace_handler(prof):\n    print(prof.key_averages().table(\n        sort_by=\"self_cuda_time_total\", row_limit=-1))\n    # prof.export_chrome_trace(\"/tmp/test_trace_\" + str(prof.step_num) + \".json\")\n\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ],\n\n    # In this example with wait=1, warmup=1, active=2,\n    # profiler will skip the first step/iteration,\n    # start warming up on the second, record\n    # the third and the forth iterations,\n    # after which the trace will become available\n    # and on_trace_ready (when set) is called;\n    # the cycle repeats starting with the next step\n\n    schedule=torch.profiler.schedule(\n        wait=1,\n        warmup=1,\n        active=2),\n    on_trace_ready=trace_handler\n    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')\n    # used when outputting for tensorboard\n    ) as p:\n        for iter in range(N):\n            code_iteration_to_profile(iter)\n            # send a signal to the profiler that the next iteration has started\n            p.step()",
        "source": "https://pytorch.org/docs/stable/profiler.html"
    },
    {
        "X": "How to use torch.unique, give an example?",
        "Y": ">>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n>>> output\ntensor([ 2,  3,  1])\n\n>>> output, inverse_indices = torch.unique(\n...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n>>> output\ntensor([ 1,  2,  3])\n>>> inverse_indices\ntensor([ 0,  2,  1,  2])\n\n>>> output, inverse_indices = torch.unique(\n...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n>>> output\ntensor([ 1,  2,  3])\n>>> inverse_indices\ntensor([[ 0,  2],\n        [ 1,  2]])",
        "Z": ">>> output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))\n>>> output\ntensor([ 2,  3,  1])\n\n>>> output, inverse_indices = torch.unique(\n...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)\n>>> output\ntensor([ 1,  2,  3])\n>>> inverse_indices\ntensor([ 0,  2,  1,  2])\n\n>>> output, inverse_indices = torch.unique(\n...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)\n>>> output\ntensor([ 1,  2,  3])\n>>> inverse_indices\ntensor([[ 0,  2],\n        [ 1,  2]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.unique.html#torch.unique"
    },
    {
        "X": "How to use torch.asin, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n>>> torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.5962,  1.4985, -0.4396,  1.4525])\n>>> torch.asin(a)\ntensor([-0.6387,     nan, -0.4552,     nan])",
        "source": "https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin"
    },
    {
        "X": "How to use To construct an Optimizer you have to give it an iterable containing the\nparameters (all should be Variable s) to optimize. Then,\nyou can specify optimizer-specific options such as the learning rate, weight decay, etc., give an example?",
        "Y": "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr=0.0001)",
        "Z": "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\noptimizer = optim.Adam([var1, var2], lr=0.0001)",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use Optimizer s also support specifying per-parameter options. To do this, instead\nof passing an iterable of Variable s, pass in an iterable of\ndict s. Each of them will define a separate parameter group, and should contain\na params key, containing a list of parameters belonging to it. Other keys\nshould match the keyword arguments accepted by the optimizers, and will be used\nas optimization options for this group.For example, this is very useful when one wants to specify per-layer learning rates:, give an example?",
        "Y": "optim.SGD([\n                {'params': model.base.parameters()},\n                {'params': model.classifier.parameters(), 'lr': 1e-3}\n            ], lr=1e-2, momentum=0.9)",
        "Z": "For example, this is very useful when one wants to specify per-layer learning rates: optim.SGD([\n                {'params': model.base.parameters()},\n                {'params': model.classifier.parameters(), 'lr': 1e-3}\n            ], lr=1e-2, momentum=0.9)",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use This is a simplified version supported by most optimizers. The function can be\ncalled once the gradients are computed using e.g.\nbackward()., give an example?",
        "Y": "for input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()",
        "Z": "for input, target in dataset:\n    optimizer.zero_grad()\n    output = model(input)\n    loss = loss_fn(output, target)\n    loss.backward()\n    optimizer.step()",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use Some optimization algorithms such as Conjugate Gradient and LBFGS need to\nreevaluate the function multiple times, so you have to pass in a closure that\nallows them to recompute your model. The closure should clear the gradients,\ncompute the loss, and return it., give an example?",
        "Y": "for input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure)",
        "Z": "for input, target in dataset:\n    def closure():\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        return loss\n    optimizer.step(closure)",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way:, give an example?",
        "Y": "model = [Parameter(torch.randn(2, 2, requires_grad=True))]\noptimizer = SGD(model, 0.1)\nscheduler = ExponentialLR(optimizer, gamma=0.9)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()",
        "Z": "Learning rate scheduling should be applied after optimizer\u2019s update; e.g., you\nshould write your code this way: model = [Parameter(torch.randn(2, 2, requires_grad=True))]\noptimizer = SGD(model, 0.1)\nscheduler = ExponentialLR(optimizer, gamma=0.9)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler.step()",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it., give an example?",
        "Y": "model = [Parameter(torch.randn(2, 2, requires_grad=True))]\noptimizer = SGD(model, 0.1)\nscheduler1 = ExponentialLR(optimizer, gamma=0.9)\nscheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler1.step()\n    scheduler2.step()",
        "Z": "Most learning rate schedulers can be called back-to-back (also referred to as\nchaining schedulers). The result is that each scheduler is applied one after the\nother on the learning rate obtained by the one preceding it. model = [Parameter(torch.randn(2, 2, requires_grad=True))]\noptimizer = SGD(model, 0.1)\nscheduler1 = ExponentialLR(optimizer, gamma=0.9)\nscheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n\nfor epoch in range(20):\n    for input, target in dataset:\n        optimizer.zero_grad()\n        output = model(input)\n        loss = loss_fn(output, target)\n        loss.backward()\n        optimizer.step()\n    scheduler1.step()\n    scheduler2.step()",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms., give an example?",
        "Y": ">>> scheduler = ...\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()",
        "Z": "In many places in the documentation, we will use the following template to refer to schedulers\nalgorithms. >>> scheduler = ...\n>>> for epoch in range(100):\n>>>     train(...)\n>>>     validate(...)\n>>>     scheduler.step()",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use AveragedModel class serves to compute the weights of the SWA model. You can create an\naveraged model by running:, give an example?",
        "Y": ">>> swa_model = AveragedModel(model)",
        "Z": "AveragedModel class serves to compute the weights of the SWA model. You can create an\naveraged model by running: >>> swa_model = AveragedModel(model)",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use AveragedModel class serves to compute the weights of the SWA model. You can create an\naveraged model by running:Here the model model can be an arbitrary torch.nn.Module object. swa_model\nwill keep track of the running averages of the parameters of the model. To update these\naverages, you can use the update_parameters() function:, give an example?",
        "Y": ">>> swa_model.update_parameters(model)",
        "Z": "Here the model model can be an arbitrary torch.nn.Module object. swa_model\nwill keep track of the running averages of the parameters of the model. To update these\naverages, you can use the update_parameters() function: >>> swa_model.update_parameters(model)",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use Typically, in SWA the learning rate is set to a high constant value. SWALR is a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group:, give an example?",
        "Y": ">>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)",
        "Z": "Typically, in SWA the learning rate is set to a high constant value. SWALR is a\nlearning rate scheduler that anneals the learning rate to a fixed value, and then keeps it\nconstant. For example, the following code creates a scheduler that linearly anneals the\nlearning rate from its initial value to 0.05 in 5 epochs within each parameter group: >>> swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, \\\n>>>         anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=0.05)",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use update_bn() is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloader loader at the end of training:, give an example?",
        "Y": ">>> torch.optim.swa_utils.update_bn(loader, swa_model)",
        "Z": "update_bn() is a utility function that allows to compute the batchnorm statistics for the SWA model\non a given dataloader loader at the end of training: >>> torch.optim.swa_utils.update_bn(loader, swa_model)",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use By default, torch.optim.swa_utils.AveragedModel computes a running equal average of\nthe parameters that you provide, but you can also use custom averaging functions with the\navg_fn parameter. In the following example ema_model computes an exponential moving average., give an example?",
        "Y": ">>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.1 * averaged_model_parameter + 0.9 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)",
        "Z": ">>> ema_avg = lambda averaged_model_parameter, model_parameter, num_averaged:\\\n>>>         0.1 * averaged_model_parameter + 0.9 * model_parameter\n>>> ema_model = torch.optim.swa_utils.AveragedModel(model, avg_fn=ema_avg)",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use In the example below, swa_model is the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160:, give an example?",
        "Y": ">>> loader, optimizer, model, loss_fn = ...\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n>>> swa_start = 160\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>       if epoch > swa_start:\n>>>           swa_model.update_parameters(model)\n>>>           swa_scheduler.step()\n>>>       else:\n>>>           scheduler.step()\n>>>\n>>> # Update bn statistics for the swa_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n>>> # Use swa_model to make predictions on test data\n>>> preds = swa_model(test_input)",
        "Z": "In the example below, swa_model is the SWA model that accumulates the averages of the weights.\nWe train the model for a total of 300 epochs and we switch to the SWA learning rate schedule\nand start to collect SWA averages of the parameters at epoch 160: >>> loader, optimizer, model, loss_fn = ...\n>>> swa_model = torch.optim.swa_utils.AveragedModel(model)\n>>> scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=300)\n>>> swa_start = 160\n>>> swa_scheduler = SWALR(optimizer, swa_lr=0.05)\n>>>\n>>> for epoch in range(300):\n>>>       for input, target in loader:\n>>>           optimizer.zero_grad()\n>>>           loss_fn(model(input), target).backward()\n>>>           optimizer.step()\n>>>       if epoch > swa_start:\n>>>           swa_model.update_parameters(model)\n>>>           swa_scheduler.step()\n>>>       else:\n>>>           scheduler.step()\n>>>\n>>> # Update bn statistics for the swa_model at the end\n>>> torch.optim.swa_utils.update_bn(loader, swa_model)\n>>> # Use swa_model to make predictions on test data\n>>> preds = swa_model(test_input)",
        "source": "https://pytorch.org/docs/stable/optim.html"
    },
    {
        "X": "How to use torch.nanquantile, give an example?",
        "Y": ">>> t = torch.tensor([float('nan'), 1, 2])\n>>> t.quantile(0.5)\ntensor(nan)\n>>> t.nanquantile(0.5)\ntensor(1.5000)\n>>> t = torch.tensor([[float('nan'), float('nan')], [1, 2]])\n>>> t\ntensor([[nan, nan],\n        [1., 2.]])\n>>> t.nanquantile(0.5, dim=0)\ntensor([1., 2.])\n>>> t.nanquantile(0.5, dim=1)\ntensor([   nan, 1.5000])",
        "Z": ">>> t = torch.tensor([float('nan'), 1, 2])\n>>> t.quantile(0.5)\ntensor(nan)\n>>> t.nanquantile(0.5)\ntensor(1.5000)\n>>> t = torch.tensor([[float('nan'), float('nan')], [1, 2]])\n>>> t\ntensor([[nan, nan],\n        [1., 2.]])\n>>> t.nanquantile(0.5, dim=0)\ntensor([1., 2.])\n>>> t.nanquantile(0.5, dim=1)\ntensor([   nan, 1.5000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"
    },
    {
        "X": "How to use torch.symeig, give an example?",
        "Y": "UPLO = \"U\" if upper else \"L\"\nL = torch.linalg.eigvalsh(A, UPLO=UPLO)",
        "Z": "torch.symeig() is deprecated in favor of torch.linalg.eigh()\nand will be removed in a future PyTorch release. The default behavior has changed\nfrom using the upper triangular portion of the matrix by default to using the\nlower triangular portion.L, _ = torch.symeig(A, upper=upper) should be replaced with UPLO = \"U\" if upper else \"L\"\nL = torch.linalg.eigvalsh(A, UPLO=UPLO)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "How  L, _ = torch.symeig(A, upper=upper) should be replaced withL, V = torch.symeig(A, eigenvectors=True, upper=upper) should be replaced with, give an example?",
        "Y": "UPLO = \"U\" if upper else \"L\"\nL, V = torch.linalg.eigh(A, UPLO=UPLO)",
        "Z": "L, _ = torch.symeig(A, upper=upper) should be replaced withL, V = torch.symeig(A, eigenvectors=True, upper=upper) should be replaced with UPLO = \"U\" if upper else \"L\"\nL, V = torch.linalg.eigh(A, UPLO=UPLO)",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "How  If upper is False, then lower triangular portion is used., give an example?",
        "Y": ">>> a = torch.randn(5, 5)\n>>> a = a + a.t()  # To make a symmetric\n>>> a\ntensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],\n        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],\n        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],\n        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],\n        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])\n>>> e, v = torch.symeig(a, eigenvectors=True)\n>>> e\ntensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])\n>>> v\ntensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],\n        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],\n        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],\n        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],\n        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])\n>>> a_big = torch.randn(5, 2, 2)\n>>> a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric\n>>> e, v = a_big.symeig(eigenvectors=True)\n>>> torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)\nTrue",
        "Z": "If upper is False, then lower triangular portion is used. >>> a = torch.randn(5, 5)\n>>> a = a + a.t()  # To make a symmetric\n>>> a\ntensor([[-5.7827,  4.4559, -0.2344, -1.7123, -1.8330],\n        [ 4.4559,  1.4250, -2.8636, -3.2100, -0.1798],\n        [-0.2344, -2.8636,  1.7112, -5.5785,  7.1988],\n        [-1.7123, -3.2100, -5.5785, -2.6227,  3.1036],\n        [-1.8330, -0.1798,  7.1988,  3.1036, -5.1453]])\n>>> e, v = torch.symeig(a, eigenvectors=True)\n>>> e\ntensor([-13.7012,  -7.7497,  -2.3163,   5.2477,   8.1050])\n>>> v\ntensor([[ 0.1643,  0.9034, -0.0291,  0.3508,  0.1817],\n        [-0.2417, -0.3071, -0.5081,  0.6534,  0.4026],\n        [-0.5176,  0.1223, -0.0220,  0.3295, -0.7798],\n        [-0.4850,  0.2695, -0.5773, -0.5840,  0.1337],\n        [ 0.6415, -0.0447, -0.6381, -0.0193, -0.4230]])\n>>> a_big = torch.randn(5, 2, 2)\n>>> a_big = a_big + a_big.transpose(-2, -1)  # To make a_big symmetric\n>>> e, v = a_big.symeig(eigenvectors=True)\n>>> torch.allclose(torch.matmul(v, torch.matmul(e.diag_embed(), v.transpose(-2, -1))), a_big)\nTrue",
        "source": "https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"
    },
    {
        "X": "How to use torch.gcd, give an example?",
        "Y": ">>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.gcd(a, b)\ntensor([1, 2, 5])\n>>> c = torch.tensor([3])\n>>> torch.gcd(a, c)\ntensor([1, 1, 3])",
        "Z": "Both input and other must have integer types. >>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.gcd(a, b)\ntensor([1, 2, 5])\n>>> c = torch.tensor([3])\n>>> torch.gcd(a, c)\ntensor([1, 1, 3])",
        "source": "https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd"
    },
    {
        "X": "How to use torch.matrix_exp, give an example?",
        "Y": ">>> a = torch.randn(2, 2, 2)\n>>> a[0, :, :] = torch.eye(2, 2)\n>>> a[1, :, :] = 2 * torch.eye(2, 2)\n>>> a\ntensor([[[1., 0.],\n         [0., 1.]],\n\n        [[2., 0.],\n         [0., 2.]]])\n>>> torch.matrix_exp(a)\ntensor([[[2.7183, 0.0000],\n         [0.0000, 2.7183]],\n\n         [[7.3891, 0.0000],\n          [0.0000, 7.3891]]])\n\n>>> import math\n>>> x = torch.tensor([[0, math.pi/3], [-math.pi/3, 0]])\n>>> x.matrix_exp() # should be [[cos(pi/3), sin(pi/3)], [-sin(pi/3), cos(pi/3)]]\ntensor([[ 0.5000,  0.8660],\n        [-0.8660,  0.5000]])",
        "Z": "Bader, P.; Blanes, S.; Casas, F.\nComputing the Matrix Exponential with an Optimized Taylor Polynomial Approximation.\nMathematics 2019, 7, 1174. >>> a = torch.randn(2, 2, 2)\n>>> a[0, :, :] = torch.eye(2, 2)\n>>> a[1, :, :] = 2 * torch.eye(2, 2)\n>>> a\ntensor([[[1., 0.],\n         [0., 1.]],\n\n        [[2., 0.],\n         [0., 2.]]])\n>>> torch.matrix_exp(a)\ntensor([[[2.7183, 0.0000],\n         [0.0000, 2.7183]],\n\n         [[7.3891, 0.0000],\n          [0.0000, 7.3891]]])\n\n>>> import math\n>>> x = torch.tensor([[0, math.pi/3], [-math.pi/3, 0]])\n>>> x.matrix_exp() # should be [[cos(pi/3), sin(pi/3)], [-sin(pi/3), cos(pi/3)]]\ntensor([[ 0.5000,  0.8660],\n        [-0.8660,  0.5000]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp"
    },
    {
        "X": "How to use Pytorch Hub supports publishing pre-trained models(model definitions and pre-trained weights)\nto a github repository by adding a simple hubconf.py file;hubconf.py can have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish)., give an example?",
        "Y": "def entrypoint_name(*args, **kwargs):\n    # args & kwargs are optional, for models which take positional/keyword arguments.\n    ...",
        "Z": "hubconf.py can have multiple entrypoints. Each entrypoint is defined as a python function\n(example: a pre-trained model you want to publish). def entrypoint_name(*args, **kwargs):\n    # args & kwargs are optional, for models which take positional/keyword arguments.\n    ...",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How to use Here is a code snippet specifies an entrypoint for resnet18 model if we expand\nthe implementation in pytorch/vision/hubconf.py.\nIn most case importing the right function in hubconf.py is sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script in\npytorch/vision repo, give an example?",
        "Y": "dependencies = ['torch']\nfrom torchvision.models.resnet import resnet18 as _resnet18\n\n# resnet18 is the name of entrypoint\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    Resnet18 model\n    pretrained (bool): kwargs, load pretrained weights into the model\n    \"\"\"\n    # Call the model, load pretrained weights\n    model = _resnet18(pretrained=pretrained, **kwargs)\n    return model",
        "Z": "Here is a code snippet specifies an entrypoint for resnet18 model if we expand\nthe implementation in pytorch/vision/hubconf.py.\nIn most case importing the right function in hubconf.py is sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script in\npytorch/vision repo dependencies = ['torch']\nfrom torchvision.models.resnet import resnet18 as _resnet18\n\n# resnet18 is the name of entrypoint\ndef resnet18(pretrained=False, **kwargs):\n    \"\"\" # This docstring shows up in hub.help()\n    Resnet18 model\n    pretrained (bool): kwargs, load pretrained weights into the model\n    \"\"\"\n    # Call the model, load pretrained weights\n    model = _resnet18(pretrained=pretrained, **kwargs)\n    return model",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How  Here is a code snippet specifies an entrypoint for resnet18 model if we expand\nthe implementation in pytorch/vision/hubconf.py.\nIn most case importing the right function in hubconf.py is sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script in\npytorch/vision repo, give an example?",
        "Y": "if pretrained:\n    # For checkpoint saved in local github repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n    dirname = os.path.dirname(__file__)\n    checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\n    state_dict = torch.load(checkpoint)\n    model.load_state_dict(state_dict)\n\n    # For checkpoint saved elsewhere\n    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))",
        "Z": "Here is a code snippet specifies an entrypoint for resnet18 model if we expand\nthe implementation in pytorch/vision/hubconf.py.\nIn most case importing the right function in hubconf.py is sufficient. Here we\njust want to use the expanded version as an example to show how it works.\nYou can see the full script in\npytorch/vision repo if pretrained:\n    # For checkpoint saved in local github repo, e.g. <RELATIVE_PATH_TO_CHECKPOINT>=weights/save.pth\n    dirname = os.path.dirname(__file__)\n    checkpoint = os.path.join(dirname, <RELATIVE_PATH_TO_CHECKPOINT>)\n    state_dict = torch.load(checkpoint)\n    model.load_state_dict(state_dict)\n\n    # For checkpoint saved elsewhere\n    checkpoint = 'https://download.pytorch.org/models/resnet18-5c106cde.pth'\n    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How to use torch.hub.list, give an example?",
        "Y": ">>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)",
        "Z": ">>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How to use torch.hub.help, give an example?",
        "Y": ">>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))",
        "Z": ">>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How to use torch.hub.load, give an example?",
        "Y": ">>> # from a github repo\n>>> repo = 'pytorch/vision'\n>>> model = torch.hub.load(repo, 'resnet50', pretrained=True)\n>>> # from a local directory\n>>> path = '/some/local/path/pytorch/vision'\n>>> model = torch.hub.load(path, 'resnet50', pretrained=True)",
        "Z": "If source is 'local', repo_or_dir is expected to be a\npath to a local directory. >>> # from a github repo\n>>> repo = 'pytorch/vision'\n>>> model = torch.hub.load(repo, 'resnet50', pretrained=True)\n>>> # from a local directory\n>>> path = '/some/local/path/pytorch/vision'\n>>> model = torch.hub.load(path, 'resnet50', pretrained=True)",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How to use torch.hub.download_url_to_file, give an example?",
        "Y": ">>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')",
        "Z": ">>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How to use torch.hub.load_state_dict_from_url, give an example?",
        "Y": ">>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')",
        "Z": "If the object is already present in model_dir, it\u2019s deserialized and\nreturned.\nThe default value of model_dir is <hub_dir>/checkpoints where\nhub_dir is the directory returned by get_dir(). >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')",
        "source": "https://pytorch.org/docs/stable/hub.html"
    },
    {
        "X": "How to use torch.log10, give an example?",
        "Y": ">>> a = torch.rand(5)\n>>> a\ntensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\n\n\n>>> torch.log10(a)\ntensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])",
        "Z": ">>> a = torch.rand(5)\n>>> a\ntensor([ 0.5224,  0.9354,  0.7257,  0.1301,  0.2251])\n\n\n>>> torch.log10(a)\ntensor([-0.2820, -0.0290, -0.1392, -0.8857, -0.6476])",
        "source": "https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10"
    },
    {
        "X": "How to use torch.fmin, give an example?",
        "Y": ">>> a = torch.tensor([2.2, float('nan'), 2.1, float('nan')])\n>>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])\n>>> torch.fmin(a, b)\ntensor([-9.3000, 0.1000, 2.1000,    nan])",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. >>> a = torch.tensor([2.2, float('nan'), 2.1, float('nan')])\n>>> b = torch.tensor([-9.3, 0.1, float('nan'), float('nan')])\n>>> torch.fmin(a, b)\ntensor([-9.3000, 0.1000, 2.1000,    nan])",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"
    },
    {
        "X": "How to use torch.outer, give an example?",
        "Y": ">>> v1 = torch.arange(1., 5.)\n>>> v2 = torch.arange(1., 4.)\n>>> torch.outer(v1, v2)\ntensor([[  1.,   2.,   3.],\n        [  2.,   4.,   6.],\n        [  3.,   6.,   9.],\n        [  4.,   8.,  12.]])",
        "Z": ">>> v1 = torch.arange(1., 5.)\n>>> v2 = torch.arange(1., 4.)\n>>> torch.outer(v1, v2)\ntensor([[  1.,   2.,   3.],\n        [  2.,   4.,   6.],\n        [  3.,   6.,   9.],\n        [  4.,   8.,  12.]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer"
    },
    {
        "X": "How to use torch.tan, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-1.2027, -1.7687,  0.4412, -1.3856])\n>>> torch.tan(a)\ntensor([-2.5930,  4.9859,  0.4722, -5.3366])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([-1.2027, -1.7687,  0.4412, -1.3856])\n>>> torch.tan(a)\ntensor([-2.5930,  4.9859,  0.4722, -5.3366])",
        "source": "https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan"
    },
    {
        "X": "How to use torch.addr, give an example?",
        "Y": ">>> vec1 = torch.arange(1., 4.)\n>>> vec2 = torch.arange(1., 3.)\n>>> M = torch.zeros(3, 2)\n>>> torch.addr(M, vec1, vec2)\ntensor([[ 1.,  2.],\n        [ 2.,  4.],\n        [ 3.,  6.]])",
        "Z": "If vec1 is a vector of size n and vec2 is a vector\nof size m, then input must be\nbroadcastable with a matrix of size\n(n\u00d7m)(n \\times m)(n\u00d7m) and out will be a matrix of size\n(n\u00d7m)(n \\times m)(n\u00d7m). >>> vec1 = torch.arange(1., 4.)\n>>> vec2 = torch.arange(1., 3.)\n>>> M = torch.zeros(3, 2)\n>>> torch.addr(M, vec1, vec2)\ntensor([[ 1.,  2.],\n        [ 2.,  4.],\n        [ 3.,  6.]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr"
    },
    {
        "X": "How to use torch.cumprod, give an example?",
        "Y": ">>> a = torch.randn(10)\n>>> a\ntensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,\n        -0.2129, -0.4206,  0.1968])\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,\n         0.0014, -0.0006, -0.0001])\n\n>>> a[5] = 0.0\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,\n         0.0000, -0.0000, -0.0000])",
        "Z": "For example, if input is a vector of size N, the result will also be\na vector of size N, with elements. >>> a = torch.randn(10)\n>>> a\ntensor([ 0.6001,  0.2069, -0.1919,  0.9792,  0.6727,  1.0062,  0.4126,\n        -0.2129, -0.4206,  0.1968])\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0158, -0.0065,\n         0.0014, -0.0006, -0.0001])\n\n>>> a[5] = 0.0\n>>> torch.cumprod(a, dim=0)\ntensor([ 0.6001,  0.1241, -0.0238, -0.0233, -0.0157, -0.0000, -0.0000,\n         0.0000, -0.0000, -0.0000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod"
    },
    {
        "X": "How to use torch.inner, give an example?",
        "Y": "# Dot product\n>>> torch.inner(torch.tensor([1, 2, 3]), torch.tensor([0, 2, 1]))\ntensor(7)\n\n# Multidimensional input tensors\n>>> a = torch.randn(2, 3)\n>>> a\ntensor([[0.8173, 1.0874, 1.1784],\n        [0.3279, 0.1234, 2.7894]])\n>>> b = torch.randn(2, 4, 3)\n>>> b\ntensor([[[-0.4682, -0.7159,  0.1506],\n        [ 0.4034, -0.3657,  1.0387],\n        [ 0.9892, -0.6684,  0.1774],\n        [ 0.9482,  1.3261,  0.3917]],\n\n        [[ 0.4537,  0.7493,  1.1724],\n        [ 0.2291,  0.5749, -0.2267],\n        [-0.7920,  0.3607, -0.3701],\n        [ 1.3666, -0.5850, -1.7242]]])\n>>> torch.inner(a, b)\ntensor([[[-0.9837,  1.1560,  0.2907,  2.6785],\n        [ 2.5671,  0.5452, -0.6912, -1.5509]],\n\n        [[ 0.1782,  2.9843,  0.7366,  1.5672],\n        [ 3.5115, -0.4864, -1.2476, -4.4337]]])\n\n# Scalar input\n>>> torch.inner(a, torch.tensor(2))\ntensor([[1.6347, 2.1748, 2.3567],\n        [0.6558, 0.2469, 5.5787]])",
        "Z": "# Dot product\n>>> torch.inner(torch.tensor([1, 2, 3]), torch.tensor([0, 2, 1]))\ntensor(7)\n\n# Multidimensional input tensors\n>>> a = torch.randn(2, 3)\n>>> a\ntensor([[0.8173, 1.0874, 1.1784],\n        [0.3279, 0.1234, 2.7894]])\n>>> b = torch.randn(2, 4, 3)\n>>> b\ntensor([[[-0.4682, -0.7159,  0.1506],\n        [ 0.4034, -0.3657,  1.0387],\n        [ 0.9892, -0.6684,  0.1774],\n        [ 0.9482,  1.3261,  0.3917]],\n\n        [[ 0.4537,  0.7493,  1.1724],\n        [ 0.2291,  0.5749, -0.2267],\n        [-0.7920,  0.3607, -0.3701],\n        [ 1.3666, -0.5850, -1.7242]]])\n>>> torch.inner(a, b)\ntensor([[[-0.9837,  1.1560,  0.2907,  2.6785],\n        [ 2.5671,  0.5452, -0.6912, -1.5509]],\n\n        [[ 0.1782,  2.9843,  0.7366,  1.5672],\n        [ 3.5115, -0.4864, -1.2476, -4.4337]]])\n\n# Scalar input\n>>> torch.inner(a, torch.tensor(2))\ntensor([[1.6347, 2.1748, 2.3567],\n        [0.6558, 0.2469, 5.5787]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner"
    },
    {
        "X": "How to use torch.isclose, give an example?",
        "Y": ">>> torch.isclose(torch.tensor((1., 2, 3)), torch.tensor((1 + 1e-10, 3, 4)))\ntensor([ True, False, False])\n>>> torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5)\ntensor([True, True])",
        "Z": "where input and other are finite. Where input\nand/or other are nonfinite they are close if and only if\nthey are equal, with NaNs being considered equal to each other when\nequal_nan is True. >>> torch.isclose(torch.tensor((1., 2, 3)), torch.tensor((1 + 1e-10, 3, 4)))\ntensor([ True, False, False])\n>>> torch.isclose(torch.tensor((float('inf'), 4)), torch.tensor((float('inf'), 6)), rtol=.5)\ntensor([True, True])",
        "source": "https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"
    },
    {
        "X": "How to use torch.rsqrt, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.0370,  0.2970,  1.5420, -0.9105])\n>>> torch.rsqrt(a)\ntensor([    nan,  1.8351,  0.8053,     nan])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.0370,  0.2970,  1.5420, -0.9105])\n>>> torch.rsqrt(a)\ntensor([    nan,  1.8351,  0.8053,     nan])",
        "source": "https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt"
    },
    {
        "X": "How to use torch.searchsorted, give an example?",
        "Y": ">>> sorted_sequence = torch.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]])\n>>> sorted_sequence\ntensor([[ 1,  3,  5,  7,  9],\n        [ 2,  4,  6,  8, 10]])\n>>> values = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> values\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.searchsorted(sorted_sequence, values)\ntensor([[1, 3, 4],\n        [1, 2, 4]])\n>>> torch.searchsorted(sorted_sequence, values, right=True)\ntensor([[2, 3, 5],\n        [1, 3, 4]])\n\n>>> sorted_sequence_1d = torch.tensor([1, 3, 5, 7, 9])\n>>> sorted_sequence_1d\ntensor([1, 3, 5, 7, 9])\n>>> torch.searchsorted(sorted_sequence_1d, values)\ntensor([[1, 3, 4],\n        [1, 3, 4]])",
        "Z": ">>> sorted_sequence = torch.tensor([[1, 3, 5, 7, 9], [2, 4, 6, 8, 10]])\n>>> sorted_sequence\ntensor([[ 1,  3,  5,  7,  9],\n        [ 2,  4,  6,  8, 10]])\n>>> values = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> values\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.searchsorted(sorted_sequence, values)\ntensor([[1, 3, 4],\n        [1, 2, 4]])\n>>> torch.searchsorted(sorted_sequence, values, right=True)\ntensor([[2, 3, 5],\n        [1, 3, 4]])\n\n>>> sorted_sequence_1d = torch.tensor([1, 3, 5, 7, 9])\n>>> sorted_sequence_1d\ntensor([1, 3, 5, 7, 9])\n>>> torch.searchsorted(sorted_sequence_1d, values)\ntensor([[1, 3, 4],\n        [1, 3, 4]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.searchsorted.html#torch.searchsorted"
    },
    {
        "X": "How to use torch.block_diag, give an example?",
        "Y": ">>> import torch\n>>> A = torch.tensor([[0, 1], [1, 0]])\n>>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\n>>> C = torch.tensor(7)\n>>> D = torch.tensor([1, 2, 3])\n>>> E = torch.tensor([[4], [5], [6]])\n>>> torch.block_diag(A, B, C, D, E)\ntensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n        [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])",
        "Z": ">>> import torch\n>>> A = torch.tensor([[0, 1], [1, 0]])\n>>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])\n>>> C = torch.tensor(7)\n>>> D = torch.tensor([1, 2, 3])\n>>> E = torch.tensor([[4], [5], [6]])\n>>> torch.block_diag(A, B, C, D, E)\ntensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n        [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n        [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n        [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n        [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.block_diag.html#torch.block_diag"
    },
    {
        "X": "How to use torch.linalg.inv, give an example?",
        "Y": "torch.linalg.solve(A, B) == A.inv() @ B",
        "Z": "Consider using torch.linalg.solve() if possible for multiplying a matrix on the left by\nthe inverse, as: torch.linalg.solve(A, B) == A.inv() @ B",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "How  Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if A is a batch of matrices\nthen the output has the same batch dimensions., give an example?",
        "Y": ">>> x = torch.rand(4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> z\ntensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n        [ 0.0000,  1.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  1.0000,  0.0000],\n        [ 0.0000, -0.0000, -0.0000,  1.0000]])\n>>> torch.dist(z, torch.eye(4))\ntensor(1.1921e-07)\n\n>>> # Batched inverse example\n>>> x = torch.randn(2, 3, 4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> torch.dist(z, torch.eye(4).expand_as(x))\ntensor(1.9073e-06)\n\n>>> x = torch.rand(4, 4, dtype=torch.cdouble)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> torch.dist(z, torch.eye(4, dtype=torch.cdouble))\ntensor(7.5107e-16, dtype=torch.float64)",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if A is a batch of matrices\nthen the output has the same batch dimensions. >>> x = torch.rand(4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> z\ntensor([[ 1.0000, -0.0000, -0.0000,  0.0000],\n        [ 0.0000,  1.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  1.0000,  0.0000],\n        [ 0.0000, -0.0000, -0.0000,  1.0000]])\n>>> torch.dist(z, torch.eye(4))\ntensor(1.1921e-07)\n\n>>> # Batched inverse example\n>>> x = torch.randn(2, 3, 4, 4)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> torch.dist(z, torch.eye(4).expand_as(x))\ntensor(1.9073e-06)\n\n>>> x = torch.rand(4, 4, dtype=torch.cdouble)\n>>> y = torch.linalg.inv(x)\n>>> z = x @ y\n>>> torch.dist(z, torch.eye(4, dtype=torch.cdouble))\ntensor(7.5107e-16, dtype=torch.float64)",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.inv.html#torch.linalg.inv"
    },
    {
        "X": "How to use torch.bitwise_or, give an example?",
        "Y": ">>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-1, -2,  3], dtype=torch.int8)\n>>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, True, False])",
        "Z": ">>> torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([-1, -2,  3], dtype=torch.int8)\n>>> torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ True, True, False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or"
    },
    {
        "X": "How to use torch.igammac, give an example?",
        "Y": ">>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.igammac(a1, a2)\ntensor([0.6472, 0.4335, 0.2650])\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\ntensor([1., 1., 1.])",
        "Z": "Supports broadcasting to a common shape\nand float inputs. >>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.igammac(a1, a2)\ntensor([0.6472, 0.4335, 0.2650])\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\ntensor([1., 1., 1.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac"
    },
    {
        "X": "How to use torch.randint, give an example?",
        "Y": ">>> torch.randint(3, 5, (3,))\ntensor([4, 3, 4])\n\n\n>>> torch.randint(10, (2, 2))\ntensor([[0, 2],\n        [5, 5]])\n\n\n>>> torch.randint(3, 10, (2, 2))\ntensor([[4, 5],\n        [6, 7]])",
        "Z": "The shape of the tensor is defined by the variable argument size. >>> torch.randint(3, 5, (3,))\ntensor([4, 3, 4])\n\n\n>>> torch.randint(10, (2, 2))\ntensor([[0, 2],\n        [5, 5]])\n\n\n>>> torch.randint(3, 10, (2, 2))\ntensor([[4, 5],\n        [6, 7]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.randint.html#torch.randint"
    },
    {
        "X": "How to use torch.masked_select, give an example?",
        "Y": ">>> x = torch.randn(3, 4)\n>>> x\ntensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\n        [-1.2035,  1.2252,  0.5002,  0.6248],\n        [ 0.1307, -2.0608,  0.1244,  2.0139]])\n>>> mask = x.ge(0.5)\n>>> mask\ntensor([[False, False, False, False],\n        [False, True, True, True],\n        [False, False, False, True]])\n>>> torch.masked_select(x, mask)\ntensor([ 1.2252,  0.5002,  0.6248,  2.0139])",
        "Z": "The shapes of the mask tensor and the input tensor don\u2019t need\nto match, but they must be broadcastable. >>> x = torch.randn(3, 4)\n>>> x\ntensor([[ 0.3552, -2.3825, -0.8297,  0.3477],\n        [-1.2035,  1.2252,  0.5002,  0.6248],\n        [ 0.1307, -2.0608,  0.1244,  2.0139]])\n>>> mask = x.ge(0.5)\n>>> mask\ntensor([[False, False, False, False],\n        [False, True, True, True],\n        [False, False, False, True]])\n>>> torch.masked_select(x, mask)\ntensor([ 1.2252,  0.5002,  0.6248,  2.0139])",
        "source": "https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select"
    },
    {
        "X": "How to use torch.mm, give an example?",
        "Y": ">>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.mm(mat1, mat2)\ntensor([[ 0.4851,  0.5037, -0.3633],\n        [-0.0760, -3.6705,  2.4784]])",
        "Z": "This operator supports TensorFloat32. >>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.mm(mat1, mat2)\ntensor([[ 0.4851,  0.5037, -0.3633],\n        [-0.0760, -3.6705,  2.4784]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm"
    },
    {
        "X": "How to use torch.movedim, give an example?",
        "Y": ">>> t = torch.randn(3,2,1)\n>>> t\ntensor([[[-0.3362],\n        [-0.8437]],\n\n        [[-0.9627],\n        [ 0.1727]],\n\n        [[ 0.5173],\n        [-0.1398]]])\n>>> torch.movedim(t, 1, 0).shape\ntorch.Size([2, 3, 1])\n>>> torch.movedim(t, 1, 0)\ntensor([[[-0.3362],\n        [-0.9627],\n        [ 0.5173]],\n\n        [[-0.8437],\n        [ 0.1727],\n        [-0.1398]]])\n>>> torch.movedim(t, (1, 2), (0, 1)).shape\ntorch.Size([2, 1, 3])\n>>> torch.movedim(t, (1, 2), (0, 1))\ntensor([[[-0.3362, -0.9627,  0.5173]],\n\n        [[-0.8437,  0.1727, -0.1398]]])",
        "Z": "Other dimensions of input that are not explicitly moved remain in\ntheir original order and appear at the positions not specified in destination. >>> t = torch.randn(3,2,1)\n>>> t\ntensor([[[-0.3362],\n        [-0.8437]],\n\n        [[-0.9627],\n        [ 0.1727]],\n\n        [[ 0.5173],\n        [-0.1398]]])\n>>> torch.movedim(t, 1, 0).shape\ntorch.Size([2, 3, 1])\n>>> torch.movedim(t, 1, 0)\ntensor([[[-0.3362],\n        [-0.9627],\n        [ 0.5173]],\n\n        [[-0.8437],\n        [ 0.1727],\n        [-0.1398]]])\n>>> torch.movedim(t, (1, 2), (0, 1)).shape\ntorch.Size([2, 1, 3])\n>>> torch.movedim(t, (1, 2), (0, 1))\ntensor([[[-0.3362, -0.9627,  0.5173]],\n\n        [[-0.8437,  0.1727, -0.1398]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim"
    },
    {
        "X": "How to use torch.addmm, give an example?",
        "Y": ">>> M = torch.randn(2, 3)\n>>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.addmm(M, mat1, mat2)\ntensor([[-4.8716,  1.4671, -1.3746],\n        [ 0.7573, -3.9555, -2.8681]])",
        "Z": "This operator supports TensorFloat32. >>> M = torch.randn(2, 3)\n>>> mat1 = torch.randn(2, 3)\n>>> mat2 = torch.randn(3, 3)\n>>> torch.addmm(M, mat1, mat2)\ntensor([[-4.8716,  1.4671, -1.3746],\n        [ 0.7573, -3.9555, -2.8681]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm"
    },
    {
        "X": "How to use torch.prod, give an example?",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[-0.8020,  0.5428, -1.5854]])\n>>> torch.prod(a)\ntensor(0.6902)",
        "Z": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[-0.8020,  0.5428, -1.5854]])\n>>> torch.prod(a)\ntensor(0.6902)",
        "source": "https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"
    },
    {
        "X": "How  If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input., give an example?",
        "Y": ">>> a = torch.rand(4, 2).bool()\n>>> a\ntensor([[True, True],\n        [True, False],\n        [True, True],\n        [True, True]], dtype=torch.bool)\n>>> torch.all(a, dim=1)\ntensor([ True, False,  True,  True], dtype=torch.bool)\n>>> torch.all(a, dim=0)\ntensor([ True, False], dtype=torch.bool)",
        "Z": "If keepdim is True, the output tensor is of the same size\nas input except in the dimension dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensor having 1 fewer dimension than input. >>> a = torch.rand(4, 2).bool()\n>>> a\ntensor([[True, True],\n        [True, False],\n        [True, True],\n        [True, True]], dtype=torch.bool)\n>>> torch.all(a, dim=1)\ntensor([ True, False,  True,  True], dtype=torch.bool)\n>>> torch.all(a, dim=0)\ntensor([ True, False], dtype=torch.bool)",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "How to use torch.where, give an example?",
        "Y": ">>> x = torch.randn(3, 2)\n>>> y = torch.ones(3, 2)\n>>> x\ntensor([[-0.4620,  0.3139],\n        [ 0.3898, -0.7197],\n        [ 0.0478, -0.1657]])\n>>> torch.where(x > 0, x, y)\ntensor([[ 1.0000,  0.3139],\n        [ 0.3898,  1.0000],\n        [ 0.0478,  1.0000]])\n>>> x = torch.randn(2, 2, dtype=torch.double)\n>>> x\ntensor([[ 1.0779,  0.0383],\n        [-0.8785, -1.1089]], dtype=torch.float64)\n>>> torch.where(x > 0, x, 0.)\ntensor([[1.0779, 0.0383],\n        [0.0000, 0.0000]], dtype=torch.float64)",
        "Z": "The operation is defined as: >>> x = torch.randn(3, 2)\n>>> y = torch.ones(3, 2)\n>>> x\ntensor([[-0.4620,  0.3139],\n        [ 0.3898, -0.7197],\n        [ 0.0478, -0.1657]])\n>>> torch.where(x > 0, x, y)\ntensor([[ 1.0000,  0.3139],\n        [ 0.3898,  1.0000],\n        [ 0.0478,  1.0000]])\n>>> x = torch.randn(2, 2, dtype=torch.double)\n>>> x\ntensor([[ 1.0779,  0.0383],\n        [-0.8785, -1.1089]], dtype=torch.float64)\n>>> torch.where(x > 0, x, 0.)\ntensor([[1.0779, 0.0383],\n        [0.0000, 0.0000]], dtype=torch.float64)",
        "source": "https://pytorch.org/docs/stable/generated/torch.where.html#torch.where"
    },
    {
        "X": "How to use torch.empty_like, give an example?",
        "Y": ">>> torch.empty((2,3), dtype=torch.int64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])",
        "Z": ">>> torch.empty((2,3), dtype=torch.int64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.empty_like.html#torch.empty_like"
    },
    {
        "X": "How to use torch.bmm, give an example?",
        "Y": ">>> input = torch.randn(10, 3, 4)\n>>> mat2 = torch.randn(10, 4, 5)\n>>> res = torch.bmm(input, mat2)\n>>> res.size()\ntorch.Size([10, 3, 5])",
        "Z": "This operator supports TensorFloat32. >>> input = torch.randn(10, 3, 4)\n>>> mat2 = torch.randn(10, 4, 5)\n>>> res = torch.bmm(input, mat2)\n>>> res.size()\ntorch.Size([10, 3, 5])",
        "source": "https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"
    },
    {
        "X": "How to use torch.randn, give an example?",
        "Y": ">>> torch.randn(4)\ntensor([-2.1436,  0.9966,  2.3426, -0.6366])\n>>> torch.randn(2, 3)\ntensor([[ 1.5954,  2.8929, -1.0923],\n        [ 1.1719, -0.4709, -0.1996]])",
        "Z": "The shape of the tensor is defined by the variable argument size. >>> torch.randn(4)\ntensor([-2.1436,  0.9966,  2.3426, -0.6366])\n>>> torch.randn(2, 3)\ntensor([[ 1.5954,  2.8929, -1.0923],\n        [ 1.1719, -0.4709, -0.1996]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.randn.html#torch.randn"
    },
    {
        "X": "How to use torch.poisson, give an example?",
        "Y": ">>> rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5\n>>> torch.poisson(rates)\ntensor([[9., 1., 3., 5.],\n        [8., 6., 6., 0.],\n        [0., 4., 5., 3.],\n        [2., 1., 4., 2.]])",
        "Z": ">>> rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5\n>>> torch.poisson(rates)\ntensor([[9., 1., 3., 5.],\n        [8., 6., 6., 0.],\n        [0., 4., 5., 3.],\n        [2., 1., 4., 2.]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.poisson.html#torch.poisson"
    },
    {
        "X": "How to use torch.vstack, give an example?",
        "Y": ">>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.vstack((a,b))\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.vstack((a,b))\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])",
        "Z": "This is equivalent to concatenation along the first axis after all 1-D tensors have been reshaped by torch.atleast_2d(). >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.vstack((a,b))\ntensor([[1, 2, 3],\n        [4, 5, 6]])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.vstack((a,b))\ntensor([[1],\n        [2],\n        [3],\n        [4],\n        [5],\n        [6]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.vstack.html#torch.vstack"
    },
    {
        "X": "How to use torch.reshape, give an example?",
        "Y": ">>> a = torch.arange(4.)\n>>> torch.reshape(a, (2, 2))\ntensor([[ 0.,  1.],\n        [ 2.,  3.]])\n>>> b = torch.tensor([[0, 1], [2, 3]])\n>>> torch.reshape(b, (-1,))\ntensor([ 0,  1,  2,  3])",
        "Z": "A single dimension may be -1, in which case it\u2019s inferred from the remaining\ndimensions and the number of elements in input. >>> a = torch.arange(4.)\n>>> torch.reshape(a, (2, 2))\ntensor([[ 0.,  1.],\n        [ 2.,  3.]])\n>>> b = torch.tensor([[0, 1], [2, 3]])\n>>> torch.reshape(b, (-1,))\ntensor([ 0,  1,  2,  3])",
        "source": "https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape"
    },
    {
        "X": "How to use torch.bucketize, give an example?",
        "Y": ">>> boundaries = torch.tensor([1, 3, 5, 7, 9])\n>>> boundaries\ntensor([1, 3, 5, 7, 9])\n>>> v = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> v\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.bucketize(v, boundaries)\ntensor([[1, 3, 4],\n        [1, 3, 4]])\n>>> torch.bucketize(v, boundaries, right=True)\ntensor([[2, 3, 5],\n        [2, 3, 5]])",
        "Z": ">>> boundaries = torch.tensor([1, 3, 5, 7, 9])\n>>> boundaries\ntensor([1, 3, 5, 7, 9])\n>>> v = torch.tensor([[3, 6, 9], [3, 6, 9]])\n>>> v\ntensor([[3, 6, 9],\n        [3, 6, 9]])\n>>> torch.bucketize(v, boundaries)\ntensor([[1, 3, 4],\n        [1, 3, 4]])\n>>> torch.bucketize(v, boundaries, right=True)\ntensor([[2, 3, 5],\n        [2, 3, 5]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.bucketize.html#torch.bucketize"
    },
    {
        "X": "How to use torch.copysign, give an example?",
        "Y": ">>> a = torch.randn(5)\n>>> a\ntensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244])\n>>> torch.copysign(a, 1)\ntensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244])\n>>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.7079,  0.2778, -1.0249,  0.5719],\n        [-0.0059, -0.2600, -0.4475, -1.3948],\n        [ 0.3667, -0.9567, -2.5757, -0.1751],\n        [ 0.2046, -0.0742,  0.2998, -0.1054]])\n>>> b = torch.randn(4)\ntensor([ 0.2373,  0.3120,  0.3190, -1.1128])\n>>> torch.copysign(a, b)\ntensor([[ 0.7079,  0.2778,  1.0249, -0.5719],\n        [ 0.0059,  0.2600,  0.4475, -1.3948],\n        [ 0.3667,  0.9567,  2.5757, -0.1751],\n        [ 0.2046,  0.0742,  0.2998, -0.1054]])",
        "Z": "Supports broadcasting to a common shape,\nand integer and float inputs. >>> a = torch.randn(5)\n>>> a\ntensor([-1.2557, -0.0026, -0.5387,  0.4740, -0.9244])\n>>> torch.copysign(a, 1)\ntensor([1.2557, 0.0026, 0.5387, 0.4740, 0.9244])\n>>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.7079,  0.2778, -1.0249,  0.5719],\n        [-0.0059, -0.2600, -0.4475, -1.3948],\n        [ 0.3667, -0.9567, -2.5757, -0.1751],\n        [ 0.2046, -0.0742,  0.2998, -0.1054]])\n>>> b = torch.randn(4)\ntensor([ 0.2373,  0.3120,  0.3190, -1.1128])\n>>> torch.copysign(a, b)\ntensor([[ 0.7079,  0.2778,  1.0249, -0.5719],\n        [ 0.0059,  0.2600,  0.4475, -1.3948],\n        [ 0.3667,  0.9567,  2.5757, -0.1751],\n        [ 0.2046,  0.0742,  0.2998, -0.1054]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign"
    },
    {
        "X": "How to use torch.kthvalue, give an example?",
        "Y": ">>> x = torch.arange(1., 6.)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n>>> torch.kthvalue(x, 4)\ntorch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))\n\n>>> x=torch.arange(1.,7.).resize_(2,3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.]])\n>>> torch.kthvalue(x, 2, 0, True)\ntorch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))",
        "Z": "If keepdim is True, both the values and indices tensors\nare the same size as input, except in the dimension dim where\nthey are of size 1. Otherwise, dim is squeezed\n(see torch.squeeze()), resulting in both the values and\nindices tensors having 1 fewer dimension than the input tensor. >>> x = torch.arange(1., 6.)\n>>> x\ntensor([ 1.,  2.,  3.,  4.,  5.])\n>>> torch.kthvalue(x, 4)\ntorch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))\n\n>>> x=torch.arange(1.,7.).resize_(2,3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.]])\n>>> torch.kthvalue(x, 2, 0, True)\ntorch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"
    },
    {
        "X": "How to use torch.cholesky, give an example?",
        "Y": "L = torch.linalg.cholesky(A)",
        "Z": "torch.cholesky() is deprecated in favor of torch.linalg.cholesky()\nand will be removed in a future PyTorch release.L = torch.cholesky(A) should be replaced with L = torch.linalg.cholesky(A)",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "How  L = torch.cholesky(A) should be replaced withU = torch.cholesky(A, upper=True) should be replaced with, give an example?",
        "Y": "U = torch.linalg.cholesky(A.transpose(-2, -1).conj()).transpose(-2, -1).conj()",
        "Z": "L = torch.cholesky(A) should be replaced withU = torch.cholesky(A, upper=True) should be replaced with U = torch.linalg.cholesky(A.transpose(-2, -1).conj()).transpose(-2, -1).conj()",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "How  If upper is True, and AAA is a batch of symmetric positive-definite\nmatrices, then the returned tensor will be composed of upper-triangular Cholesky factors\nof each of the individual matrices. Similarly, when upper is False, the returned\ntensor will be composed of lower-triangular Cholesky factors of each of the individual\nmatrices., give an example?",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> a\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> l\ntensor([[ 1.5528,  0.0000,  0.0000],\n        [-0.4821,  1.0592,  0.0000],\n        [ 0.9371,  0.5487,  0.7023]])\n>>> torch.mm(l, l.t())\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> a = torch.randn(3, 2, 2)\n>>> a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> z = torch.matmul(l, l.transpose(-1, -2))\n>>> torch.max(torch.abs(z - a)) # Max non-zero\ntensor(2.3842e-07)",
        "Z": "If upper is True, and AAA is a batch of symmetric positive-definite\nmatrices, then the returned tensor will be composed of upper-triangular Cholesky factors\nof each of the individual matrices. Similarly, when upper is False, the returned\ntensor will be composed of lower-triangular Cholesky factors of each of the individual\nmatrices. >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> a\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> l\ntensor([[ 1.5528,  0.0000,  0.0000],\n        [-0.4821,  1.0592,  0.0000],\n        [ 0.9371,  0.5487,  0.7023]])\n>>> torch.mm(l, l.t())\ntensor([[ 2.4112, -0.7486,  1.4551],\n        [-0.7486,  1.3544,  0.1294],\n        [ 1.4551,  0.1294,  1.6724]])\n>>> a = torch.randn(3, 2, 2)\n>>> a = torch.matmul(a, a.transpose(-1, -2)) + 1e-03 # make symmetric positive-definite\n>>> l = torch.cholesky(a)\n>>> z = torch.matmul(l, l.transpose(-1, -2))\n>>> torch.max(torch.abs(z - a)) # Max non-zero\ntensor(2.3842e-07)",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"
    },
    {
        "X": "How to use torch.histc, give an example?",
        "Y": ">>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)\ntensor([ 0.,  2.,  1.,  0.])",
        "Z": "Elements lower than min and higher than max are ignored. >>> torch.histc(torch.tensor([1., 2, 1]), bins=4, min=0, max=3)\ntensor([ 0.,  2.,  1.,  0.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc"
    },
    {
        "X": "How to use torch.eq, give an example?",
        "Y": ">>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ True, False],\n        [False, True]])",
        "Z": "The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.eq(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[ True, False],\n        [False, True]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq"
    },
    {
        "X": "How to use torch.unsqueeze, give an example?",
        "Y": ">>> x = torch.tensor([1, 2, 3, 4])\n>>> torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n>>> torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])",
        "Z": "A dim value within the range [-input.dim() - 1, input.dim() + 1)\ncan be used. Negative dim will correspond to unsqueeze()\napplied at dim = dim + input.dim() + 1. >>> x = torch.tensor([1, 2, 3, 4])\n>>> torch.unsqueeze(x, 0)\ntensor([[ 1,  2,  3,  4]])\n>>> torch.unsqueeze(x, 1)\ntensor([[ 1],\n        [ 2],\n        [ 3],\n        [ 4]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze"
    },
    {
        "X": "How to use torch.matrix_rank, give an example?",
        "Y": ">>> a = torch.eye(10)\n>>> torch.matrix_rank(a)\ntensor(10)\n>>> b = torch.eye(10)\n>>> b[0, 0] = 0\n>>> torch.matrix_rank(b)\ntensor(9)",
        "Z": "tol is the threshold below which the singular values (or the eigenvalues\nwhen symmetric is True) are considered to be 0. If tol is not\nspecified, tol is set to S.max() * max(S.size()) * eps where S is the\nsingular values (or the eigenvalues when symmetric is True), and eps\nis the epsilon value for the datatype of input. >>> a = torch.eye(10)\n>>> torch.matrix_rank(a)\ntensor(10)\n>>> b = torch.eye(10)\n>>> b[0, 0] = 0\n>>> torch.matrix_rank(b)\ntensor(9)",
        "source": "https://pytorch.org/docs/stable/generated/torch.matrix_rank.html#torch.matrix_rank"
    },
    {
        "X": "How to use torch.argmax, give an example?",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)",
        "Z": "This is the second value returned by torch.max(). See its\ndocumentation for the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a)\ntensor(0)",
        "source": "https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "How  This is the second value returned by torch.max(). See its\ndocumentation for the exact semantics of this method., give an example?",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a, dim=1)\ntensor([ 0,  2,  0,  1])",
        "Z": "This is the second value returned by torch.max(). See its\ndocumentation for the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 1.3398,  0.2663, -0.2686,  0.2450],\n        [-0.7401, -0.8805, -0.3402, -1.1936],\n        [ 0.4907, -1.3948, -1.0691, -0.3132],\n        [-1.6092,  0.5419, -0.2993,  0.3195]])\n>>> torch.argmax(a, dim=1)\ntensor([ 0,  2,  0,  1])",
        "source": "https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"
    },
    {
        "X": "How to use torch.imag, give an example?",
        "Y": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])",
        "Z": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.imag\ntensor([ 0.3553, -0.7896, -0.0633, -0.8119])",
        "source": "https://pytorch.org/docs/stable/generated/torch.imag.html#torch.imag"
    },
    {
        "X": "How to use torch.exp, give an example?",
        "Y": ">>> torch.exp(torch.tensor([0, math.log(2.)]))\ntensor([ 1.,  2.])",
        "Z": ">>> torch.exp(torch.tensor([0, math.log(2.)]))\ntensor([ 1.,  2.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp"
    },
    {
        "X": "How to use torch.angle, give an example?",
        "Y": ">>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])",
        "Z": ">>> torch.angle(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))*180/3.14159\ntensor([ 135.,  135,  -45])",
        "source": "https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle"
    },
    {
        "X": "How  Similar to SciPy\u2019s scipy.special.xlog1py., give an example?",
        "Y": ">>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlog1py(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlog1py(x, y)\ntensor([1.3863, 2.1972, 2.0794])\n>>> torch.special.xlog1py(x, 4)\ntensor([1.6094, 3.2189, 4.8283])\n>>> torch.special.xlog1py(2, y)\ntensor([2.7726, 2.1972, 1.3863])",
        "Z": "Similar to SciPy\u2019s scipy.special.xlog1py. >>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.special.xlog1py(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.special.xlog1py(x, y)\ntensor([1.3863, 2.1972, 2.0794])\n>>> torch.special.xlog1py(x, 4)\ntensor([1.6094, 3.2189, 4.8283])\n>>> torch.special.xlog1py(2, y)\ntensor([2.7726, 2.1972, 1.3863])",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.expit"
    },
    {
        "X": "How to use torch.Tensor.scatter_, give an example?",
        "Y": "self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2",
        "Z": "For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "How  Additionally accepts an optional reduce argument that allows\nspecification of an optional reduction operation, which is applied to all\nvalues in the tensor src into self at the indicies\nspecified in the index. For each value in src, the reduction\noperation is applied to an index in self which is specified by\nits index in src for dimension != dim and by the corresponding\nvalue in index for dimension = dim.Given a 3-D tensor and reduction using the multiplication operation, self\nis updated as:, give an example?",
        "Y": "self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2",
        "Z": "Additionally accepts an optional reduce argument that allows\nspecification of an optional reduction operation, which is applied to all\nvalues in the tensor src into self at the indicies\nspecified in the index. For each value in src, the reduction\noperation is applied to an index in self which is specified by\nits index in src for dimension != dim and by the corresponding\nvalue in index for dimension = dim.Given a 3-D tensor and reduction using the multiplication operation, self\nis updated as: self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "How  Reducing with the addition operation is the same as using\nscatter_add_()., give an example?",
        "Y": ">>> src = torch.arange(1, 11).reshape((2, 5))\n>>> src\ntensor([[ 1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10]])\n>>> index = torch.tensor([[0, 1, 2, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\ntensor([[1, 0, 0, 4, 0],\n        [0, 2, 0, 0, 0],\n        [0, 0, 3, 0, 0]])\n>>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\ntensor([[1, 2, 3, 0, 0],\n        [6, 7, 0, 0, 8],\n        [0, 0, 0, 0, 0]])\n\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='multiply')\ntensor([[2.0000, 2.0000, 2.4600, 2.0000],\n        [2.0000, 2.0000, 2.0000, 2.4600]])\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='add')\ntensor([[2.0000, 2.0000, 3.2300, 2.0000],\n        [2.0000, 2.0000, 2.0000, 3.2300]])",
        "Z": "Reducing with the addition operation is the same as using\nscatter_add_(). >>> src = torch.arange(1, 11).reshape((2, 5))\n>>> src\ntensor([[ 1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10]])\n>>> index = torch.tensor([[0, 1, 2, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\ntensor([[1, 0, 0, 4, 0],\n        [0, 2, 0, 0, 0],\n        [0, 0, 3, 0, 0]])\n>>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\ntensor([[1, 2, 3, 0, 0],\n        [6, 7, 0, 0, 8],\n        [0, 0, 0, 0, 0]])\n\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='multiply')\ntensor([[2.0000, 2.0000, 2.4600, 2.0000],\n        [2.0000, 2.0000, 2.0000, 2.4600]])\n>>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n...            1.23, reduce='add')\ntensor([[2.0000, 2.0000, 3.2300, 2.0000],\n        [2.0000, 2.0000, 2.0000, 3.2300]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"
    },
    {
        "X": "How to use torch.range, give an example?",
        "Y": ">>> torch.range(1, 4)\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.range(1, 4, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])",
        "Z": ">>> torch.range(1, 4)\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.range(1, 4, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000,  2.5000,  3.0000,  3.5000,  4.0000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.range.html#torch.range"
    },
    {
        "X": "How to use torch.broadcast_to, give an example?",
        "Y": ">>> x = torch.tensor([1, 2, 3])\n>>> torch.broadcast_to(x, (3, 3))\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])",
        "Z": ">>> x = torch.tensor([1, 2, 3])\n>>> torch.broadcast_to(x, (3, 3))\ntensor([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to"
    },
    {
        "X": "How to use torch.squeeze, give an example?",
        "Y": ">>> x = torch.zeros(2, 1, 2, 1, 2)\n>>> x.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x)\n>>> y.size()\ntorch.Size([2, 2, 2])\n>>> y = torch.squeeze(x, 0)\n>>> y.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x, 1)\n>>> y.size()\ntorch.Size([2, 2, 1, 2])",
        "Z": "When dim is given, a squeeze operation is done only in the given\ndimension. If input is of shape: (A\u00d71\u00d7B)(A \\times 1 \\times B)(A\u00d71\u00d7B),\nsqueeze(input, 0) leaves the tensor unchanged, but squeeze(input, 1)\nwill squeeze the tensor to the shape (A\u00d7B)(A \\times B)(A\u00d7B). >>> x = torch.zeros(2, 1, 2, 1, 2)\n>>> x.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x)\n>>> y.size()\ntorch.Size([2, 2, 2])\n>>> y = torch.squeeze(x, 0)\n>>> y.size()\ntorch.Size([2, 1, 2, 1, 2])\n>>> y = torch.squeeze(x, 1)\n>>> y.size()\ntorch.Size([2, 2, 1, 2])",
        "source": "https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"
    },
    {
        "X": "How to use You can use torch.manual_seed() to seed the RNG for all devices (both\nCPU and CUDA):, give an example?",
        "Y": "import torch\ntorch.manual_seed(0)",
        "Z": "You can use torch.manual_seed() to seed the RNG for all devices (both\nCPU and CUDA): import torch\ntorch.manual_seed(0)",
        "source": "https://pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "How to use For custom operators, you might need to set python seed as well:, give an example?",
        "Y": "import random\nrandom.seed(0)",
        "Z": "For custom operators, you might need to set python seed as well: import random\nrandom.seed(0)",
        "source": "https://pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "How to use If you or any of the libraries you are using rely on NumPy, you can seed the global\nNumPy RNG with:, give an example?",
        "Y": "import numpy as np\nnp.random.seed(0)",
        "Z": "If you or any of the libraries you are using rely on NumPy, you can seed the global\nNumPy RNG with: import numpy as np\nnp.random.seed(0)",
        "source": "https://pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "How to use Please check the documentation for torch.use_deterministic_algorithms()\nfor a full list of affected operations. If an operation does not act correctly\naccording to the documentation, or if you need a deterministic implementation\nof an operation that does not have one, please submit an issue:\nhttps://github.com/pytorch/pytorch/issues?q=label:%22topic:%20determinism%22For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_()\nwill throw an error:, give an example?",
        "Y": ">>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set\n'torch.use_deterministic_algorithms(True)'. ...",
        "Z": "Please check the documentation for torch.use_deterministic_algorithms()\nfor a full list of affected operations. If an operation does not act correctly\naccording to the documentation, or if you need a deterministic implementation\nof an operation that does not have one, please submit an issue:\nhttps://github.com/pytorch/pytorch/issues?q=label:%22topic:%20determinism%22For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_()\nwill throw an error: >>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.randn(2, 2).cuda().index_add_(0, torch.tensor([0, 1]), torch.randn(2, 2))\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: index_add_cuda_ does not have a deterministic implementation, but you set\n'torch.use_deterministic_algorithms(True)'. ...",
        "source": "https://pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "How to use For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_()\nwill throw an error:When torch.bmm() is called with sparse-dense CUDA tensors it typically uses a\nnondeterministic algorithm, but when the deterministic flag is turned on, its alternate\ndeterministic implementation will be used:, give an example?",
        "Y": ">>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())\ntensor([[[ 1.1900, -2.3409],\n         [ 0.4796,  0.8003]],\n        [[ 0.1509,  1.8027],\n         [ 0.0333, -1.1444]]], device='cuda:0')",
        "Z": "For example, running the nondeterministic CUDA implementation of torch.Tensor.index_add_()\nwill throw an error:When torch.bmm() is called with sparse-dense CUDA tensors it typically uses a\nnondeterministic algorithm, but when the deterministic flag is turned on, its alternate\ndeterministic implementation will be used: >>> import torch\n>>> torch.use_deterministic_algorithms(True)\n>>> torch.bmm(torch.randn(2, 2, 2).to_sparse().cuda(), torch.randn(2, 2, 2).cuda())\ntensor([[[ 1.1900, -2.3409],\n         [ 0.4796,  0.8003]],\n        [[ 0.1509,  1.8027],\n         [ 0.0333, -1.1444]]], device='cuda:0')",
        "source": "https://pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "How to use DataLoader will reseed workers following Randomness in multi-process data loading algorithm.\nUse worker_init_fn() and generator to preserve reproducibility:, give an example?",
        "Y": "def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)\n\nDataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    worker_init_fn=seed_worker\n    generator=g,\n)",
        "Z": "DataLoader will reseed workers following Randomness in multi-process data loading algorithm.\nUse worker_init_fn() and generator to preserve reproducibility: def seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    numpy.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ng = torch.Generator()\ng.manual_seed(0)\n\nDataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    num_workers=num_workers,\n    worker_init_fn=seed_worker\n    generator=g,\n)",
        "source": "https://pytorch.org/docs/stable/notes/randomness.html"
    },
    {
        "X": "How to use torch.dsplit, give an example?",
        "Y": ">>> t = torch.arange(16.0).reshape(2, 2, 4)\n>>> t\ntensor([[[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.]],\n        [[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]])\n>>> torch.dsplit(t, 2)\n(tensor([[[ 0.,  1.],\n        [ 4.,  5.]],\n       [[ 8.,  9.],\n        [12., 13.]]]),\n tensor([[[ 2.,  3.],\n          [ 6.,  7.]],\n         [[10., 11.],\n          [14., 15.]]]))",
        "Z": ">>> t = torch.arange(16.0).reshape(2, 2, 4)\n>>> t\ntensor([[[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.]],\n        [[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]])\n>>> torch.dsplit(t, 2)\n(tensor([[[ 0.,  1.],\n        [ 4.,  5.]],\n       [[ 8.,  9.],\n        [12., 13.]]]),\n tensor([[[ 2.,  3.],\n          [ 6.,  7.]],\n         [[10., 11.],\n          [14., 15.]]]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit"
    },
    {
        "X": "How to use torch.can_cast, give an example?",
        "Y": ">>> torch.can_cast(torch.double, torch.float)\nTrue\n>>> torch.can_cast(torch.float, torch.int)\nFalse",
        "Z": ">>> torch.can_cast(torch.double, torch.float)\nTrue\n>>> torch.can_cast(torch.float, torch.int)\nFalse",
        "source": "https://pytorch.org/docs/stable/generated/torch.can_cast.html#torch.can_cast"
    },
    {
        "X": "How to use torch.linalg.slogdet, give an example?",
        "Y": ">>> A = torch.randn(3, 3)\n>>> A\ntensor([[ 0.0032, -0.2239, -1.1219],\n        [-0.6690,  0.1161,  0.4053],\n        [-1.6218, -0.9273, -0.0082]])\n>>> torch.linalg.det(A)\ntensor(-0.7576)\n>>> torch.linalg.logdet(A)\ntensor(nan)\n>>> torch.linalg.slogdet(A)\ntorch.return_types.linalg_slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if A is a batch of matrices then\nthe output has the same batch dimensions. >>> A = torch.randn(3, 3)\n>>> A\ntensor([[ 0.0032, -0.2239, -1.1219],\n        [-0.6690,  0.1161,  0.4053],\n        [-1.6218, -0.9273, -0.0082]])\n>>> torch.linalg.det(A)\ntensor(-0.7576)\n>>> torch.linalg.logdet(A)\ntensor(nan)\n>>> torch.linalg.slogdet(A)\ntorch.return_types.linalg_slogdet(sign=tensor(-1.), logabsdet=tensor(-0.2776))",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.slogdet.html#torch.linalg.slogdet"
    },
    {
        "X": "How to use torch.qr, give an example?",
        "Y": "Q, R = torch.linalg.qr(A)",
        "Z": "torch.qr() is deprecated in favor of torch.linalg.qr()\nand will be removed in a future PyTorch release. The boolean parameter some has been\nreplaced with a string parameter mode.Q, R = torch.qr(A) should be replaced with Q, R = torch.linalg.qr(A)",
        "source": "https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr"
    },
    {
        "X": "How  Q, R = torch.qr(A) should be replaced withQ, R = torch.qr(A, some=False) should be replaced with, give an example?",
        "Y": "Q, R = torch.linalg.qr(A, mode=\"complete\")",
        "Z": "Q, R = torch.qr(A) should be replaced withQ, R = torch.qr(A, some=False) should be replaced with Q, R = torch.linalg.qr(A, mode=\"complete\")",
        "source": "https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr"
    },
    {
        "X": "How  If some is True, then this function returns the thin (reduced) QR factorization.\nOtherwise, if some is False, this function returns the complete QR factorization., give an example?",
        "Y": ">>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n>>> q, r = torch.qr(a)\n>>> q\ntensor([[-0.8571,  0.3943,  0.3314],\n        [-0.4286, -0.9029, -0.0343],\n        [ 0.2857, -0.1714,  0.9429]])\n>>> r\ntensor([[ -14.0000,  -21.0000,   14.0000],\n        [   0.0000, -175.0000,   70.0000],\n        [   0.0000,    0.0000,  -35.0000]])\n>>> torch.mm(q, r).round()\ntensor([[  12.,  -51.,    4.],\n        [   6.,  167.,  -68.],\n        [  -4.,   24.,  -41.]])\n>>> torch.mm(q.t(), q).round()\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1., -0.],\n        [ 0., -0.,  1.]])\n>>> a = torch.randn(3, 4, 5)\n>>> q, r = torch.qr(a, some=False)\n>>> torch.allclose(torch.matmul(q, r), a)\nTrue\n>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))\nTrue",
        "Z": "If some is True, then this function returns the thin (reduced) QR factorization.\nOtherwise, if some is False, this function returns the complete QR factorization. >>> a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])\n>>> q, r = torch.qr(a)\n>>> q\ntensor([[-0.8571,  0.3943,  0.3314],\n        [-0.4286, -0.9029, -0.0343],\n        [ 0.2857, -0.1714,  0.9429]])\n>>> r\ntensor([[ -14.0000,  -21.0000,   14.0000],\n        [   0.0000, -175.0000,   70.0000],\n        [   0.0000,    0.0000,  -35.0000]])\n>>> torch.mm(q, r).round()\ntensor([[  12.,  -51.,    4.],\n        [   6.,  167.,  -68.],\n        [  -4.,   24.,  -41.]])\n>>> torch.mm(q.t(), q).round()\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1., -0.],\n        [ 0., -0.,  1.]])\n>>> a = torch.randn(3, 4, 5)\n>>> q, r = torch.qr(a, some=False)\n>>> torch.allclose(torch.matmul(q, r), a)\nTrue\n>>> torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))\nTrue",
        "source": "https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr"
    },
    {
        "X": "How to use torch.nan_to_num, give an example?",
        "Y": ">>> x = torch.tensor([float('nan'), float('inf'), -float('inf'), 3.14])\n>>> torch.nan_to_num(x)\ntensor([ 0.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\n>>> torch.nan_to_num(x, nan=2.0)\ntensor([ 2.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\n>>> torch.nan_to_num(x, nan=2.0, posinf=1.0)\ntensor([ 2.0000e+00,  1.0000e+00, -3.4028e+38,  3.1400e+00])",
        "Z": ">>> x = torch.tensor([float('nan'), float('inf'), -float('inf'), 3.14])\n>>> torch.nan_to_num(x)\ntensor([ 0.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\n>>> torch.nan_to_num(x, nan=2.0)\ntensor([ 2.0000e+00,  3.4028e+38, -3.4028e+38,  3.1400e+00])\n>>> torch.nan_to_num(x, nan=2.0, posinf=1.0)\ntensor([ 2.0000e+00,  1.0000e+00, -3.4028e+38,  3.1400e+00])",
        "source": "https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"
    },
    {
        "X": "How to use torch.div, give an example?",
        "Y": ">>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\n>>> torch.div(x, 0.5)\ntensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])\n\n>>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],\n...                   [ 0.1815, -1.0111,  0.9805, -1.5923],\n...                   [ 0.1062,  1.4581,  0.7759, -1.2344],\n...                   [-0.1830, -0.0313,  1.1908, -1.4757]])\n>>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])\n>>> torch.div(a, b)\ntensor([[-0.4620, -6.6051,  0.5676,  1.2639],\n        [ 0.2260, -3.4509, -1.2086,  6.8990],\n        [ 0.1322,  4.9764, -0.9564,  5.3484],\n        [-0.2278, -0.1068, -1.4678,  6.3938]])\n\n>>> torch.div(a, b, rounding_mode='trunc')\ntensor([[-0., -6.,  0.,  1.],\n        [ 0., -3., -1.,  6.],\n        [ 0.,  4., -0.,  5.],\n        [-0., -0., -1.,  6.]])\n\n>>> torch.div(a, b, rounding_mode='floor')\ntensor([[-1., -7.,  0.,  1.],\n        [ 0., -4., -2.,  6.],\n        [ 0.,  4., -1.,  5.],\n        [-1., -1., -2.,  6.]])",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer, float, and complex inputs.\nAlways promotes integer types to the default scalar type. >>> x = torch.tensor([ 0.3810,  1.2774, -0.2972, -0.3719,  0.4637])\n>>> torch.div(x, 0.5)\ntensor([ 0.7620,  2.5548, -0.5944, -0.7438,  0.9274])\n\n>>> a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],\n...                   [ 0.1815, -1.0111,  0.9805, -1.5923],\n...                   [ 0.1062,  1.4581,  0.7759, -1.2344],\n...                   [-0.1830, -0.0313,  1.1908, -1.4757]])\n>>> b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])\n>>> torch.div(a, b)\ntensor([[-0.4620, -6.6051,  0.5676,  1.2639],\n        [ 0.2260, -3.4509, -1.2086,  6.8990],\n        [ 0.1322,  4.9764, -0.9564,  5.3484],\n        [-0.2278, -0.1068, -1.4678,  6.3938]])\n\n>>> torch.div(a, b, rounding_mode='trunc')\ntensor([[-0., -6.,  0.,  1.],\n        [ 0., -3., -1.,  6.],\n        [ 0.,  4., -0.,  5.],\n        [-0., -0., -1.,  6.]])\n\n>>> torch.div(a, b, rounding_mode='floor')\ntensor([[-1., -7.,  0.,  1.],\n        [ 0., -4., -2.,  6.],\n        [ 0.,  4., -1.,  5.],\n        [-1., -1., -2.,  6.]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"
    },
    {
        "X": "How to use Mixing Tracing and Scripting, give an example?",
        "Y": "import torch\n\ndef foo(x, y):\n    return 2 * x + y\n\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n@torch.jit.script\ndef bar(x):\n    return traced_foo(x, x)",
        "Z": "import torch\n\ndef foo(x, y):\n    return 2 * x + y\n\ntraced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))\n\n@torch.jit.script\ndef bar(x):\n    return traced_foo(x, x)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use Setting the environment variable PYTORCH_JIT=0 will disable all script\nand tracing annotations. If there is hard-to-debug error in one of your\nTorchScript models, you can use this flag to force everything to run using native\nPython. Since TorchScript (scripting and tracing) is disabled with this flag,\nyou can use tools like pdb to debug the model code.  For example:, give an example?",
        "Y": "@torch.jit.script\ndef scripted_fn(x : torch.Tensor):\n    for i in range(12):\n        x = x + x\n    return x\n\ndef fn(x):\n    x = torch.neg(x)\n    import pdb; pdb.set_trace()\n    return scripted_fn(x)\n\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\ntraced_fn(torch.rand(3, 4))",
        "Z": "Setting the environment variable PYTORCH_JIT=0 will disable all script\nand tracing annotations. If there is hard-to-debug error in one of your\nTorchScript models, you can use this flag to force everything to run using native\nPython. Since TorchScript (scripting and tracing) is disabled with this flag,\nyou can use tools like pdb to debug the model code.  For example: @torch.jit.script\ndef scripted_fn(x : torch.Tensor):\n    for i in range(12):\n        x = x + x\n    return x\n\ndef fn(x):\n    x = torch.neg(x)\n    import pdb; pdb.set_trace()\n    return scripted_fn(x)\n\ntraced_fn = torch.jit.trace(fn, (torch.rand(4, 5),))\ntraced_fn(torch.rand(3, 4))",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use Setting the environment variable PYTORCH_JIT=0 will disable all script\nand tracing annotations. If there is hard-to-debug error in one of your\nTorchScript models, you can use this flag to force everything to run using native\nPython. Since TorchScript (scripting and tracing) is disabled with this flag,\nyou can use tools like pdb to debug the model code.  For example:Debugging this script with pdb works except for when we invoke the\n@torch.jit.script function. We can globally disable\nJIT, so that we can call the @torch.jit.script\nfunction as a normal Python function and not compile it. If the above script\nis called disable_jit_example.py, we can invoke it like so:, give an example?",
        "Y": "$ PYTORCH_JIT=0 python disable_jit_example.py",
        "Z": "Debugging this script with pdb works except for when we invoke the\n@torch.jit.script function. We can globally disable\nJIT, so that we can call the @torch.jit.script\nfunction as a normal Python function and not compile it. If the above script\nis called disable_jit_example.py, we can invoke it like so: $ PYTORCH_JIT=0 python disable_jit_example.py",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use Inspecting Code, give an example?",
        "Y": "@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.code)",
        "Z": "@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.code)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use TorchScript provides a code pretty-printer for all ScriptModule instances. This\npretty-printer gives an interpretation of the script method\u2019s code as valid\nPython syntax. For example:A ScriptModule with a single forward method will have an attribute\ncode, which you can use to inspect the ScriptModule\u2019s code.\nIf the ScriptModule has more than one method, you will need to access\n.code on the method itself and not the module. We can inspect the\ncode of a method named foo on a ScriptModule by accessing .foo.code.\nThe example above produces this output:, give an example?",
        "Y": "def foo(len: int) -> Tensor:\n    rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\n    rv0 = rv\n    for i in range(len):\n        if torch.lt(i, 10):\n            rv1 = torch.sub(rv0, 1., 1)\n        else:\n            rv1 = torch.add(rv0, 1., 1)\n        rv0 = rv1\n    return rv0",
        "Z": "A ScriptModule with a single forward method will have an attribute\ncode, which you can use to inspect the ScriptModule\u2019s code.\nIf the ScriptModule has more than one method, you will need to access\n.code on the method itself and not the module. We can inspect the\ncode of a method named foo on a ScriptModule by accessing .foo.code.\nThe example above produces this output: def foo(len: int) -> Tensor:\n    rv = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\n    rv0 = rv\n    for i in range(len):\n        if torch.lt(i, 10):\n            rv1 = torch.sub(rv0, 1., 1)\n        else:\n            rv1 = torch.add(rv0, 1., 1)\n        rv0 = rv1\n    return rv0",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use Interpreting Graphs, give an example?",
        "Y": "@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.graph)",
        "Z": "@torch.jit.script\ndef foo(len):\n    # type: (int) -> torch.Tensor\n    rv = torch.zeros(3, 4)\n    for i in range(len):\n        if i < 10:\n            rv = rv - 1.0\n        else:\n            rv = rv + 1.0\n    return rv\n\nprint(foo.graph)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use graph follows the same rules described in the Inspecting Code section\nwith regard to forward method lookup.The example script above produces the graph:, give an example?",
        "Y": "graph(%len.1 : int):\n  %24 : int = prim::Constant[value=1]()\n  %17 : bool = prim::Constant[value=1]() # test.py:10:5\n  %12 : bool? = prim::Constant()\n  %10 : Device? = prim::Constant()\n  %6 : int? = prim::Constant()\n  %1 : int = prim::Constant[value=3]() # test.py:9:22\n  %2 : int = prim::Constant[value=4]() # test.py:9:25\n  %20 : int = prim::Constant[value=10]() # test.py:11:16\n  %23 : float = prim::Constant[value=1]() # test.py:12:23\n  %4 : int[] = prim::ListConstruct(%1, %2)\n  %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10\n  %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5\n    block0(%i.1 : int, %rv.14 : Tensor):\n      %21 : bool = aten::lt(%i.1, %20) # test.py:11:12\n      %rv.13 : Tensor = prim::If(%21) # test.py:11:9\n        block0():\n          %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18\n          -> (%rv.3)\n        block1():\n          %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18\n          -> (%rv.6)\n      -> (%17, %rv.13)\n  return (%rv)",
        "Z": "graph follows the same rules described in the Inspecting Code section\nwith regard to forward method lookup.The example script above produces the graph: graph(%len.1 : int):\n  %24 : int = prim::Constant[value=1]()\n  %17 : bool = prim::Constant[value=1]() # test.py:10:5\n  %12 : bool? = prim::Constant()\n  %10 : Device? = prim::Constant()\n  %6 : int? = prim::Constant()\n  %1 : int = prim::Constant[value=3]() # test.py:9:22\n  %2 : int = prim::Constant[value=4]() # test.py:9:25\n  %20 : int = prim::Constant[value=10]() # test.py:11:16\n  %23 : float = prim::Constant[value=1]() # test.py:12:23\n  %4 : int[] = prim::ListConstruct(%1, %2)\n  %rv.1 : Tensor = aten::zeros(%4, %6, %6, %10, %12) # test.py:9:10\n  %rv : Tensor = prim::Loop(%len.1, %17, %rv.1) # test.py:10:5\n    block0(%i.1 : int, %rv.14 : Tensor):\n      %21 : bool = aten::lt(%i.1, %20) # test.py:11:12\n      %rv.13 : Tensor = prim::If(%21) # test.py:11:9\n        block0():\n          %rv.3 : Tensor = aten::sub(%rv.14, %23, %24) # test.py:12:18\n          -> (%rv.3)\n        block1():\n          %rv.6 : Tensor = aten::add(%rv.14, %23, %24) # test.py:14:18\n          -> (%rv.6)\n      -> (%17, %rv.13)\n  return (%rv)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use One way to automatically catch many errors in traces is by using check_inputs\non the torch.jit.trace() API. check_inputs takes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example:, give an example?",
        "Y": "def loop_in_traced_fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\ntraced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)",
        "Z": "One way to automatically catch many errors in traces is by using check_inputs\non the torch.jit.trace() API. check_inputs takes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example: def loop_in_traced_fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\ntraced = torch.jit.trace(loop_in_traced_fn, inputs, check_inputs=check_inputs)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use One way to automatically catch many errors in traces is by using check_inputs\non the torch.jit.trace() API. check_inputs takes a list of tuples\nof inputs that will be used to re-trace the computation and verify the\nresults. For example:Gives us the following diagnostic information:, give an example?",
        "Y": "ERROR: Graphs differed across invocations!\nGraph diff:\n\n            graph(%x : Tensor) {\n            %1 : int = prim::Constant[value=0]()\n            %2 : int = prim::Constant[value=0]()\n            %result.1 : Tensor = aten::select(%x, %1, %2)\n            %4 : int = prim::Constant[value=0]()\n            %5 : int = prim::Constant[value=0]()\n            %6 : Tensor = aten::select(%x, %4, %5)\n            %result.2 : Tensor = aten::mul(%result.1, %6)\n            %8 : int = prim::Constant[value=0]()\n            %9 : int = prim::Constant[value=1]()\n            %10 : Tensor = aten::select(%x, %8, %9)\n        -   %result : Tensor = aten::mul(%result.2, %10)\n        +   %result.3 : Tensor = aten::mul(%result.2, %10)\n        ?          ++\n            %12 : int = prim::Constant[value=0]()\n            %13 : int = prim::Constant[value=2]()\n            %14 : Tensor = aten::select(%x, %12, %13)\n        +   %result : Tensor = aten::mul(%result.3, %14)\n        +   %16 : int = prim::Constant[value=0]()\n        +   %17 : int = prim::Constant[value=3]()\n        +   %18 : Tensor = aten::select(%x, %16, %17)\n        -   %15 : Tensor = aten::mul(%result, %14)\n        ?     ^                                 ^\n        +   %19 : Tensor = aten::mul(%result, %18)\n        ?     ^                                 ^\n        -   return (%15);\n        ?             ^\n        +   return (%19);\n        ?             ^\n            }",
        "Z": "Gives us the following diagnostic information: ERROR: Graphs differed across invocations!\nGraph diff:\n\n            graph(%x : Tensor) {\n            %1 : int = prim::Constant[value=0]()\n            %2 : int = prim::Constant[value=0]()\n            %result.1 : Tensor = aten::select(%x, %1, %2)\n            %4 : int = prim::Constant[value=0]()\n            %5 : int = prim::Constant[value=0]()\n            %6 : Tensor = aten::select(%x, %4, %5)\n            %result.2 : Tensor = aten::mul(%result.1, %6)\n            %8 : int = prim::Constant[value=0]()\n            %9 : int = prim::Constant[value=1]()\n            %10 : Tensor = aten::select(%x, %8, %9)\n        -   %result : Tensor = aten::mul(%result.2, %10)\n        +   %result.3 : Tensor = aten::mul(%result.2, %10)\n        ?          ++\n            %12 : int = prim::Constant[value=0]()\n            %13 : int = prim::Constant[value=2]()\n            %14 : Tensor = aten::select(%x, %12, %13)\n        +   %result : Tensor = aten::mul(%result.3, %14)\n        +   %16 : int = prim::Constant[value=0]()\n        +   %17 : int = prim::Constant[value=3]()\n        +   %18 : Tensor = aten::select(%x, %16, %17)\n        -   %15 : Tensor = aten::mul(%result, %14)\n        ?     ^                                 ^\n        +   %19 : Tensor = aten::mul(%result, %18)\n        ?     ^                                 ^\n        -   return (%15);\n        ?             ^\n        +   return (%19);\n        ?             ^\n            }",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use  , give an example?",
        "Y": "def fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\nscripted_fn = torch.jit.script(fn)\nprint(scripted_fn.graph)\n#print(str(scripted_fn.graph).strip())\n\nfor input_tuple in [inputs] + check_inputs:\n    torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple))",
        "Z": "def fn(x):\n    result = x[0]\n    for i in range(x.size(0)):\n        result = result * x[i]\n    return result\n\ninputs = (torch.rand(3, 4, 5),)\ncheck_inputs = [(torch.rand(4, 5, 6),), (torch.rand(2, 3, 4),)]\n\nscripted_fn = torch.jit.script(fn)\nprint(scripted_fn.graph)\n#print(str(scripted_fn.graph).strip())\n\nfor input_tuple in [inputs] + check_inputs:\n    torch.testing.assert_allclose(fn(*input_tuple), scripted_fn(*input_tuple))",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use In this case, data-dependent control flow like this can be captured using\ntorch.jit.script() instead:Which produces:, give an example?",
        "Y": "graph(%x : Tensor) {\n    %5 : bool = prim::Constant[value=1]()\n    %1 : int = prim::Constant[value=0]()\n    %result.1 : Tensor = aten::select(%x, %1, %1)\n    %4 : int = aten::size(%x, %1)\n    %result : Tensor = prim::Loop(%4, %5, %result.1)\n    block0(%i : int, %7 : Tensor) {\n        %10 : Tensor = aten::select(%x, %1, %i)\n        %result.2 : Tensor = aten::mul(%7, %10)\n        -> (%5, %result.2)\n    }\n    return (%result);\n}",
        "Z": "In this case, data-dependent control flow like this can be captured using\ntorch.jit.script() instead:Which produces: graph(%x : Tensor) {\n    %5 : bool = prim::Constant[value=1]()\n    %1 : int = prim::Constant[value=0]()\n    %result.1 : Tensor = aten::select(%x, %1, %1)\n    %4 : int = aten::size(%x, %1)\n    %result : Tensor = prim::Loop(%4, %5, %result.1)\n    block0(%i : int, %7 : Tensor) {\n        %10 : Tensor = aten::select(%x, %1, %i)\n        %result.2 : Tensor = aten::mul(%7, %10)\n        -> (%5, %result.2)\n    }\n    return (%result);\n}",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use The tracer produces warnings for several problematic patterns in traced\ncomputation. As an example, take a trace of a function that contains an\nin-place assignment on a slice (a view) of a Tensor:Produces several warnings and a graph which simply returns the input:, give an example?",
        "Y": "fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n    x[0] = torch.rand(*x.shape[1:2])\nfill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)\n    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\ngraph(%0 : Float(3, 4)) {\n    return (%0);\n}",
        "Z": "Produces several warnings and a graph which simply returns the input: fill_row_zero.py:4: TracerWarning: There are 2 live references to the data region being modified when tracing in-place operator copy_ (possibly due to an assignment). This might cause the trace to be incorrect, because all other views that also reference this data will not reflect this change in the trace! On the other hand, if all other views use the same memory chunk, but are disjoint (e.g. are outputs of torch.split), this might still be safe.\n    x[0] = torch.rand(*x.shape[1:2])\nfill_row_zero.py:6: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 1] (0.09115803241729736 vs. 0.6782537698745728) and 3 other locations (33.00%)\n    traced = torch.jit.trace(fill_row_zero, (torch.rand(3, 4),))\ngraph(%0 : Float(3, 4)) {\n    return (%0);\n}",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use First convert your model from GPU to CPU and then save it, like so:, give an example?",
        "Y": "cpu_model = gpu_model.cpu()\nsample_input_cpu = sample_input_gpu.cpu()\ntraced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\ntorch.jit.save(traced_cpu, \"cpu.pt\")\n\ntraced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\ntorch.jit.save(traced_gpu, \"gpu.pt\")\n\n# ... later, when using the model:\n\nif use_gpu:\n  model = torch.jit.load(\"gpu.pt\")\nelse:\n  model = torch.jit.load(\"cpu.pt\")\n\nmodel(input)",
        "Z": "First convert your model from GPU to CPU and then save it, like so: cpu_model = gpu_model.cpu()\nsample_input_cpu = sample_input_gpu.cpu()\ntraced_cpu = torch.jit.trace(cpu_model, sample_input_cpu)\ntorch.jit.save(traced_cpu, \"cpu.pt\")\n\ntraced_gpu = torch.jit.trace(gpu_model, sample_input_gpu)\ntorch.jit.save(traced_gpu, \"gpu.pt\")\n\n# ... later, when using the model:\n\nif use_gpu:\n  model = torch.jit.load(\"gpu.pt\")\nelse:\n  model = torch.jit.load(\"cpu.pt\")\n\nmodel(input)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use Frequently Asked Questions, give an example?",
        "Y": "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.x = 2\n\n    def forward(self):\n        return self.x\n\nm = torch.jit.script(Model())",
        "Z": "import torch\n\nclass Model(torch.nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.x = 2\n\n    def forward(self):\n        return self.x\n\nm = torch.jit.script(Model())",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use Migrating to PyTorch 1 2 Recursive Scripting API, give an example?",
        "Y": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\nmy_model = Model()\nmy_scripted_model = torch.jit.script(my_model)",
        "Z": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\n\nmy_model = Model()\nmy_scripted_model = torch.jit.script(my_model)",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use Old API:New API:, give an example?",
        "Y": "try:\n    from typing_extensions import Final\nexcept:\n    # If you don't have `typing_extensions` installed, you can use a\n    # polyfill from `torch.jit`.\n    from torch.jit import Final\n\nclass MyModule(torch.nn.Module):\n\n    my_constant: Final[int]\n\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule())",
        "Z": "Old API:New API: try:\n    from typing_extensions import Final\nexcept:\n    # If you don't have `typing_extensions` installed, you can use a\n    # polyfill from `torch.jit`.\n    from torch.jit import Final\n\nclass MyModule(torch.nn.Module):\n\n    my_constant: Final[int]\n\n    def __init__(self):\n        super(MyModule, self).__init__()\n        self.my_constant = 2\n\n    def forward(self):\n        pass\n\nm = torch.jit.script(MyModule())",
        "source": "https://pytorch.org/docs/stable/jit.html"
    },
    {
        "X": "How to use torch.column_stack, give an example?",
        "Y": ">>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.column_stack((a, b))\ntensor([[1, 4],\n    [2, 5],\n    [3, 6]])\n>>> a = torch.arange(5)\n>>> b = torch.arange(10).reshape(5, 2)\n>>> torch.column_stack((a, b, b))\ntensor([[0, 0, 1, 0, 1],\n        [1, 2, 3, 2, 3],\n        [2, 4, 5, 4, 5],\n        [3, 6, 7, 6, 7],\n        [4, 8, 9, 8, 9]])",
        "Z": "Equivalent to torch.hstack(tensors), except each zero or one dimensional tensor t\nin tensors is first reshaped into a (t.numel(), 1) column before being stacked horizontally. >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.column_stack((a, b))\ntensor([[1, 4],\n    [2, 5],\n    [3, 6]])\n>>> a = torch.arange(5)\n>>> b = torch.arange(10).reshape(5, 2)\n>>> torch.column_stack((a, b, b))\ntensor([[0, 0, 1, 0, 1],\n        [1, 2, 3, 2, 3],\n        [2, 4, 5, 4, 5],\n        [3, 6, 7, 6, 7],\n        [4, 8, 9, 8, 9]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.column_stack.html#torch.column_stack"
    },
    {
        "X": "How to use torch.is_tensor, give an example?",
        "Y": ">>> x=torch.tensor([1,2,3])\n>>> torch.is_tensor(x)\nTrue",
        "Z": "Note that this function is simply doing isinstance(obj, Tensor).\nUsing that isinstance check is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead of\nis_tensor. >>> x=torch.tensor([1,2,3])\n>>> torch.is_tensor(x)\nTrue",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html"
    },
    {
        "X": "How to use torch.set_flush_denormal, give an example?",
        "Y": ">>> torch.set_flush_denormal(True)\nTrue\n>>> torch.tensor([1e-323], dtype=torch.float64)\ntensor([ 0.], dtype=torch.float64)\n>>> torch.set_flush_denormal(False)\nTrue\n>>> torch.tensor([1e-323], dtype=torch.float64)\ntensor(9.88131e-324 *\n       [ 1.0000], dtype=torch.float64)",
        "Z": "Returns True if your system supports flushing denormal numbers and it\nsuccessfully configures flush denormal mode.  set_flush_denormal()\nis only supported on x86 architectures supporting SSE3. >>> torch.set_flush_denormal(True)\nTrue\n>>> torch.tensor([1e-323], dtype=torch.float64)\ntensor([ 0.], dtype=torch.float64)\n>>> torch.set_flush_denormal(False)\nTrue\n>>> torch.tensor([1e-323], dtype=torch.float64)\ntensor(9.88131e-324 *\n       [ 1.0000], dtype=torch.float64)",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_flush_denormal.html#torch.set_flush_denormal"
    },
    {
        "X": "How to use torch.einsum, give an example?",
        "Y": "# trace\n>>> torch.einsum('ii', torch.randn(4, 4))\ntensor(-1.2104)\n\n# diagonal\n>>> torch.einsum('ii->i', torch.randn(4, 4))\ntensor([-0.1034,  0.7952, -0.2433,  0.4545])\n\n# outer product\n>>> x = torch.randn(5)\n>>> y = torch.randn(4)\n>>> torch.einsum('i,j->ij', x, y)\ntensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\n        [-0.3744,  0.9381,  1.2685, -1.6070],\n        [ 0.7208, -1.8058, -2.4419,  3.0936],\n        [ 0.1713, -0.4291, -0.5802,  0.7350],\n        [ 0.5704, -1.4290, -1.9323,  2.4480]])\n\n# batch matrix multiplication\n>>> As = torch.randn(3,2,5)\n>>> Bs = torch.randn(3,5,4)\n>>> torch.einsum('bij,bjk->bik', As, Bs)\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n        [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n        [[ 4.2239,  0.3107, -0.5756, -0.2354],\n        [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n        [[ 2.8153,  1.8787, -4.3839, -1.2112],\n        [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n# batch permute\n>>> A = torch.randn(2, 3, 4, 5)\n>>> torch.einsum('...ij->...ji', A).shape\ntorch.Size([2, 3, 5, 4])\n\n# equivalent to torch.nn.functional.bilinear\n>>> A = torch.randn(3,5,4)\n>>> l = torch.randn(2,5)\n>>> r = torch.randn(2,4)\n>>> torch.einsum('bn,anm,bm->ba', l, A, r)\ntensor([[-0.3430, -5.2405,  0.4494],\n        [ 0.3311,  5.5201, -3.0356]])",
        "Z": "Equation: # trace\n>>> torch.einsum('ii', torch.randn(4, 4))\ntensor(-1.2104)\n\n# diagonal\n>>> torch.einsum('ii->i', torch.randn(4, 4))\ntensor([-0.1034,  0.7952, -0.2433,  0.4545])\n\n# outer product\n>>> x = torch.randn(5)\n>>> y = torch.randn(4)\n>>> torch.einsum('i,j->ij', x, y)\ntensor([[ 0.1156, -0.2897, -0.3918,  0.4963],\n        [-0.3744,  0.9381,  1.2685, -1.6070],\n        [ 0.7208, -1.8058, -2.4419,  3.0936],\n        [ 0.1713, -0.4291, -0.5802,  0.7350],\n        [ 0.5704, -1.4290, -1.9323,  2.4480]])\n\n# batch matrix multiplication\n>>> As = torch.randn(3,2,5)\n>>> Bs = torch.randn(3,5,4)\n>>> torch.einsum('bij,bjk->bik', As, Bs)\ntensor([[[-1.0564, -1.5904,  3.2023,  3.1271],\n        [-1.6706, -0.8097, -0.8025, -2.1183]],\n\n        [[ 4.2239,  0.3107, -0.5756, -0.2354],\n        [-1.4558, -0.3460,  1.5087, -0.8530]],\n\n        [[ 2.8153,  1.8787, -4.3839, -1.2112],\n        [ 0.3728, -2.1131,  0.0921,  0.8305]]])\n\n# batch permute\n>>> A = torch.randn(2, 3, 4, 5)\n>>> torch.einsum('...ij->...ji', A).shape\ntorch.Size([2, 3, 5, 4])\n\n# equivalent to torch.nn.functional.bilinear\n>>> A = torch.randn(3,5,4)\n>>> l = torch.randn(2,5)\n>>> r = torch.randn(2,4)\n>>> torch.einsum('bn,anm,bm->ba', l, A, r)\ntensor([[-0.3430, -5.2405,  0.4494],\n        [ 0.3311,  5.5201, -3.0356]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.einsum.html#torch.einsum"
    },
    {
        "X": "How to use torch.lerp, give an example?",
        "Y": ">>> start = torch.arange(1., 5.)\n>>> end = torch.empty(4).fill_(10)\n>>> start\ntensor([ 1.,  2.,  3.,  4.])\n>>> end\ntensor([ 10.,  10.,  10.,  10.])\n>>> torch.lerp(start, end, 0.5)\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n>>> torch.lerp(start, end, torch.full_like(start, 0.5))\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])",
        "Z": "The shapes of start and end must be\nbroadcastable. If weight is a tensor, then\nthe shapes of weight, start, and end must be broadcastable. >>> start = torch.arange(1., 5.)\n>>> end = torch.empty(4).fill_(10)\n>>> start\ntensor([ 1.,  2.,  3.,  4.])\n>>> end\ntensor([ 10.,  10.,  10.,  10.])\n>>> torch.lerp(start, end, 0.5)\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])\n>>> torch.lerp(start, end, torch.full_like(start, 0.5))\ntensor([ 5.5000,  6.0000,  6.5000,  7.0000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp"
    },
    {
        "X": "How to use torch.maximum, give an example?",
        "Y": ">>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.maximum(a, b)\ntensor([3, 2, 4])",
        "Z": ">>> a = torch.tensor((1, 2, -1))\n>>> b = torch.tensor((3, 0, 4))\n>>> torch.maximum(a, b)\ntensor([3, 2, 4])",
        "source": "https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum"
    },
    {
        "X": "How to use torch.norm, give an example?",
        "Y": ">>> import torch\n>>> a = torch.arange(9, dtype= torch.float) - 4\n>>> b = a.reshape((3, 3))\n>>> torch.norm(a)\ntensor(7.7460)\n>>> torch.norm(b)\ntensor(7.7460)\n>>> torch.norm(a, float('inf'))\ntensor(4.)\n>>> torch.norm(b, float('inf'))\ntensor(4.)\n>>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\n>>> torch.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000])\n>>> torch.norm(c, dim=1)\ntensor([3.7417, 4.2426])\n>>> torch.norm(c, p=1, dim=1)\ntensor([6., 6.])\n>>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n>>> torch.norm(d, dim=(1,2))\ntensor([ 3.7417, 11.2250])\n>>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n(tensor(3.7417), tensor(11.2250))",
        "Z": ">>> import torch\n>>> a = torch.arange(9, dtype= torch.float) - 4\n>>> b = a.reshape((3, 3))\n>>> torch.norm(a)\ntensor(7.7460)\n>>> torch.norm(b)\ntensor(7.7460)\n>>> torch.norm(a, float('inf'))\ntensor(4.)\n>>> torch.norm(b, float('inf'))\ntensor(4.)\n>>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)\n>>> torch.norm(c, dim=0)\ntensor([1.4142, 2.2361, 5.0000])\n>>> torch.norm(c, dim=1)\ntensor([3.7417, 4.2426])\n>>> torch.norm(c, p=1, dim=1)\ntensor([6., 6.])\n>>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)\n>>> torch.norm(d, dim=(1,2))\ntensor([ 3.7417, 11.2250])\n>>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])\n(tensor(3.7417), tensor(11.2250))",
        "source": "https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"
    },
    {
        "X": "How to use torch.zeros, give an example?",
        "Y": ">>> torch.zeros(2, 3)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n\n>>> torch.zeros(5)\ntensor([ 0.,  0.,  0.,  0.,  0.])",
        "Z": ">>> torch.zeros(2, 3)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])\n\n>>> torch.zeros(5)\ntensor([ 0.,  0.,  0.,  0.,  0.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros"
    },
    {
        "X": "How to use torch.any, give an example?",
        "Y": ">>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.any(a)\ntensor(True, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.any(a)\ntensor(True)",
        "Z": ">>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.any(a)\ntensor(True, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.any(a)\ntensor(True)",
        "source": "https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"
    },
    {
        "X": "How to use torch.lu_solve, give an example?",
        "Y": ">>> A = torch.randn(2, 3, 3)\n>>> b = torch.randn(2, 3, 1)\n>>> A_LU = torch.lu(A)\n>>> x = torch.lu_solve(b, *A_LU)\n>>> torch.norm(torch.bmm(A, x) - b)\ntensor(1.00000e-07 *\n       2.8312)",
        "Z": "This function supports float, double, cfloat and cdouble dtypes for input. >>> A = torch.randn(2, 3, 3)\n>>> b = torch.randn(2, 3, 1)\n>>> A_LU = torch.lu(A)\n>>> x = torch.lu_solve(b, *A_LU)\n>>> torch.norm(torch.bmm(A, x) - b)\ntensor(1.00000e-07 *\n       2.8312)",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"
    },
    {
        "X": "How to use torch.argsort, give an example?",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\n        [ 0.1598,  0.0788, -0.0745, -1.2700],\n        [ 1.2208,  1.0722, -0.7064,  1.2564],\n        [ 0.0669, -0.2318, -0.8229, -0.9280]])\n\n\n>>> torch.argsort(a, dim=1)\ntensor([[2, 0, 3, 1],\n        [3, 2, 1, 0],\n        [2, 1, 0, 3],\n        [3, 2, 1, 0]])",
        "Z": "This is the second value returned by torch.sort().  See its documentation\nfor the exact semantics of this method. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0785,  1.5267, -0.8521,  0.4065],\n        [ 0.1598,  0.0788, -0.0745, -1.2700],\n        [ 1.2208,  1.0722, -0.7064,  1.2564],\n        [ 0.0669, -0.2318, -0.8229, -0.9280]])\n\n\n>>> torch.argsort(a, dim=1)\ntensor([[2, 0, 3, 1],\n        [3, 2, 1, 0],\n        [2, 1, 0, 3],\n        [3, 2, 1, 0]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort"
    },
    {
        "X": "How to use torch.cummax, give an example?",
        "Y": ">>> a = torch.randn(10)\n>>> a\ntensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,\n     1.9946, -0.8209])\n>>> torch.cummax(a, dim=0)\ntorch.return_types.cummax(\n    values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,\n     1.9946,  1.9946]),\n    indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))",
        "Z": ">>> a = torch.randn(10)\n>>> a\ntensor([-0.3449, -1.5447,  0.0685, -1.5104, -1.1706,  0.2259,  1.4696, -1.3284,\n     1.9946, -0.8209])\n>>> torch.cummax(a, dim=0)\ntorch.return_types.cummax(\n    values=tensor([-0.3449, -0.3449,  0.0685,  0.0685,  0.0685,  0.2259,  1.4696,  1.4696,\n     1.9946,  1.9946]),\n    indices=tensor([0, 0, 2, 2, 2, 5, 6, 6, 8, 8]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax"
    },
    {
        "X": "How to use torch.gather, give an example?",
        "Y": "out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\nout[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\nout[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2",
        "Z": "For a 3-D tensor the output is specified by: out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0\nout[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1\nout[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2",
        "source": "https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather"
    },
    {
        "X": "How  input and index must have the same number of dimensions.\nIt is also required that index.size(d) <= input.size(d) for all\ndimensions d != dim.  out will have the same shape as index.\nNote that input and index do not broadcast against each other., give an example?",
        "Y": ">>> t = torch.tensor([[1, 2], [3, 4]])\n>>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\ntensor([[ 1,  1],\n        [ 4,  3]])",
        "Z": "input and index must have the same number of dimensions.\nIt is also required that index.size(d) <= input.size(d) for all\ndimensions d != dim.  out will have the same shape as index.\nNote that input and index do not broadcast against each other. >>> t = torch.tensor([[1, 2], [3, 4]])\n>>> torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))\ntensor([[ 1,  1],\n        [ 4,  3]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather"
    },
    {
        "X": "How to use torch.isreal, give an example?",
        "Y": ">>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))\ntensor([True, False, True])",
        "Z": ">>> torch.isreal(torch.tensor([1, 1+1j, 2+0j]))\ntensor([True, False, True])",
        "source": "https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal"
    },
    {
        "X": "How to use torch.nn.init.calculate_gain, give an example?",
        "Y": ">>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2",
        "Z": ">>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.uniform_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.uniform_(w)",
        "Z": ">>> w = torch.empty(3, 5)\n>>> nn.init.uniform_(w)",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.normal_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.normal_(w)",
        "Z": ">>> w = torch.empty(3, 5)\n>>> nn.init.normal_(w)",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.constant_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.constant_(w, 0.3)",
        "Z": ">>> w = torch.empty(3, 5)\n>>> nn.init.constant_(w, 0.3)",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.ones_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.ones_(w)",
        "Z": ">>> w = torch.empty(3, 5)\n>>> nn.init.ones_(w)",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.zeros_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)",
        "Z": ">>> w = torch.empty(3, 5)\n>>> nn.init.zeros_(w)",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.eye_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.eye_(w)",
        "Z": ">>> w = torch.empty(3, 5)\n>>> nn.init.eye_(w)",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.dirac_, give an example?",
        "Y": ">>> w = torch.empty(3, 16, 5, 5)\n>>> nn.init.dirac_(w)\n>>> w = torch.empty(3, 24, 5, 5)\n>>> nn.init.dirac_(w, 3)",
        "Z": ">>> w = torch.empty(3, 16, 5, 5)\n>>> nn.init.dirac_(w)\n>>> w = torch.empty(3, 24, 5, 5)\n>>> nn.init.dirac_(w, 3)",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.xavier_uniform_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))",
        "Z": "Also known as Glorot initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.xavier_normal_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.xavier_normal_(w)",
        "Z": "Also known as Glorot initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.xavier_normal_(w)",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.kaiming_uniform_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')",
        "Z": "Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.kaiming_normal_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')",
        "Z": "Also known as He initialization. >>> w = torch.empty(3, 5)\n>>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.orthogonal_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.orthogonal_(w)",
        "Z": ">>> w = torch.empty(3, 5)\n>>> nn.init.orthogonal_(w)",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.nn.init.sparse_, give an example?",
        "Y": ">>> w = torch.empty(3, 5)\n>>> nn.init.sparse_(w, sparsity=0.1)",
        "Z": ">>> w = torch.empty(3, 5)\n>>> nn.init.sparse_(w, sparsity=0.1)",
        "source": "https://pytorch.org/docs/stable/nn.init.html"
    },
    {
        "X": "How to use torch.addbmm, give an example?",
        "Y": ">>> M = torch.randn(3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.addbmm(M, batch1, batch2)\ntensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\n        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\n        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])",
        "Z": "This operator supports TensorFloat32. >>> M = torch.randn(3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.addbmm(M, batch1, batch2)\ntensor([[  6.6311,   0.0503,   6.9768, -12.0362,  -2.1653],\n        [ -4.8185,  -1.4255,  -6.6760,   8.9453,   2.5743],\n        [ -3.8202,   4.3691,   1.0943,  -1.1109,   5.4730]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"
    },
    {
        "X": "How to use torch.linalg.pinv, give an example?",
        "Y": "torch.linalg.lstsq(A, B).solution == A.pinv() @ B",
        "Z": "Consider using torch.linalg.lstsq() if possible for multiplying a matrix on the left by\nthe the pseudoinverse, as: torch.linalg.lstsq(A, B).solution == A.pinv() @ B",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "How  The singular values (or the norm of the eigenvalues when hermitian= True)\nthat are below the specified rcond threshold are treated as zero and discarded in the computation., give an example?",
        "Y": ">>> A = torch.randn(3, 5)\n>>> A\ntensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n>>> torch.linalg.pinv(A)\ntensor([[ 0.0600, -0.1933, -0.2090],\n        [-0.0903, -0.0817, -0.4752],\n        [-0.7124, -0.1631, -0.2272],\n        [ 0.1356,  0.3933, -0.5023],\n        [-0.0308, -0.1725, -0.5216]])\n\nBatched linalg.pinv example\n>>> A = torch.randn(2, 6, 3)\n>>> B = torch.linalg.pinv(A)\n>>> torch.matmul(B, A).round()\ntensor([[[1., -0., 0.],\n         [0., 1., -0.],\n         [0., 0., 1.]],\n\n        [[1., -0., 0.],\n         [-0., 1., 0.],\n         [-0., -0., 1.]]])\n\nHermitian input example\n>>> A = torch.randn(3, 3, dtype=torch.complex64)\n>>> A = A + A.t().conj()  # creates a Hermitian matrix\n>>> B = torch.linalg.pinv(A, hermitian=True)\n>>> torch.matmul(B, A)\ntensor([[ 1.0000e+00+0.0000e+00j, -1.1921e-07-2.3842e-07j,\n        5.9605e-08-2.3842e-07j],\n        [ 5.9605e-08+2.3842e-07j,  1.0000e+00+2.3842e-07j,\n        -4.7684e-07+1.1921e-07j],\n        [-1.1921e-07+0.0000e+00j, -2.3842e-07-2.9802e-07j,\n        1.0000e+00-1.7897e-07j]])\n\nNon-default rcond example\n>>> rcond = 0.5\n>>> A = torch.randn(3, 3)\n>>> torch.linalg.pinv(A)\ntensor([[ 0.2971, -0.4280, -2.0111],\n        [-0.0090,  0.6426, -0.1116],\n        [-0.7832, -0.2465,  1.0994]])\n>>> torch.linalg.pinv(A, rcond)\ntensor([[-0.2672, -0.2351, -0.0539],\n        [-0.0211,  0.6467, -0.0698],\n        [-0.4400, -0.3638, -0.0910]])\n\nMatrix-wise rcond example\n>>> A = torch.randn(5, 6, 2, 3, 3)\n>>> rcond = torch.rand(2)  # different rcond values for each matrix in a[:, :, 0] and a[:, :, 1]\n>>> torch.linalg.pinv(A, rcond)\n>>> rcond = torch.randn(5, 6, 2) # different rcond value for each matrix in 'a'\n>>> torch.linalg.pinv(A, rcond)",
        "Z": "The singular values (or the norm of the eigenvalues when hermitian= True)\nthat are below the specified rcond threshold are treated as zero and discarded in the computation. >>> A = torch.randn(3, 5)\n>>> A\ntensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],\n        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],\n        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])\n>>> torch.linalg.pinv(A)\ntensor([[ 0.0600, -0.1933, -0.2090],\n        [-0.0903, -0.0817, -0.4752],\n        [-0.7124, -0.1631, -0.2272],\n        [ 0.1356,  0.3933, -0.5023],\n        [-0.0308, -0.1725, -0.5216]])\n\nBatched linalg.pinv example\n>>> A = torch.randn(2, 6, 3)\n>>> B = torch.linalg.pinv(A)\n>>> torch.matmul(B, A).round()\ntensor([[[1., -0., 0.],\n         [0., 1., -0.],\n         [0., 0., 1.]],\n\n        [[1., -0., 0.],\n         [-0., 1., 0.],\n         [-0., -0., 1.]]])\n\nHermitian input example\n>>> A = torch.randn(3, 3, dtype=torch.complex64)\n>>> A = A + A.t().conj()  # creates a Hermitian matrix\n>>> B = torch.linalg.pinv(A, hermitian=True)\n>>> torch.matmul(B, A)\ntensor([[ 1.0000e+00+0.0000e+00j, -1.1921e-07-2.3842e-07j,\n        5.9605e-08-2.3842e-07j],\n        [ 5.9605e-08+2.3842e-07j,  1.0000e+00+2.3842e-07j,\n        -4.7684e-07+1.1921e-07j],\n        [-1.1921e-07+0.0000e+00j, -2.3842e-07-2.9802e-07j,\n        1.0000e+00-1.7897e-07j]])\n\nNon-default rcond example\n>>> rcond = 0.5\n>>> A = torch.randn(3, 3)\n>>> torch.linalg.pinv(A)\ntensor([[ 0.2971, -0.4280, -2.0111],\n        [-0.0090,  0.6426, -0.1116],\n        [-0.7832, -0.2465,  1.0994]])\n>>> torch.linalg.pinv(A, rcond)\ntensor([[-0.2672, -0.2351, -0.0539],\n        [-0.0211,  0.6467, -0.0698],\n        [-0.4400, -0.3638, -0.0910]])\n\nMatrix-wise rcond example\n>>> A = torch.randn(5, 6, 2, 3, 3)\n>>> rcond = torch.rand(2)  # different rcond values for each matrix in a[:, :, 0] and a[:, :, 1]\n>>> torch.linalg.pinv(A, rcond)\n>>> rcond = torch.randn(5, 6, 2) # different rcond value for each matrix in 'a'\n>>> torch.linalg.pinv(A, rcond)",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.pinv.html#torch.linalg.pinv"
    },
    {
        "X": "How to use torch.result_type, give an example?",
        "Y": ">>> torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)\ntorch.float32\n>>> torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))\ntorch.uint8",
        "Z": ">>> torch.result_type(torch.tensor([1, 2], dtype=torch.int), 1.0)\ntorch.float32\n>>> torch.result_type(torch.tensor([1, 2], dtype=torch.uint8), torch.tensor(1))\ntorch.uint8",
        "source": "https://pytorch.org/docs/stable/generated/torch.result_type.html#torch.result_type"
    },
    {
        "X": "How to use torch.set_default_dtype, give an example?",
        "Y": ">>> # initial default for floating point is torch.float32\n>>> torch.tensor([1.2, 3]).dtype\ntorch.float32\n>>> # initial default for floating point is torch.complex64\n>>> torch.tensor([1.2, 3j]).dtype\ntorch.complex64\n>>> torch.set_default_dtype(torch.float64)\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64\n>>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor\ntorch.complex128",
        "Z": "The default floating point dtype is initially torch.float32. >>> # initial default for floating point is torch.float32\n>>> torch.tensor([1.2, 3]).dtype\ntorch.float32\n>>> # initial default for floating point is torch.complex64\n>>> torch.tensor([1.2, 3j]).dtype\ntorch.complex64\n>>> torch.set_default_dtype(torch.float64)\n>>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor\ntorch.float64\n>>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor\ntorch.complex128",
        "source": "https://pytorch.org/docs/stable/generated/torch.set_default_dtype.html#torch.set_default_dtype"
    },
    {
        "X": "How to use torch.cummin, give an example?",
        "Y": ">>> a = torch.randn(10)\n>>> a\ntensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,\n     0.9165,  1.6684])\n>>> torch.cummin(a, dim=0)\ntorch.return_types.cummin(\n    values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,\n    -1.3298, -1.3298]),\n    indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))",
        "Z": ">>> a = torch.randn(10)\n>>> a\ntensor([-0.2284, -0.6628,  0.0975,  0.2680, -1.3298, -0.4220, -0.3885,  1.1762,\n     0.9165,  1.6684])\n>>> torch.cummin(a, dim=0)\ntorch.return_types.cummin(\n    values=tensor([-0.2284, -0.6628, -0.6628, -0.6628, -1.3298, -1.3298, -1.3298, -1.3298,\n    -1.3298, -1.3298]),\n    indices=tensor([0, 1, 1, 1, 4, 4, 4, 4, 4, 4]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin"
    },
    {
        "X": "How to use torch.mode, give an example?",
        "Y": ">>> a = torch.randint(10, (5,))\n>>> a\ntensor([6, 5, 1, 0, 2])\n>>> b = a + (torch.randn(50, 1) * 5).long()\n>>> torch.mode(b, 0)\ntorch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))",
        "Z": "If keepdim is True, the output tensors are of the same size as\ninput except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting\nin the output tensors having 1 fewer dimension than input. >>> a = torch.randint(10, (5,))\n>>> a\ntensor([6, 5, 1, 0, 2])\n>>> b = a + (torch.randn(50, 1) * 5).long()\n>>> torch.mode(b, 0)\ntorch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"
    },
    {
        "X": "How  Note that this function is simply doing isinstance(obj, Tensor).\nUsing that isinstance check is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead of\nis_tensor., give an example?",
        "Y": ">>> x=torch.tensor([1,2,3])\n>>> torch.is_tensor(x)\nTrue",
        "Z": "Note that this function is simply doing isinstance(obj, Tensor).\nUsing that isinstance check is better for typechecking with mypy,\nand more explicit - so it\u2019s recommended to use that instead of\nis_tensor. >>> x=torch.tensor([1,2,3])\n>>> torch.is_tensor(x)\nTrue",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_tensor.html#torch.is_tensor"
    },
    {
        "X": "How to use torch.distributed.optim.ZeroRedundancyOptimizer, give an example?",
        "Y": ">>> import torch.nn as nn\n>>> from torch.distributed.optim import ZeroRedundancyOptimizer\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n\n>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n>>> ddp = DDP(model, device_ids=[rank])\n>>> opt = ZeroRedundancyOptimizer(\n>>>     ddp.parameters(),\n>>>     optimizer_class=torch.optim.Adam,\n>>>     lr=0.01\n>>> )\n>>> ddp(inputs).sum().backward()\n>>> opt.step()",
        "Z": "ZeroRedundancyOptimizer use a greedy algorithm to pack a number of\nparameters at each rank. Each parameter belongs to a single rank and is not\ndivided among ranks. The partition is arbitrary and might not match the\nthe parameter registration or usage order. >>> import torch.nn as nn\n>>> from torch.distributed.optim import ZeroRedundancyOptimizer\n>>> from torch.nn.parallel import DistributedDataParallel as DDP\n\n>>> model = nn.Sequential(*[nn.Linear(2000, 2000).to(rank) for _ in range(20)])\n>>> ddp = DDP(model, device_ids=[rank])\n>>> opt = ZeroRedundancyOptimizer(\n>>>     ddp.parameters(),\n>>>     optimizer_class=torch.optim.Adam,\n>>>     lr=0.01\n>>> )\n>>> ddp(inputs).sum().backward()\n>>> opt.step()",
        "source": "https://pytorch.org/docs/stable/distributed.optim.html"
    },
    {
        "X": "How to use torch.arange, give an example?",
        "Y": ">>> torch.arange(5)\ntensor([ 0,  1,  2,  3,  4])\n>>> torch.arange(1, 4)\ntensor([ 1,  2,  3])\n>>> torch.arange(1, 2.5, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000])",
        "Z": "Note that non-integer step is subject to floating point rounding errors when\ncomparing against end; to avoid inconsistency, we advise adding a small epsilon to end\nin such cases. >>> torch.arange(5)\ntensor([ 0,  1,  2,  3,  4])\n>>> torch.arange(1, 4)\ntensor([ 1,  2,  3])\n>>> torch.arange(1, 2.5, 0.5)\ntensor([ 1.0000,  1.5000,  2.0000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.arange.html#torch.arange"
    },
    {
        "X": "How to use torch.hypot, give an example?",
        "Y": ">>> a = torch.hypot(torch.tensor([4.0]), torch.tensor([3.0, 4.0, 5.0]))\ntensor([5.0000, 5.6569, 6.4031])",
        "Z": "The shapes of input and other must be\nbroadcastable. >>> a = torch.hypot(torch.tensor([4.0]), torch.tensor([3.0, 4.0, 5.0]))\ntensor([5.0000, 5.6569, 6.4031])",
        "source": "https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot"
    },
    {
        "X": "How to use torch.as_strided, give an example?",
        "Y": ">>> x = torch.randn(3, 3)\n>>> x\ntensor([[ 0.9039,  0.6291,  1.0795],\n        [ 0.1586,  2.1939, -0.4900],\n        [-0.1909, -0.7503,  1.9355]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2))\n>>> t\ntensor([[0.9039, 1.0795],\n        [0.6291, 0.1586]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2), 1)\ntensor([[0.6291, 0.1586],\n        [1.0795, 2.1939]])",
        "Z": ">>> x = torch.randn(3, 3)\n>>> x\ntensor([[ 0.9039,  0.6291,  1.0795],\n        [ 0.1586,  2.1939, -0.4900],\n        [-0.1909, -0.7503,  1.9355]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2))\n>>> t\ntensor([[0.9039, 1.0795],\n        [0.6291, 0.1586]])\n>>> t = torch.as_strided(x, (2, 2), (1, 2), 1)\ntensor([[0.6291, 0.1586],\n        [1.0795, 2.1939]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"
    },
    {
        "X": "How to use torch.transpose, give an example?",
        "Y": ">>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 1.0028, -0.9893,  0.5809],\n        [-0.1669,  0.7299,  0.4942]])\n>>> torch.transpose(x, 0, 1)\ntensor([[ 1.0028, -0.1669],\n        [-0.9893,  0.7299],\n        [ 0.5809,  0.4942]])",
        "Z": "The resulting out tensor shares its underlying storage with the\ninput tensor, so changing the content of one would change the content\nof the other. >>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 1.0028, -0.9893,  0.5809],\n        [-0.1669,  0.7299,  0.4942]])\n>>> torch.transpose(x, 0, 1)\ntensor([[ 1.0028, -0.1669],\n        [-0.9893,  0.7299],\n        [ 0.5809,  0.4942]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose"
    },
    {
        "X": "How to use torch.cross, give an example?",
        "Y": ">>> a = torch.randn(4, 3)\n>>> a\ntensor([[-0.3956,  1.1455,  1.6895],\n        [-0.5849,  1.3672,  0.3599],\n        [-1.1626,  0.7180, -0.0521],\n        [-0.1339,  0.9902, -2.0225]])\n>>> b = torch.randn(4, 3)\n>>> b\ntensor([[-0.0257, -1.4725, -1.2251],\n        [-1.1479, -0.7005, -1.9757],\n        [-1.3904,  0.3726, -1.1836],\n        [-0.9688, -0.7153,  0.2159]])\n>>> torch.cross(a, b, dim=1)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n>>> torch.cross(a, b)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])",
        "Z": "If dim is not given, it defaults to the first dimension found with the\nsize 3. Note that this might be unexpected. >>> a = torch.randn(4, 3)\n>>> a\ntensor([[-0.3956,  1.1455,  1.6895],\n        [-0.5849,  1.3672,  0.3599],\n        [-1.1626,  0.7180, -0.0521],\n        [-0.1339,  0.9902, -2.0225]])\n>>> b = torch.randn(4, 3)\n>>> b\ntensor([[-0.0257, -1.4725, -1.2251],\n        [-1.1479, -0.7005, -1.9757],\n        [-1.3904,  0.3726, -1.1836],\n        [-0.9688, -0.7153,  0.2159]])\n>>> torch.cross(a, b, dim=1)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])\n>>> torch.cross(a, b)\ntensor([[ 1.0844, -0.5281,  0.6120],\n        [-2.4490, -1.5687,  1.9792],\n        [-0.8304, -1.3037,  0.5650],\n        [-1.2329,  1.9883,  1.0551]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross"
    },
    {
        "X": "How to use torch.rot90, give an example?",
        "Y": ">>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.rot90(x, 1, [0, 1])\ntensor([[1, 3],\n        [0, 2]])\n\n>>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[0, 1],\n         [2, 3]],\n\n        [[4, 5],\n         [6, 7]]])\n>>> torch.rot90(x, 1, [1, 2])\ntensor([[[1, 3],\n         [0, 2]],\n\n        [[5, 7],\n         [4, 6]]])",
        "Z": ">>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.rot90(x, 1, [0, 1])\ntensor([[1, 3],\n        [0, 2]])\n\n>>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[0, 1],\n         [2, 3]],\n\n        [[4, 5],\n         [6, 7]]])\n>>> torch.rot90(x, 1, [1, 2])\ntensor([[[1, 3],\n         [0, 2]],\n\n        [[5, 7],\n         [4, 6]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90"
    },
    {
        "X": "How to use torch.Tensor.scatter_add_, give an example?",
        "Y": "self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2",
        "Z": "For a 3-D tensor, self is updated as: self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "How  self, index and src should have same number of\ndimensions. It is also required that index.size(d) <= src.size(d) for all\ndimensions d, and that index.size(d) <= self.size(d) for all dimensions\nd != dim. Note that index and src do not broadcast., give an example?",
        "Y": ">>> src = torch.ones((2, 5))\n>>> index = torch.tensor([[0, 1, 2, 0, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[1., 0., 0., 1., 1.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.]])\n>>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[2., 0., 0., 1., 1.],\n        [0., 2., 0., 0., 0.],\n        [0., 0., 2., 1., 1.]])",
        "Z": "self, index and src should have same number of\ndimensions. It is also required that index.size(d) <= src.size(d) for all\ndimensions d, and that index.size(d) <= self.size(d) for all dimensions\nd != dim. Note that index and src do not broadcast. >>> src = torch.ones((2, 5))\n>>> index = torch.tensor([[0, 1, 2, 0, 0]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[1., 0., 0., 1., 1.],\n        [0., 1., 0., 0., 0.],\n        [0., 0., 1., 0., 0.]])\n>>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n>>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\ntensor([[2., 0., 0., 1., 1.],\n        [0., 2., 0., 0., 0.],\n        [0., 0., 2., 1., 1.]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"
    },
    {
        "X": "How to use torch.triu, give an example?",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.3480, -0.5211, -0.4573]])\n>>> torch.triu(a)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.0000, -1.0680,  0.6602],\n        [ 0.0000,  0.0000, -0.4573]])\n>>> torch.triu(a, diagonal=1)\ntensor([[ 0.0000,  0.5207,  2.0049],\n        [ 0.0000,  0.0000,  0.6602],\n        [ 0.0000,  0.0000,  0.0000]])\n>>> torch.triu(a, diagonal=-1)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.0000, -0.5211, -0.4573]])\n\n>>> b = torch.randn(4, 6)\n>>> b\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])\n>>> torch.triu(b, diagonal=1)\ntensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])\n>>> torch.triu(b, diagonal=-1)\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])",
        "Z": "The argument diagonal controls which diagonal to consider. If\ndiagonal = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121] where\nd1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. >>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.3480, -0.5211, -0.4573]])\n>>> torch.triu(a)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.0000, -1.0680,  0.6602],\n        [ 0.0000,  0.0000, -0.4573]])\n>>> torch.triu(a, diagonal=1)\ntensor([[ 0.0000,  0.5207,  2.0049],\n        [ 0.0000,  0.0000,  0.6602],\n        [ 0.0000,  0.0000,  0.0000]])\n>>> torch.triu(a, diagonal=-1)\ntensor([[ 0.2309,  0.5207,  2.0049],\n        [ 0.2072, -1.0680,  0.6602],\n        [ 0.0000, -0.5211, -0.4573]])\n\n>>> b = torch.randn(4, 6)\n>>> b\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.4333,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [-0.9888,  1.0679, -1.3337, -1.6556,  0.4798,  0.2830]])\n>>> torch.triu(b, diagonal=1)\ntensor([[ 0.0000, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [ 0.0000,  0.0000, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.0000,  0.0000, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.4798,  0.2830]])\n>>> torch.triu(b, diagonal=-1)\ntensor([[ 0.5876, -0.0794, -1.8373,  0.6654,  0.2604,  1.5235],\n        [-0.2447,  0.9556, -1.2919,  1.3378, -0.1768, -1.0857],\n        [ 0.0000,  0.3146,  0.6576, -1.0432,  0.9348, -0.4410],\n        [ 0.0000,  0.0000, -1.3337, -1.6556,  0.4798,  0.2830]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu"
    },
    {
        "X": "How to use torch.polygamma, give an example?",
        "Y": ">>> a = torch.tensor([1, 0.5])\n>>> torch.polygamma(1, a)\ntensor([1.64493, 4.9348])\n>>> torch.polygamma(2, a)\ntensor([ -2.4041, -16.8288])\n>>> torch.polygamma(3, a)\ntensor([ 6.4939, 97.4091])\n>>> torch.polygamma(4, a)\ntensor([ -24.8863, -771.4742])",
        "Z": ">>> a = torch.tensor([1, 0.5])\n>>> torch.polygamma(1, a)\ntensor([1.64493, 4.9348])\n>>> torch.polygamma(2, a)\ntensor([ -2.4041, -16.8288])\n>>> torch.polygamma(3, a)\ntensor([ 6.4939, 97.4091])\n>>> torch.polygamma(4, a)\ntensor([ -24.8863, -771.4742])",
        "source": "https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma"
    },
    {
        "X": "How to use torch.var, give an example?",
        "Y": ">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.var(a, unbiased=False)\ntensor(0.1754)",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. >>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.var(a, unbiased=False)\ntensor(0.1754)",
        "source": "https://pytorch.org/docs/stable/generated/torch.var.html#torch.var"
    },
    {
        "X": "How to use torch.allclose, give an example?",
        "Y": ">>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\nFalse\n>>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\nTrue\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\nFalse\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\nTrue",
        "Z": "elementwise, for all elements of input and other. The behaviour of this function is analogous to\nnumpy.allclose >>> torch.allclose(torch.tensor([10000., 1e-07]), torch.tensor([10000.1, 1e-08]))\nFalse\n>>> torch.allclose(torch.tensor([10000., 1e-08]), torch.tensor([10000.1, 1e-09]))\nTrue\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]))\nFalse\n>>> torch.allclose(torch.tensor([1.0, float('nan')]), torch.tensor([1.0, float('nan')]), equal_nan=True)\nTrue",
        "source": "https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose"
    },
    {
        "X": "How to use torch.matmul, give an example?",
        "Y": ">>> # vector x vector\n>>> tensor1 = torch.randn(3)\n>>> tensor2 = torch.randn(3)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([])\n>>> # matrix x vector\n>>> tensor1 = torch.randn(3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([3])\n>>> # batched matrix x broadcasted vector\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3])\n>>> # batched matrix x batched matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(10, 4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n>>> # batched matrix x broadcasted matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])",
        "Z": "This operator supports TensorFloat32. >>> # vector x vector\n>>> tensor1 = torch.randn(3)\n>>> tensor2 = torch.randn(3)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([])\n>>> # matrix x vector\n>>> tensor1 = torch.randn(3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([3])\n>>> # batched matrix x broadcasted vector\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3])\n>>> # batched matrix x batched matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(10, 4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])\n>>> # batched matrix x broadcasted matrix\n>>> tensor1 = torch.randn(10, 3, 4)\n>>> tensor2 = torch.randn(4, 5)\n>>> torch.matmul(tensor1, tensor2).size()\ntorch.Size([10, 3, 5])",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "How to use torch.count_nonzero, give an example?",
        "Y": ">>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])",
        "Z": ">>> x = torch.zeros(3,3)\n>>> x[torch.randn(3,3) > 0.5] = 1\n>>> x\ntensor([[0., 1., 1.],\n        [0., 0., 0.],\n        [0., 0., 1.]])\n>>> torch.count_nonzero(x)\ntensor(3)\n>>> torch.count_nonzero(x, dim=0)\ntensor([0, 1, 2])",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "X": "How to use torch.meshgrid, give an example?",
        "Y": ">>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([4, 5, 6])\n>>> grid_x, grid_y = torch.meshgrid(x, y)\n>>> grid_x\ntensor([[1, 1, 1],\n        [2, 2, 2],\n        [3, 3, 3]])\n>>> grid_y\ntensor([[4, 5, 6],\n        [4, 5, 6],\n        [4, 5, 6]])",
        "Z": ">>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([4, 5, 6])\n>>> grid_x, grid_y = torch.meshgrid(x, y)\n>>> grid_x\ntensor([[1, 1, 1],\n        [2, 2, 2],\n        [3, 3, 3]])\n>>> grid_y\ntensor([[4, 5, 6],\n        [4, 5, 6],\n        [4, 5, 6]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "How to use torch.is_nonzero, give an example?",
        "Y": ">>> torch.is_nonzero(torch.tensor([0.]))\nFalse\n>>> torch.is_nonzero(torch.tensor([1.5]))\nTrue\n>>> torch.is_nonzero(torch.tensor([False]))\nFalse\n>>> torch.is_nonzero(torch.tensor([3]))\nTrue\n>>> torch.is_nonzero(torch.tensor([1, 3, 5]))\nTraceback (most recent call last):\n...\nRuntimeError: bool value of Tensor with more than one value is ambiguous\n>>> torch.is_nonzero(torch.tensor([]))\nTraceback (most recent call last):\n...\nRuntimeError: bool value of Tensor with no values is ambiguous",
        "Z": ">>> torch.is_nonzero(torch.tensor([0.]))\nFalse\n>>> torch.is_nonzero(torch.tensor([1.5]))\nTrue\n>>> torch.is_nonzero(torch.tensor([False]))\nFalse\n>>> torch.is_nonzero(torch.tensor([3]))\nTrue\n>>> torch.is_nonzero(torch.tensor([1, 3, 5]))\nTraceback (most recent call last):\n...\nRuntimeError: bool value of Tensor with more than one value is ambiguous\n>>> torch.is_nonzero(torch.tensor([]))\nTraceback (most recent call last):\n...\nRuntimeError: bool value of Tensor with no values is ambiguous",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero"
    },
    {
        "X": "How to use A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor:, give an example?",
        "Y": ">>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: >>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How to use A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor:A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op:, give an example?",
        "Y": ">>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')",
        "Z": "A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: >>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How to use For more information about building Tensors, see Creation OpsThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:, give an example?",
        "Y": ">>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])",
        "Z": "For more information about building Tensors, see Creation OpsThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How to use The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value:, give an example?",
        "Y": ">>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: >>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How to use For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation., give an example?",
        "Y": ">>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])",
        "Z": "For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How to use A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor().Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write:, give an example?",
        "Y": ">>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [3, 4, 5]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3, 4, 5]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n>>> s.to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: >>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [3, 4, 5]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3, 4, 5]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n>>> s.to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write:Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:, give an example?",
        "Y": ">>> i = [[0, 2], [1, 0], [1, 2]]\n>>> v =  [3,      4,      5    ]\n>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))\n>>> # Or another equivalent formulation to get s\n>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))\n>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write:Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: >>> i = [[0, 2], [1, 0], [1, 2]]\n>>> v =  [3,      4,      5    ]\n>>> s = torch.sparse_coo_tensor(list(zip(*i)), v, (2, 3))\n>>> # Or another equivalent formulation to get s\n>>> s = torch.sparse_coo_tensor(torch.tensor(i).t(), v, (2, 3))\n>>> torch.sparse_coo_tensor(i.t(), v, torch.Size([2,3])).to_dense()\ntensor([[0, 0, 3],\n        [4, 0, 5]])",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:An empty sparse COO tensor can be constructed by specifying its size\nonly:, give an example?",
        "Y": ">>> torch.sparse_coo_tensor(size=(2, 3))\ntensor(indices=tensor([], size=(2, 0)),\n       values=tensor([], size=(0,)),\n       size=(2, 3), nnz=0, layout=torch.sparse_coo)",
        "Z": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor:An empty sparse COO tensor can be constructed by specifying its size\nonly: >>> torch.sparse_coo_tensor(size=(2, 3))\ntensor(indices=tensor([], size=(2, 0)),\n       values=tensor([], size=(0,)),\n       size=(2, 3), nnz=0, layout=torch.sparse_coo)",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write, give an example?",
        "Y": ">>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([[3, 4],\n                      [5, 6],\n                      [7, 8]]),\n       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write >>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))\n>>> s\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([[3, 4],\n                      [5, 6],\n                      [7, 8]]),\n       size=(2, 3, 2), nnz=3, layout=torch.sparse_coo)",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How  PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write, give an example?",
        "Y": ">>> s.to_dense()\ntensor([[[0, 0],\n         [0, 0],\n         [3, 4]],\n        [[5, 6],\n         [0, 0],\n         [7, 8]]])",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave:Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write >>> s.to_dense()\ntensor([[[0, 0],\n         [0, 0],\n         [3, 4]],\n        [[5, 6],\n         [0, 0],\n         [7, 8]]])",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor:, give an example?",
        "Y": ">>> i = [[1, 1]]\n>>> v =  [3, 4]\n>>> s=torch.sparse_coo_tensor(i, v, (3,))\n>>> s\ntensor(indices=tensor([[1, 1]]),\n       values=tensor(  [3, 4]),\n       size=(3,), nnz=2, layout=torch.sparse_coo)",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: >>> i = [[1, 1]]\n>>> v =  [3, 4]\n>>> s=torch.sparse_coo_tensor(i, v, (3,))\n>>> s\ntensor(indices=tensor([[1, 1]]),\n       values=tensor(  [3, 4]),\n       size=(3,), nnz=2, layout=torch.sparse_coo)",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor:while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation:, give an example?",
        "Y": ">>> s.coalesce()\ntensor(indices=tensor([[1]]),\n       values=tensor([7]),\n       size=(3,), nnz=1, layout=torch.sparse_coo)",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: >>> s.coalesce()\ntensor(indices=tensor([[1]]),\n       values=tensor([7]),\n       size=(3,), nnz=1, layout=torch.sparse_coo)",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors:, give an example?",
        "Y": ">>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\n>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\n>>> a + b\ntensor(indices=tensor([[0, 0, 1, 1]]),\n       values=tensor([7, 8, 5, 6]),\n       size=(2,), nnz=4, layout=torch.sparse_coo)",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: >>> a = torch.sparse_coo_tensor([[1, 1]], [5, 6], (2,))\n>>> b = torch.sparse_coo_tensor([[0, 0]], [7, 8], (2,))\n>>> a + b\ntensor(indices=tensor([[0, 0, 1, 1]]),\n       values=tensor([7, 8, 5, 6]),\n       size=(2,), nnz=4, layout=torch.sparse_coo)",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use Let\u2019s consider the following example:, give an example?",
        "Y": ">>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))",
        "Z": "Let\u2019s consider the following example: >>> i = [[0, 1, 1],\n         [2, 0, 2]]\n>>> v =  [[3, 4], [5, 6], [7, 8]]\n>>> s = torch.sparse_coo_tensor(i, v, (2, 3, 2))",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use Let\u2019s consider the following example:As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties:, give an example?",
        "Y": ">>> isinstance(s, torch.Tensor)\nTrue\n>>> s.is_sparse\nTrue\n>>> s.layout == torch.sparse_coo\nTrue",
        "Z": "As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: >>> isinstance(s, torch.Tensor)\nTrue\n>>> s.is_sparse\nTrue\n>>> s.layout == torch.sparse_coo\nTrue",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties:The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance:, give an example?",
        "Y": ">>> s.sparse_dim(), s.dense_dim()\n(2, 1)",
        "Z": "As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties:The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: >>> s.sparse_dim(), s.dense_dim()\n(2, 1)",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use NoteCurrently, one can acquire the COO format data only when the tensor\ninstance is coalesced:, give an example?",
        "Y": ">>> s.indices()\nRuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first",
        "Z": "Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: >>> s.indices()\nRuntimeError: Cannot get indices on an uncoalesced tensor, please call .coalesce() first",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced:For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices():, give an example?",
        "Y": ">>> s._indices()\ntensor([[0, 1, 1],\n        [2, 0, 2]])",
        "Z": "Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced:For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): >>> s._indices()\ntensor([[0, 1, 1],\n        [2, 0, 2]])",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values().Constructing a new sparse COO tensor results a tensor that is not\ncoalesced:, give an example?",
        "Y": ">>> s.is_coalesced()\nFalse",
        "Z": "If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values().Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: >>> s.is_coalesced()\nFalse",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use Constructing a new sparse COO tensor results a tensor that is not\ncoalesced:but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:, give an example?",
        "Y": ">>> s2 = s.coalesce()\n>>> s2.indices()\ntensor([[0, 1, 1],\n       [2, 0, 2]])",
        "Z": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced:but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: >>> s2 = s.coalesce()\n>>> s2.indices()\ntensor([[0, 1, 1],\n       [2, 0, 2]])",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general.Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:, give an example?",
        "Y": ">>> s[1]\ntensor(indices=tensor([[0, 2]]),\n       values=tensor([[5, 6],\n                      [7, 8]]),\n       size=(3, 2), nnz=2, layout=torch.sparse_coo)\n>>> s[1, 0, 1]\ntensor(6)\n>>> s[1, 0, 1:]\ntensor([6])",
        "Z": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general.Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: >>> s[1]\ntensor(indices=tensor([[0, 2]]),\n       values=tensor([[5, 6],\n                      [7, 8]]),\n       size=(3, 2), nnz=2, layout=torch.sparse_coo)\n>>> s[1, 0, 1]\ntensor(6)\n>>> s[1, 0, 1:]\ntensor([6])",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present., give an example?",
        "Y": ">>> crow_indices = torch.tensor([0, 2, 4])\n>>> col_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csr = torch._sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)\n>>> csr\ntensor(crow_indices=tensor([0, 2, 4]),\n      col_indices=tensor([0, 1, 0, 1]),\n      values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n      dtype=torch.float64)\n>>> csr.to_dense()\ntensor([[1., 2.],\n        [3., 4.]], dtype=torch.float64)",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. >>> crow_indices = torch.tensor([0, 2, 4])\n>>> col_indices = torch.tensor([0, 1, 0, 1])\n>>> values = torch.tensor([1, 2, 3, 4])\n>>> csr = torch._sparse_csr_tensor(crow_indices, col_indices, values, dtype=torch.double)\n>>> csr\ntensor(crow_indices=tensor([0, 2, 4]),\n      col_indices=tensor([0, 1, 0, 1]),\n      values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,\n      dtype=torch.float64)\n>>> csr.to_dense()\ntensor([[1., 2.],\n        [3., 4.]], dtype=torch.float64)",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:, give an example?",
        "Y": ">>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype = torch.float64)\n>>> sp = a._to_sparse_csr()\n>>> sp\ntensor(crow_indices=tensor([0, 1, 3, 3]),\n      col_indices=tensor([2, 0, 1]),\n      values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)",
        "Z": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: >>> a = torch.tensor([[0, 0, 1, 0], [1, 2, 0, 0], [0, 0, 0, 0]], dtype = torch.float64)\n>>> sp = a._to_sparse_csr()\n>>> sp\ntensor(crow_indices=tensor([0, 1, 3, 3]),\n      col_indices=tensor([2, 0, 1]),\n      values=tensor([1., 1., 2.]), size=(3, 4), nnz=3, dtype=torch.float64)",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors., give an example?",
        "Y": ">>> vec = torch.randn(4, 1, dtype=torch.float64)\n>>> sp.matmul(vec)\ntensor([[0.9078],\n        [1.3180],\n        [0.0000]], dtype=torch.float64)",
        "Z": "The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors. >>> vec = torch.randn(4, 1, dtype=torch.float64)\n>>> sp.matmul(vec)\ntensor([[0.9078],\n        [1.3180],\n        [0.0000]], dtype=torch.float64)",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How to use torch.sin, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.5461,  0.1347, -2.7266, -0.2746])\n>>> torch.sin(a)\ntensor([-0.5194,  0.1343, -0.4032, -0.2711])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.5461,  0.1347, -2.7266, -0.2746])\n>>> torch.sin(a)\ntensor([-0.5194,  0.1343, -0.4032, -0.2711])",
        "source": "https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin"
    },
    {
        "X": "How to use torch.gradient, give an example?",
        "Y": ">>> t = torch.tensor([1, 2, 4, 7, 11, 16], dtype=torch.float)\n>>> torch.gradient(t)\ntensor([1. , 1.5, 2.5, 3.5, 4.5, 5. ])\n>>> coords = torch.tensor([0., 1., 1.5, 3.5, 4., 6.], dtype=torch.float)\n>>> torch.gradient(t, spacing=(coords,))\ntensor([1. ,  3. ,  3.5,  6.7,  6.9,  2.5])",
        "Z": ">>> t = torch.tensor([1, 2, 4, 7, 11, 16], dtype=torch.float)\n>>> torch.gradient(t)\ntensor([1. , 1.5, 2.5, 3.5, 4.5, 5. ])\n>>> coords = torch.tensor([0., 1., 1.5, 3.5, 4., 6.], dtype=torch.float)\n>>> torch.gradient(t, spacing=(coords,))\ntensor([1. ,  3. ,  3.5,  6.7,  6.9,  2.5])",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "X": "How to use torch.bincount, give an example?",
        "Y": ">>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n>>> weights = torch.linspace(0, 1, steps=5)\n>>> input, weights\n(tensor([4, 3, 6, 3, 4]),\n tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n>>> torch.bincount(input)\ntensor([0, 0, 0, 2, 2, 0, 1])\n\n>>> input.bincount(weights)\ntensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])",
        "Z": "The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n>>> weights = torch.linspace(0, 1, steps=5)\n>>> input, weights\n(tensor([4, 3, 6, 3, 4]),\n tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n>>> torch.bincount(input)\ntensor([0, 0, 0, 2, 2, 0, 1])\n\n>>> input.bincount(weights)\ntensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "How to use torch.conj, give an example?",
        "Y": ">>> torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\ntensor([-1 - 1j, -2 - 2j, 3 + 3j])",
        "Z": ">>> torch.conj(torch.tensor([-1 + 1j, -2 + 2j, 3 - 3j]))\ntensor([-1 - 1j, -2 - 2j, 3 + 3j])",
        "source": "https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"
    },
    {
        "X": "How to use torch.positive, give an example?",
        "Y": ">>> t = torch.randn(5)\n>>> t\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n>>> torch.positive(t)\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])",
        "Z": ">>> t = torch.randn(5)\n>>> t\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n>>> torch.positive(t)\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])",
        "source": "https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive"
    },
    {
        "X": "How to use torch.atan, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.2341,  0.2539, -0.6256, -0.6448])\n>>> torch.atan(a)\ntensor([ 0.2299,  0.2487, -0.5591, -0.5727])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.2341,  0.2539, -0.6256, -0.6448])\n>>> torch.atan(a)\ntensor([ 0.2299,  0.2487, -0.5591, -0.5727])",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan"
    },
    {
        "X": "How to use torch.tensor, give an example?",
        "Y": ">>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\ntensor([[ 0.1000,  1.2000],\n        [ 2.2000,  3.1000],\n        [ 4.9000,  5.2000]])\n\n>>> torch.tensor([0, 1])  # Type inference on data\ntensor([ 0,  1])\n\n>>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n...              dtype=torch.float64,\n...              device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\ntensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n\n>>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\ntensor(3.1416)\n\n>>> torch.tensor([])  # Create an empty tensor (of size (0,))\ntensor([])",
        "Z": ">>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\ntensor([[ 0.1000,  1.2000],\n        [ 2.2000,  3.1000],\n        [ 4.9000,  5.2000]])\n\n>>> torch.tensor([0, 1])  # Type inference on data\ntensor([ 0,  1])\n\n>>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n...              dtype=torch.float64,\n...              device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\ntensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n\n>>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\ntensor(3.1416)\n\n>>> torch.tensor([])  # Create an empty tensor (of size (0,))\ntensor([])",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "How to use torch.triu_indices, give an example?",
        "Y": ">>> a = torch.triu_indices(3, 3)\n>>> a\ntensor([[0, 0, 0, 1, 1, 2],\n        [0, 1, 2, 1, 2, 2]])\n\n>>> a = torch.triu_indices(4, 3, -1)\n>>> a\ntensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],\n        [0, 1, 2, 0, 1, 2, 1, 2, 2]])\n\n>>> a = torch.triu_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1],\n        [1, 2, 2]])",
        "Z": "The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. >>> a = torch.triu_indices(3, 3)\n>>> a\ntensor([[0, 0, 0, 1, 1, 2],\n        [0, 1, 2, 1, 2, 2]])\n\n>>> a = torch.triu_indices(4, 3, -1)\n>>> a\ntensor([[0, 0, 0, 1, 1, 1, 2, 2, 3],\n        [0, 1, 2, 0, 1, 2, 1, 2, 2]])\n\n>>> a = torch.triu_indices(4, 3, 1)\n>>> a\ntensor([[0, 0, 1],\n        [1, 2, 2]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "How to use torch.remainder, give an example?",
        "Y": ">>> torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([ 1.,  0.,  1.,  1.,  0.,  1.])\n>>> torch.remainder(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. >>> torch.remainder(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([ 1.,  0.,  1.,  1.,  0.,  1.])\n>>> torch.remainder(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([ 1.0000,  0.5000,  0.0000,  1.0000,  0.5000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "How to use torch.eye, give an example?",
        "Y": ">>> torch.eye(3)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  0.],\n        [ 0.,  0.,  1.]])",
        "Z": ">>> torch.eye(3)\ntensor([[ 1.,  0.,  0.],\n        [ 0.,  1.,  0.],\n        [ 0.,  0.,  1.]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "How to use torch.trunc, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 3.4742,  0.5466, -0.8008, -0.9079])\n>>> torch.trunc(a)\ntensor([ 3.,  0., -0., -0.])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([ 3.4742,  0.5466, -0.8008, -0.9079])\n>>> torch.trunc(a)\ntensor([ 3.,  0., -0., -0.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc"
    },
    {
        "X": "How to use torch.log2, give an example?",
        "Y": ">>> a = torch.rand(5)\n>>> a\ntensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])\n\n\n>>> torch.log2(a)\ntensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])",
        "Z": ">>> a = torch.rand(5)\n>>> a\ntensor([ 0.8419,  0.8003,  0.9971,  0.5287,  0.0490])\n\n\n>>> torch.log2(a)\ntensor([-0.2483, -0.3213, -0.0042, -0.9196, -4.3504])",
        "source": "https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2"
    },
    {
        "X": "How to use torch.sparse_coo_tensor, give an example?",
        "Y": ">>> i = torch.tensor([[0, 1, 1],\n...                   [2, 0, 2]])\n>>> v = torch.tensor([3, 4, 5], dtype=torch.float32)\n>>> torch.sparse_coo_tensor(i, v, [2, 4])\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 4), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v)  # Shape inference\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v, [2, 4],\n...                         dtype=torch.float64,\n...                         device=torch.device('cuda:0'))\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,\n       layout=torch.sparse_coo)\n\n# Create an empty sparse tensor with the following invariants:\n#   1. sparse_dim + dense_dim = len(SparseTensor.shape)\n#   2. SparseTensor._indices().shape = (sparse_dim, nnz)\n#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])\n#\n# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and\n# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0,)),\n       size=(1,), nnz=0, layout=torch.sparse_coo)\n\n# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and\n# sparse_dim = 1\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0, 2)),\n       size=(1, 2), nnz=0, layout=torch.sparse_coo)",
        "Z": ">>> i = torch.tensor([[0, 1, 1],\n...                   [2, 0, 2]])\n>>> v = torch.tensor([3, 4, 5], dtype=torch.float32)\n>>> torch.sparse_coo_tensor(i, v, [2, 4])\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 4), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v)  # Shape inference\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       size=(2, 3), nnz=3, layout=torch.sparse_coo)\n\n>>> torch.sparse_coo_tensor(i, v, [2, 4],\n...                         dtype=torch.float64,\n...                         device=torch.device('cuda:0'))\ntensor(indices=tensor([[0, 1, 1],\n                       [2, 0, 2]]),\n       values=tensor([3., 4., 5.]),\n       device='cuda:0', size=(2, 4), nnz=3, dtype=torch.float64,\n       layout=torch.sparse_coo)\n\n# Create an empty sparse tensor with the following invariants:\n#   1. sparse_dim + dense_dim = len(SparseTensor.shape)\n#   2. SparseTensor._indices().shape = (sparse_dim, nnz)\n#   3. SparseTensor._values().shape = (nnz, SparseTensor.shape[sparse_dim:])\n#\n# For instance, to create an empty sparse tensor with nnz = 0, dense_dim = 0 and\n# sparse_dim = 1 (hence indices is a 2D tensor of shape = (1, 0))\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), [], [1])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0,)),\n       size=(1,), nnz=0, layout=torch.sparse_coo)\n\n# and to create an empty sparse tensor with nnz = 0, dense_dim = 1 and\n# sparse_dim = 1\n>>> S = torch.sparse_coo_tensor(torch.empty([1, 0]), torch.empty([0, 2]), [1, 2])\ntensor(indices=tensor([], size=(1, 0)),\n       values=tensor([], size=(0, 2)),\n       size=(1, 2), nnz=0, layout=torch.sparse_coo)",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "How to use torch.logical_or, give an example?",
        "Y": ">>> torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_or(a, b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b.double())\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True,  True, False])",
        "Z": ">>> torch.logical_or(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False,  True])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_or(a, b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b.double())\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a.double(), b)\ntensor([ True,  True,  True, False])\n>>> torch.logical_or(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([ True,  True,  True, False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "X": "How to use torch.solve, give an example?",
        "Y": "X = torch.linalg.solve(A, B)",
        "Z": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack().X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B)",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "How  Supports real-valued and complex-valued inputs., give an example?",
        "Y": ">>> A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],\n...                   [-6.05, -3.30,  5.36, -4.44,  1.08],\n...                   [-0.45,  2.58, -2.70,  0.27,  9.04],\n...                   [8.32,  2.71,  4.35,  -7.17,  2.14],\n...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()\n>>> B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],\n...                   [-1.56,  4.00, -8.67,  1.75,  2.86],\n...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, torch.mm(A, X))\ntensor(1.00000e-06 *\n       7.0977)\n\n>>> # Batched solver example\n>>> A = torch.randn(2, 3, 1, 4, 4)\n>>> B = torch.randn(2, 3, 1, 4, 6)\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, A.matmul(X))\ntensor(1.00000e-06 *\n   3.6386)",
        "Z": "Supports real-valued and complex-valued inputs. >>> A = torch.tensor([[6.80, -2.11,  5.66,  5.97,  8.23],\n...                   [-6.05, -3.30,  5.36, -4.44,  1.08],\n...                   [-0.45,  2.58, -2.70,  0.27,  9.04],\n...                   [8.32,  2.71,  4.35,  -7.17,  2.14],\n...                   [-9.67, -5.14, -7.26,  6.08, -6.87]]).t()\n>>> B = torch.tensor([[4.02,  6.19, -8.22, -7.57, -3.03],\n...                   [-1.56,  4.00, -8.67,  1.75,  2.86],\n...                   [9.81, -4.09, -4.57, -8.61,  8.99]]).t()\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, torch.mm(A, X))\ntensor(1.00000e-06 *\n       7.0977)\n\n>>> # Batched solver example\n>>> A = torch.randn(2, 3, 1, 4, 4)\n>>> B = torch.randn(2, 3, 1, 4, 6)\n>>> X, LU = torch.solve(B, A)\n>>> torch.dist(B, A.matmul(X))\ntensor(1.00000e-06 *\n   3.6386)",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "How to use torch.vander, give an example?",
        "Y": ">>> x = torch.tensor([1, 2, 3, 5])\n>>> torch.vander(x)\ntensor([[  1,   1,   1,   1],\n        [  8,   4,   2,   1],\n        [ 27,   9,   3,   1],\n        [125,  25,   5,   1]])\n>>> torch.vander(x, N=3)\ntensor([[ 1,  1,  1],\n        [ 4,  2,  1],\n        [ 9,  3,  1],\n        [25,  5,  1]])\n>>> torch.vander(x, N=3, increasing=True)\ntensor([[ 1,  1,  1],\n        [ 1,  2,  4],\n        [ 1,  3,  9],\n        [ 1,  5, 25]])",
        "Z": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. >>> x = torch.tensor([1, 2, 3, 5])\n>>> torch.vander(x)\ntensor([[  1,   1,   1,   1],\n        [  8,   4,   2,   1],\n        [ 27,   9,   3,   1],\n        [125,  25,   5,   1]])\n>>> torch.vander(x, N=3)\ntensor([[ 1,  1,  1],\n        [ 4,  2,  1],\n        [ 9,  3,  1],\n        [25,  5,  1]])\n>>> torch.vander(x, N=3, increasing=True)\ntensor([[ 1,  1,  1],\n        [ 1,  2,  4],\n        [ 1,  3,  9],\n        [ 1,  5, 25]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "How to use torch.frexp, give an example?",
        "Y": ">>> x = torch.arange(9.)\n>>> mantissa, exponent = torch.frexp(x)\n>>> mantissa\ntensor([0.0000, 0.5000, 0.5000, 0.7500, 0.5000, 0.6250, 0.7500, 0.8750, 0.5000])\n>>> exponent\ntensor([0, 1, 2, 2, 3, 3, 3, 3, 4], dtype=torch.int32)\n>>> torch.ldexp(mantissa, exponent)\ntensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])",
        "Z": "Supports float inputs. >>> x = torch.arange(9.)\n>>> mantissa, exponent = torch.frexp(x)\n>>> mantissa\ntensor([0.0000, 0.5000, 0.5000, 0.7500, 0.5000, 0.6250, 0.7500, 0.8750, 0.5000])\n>>> exponent\ntensor([0, 1, 2, 2, 3, 3, 3, 3, 4], dtype=torch.int32)\n>>> torch.ldexp(mantissa, exponent)\ntensor([0., 1., 2., 3., 4., 5., 6., 7., 8.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "X": "How to use torch.addcdiv, give an example?",
        "Y": ">>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcdiv(t, t1, t2, value=0.1)\ntensor([[-0.2312, -3.6496,  0.1312],\n        [-1.0428,  3.4292, -0.1030],\n        [-0.5369, -0.9829,  0.0430]])",
        "Z": "For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. >>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcdiv(t, t1, t2, value=0.1)\ntensor([[-0.2312, -3.6496,  0.1312],\n        [-1.0428,  3.4292, -0.1030],\n        [-0.5369, -0.9829,  0.0430]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "How to use torch.moveaxis, give an example?",
        "Y": ">>> t = torch.randn(3,2,1)\n>>> t\ntensor([[[-0.3362],\n        [-0.8437]],\n\n        [[-0.9627],\n        [ 0.1727]],\n\n        [[ 0.5173],\n        [-0.1398]]])\n>>> torch.moveaxis(t, 1, 0).shape\ntorch.Size([2, 3, 1])\n>>> torch.moveaxis(t, 1, 0)\ntensor([[[-0.3362],\n        [-0.9627],\n        [ 0.5173]],\n\n        [[-0.8437],\n        [ 0.1727],\n        [-0.1398]]])\n>>> torch.moveaxis(t, (1, 2), (0, 1)).shape\ntorch.Size([2, 1, 3])\n>>> torch.moveaxis(t, (1, 2), (0, 1))\ntensor([[[-0.3362, -0.9627,  0.5173]],\n\n        [[-0.8437,  0.1727, -0.1398]]])",
        "Z": "This function is equivalent to NumPy\u2019s moveaxis function. >>> t = torch.randn(3,2,1)\n>>> t\ntensor([[[-0.3362],\n        [-0.8437]],\n\n        [[-0.9627],\n        [ 0.1727]],\n\n        [[ 0.5173],\n        [-0.1398]]])\n>>> torch.moveaxis(t, 1, 0).shape\ntorch.Size([2, 3, 1])\n>>> torch.moveaxis(t, 1, 0)\ntensor([[[-0.3362],\n        [-0.9627],\n        [ 0.5173]],\n\n        [[-0.8437],\n        [ 0.1727],\n        [-0.1398]]])\n>>> torch.moveaxis(t, (1, 2), (0, 1)).shape\ntorch.Size([2, 1, 3])\n>>> torch.moveaxis(t, (1, 2), (0, 1))\ntensor([[[-0.3362, -0.9627,  0.5173]],\n\n        [[-0.8437,  0.1727, -0.1398]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis"
    },
    {
        "X": "How to use torch.lgamma, give an example?",
        "Y": ">>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.lgamma(a)\ntensor([ 0.5724,  0.0000, -0.1208])",
        "Z": ">>> a = torch.arange(0.5, 2, 0.5)\n>>> torch.lgamma(a)\ntensor([ 0.5724,  0.0000, -0.1208])",
        "source": "https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma"
    },
    {
        "X": "How to use torch.futures.Future.add_done_callback, give an example?",
        "Y": ">>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"This will run after the future has finished.\")\n>>>     print(fut.wait())\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.add_done_callback(callback)\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> This will run after the future has finished.\n>>> 5",
        "Z": ">>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"This will run after the future has finished.\")\n>>>     print(fut.wait())\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.add_done_callback(callback)\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> This will run after the future has finished.\n>>> 5",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How to use torch.futures.Future.set_exception, give an example?",
        "Y": ">>> import torch\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\n>>>\n>>> # Output:\n>>> # This will run after the future has finished.\n>>> ValueError: foo",
        "Z": ">>> import torch\n>>>\n>>> fut = torch.futures.Future()\n>>> fut.set_exception(ValueError(\"foo\"))\n>>> fut.wait()\n>>>\n>>> # Output:\n>>> # This will run after the future has finished.\n>>> ValueError: foo",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How to use torch.futures.Future.set_result, give an example?",
        "Y": ">>> import threading\n>>> import time\n>>> import torch\n>>>\n>>> def slow_set_future(fut, value):\n>>>     time.sleep(0.5)\n>>>     fut.set_result(value)\n>>>\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n>>>     target=slow_set_future,\n>>>     args=(fut, torch.ones(2) * 3)\n>>> )\n>>> t.start()\n>>>\n>>> print(fut.wait())  # tensor([3., 3.])\n>>> t.join()",
        "Z": ">>> import threading\n>>> import time\n>>> import torch\n>>>\n>>> def slow_set_future(fut, value):\n>>>     time.sleep(0.5)\n>>>     fut.set_result(value)\n>>>\n>>> fut = torch.futures.Future()\n>>> t = threading.Thread(\n>>>     target=slow_set_future,\n>>>     args=(fut, torch.ones(2) * 3)\n>>> )\n>>> t.start()\n>>>\n>>> print(fut.wait())  # tensor([3., 3.])\n>>> t.join()",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How to use torch.futures.Future.then, give an example?",
        "Y": ">>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"RPC return value is {fut.wait()}.\")\n>>>\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\n>>> )\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> # RPC return value is 5.\n>>> # Chained cb done. None",
        "Z": ">>> import torch\n>>>\n>>> def callback(fut):\n>>>     print(f\"RPC return value is {fut.wait()}.\")\n>>>\n>>> fut = torch.futures.Future()\n>>> # The inserted callback will print the return value when\n>>> # receiving the response from \"worker1\"\n>>> cb_fut = fut.then(callback)\n>>> chain_cb_fut = cb_fut.then(\n>>>     lambda x : print(f\"Chained cb done. {x.wait()}\")\n>>> )\n>>> fut.set_result(5)\n>>>\n>>> # Outputs are:\n>>> # RPC return value is 5.\n>>> # Chained cb done. None",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How to use torch.futures.collect_all, give an example?",
        "Y": ">>> import torch\n>>>\n>>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>>\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>>\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>>\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\n>>> # outputs:\n>>> # fut0 result = 0\n>>> # fut1 result = 1",
        "Z": ">>> import torch\n>>>\n>>> fut0 = torch.futures.Future()\n>>> fut1 = torch.futures.Future()\n>>>\n>>> fut = torch.futures.collect_all([fut0, fut1])\n>>>\n>>> fut0.set_result(0)\n>>> fut1.set_result(1)\n>>>\n>>> fut_list = fut.wait()\n>>> print(f\"fut0 result = {fut_list[0].wait()}\")\n>>> print(f\"fut1 result = {fut_list[1].wait()}\")\n>>> # outputs:\n>>> # fut0 result = 0\n>>> # fut1 result = 1",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How to use torch.fmod, give an example?",
        "Y": ">>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([-1., -0., -1.,  1.,  0.,  1.])\n>>> torch.fmod(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000])",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. >>> torch.fmod(torch.tensor([-3., -2, -1, 1, 2, 3]), 2)\ntensor([-1., -0., -1.,  1.,  0.,  1.])\n>>> torch.fmod(torch.tensor([1, 2, 3, 4, 5]), 1.5)\ntensor([1.0000, 0.5000, 0.0000, 1.0000, 0.5000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "How to use torch.dist, give an example?",
        "Y": ">>> x = torch.randn(4)\n>>> x\ntensor([-1.5393, -0.8675,  0.5916,  1.6321])\n>>> y = torch.randn(4)\n>>> y\ntensor([ 0.0967, -1.0511,  0.6295,  0.8360])\n>>> torch.dist(x, y, 3.5)\ntensor(1.6727)\n>>> torch.dist(x, y, 3)\ntensor(1.6973)\n>>> torch.dist(x, y, 0)\ntensor(inf)\n>>> torch.dist(x, y, 1)\ntensor(2.6537)",
        "Z": "The shapes of input and other must be\nbroadcastable. >>> x = torch.randn(4)\n>>> x\ntensor([-1.5393, -0.8675,  0.5916,  1.6321])\n>>> y = torch.randn(4)\n>>> y\ntensor([ 0.0967, -1.0511,  0.6295,  0.8360])\n>>> torch.dist(x, y, 3.5)\ntensor(1.6727)\n>>> torch.dist(x, y, 3)\ntensor(1.6973)\n>>> torch.dist(x, y, 0)\ntensor(inf)\n>>> torch.dist(x, y, 1)\ntensor(2.6537)",
        "source": "https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist"
    },
    {
        "X": "How to use torch.empty, give an example?",
        "Y": ">>> a=torch.empty((2,3), dtype=torch.int32, device = 'cuda')\n>>> torch.empty_like(a)\ntensor([[0, 0, 0],\n        [0, 0, 0]], device='cuda:0', dtype=torch.int32)",
        "Z": ">>> a=torch.empty((2,3), dtype=torch.int32, device = 'cuda')\n>>> torch.empty_like(a)\ntensor([[0, 0, 0],\n        [0, 0, 0]], device='cuda:0', dtype=torch.int32)",
        "source": "https://pytorch.org/docs/stable/generated/torch.empty.html#torch.empty"
    },
    {
        "X": "How to use torch.nansum, give an example?",
        "Y": ">>> a = torch.tensor([1., 2., float('nan'), 4.])\n>>> torch.nansum(a)\ntensor(7.)",
        "Z": ">>> a = torch.tensor([1., 2., float('nan'), 4.])\n>>> torch.nansum(a)\ntensor(7.)",
        "source": "https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum"
    },
    {
        "X": "How  If keepdim is True, the output tensor is of the same size\nas input except in the dimension(s) dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in the\noutput tensor having 1 (or len(dim)) fewer dimension(s)., give an example?",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n        [-0.2993,  0.9138,  0.9337, -1.6864],\n        [ 0.1132,  0.7892, -0.1003,  0.5688],\n        [ 0.3637, -0.9906, -0.4752, -1.5197]])\n>>> torch.sum(a, 1)\ntensor([-0.4598, -0.1381,  1.3708, -2.6217])\n>>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n>>> torch.sum(b, (2, 1))\ntensor([  435.,  1335.,  2235.,  3135.])",
        "Z": "If keepdim is True, the output tensor is of the same size\nas input except in the dimension(s) dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in the\noutput tensor having 1 (or len(dim)) fewer dimension(s). >>> a = torch.randn(4, 4)\n>>> a\ntensor([[ 0.0569, -0.2475,  0.0737, -0.3429],\n        [-0.2993,  0.9138,  0.9337, -1.6864],\n        [ 0.1132,  0.7892, -0.1003,  0.5688],\n        [ 0.3637, -0.9906, -0.4752, -1.5197]])\n>>> torch.sum(a, 1)\ntensor([-0.4598, -0.1381,  1.3708, -2.6217])\n>>> b = torch.arange(4 * 5 * 6).view(4, 5, 6)\n>>> torch.sum(b, (2, 1))\ntensor([  435.,  1335.,  2235.,  3135.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "How to use torch.diagonal, give an example?",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0854,  1.1431, -0.1752],\n        [ 0.8536, -0.0905,  0.0360],\n        [ 0.6927, -0.3735, -0.4945]])\n\n\n>>> torch.diagonal(a, 0)\ntensor([-1.0854, -0.0905, -0.4945])\n\n\n>>> torch.diagonal(a, 1)\ntensor([ 1.1431,  0.0360])\n\n\n>>> x = torch.randn(2, 5, 4, 2)\n>>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n         [-1.1065,  1.0401, -0.2235, -0.7938]],\n\n        [[-1.7325, -0.3081,  0.6166,  0.2335],\n         [ 1.0500,  0.7336, -0.3836, -1.1015]]])",
        "Z": "Applying torch.diag_embed() to the output of this function with\nthe same arguments yields a diagonal matrix with the diagonal entries\nof the input. However, torch.diag_embed() has different default\ndimensions, so those need to be explicitly specified. >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0854,  1.1431, -0.1752],\n        [ 0.8536, -0.0905,  0.0360],\n        [ 0.6927, -0.3735, -0.4945]])\n\n\n>>> torch.diagonal(a, 0)\ntensor([-1.0854, -0.0905, -0.4945])\n\n\n>>> torch.diagonal(a, 1)\ntensor([ 1.1431,  0.0360])\n\n\n>>> x = torch.randn(2, 5, 4, 2)\n>>> torch.diagonal(x, offset=-1, dim1=1, dim2=2)\ntensor([[[-1.2631,  0.3755, -1.5977, -1.8172],\n         [-1.1065,  1.0401, -0.2235, -0.7938]],\n\n        [[-1.7325, -0.3081,  0.6166,  0.2335],\n         [ 1.0500,  0.7336, -0.3836, -1.1015]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal"
    },
    {
        "X": "How to use torch.le, give an example?",
        "Y": ">>> torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, False], [True, True]])",
        "Z": "The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.le(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, False], [True, True]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.le.html#torch.le"
    },
    {
        "X": "How to use torch.rand, give an example?",
        "Y": ">>> torch.rand(4)\ntensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n>>> torch.rand(2, 3)\ntensor([[ 0.8237,  0.5781,  0.6879],\n        [ 0.3816,  0.7249,  0.0998]])",
        "Z": "The shape of the tensor is defined by the variable argument size. >>> torch.rand(4)\ntensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n>>> torch.rand(2, 3)\ntensor([[ 0.8237,  0.5781,  0.6879],\n        [ 0.3816,  0.7249,  0.0998]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand"
    },
    {
        "X": "How to use torch.take_along_dim, give an example?",
        "Y": ">>> t = torch.tensor([[10, 30, 20], [60, 40, 50]])\n>>> max_idx = torch.argmax(t)\n>>> torch.take_along_dim(t, max_idx)\ntensor([60])\n>>> sorted_idx = torch.argsort(t, dim=1)\n>>> torch.take_along_dim(t, sorted_idx, dim=1)\ntensor([[10, 20, 30],\n        [40, 50, 60]])",
        "Z": "Functions that return indices along a dimension, like torch.argmax() and torch.argsort(),\nare designed to work with this function. See the examples below. >>> t = torch.tensor([[10, 30, 20], [60, 40, 50]])\n>>> max_idx = torch.argmax(t)\n>>> torch.take_along_dim(t, max_idx)\ntensor([60])\n>>> sorted_idx = torch.argsort(t, dim=1)\n>>> torch.take_along_dim(t, sorted_idx, dim=1)\ntensor([[10, 20, 30],\n        [40, 50, 60]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim"
    },
    {
        "X": "How to use torch.isposinf, give an example?",
        "Y": ">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\n>>> torch.isposinf(a)\ntensor([False,  True, False])",
        "Z": ">>> a = torch.tensor([-float('inf'), float('inf'), 1.2])\n>>> torch.isposinf(a)\ntensor([False,  True, False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf"
    },
    {
        "X": "How to use torch.logical_and, give an example?",
        "Y": ">>> torch.logical_and(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False, False])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_and(a, b)\ntensor([False, False,  True, False])\n>>> torch.logical_and(a.double(), b.double())\ntensor([False, False,  True, False])\n>>> torch.logical_and(a.double(), b)\ntensor([False, False,  True, False])\n>>> torch.logical_and(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([False, False,  True, False])",
        "Z": ">>> torch.logical_and(torch.tensor([True, False, True]), torch.tensor([True, False, False]))\ntensor([ True, False, False])\n>>> a = torch.tensor([0, 1, 10, 0], dtype=torch.int8)\n>>> b = torch.tensor([4, 0, 1, 0], dtype=torch.int8)\n>>> torch.logical_and(a, b)\ntensor([False, False,  True, False])\n>>> torch.logical_and(a.double(), b.double())\ntensor([False, False,  True, False])\n>>> torch.logical_and(a.double(), b)\ntensor([False, False,  True, False])\n>>> torch.logical_and(a, b, out=torch.empty(4, dtype=torch.bool))\ntensor([False, False,  True, False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and"
    },
    {
        "X": "How to use torch.neg, give an example?",
        "Y": ">>> a = torch.randn(5)\n>>> a\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n>>> torch.neg(a)\ntensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])",
        "Z": ">>> a = torch.randn(5)\n>>> a\ntensor([ 0.0090, -0.2262, -0.0682, -0.2866,  0.3940])\n>>> torch.neg(a)\ntensor([-0.0090,  0.2262,  0.0682,  0.2866, -0.3940])",
        "source": "https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg"
    },
    {
        "X": "How to use torch.trace, give an example?",
        "Y": ">>> x = torch.arange(1., 10.).view(3, 3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.],\n        [ 7.,  8.,  9.]])\n>>> torch.trace(x)\ntensor(15.)",
        "Z": ">>> x = torch.arange(1., 10.).view(3, 3)\n>>> x\ntensor([[ 1.,  2.,  3.],\n        [ 4.,  5.,  6.],\n        [ 7.,  8.,  9.]])\n>>> torch.trace(x)\ntensor(15.)",
        "source": "https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace"
    },
    {
        "X": "How to use There are two ways to initialize using TCP, both requiring a network address\nreachable from all processes and a desired world_size. The first way\nrequires specifying an address that belongs to the rank 0 process. This\ninitialization method requires that all processes have manually specified ranks.Note that multicast address is not supported anymore in the latest distributed\npackage. group_name is deprecated as well., give an example?",
        "Y": "import torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)",
        "Z": "Note that multicast address is not supported anymore in the latest distributed\npackage. group_name is deprecated as well. import torch.distributed as dist\n\n# Use address of one of the machines\ndist.init_process_group(backend, init_method='tcp://10.1.1.20:23456',\n                        rank=args.rank, world_size=4)",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use Another initialization method makes use of a file system that is shared and\nvisible from all machines in a group, along with a desired world_size. The URL should start\nwith file:// and contain a path to a non-existent file (in an existing\ndirectory) on a shared file system. File-system initialization will automatically\ncreate that file if it doesn\u2019t exist, but will not delete the file. Therefore, it\nis your responsibility to make sure that the file is cleaned up before the next\ninit_process_group() call on the same file path/name.Note that automatic rank assignment is not supported anymore in the latest\ndistributed package and group_name is deprecated as well., give an example?",
        "Y": "import torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)",
        "Z": "Note that automatic rank assignment is not supported anymore in the latest\ndistributed package and group_name is deprecated as well. import torch.distributed as dist\n\n# rank should always be specified\ndist.init_process_group(backend, init_method='file:///mnt/nfs/sharedfile',\n                        world_size=4, rank=args.rank)",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use Distributed Key Value Store, give an example?",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")",
        "Z": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Run on process 1 (server)\n>>> server_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, True, timedelta(seconds=30))\n>>> # Run on process 2 (client)\n>>> client_store = dist.TCPStore(\"127.0.0.1\", 1234, 2, False)\n>>> # Use any of the store methods from either the client or server after initialization\n>>> server_store.set(\"first_key\", \"first_value\")\n>>> client_store.get(\"first_key\")",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.Store.set, give an example?",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")",
        "Z": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.Store.get, give an example?",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")",
        "Z": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # Should return \"first_value\"\n>>> store.get(\"first_key\")",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.Store.add, give an example?",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")",
        "Z": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.add(\"first_key\", 1)\n>>> store.add(\"first_key\", 6)\n>>> # Should return 7\n>>> store.get(\"first_key\")",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.Store.wait, give an example?",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])",
        "Z": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> # This will throw an exception after 30 seconds\n>>> store.wait([\"bad_key\"])",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.Store.num_keys, give an example?",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()",
        "Z": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\", \"first_value\")\n>>> # This should return 2\n>>> store.num_keys()",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.Store.delete_key, give an example?",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")",
        "Z": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, HashStore can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set(\"first_key\")\n>>> # This should return true\n>>> store.delete_key(\"first_key\")\n>>> # This should return false\n>>> store.delete_key(\"bad_key\")",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.Store.set_timeout, give an example?",
        "Y": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])",
        "Z": ">>> import torch.distributed as dist\n>>> from datetime import timedelta\n>>> # Using TCPStore as an example, other store types can also be used\n>>> store = dist.TCPStore(\"127.0.0.1\", 0, 1, True, timedelta(seconds=30))\n>>> store.set_timeout(timedelta(seconds=10))\n>>> # This will throw an exception after 10 seconds\n>>> store.wait([\"bad_key\"])",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams:, give an example?",
        "Y": "# Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)",
        "Z": "The following code can serve as a reference regarding semantics for CUDA operations when using distributed collectives.\nIt shows the explicit need to synchronize when using collective outputs on different CUDA streams: # Code runs on each rank.\ndist.init_process_group(\"nccl\", rank=rank, world_size=2)\noutput = torch.tensor([rank]).cuda(rank)\ns = torch.cuda.Stream()\nhandle = dist.all_reduce(output, async_op=True)\n# Wait ensures the operation is enqueued, but not necessarily complete.\nhandle.wait()\n# Using result on non-default stream.\nwith torch.cuda.stream(s):\n    s.wait_stream(torch.cuda.default_stream())\n    output.add_(100)\nif rank == 0:\n    # if the explicit call to wait_stream was omitted, the output below will be\n    # non-deterministically 1 or 101, depending on whether the allreduce overwrote\n    # the value after the add completed.\n    print(output)",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.broadcast_object_list, give an example?",
        "Y": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> dist.broadcast_object_list(objects, src=0)\n>>> broadcast_objects\n['foo', 12, {1: 2}]",
        "Z": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     objects = [None, None, None]\n>>> dist.broadcast_object_list(objects, src=0)\n>>> broadcast_objects\n['foo', 12, {1: 2}]",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.all_reduce, give an example?",
        "Y": ">>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1",
        "Z": "Complex tensors are supported. >>> # All tensors below are of torch.int64 type.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_reduce(tensor, op=ReduceOp.SUM)\n>>> tensor\ntensor([4, 6]) # Rank 0\ntensor([4, 6]) # Rank 1",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How  Complex tensors are supported., give an example?",
        "Y": ">>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1",
        "Z": "Complex tensors are supported. >>> # All tensors below are of torch.cfloat dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.cfloat) for _ in range(2)]\n>>> tensor_list\n[tensor([0.+0.j, 0.+0.j]), tensor([0.+0.j, 0.+0.j])] # Rank 0 and 1\n>>> tensor = torch.tensor([1+1j, 2+2j], dtype=torch.cfloat) + 2 * rank * (1+1j)\n>>> tensor\ntensor([1.+1.j, 2.+2.j]) # Rank 0\ntensor([3.+3.j, 4.+4.j]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 0\n[tensor([1.+1.j, 2.+2.j]), tensor([3.+3.j, 4.+4.j])] # Rank 1",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.all_gather, give an example?",
        "Y": ">>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1",
        "Z": "Complex tensors are supported. >>> # All tensors below are of torch.int64 dtype.\n>>> # We have 2 process groups, 2 ranks.\n>>> tensor_list = [torch.zero(2, dtype=torch.int64) for _ in range(2)]\n>>> tensor_list\n[tensor([0, 0]), tensor([0, 0])] # Rank 0 and 1\n>>> tensor = torch.arange(2, dtype=torch.int64) + 1 + 2 * rank\n>>> tensor\ntensor([1, 2]) # Rank 0\ntensor([3, 4]) # Rank 1\n>>> dist.all_gather(tensor_list, tensor)\n>>> tensor_list\n[tensor([1, 2]), tensor([3, 4])] # Rank 0\n[tensor([1, 2]), tensor([3, 4])] # Rank 1",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.all_gather_object, give an example?",
        "Y": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]",
        "Z": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.all_gather_object(output, gather_objects[dist.get_rank()])\n>>> output\n['foo', 12, {1: 2}]",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.gather_object, give an example?",
        "Y": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n        gather_objects[dist.get_rank()],\n        output if dist.get_rank() == 0 else None,\n        dst=0\n    )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]",
        "Z": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> # Assumes world_size of 3.\n>>> gather_objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> output = [None for _ in gather_objects]\n>>> dist.gather_object(\n        gather_objects[dist.get_rank()],\n        output if dist.get_rank() == 0 else None,\n        dst=0\n    )\n>>> # On rank 0\n>>> output\n['foo', 12, {1: 2}]",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.scatter_object_list, give an example?",
        "Y": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]",
        "Z": ">>> # Note: Process group initialization omitted on each rank.\n>>> import torch.distributed as dist\n>>> if dist.get_rank() == 0:\n>>>     # Assumes world_size of 3.\n>>>     objects = [\"foo\", 12, {1: 2}] # any picklable object\n>>> else:\n>>>     # Can be any list on non-src ranks, elements are not used.\n>>>     objects = [None, None, None]\n>>> output_list = [None]\n>>> dist.scatter_object_list(output_list, objects, src=0)\n>>> # Rank i gets objects[i]. For example, on rank 2:\n>>> output_list\n[{1: 2}]",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.distributed.all_to_all, give an example?",
        "Y": ">>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3",
        "Z": ">>> input = torch.arange(4) + rank * 4\n>>> input = list(input.chunk(4))\n>>> input\n[tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0\n[tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1\n[tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2\n[tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3\n>>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))\n>>> dist.all_to_all(output, input)\n>>> output\n[tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0\n[tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1\n[tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2\n[tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use Note that you can use torch.profiler (recommended, only available after 1.8.1)  or torch.autograd.profiler to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,\nnccl, mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator:, give an example?",
        "Y": "import torch\nimport torch.distributed as dist\nwith torch.profiler():\n    tensor = torch.randn(20, 10)\n    dist.all_reduce(tensor)",
        "Z": "Note that you can use torch.profiler (recommended, only available after 1.8.1)  or torch.autograd.profiler to profile collective communication and point-to-point communication APIs mentioned here. All out-of-the-box backends (gloo,\nnccl, mpi) are supported and collective communication usage will be rendered as expected in profiling output/traces. Profiling your code is the same as any regular torch operator: import torch\nimport torch.distributed as dist\nwith torch.profiler():\n    tensor = torch.randn(20, 10)\n    dist.all_reduce(tensor)",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use For example, if the system we use for distributed training has 2 nodes, each\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\nlike to all-reduce. The following code can serve as a reference:Code running on Node 0, give an example?",
        "Y": "import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=0)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)",
        "Z": "For example, if the system we use for distributed training has 2 nodes, each\nof which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would\nlike to all-reduce. The following code can serve as a reference:Code running on Node 0 import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=0)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use Code running on Node 0Code running on Node 1, give an example?",
        "Y": "import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=1)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)",
        "Z": "Code running on Node 0Code running on Node 1 import torch\nimport torch.distributed as dist\n\ndist.init_process_group(backend=\"nccl\",\n                        init_method=\"file:///distributed_test\",\n                        world_size=2,\n                        rank=1)\ntensor_list = []\nfor dev_idx in range(torch.cuda.device_count()):\n    tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))\n\ndist.all_reduce_multigpu(tensor_list)",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use In both cases of single-node distributed training or multi-node distributed\ntraining, this utility will launch the given number of processes per node\n(--nproc_per_node). If used for GPU training, this number needs to be less\nor equal to the number of GPUs on the current system (nproc_per_node),\nand each process will be operating on a single GPU from GPU 0 to\nGPU (nproc_per_node - 1).How to use this module:, give an example?",
        "Y": ">>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n           arguments of your training script)",
        "Z": "In both cases of single-node distributed training or multi-node distributed\ntraining, this utility will launch the given number of processes per node\n(--nproc_per_node). If used for GPU training, this number needs to be less\nor equal to the number of GPUs on the current system (nproc_per_node),\nand each process will be operating on a single GPU from GPU 0 to\nGPU (nproc_per_node - 1).How to use this module: >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other\n           arguments of your training script)",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use How to use this module:Node 1: (IP: 192.168.1.1, and has a free port: 1234), give an example?",
        "Y": ">>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)",
        "Z": "How to use this module:Node 1: (IP: 192.168.1.1, and has a free port: 1234) >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=0 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2:, give an example?",
        "Y": ">>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)",
        "Z": "Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2: >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE\n           --nnodes=2 --node_rank=1 --master_addr=\"192.168.1.1\"\n           --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3\n           and all other arguments of your training script)",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How  Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2:, give an example?",
        "Y": ">>> python -m torch.distributed.launch --help",
        "Z": "Node 1: (IP: 192.168.1.1, and has a free port: 1234)Node 2: >>> python -m torch.distributed.launch --help",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use 2. In your training program, you must parse the command-line argument:\n--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.\nIf your training program uses GPUs, you should ensure that your code only\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:Parsing the local_rank argument, give an example?",
        "Y": ">>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local_rank\", type=int)\n>>> args = parser.parse_args()",
        "Z": "2. In your training program, you must parse the command-line argument:\n--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.\nIf your training program uses GPUs, you should ensure that your code only\nruns on the GPU device of LOCAL_PROCESS_RANK. This can be done by:Parsing the local_rank argument >>> import argparse\n>>> parser = argparse.ArgumentParser()\n>>> parser.add_argument(\"--local_rank\", type=int)\n>>> args = parser.parse_args()",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use Parsing the local_rank argumentSet your device to local rank using either, give an example?",
        "Y": ">>> torch.cuda.set_device(args.local_rank)  # before your code runs",
        "Z": "Parsing the local_rank argumentSet your device to local rank using either >>> torch.cuda.set_device(args.local_rank)  # before your code runs",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use Set your device to local rank using eitheror, give an example?",
        "Y": ">>> with torch.cuda.device(args.local_rank):\n>>>    # your code to run",
        "Z": "Set your device to local rank using eitheror >>> with torch.cuda.device(args.local_rank):\n>>>    # your code to run",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use or3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses env://, which is the only supported init_method\nby this module., give an example?",
        "Y": "torch.distributed.init_process_group(backend='YOUR BACKEND',\n                                     init_method='env://')",
        "Z": "or3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses env://, which is the only supported init_method\nby this module. torch.distributed.init_process_group(backend='YOUR BACKEND',\n                                     init_method='env://')",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use 3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses env://, which is the only supported init_method\nby this module.4. In your training program, you can either use regular distributed functions\nor use torch.nn.parallel.DistributedDataParallel() module. If your\ntraining program uses GPUs for training and you would like to use\ntorch.nn.parallel.DistributedDataParallel() module,\nhere is how to configure it., give an example?",
        "Y": "model = torch.nn.parallel.DistributedDataParallel(model,\n                                                  device_ids=[args.local_rank],\n                                                  output_device=args.local_rank)",
        "Z": "3. In your training program, you are supposed to call the following function\nat the beginning to start the distributed backend. You need to make sure that\nthe init_method uses env://, which is the only supported init_method\nby this module.4. In your training program, you can either use regular distributed functions\nor use torch.nn.parallel.DistributedDataParallel() module. If your\ntraining program uses GPUs for training and you would like to use\ntorch.nn.parallel.DistributedDataParallel() module,\nhere is how to configure it. model = torch.nn.parallel.DistributedDataParallel(model,\n                                                  device_ids=[args.local_rank],\n                                                  output_device=args.local_rank)",
        "source": "https://pytorch.org/docs/stable/distributed.html"
    },
    {
        "X": "How to use torch.ceil, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\n>>> torch.ceil(a)\ntensor([-0., -1., -1.,  1.])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.6341, -1.4208, -1.0900,  0.5826])\n>>> torch.ceil(a)\ntensor([-0., -1., -1.,  1.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil"
    },
    {
        "X": "How to use torch.linalg.det, give an example?",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.9478,  0.9158, -1.1295],\n        [ 0.9701,  0.7346, -1.8044],\n        [-0.2337,  0.0557,  0.6929]])\n>>> torch.linalg.det(a)\ntensor(0.0934)\n\n>>> out = torch.empty(0)\n>>> torch.linalg.det(a, out=out)\ntensor(0.0934)\n>>> out\ntensor(0.0934)\n\n>>> a = torch.randn(3, 2, 2)\n>>> a\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> torch.linalg.det(a)\ntensor([1.1990, 0.4099, 0.7386])",
        "Z": "Supports input of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if A is a batch of matrices then\nthe output has the same batch dimensions. >>> a = torch.randn(3, 3)\n>>> a\ntensor([[ 0.9478,  0.9158, -1.1295],\n        [ 0.9701,  0.7346, -1.8044],\n        [-0.2337,  0.0557,  0.6929]])\n>>> torch.linalg.det(a)\ntensor(0.0934)\n\n>>> out = torch.empty(0)\n>>> torch.linalg.det(a, out=out)\ntensor(0.0934)\n>>> out\ntensor(0.0934)\n\n>>> a = torch.randn(3, 2, 2)\n>>> a\ntensor([[[ 0.9254, -0.6213],\n         [-0.5787,  1.6843]],\n\n        [[ 0.3242, -0.9665],\n         [ 0.4539, -0.0887]],\n\n        [[ 1.1336, -0.4025],\n         [-0.7089,  0.9032]]])\n>>> torch.linalg.det(a)\ntensor([1.1990, 0.4099, 0.7386])",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.det.html#torch.linalg.det"
    },
    {
        "X": "How to use torch.nonzero, give an example?",
        "Y": ">>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\ntensor([[ 0],\n        [ 1],\n        [ 2],\n        [ 4]])\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n...                             [0.0, 0.4, 0.0, 0.0],\n...                             [0.0, 0.0, 1.2, 0.0],\n...                             [0.0, 0.0, 0.0,-0.4]]))\ntensor([[ 0,  0],\n        [ 1,  1],\n        [ 2,  2],\n        [ 3,  3]])\n>>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n(tensor([0, 1, 2, 4]),)\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n...                             [0.0, 0.4, 0.0, 0.0],\n...                             [0.0, 0.0, 1.2, 0.0],\n...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n>>> torch.nonzero(torch.tensor(5), as_tuple=True)\n(tensor([0]),)",
        "Z": "As a special case, when input has zero dimensions and a nonzero scalar\nvalue, it is treated as a one-dimensional tensor with one element. >>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))\ntensor([[ 0],\n        [ 1],\n        [ 2],\n        [ 4]])\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n...                             [0.0, 0.4, 0.0, 0.0],\n...                             [0.0, 0.0, 1.2, 0.0],\n...                             [0.0, 0.0, 0.0,-0.4]]))\ntensor([[ 0,  0],\n        [ 1,  1],\n        [ 2,  2],\n        [ 3,  3]])\n>>> torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)\n(tensor([0, 1, 2, 4]),)\n>>> torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],\n...                             [0.0, 0.4, 0.0, 0.0],\n...                             [0.0, 0.0, 1.2, 0.0],\n...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)\n(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))\n>>> torch.nonzero(torch.tensor(5), as_tuple=True)\n(tensor([0]),)",
        "source": "https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero"
    },
    {
        "X": "How to use torch.reciprocal, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.4595, -2.1219, -1.4314,  0.7298])\n>>> torch.reciprocal(a)\ntensor([-2.1763, -0.4713, -0.6986,  1.3702])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.4595, -2.1219, -1.4314,  0.7298])\n>>> torch.reciprocal(a)\ntensor([-2.1763, -0.4713, -0.6986,  1.3702])",
        "source": "https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal"
    },
    {
        "X": "How to use torch.sort, give an example?",
        "Y": ">>> x = torch.randn(3, 4)\n>>> sorted, indices = torch.sort(x)\n>>> sorted\ntensor([[-0.2162,  0.0608,  0.6719,  2.3332],\n        [-0.5793,  0.0061,  0.6058,  0.9497],\n        [-0.5071,  0.3343,  0.9553,  1.0960]])\n>>> indices\ntensor([[ 1,  0,  2,  3],\n        [ 3,  1,  0,  2],\n        [ 0,  3,  1,  2]])\n\n>>> sorted, indices = torch.sort(x, 0)\n>>> sorted\ntensor([[-0.5071, -0.2162,  0.6719, -0.5793],\n        [ 0.0608,  0.0061,  0.9497,  0.3343],\n        [ 0.6058,  0.9553,  1.0960,  2.3332]])\n>>> indices\ntensor([[ 2,  0,  0,  1],\n        [ 0,  1,  1,  2],\n        [ 1,  2,  2,  0]])\n>>> x = torch.tensor([0, 1] * 9)\n>>> x.sort()\ntorch.return_types.sort(\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1]))\n>>> x.sort(stable=True)\ntorch.return_types.sort(\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17]))",
        "Z": "A namedtuple of (values, indices) is returned, where the values are the\nsorted values and indices are the indices of the elements in the original\ninput tensor. >>> x = torch.randn(3, 4)\n>>> sorted, indices = torch.sort(x)\n>>> sorted\ntensor([[-0.2162,  0.0608,  0.6719,  2.3332],\n        [-0.5793,  0.0061,  0.6058,  0.9497],\n        [-0.5071,  0.3343,  0.9553,  1.0960]])\n>>> indices\ntensor([[ 1,  0,  2,  3],\n        [ 3,  1,  0,  2],\n        [ 0,  3,  1,  2]])\n\n>>> sorted, indices = torch.sort(x, 0)\n>>> sorted\ntensor([[-0.5071, -0.2162,  0.6719, -0.5793],\n        [ 0.0608,  0.0061,  0.9497,  0.3343],\n        [ 0.6058,  0.9553,  1.0960,  2.3332]])\n>>> indices\ntensor([[ 2,  0,  0,  1],\n        [ 0,  1,  1,  2],\n        [ 1,  2,  2,  0]])\n>>> x = torch.tensor([0, 1] * 9)\n>>> x.sort()\ntorch.return_types.sort(\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    indices=tensor([ 2, 16,  4,  6, 14,  8,  0, 10, 12,  9, 17, 15, 13, 11,  7,  5,  3,  1]))\n>>> x.sort(stable=True)\ntorch.return_types.sort(\n    values=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n    indices=tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16,  1,  3,  5,  7,  9, 11, 13, 15, 17]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort"
    },
    {
        "X": "How to use torch.float_power, give an example?",
        "Y": ">>> a = torch.randint(10, (4,))\n>>> a\ntensor([6, 4, 7, 1])\n>>> torch.float_power(a, 2)\ntensor([36., 16., 49.,  1.], dtype=torch.float64)\n\n>>> a = torch.arange(1, 5)\n>>> a\ntensor([ 1,  2,  3,  4])\n>>> exp = torch.tensor([2, -3, 4, -5])\n>>> exp\ntensor([ 2, -3,  4, -5])\n>>> torch.float_power(a, exp)\ntensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64)",
        "Z": ">>> a = torch.randint(10, (4,))\n>>> a\ntensor([6, 4, 7, 1])\n>>> torch.float_power(a, 2)\ntensor([36., 16., 49.,  1.], dtype=torch.float64)\n\n>>> a = torch.arange(1, 5)\n>>> a\ntensor([ 1,  2,  3,  4])\n>>> exp = torch.tensor([2, -3, 4, -5])\n>>> exp\ntensor([ 2, -3,  4, -5])\n>>> torch.float_power(a, exp)\ntensor([1.0000e+00, 1.2500e-01, 8.1000e+01, 9.7656e-04], dtype=torch.float64)",
        "source": "https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power"
    },
    {
        "X": "How to use torch.diag_embed, give an example?",
        "Y": ">>> a = torch.randn(2, 3)\n>>> torch.diag_embed(a)\ntensor([[[ 1.5410,  0.0000,  0.0000],\n         [ 0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -2.1788]],\n\n        [[ 0.5684,  0.0000,  0.0000],\n         [ 0.0000, -1.0845,  0.0000],\n         [ 0.0000,  0.0000, -1.3986]]])\n\n>>> torch.diag_embed(a, offset=1, dim1=0, dim2=2)\ntensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],\n         [ 0.0000,  0.5684,  0.0000,  0.0000]],\n\n        [[ 0.0000,  0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -1.0845,  0.0000]],\n\n        [[ 0.0000,  0.0000,  0.0000, -2.1788],\n         [ 0.0000,  0.0000,  0.0000, -1.3986]],\n\n        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])",
        "Z": "Applying torch.diagonal() to the output of this function with\nthe same arguments yields a matrix identical to input. However,\ntorch.diagonal() has different default dimensions, so those\nneed to be explicitly specified. >>> a = torch.randn(2, 3)\n>>> torch.diag_embed(a)\ntensor([[[ 1.5410,  0.0000,  0.0000],\n         [ 0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -2.1788]],\n\n        [[ 0.5684,  0.0000,  0.0000],\n         [ 0.0000, -1.0845,  0.0000],\n         [ 0.0000,  0.0000, -1.3986]]])\n\n>>> torch.diag_embed(a, offset=1, dim1=0, dim2=2)\ntensor([[[ 0.0000,  1.5410,  0.0000,  0.0000],\n         [ 0.0000,  0.5684,  0.0000,  0.0000]],\n\n        [[ 0.0000,  0.0000, -0.2934,  0.0000],\n         [ 0.0000,  0.0000, -1.0845,  0.0000]],\n\n        [[ 0.0000,  0.0000,  0.0000, -2.1788],\n         [ 0.0000,  0.0000,  0.0000, -1.3986]],\n\n        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed"
    },
    {
        "X": "How to use torch.frac, give an example?",
        "Y": ">>> torch.frac(torch.tensor([1, 2.5, -3.2]))\ntensor([ 0.0000,  0.5000, -0.2000])",
        "Z": ">>> torch.frac(torch.tensor([1, 2.5, -3.2]))\ntensor([ 0.0000,  0.5000, -0.2000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac"
    },
    {
        "X": "How to use torch.linalg.householder_product, give an example?",
        "Y": ">>> a = torch.randn(2, 2)\n>>> h, tau = torch.geqrf(a)\n>>> q = torch.linalg.householder_product(h, tau)\n>>> torch.allclose(q, torch.linalg.qr(a)[0])\nTrue\n\n>>> h = torch.randn(3, 2, 2, dtype=torch.complex128)\n>>> tau = torch.randn(3, 1, dtype=torch.complex128)\n>>> q = torch.linalg.householder_product(h, tau)\n>>> q\ntensor([[[ 1.8034+0.4184j,  0.2588-1.0174j],\n        [-0.6853+0.7953j,  2.0790+0.5620j]],\n\n        [[ 1.4581+1.6989j, -1.5360+0.1193j],\n        [ 1.3877-0.6691j,  1.3512+1.3024j]],\n\n        [[ 1.4766+0.5783j,  0.0361+0.6587j],\n        [ 0.6396+0.1612j,  1.3693+0.4481j]]], dtype=torch.complex128)",
        "Z": "Supports inputs of float, double, cfloat and cdouble dtypes.\nAlso supports batches of matrices, and if the inputs are batches of matrices then\nthe output has the same batch dimensions. >>> a = torch.randn(2, 2)\n>>> h, tau = torch.geqrf(a)\n>>> q = torch.linalg.householder_product(h, tau)\n>>> torch.allclose(q, torch.linalg.qr(a)[0])\nTrue\n\n>>> h = torch.randn(3, 2, 2, dtype=torch.complex128)\n>>> tau = torch.randn(3, 1, dtype=torch.complex128)\n>>> q = torch.linalg.householder_product(h, tau)\n>>> q\ntensor([[[ 1.8034+0.4184j,  0.2588-1.0174j],\n        [-0.6853+0.7953j,  2.0790+0.5620j]],\n\n        [[ 1.4581+1.6989j, -1.5360+0.1193j],\n        [ 1.3877-0.6691j,  1.3512+1.3024j]],\n\n        [[ 1.4766+0.5783j,  0.0361+0.6587j],\n        [ 0.6396+0.1612j,  1.3693+0.4481j]]], dtype=torch.complex128)",
        "source": "https://pytorch.org/docs/stable/generated/torch.linalg.householder_product.html#torch.linalg.householder_product"
    },
    {
        "X": "How to use torch.tril, give an example?",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0813, -0.8619,  0.7105],\n        [ 0.0935,  0.1380,  2.2112],\n        [-0.3409, -0.9828,  0.0289]])\n>>> torch.tril(a)\ntensor([[-1.0813,  0.0000,  0.0000],\n        [ 0.0935,  0.1380,  0.0000],\n        [-0.3409, -0.9828,  0.0289]])\n\n>>> b = torch.randn(4, 6)\n>>> b\ntensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],\n        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])\n>>> torch.tril(b, diagonal=1)\ntensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])\n>>> torch.tril(b, diagonal=-1)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])",
        "Z": "The argument diagonal controls which diagonal to consider. If\ndiagonal = 0, all elements on and below the main diagonal are\nretained. A positive value includes just as many diagonals above the main\ndiagonal, and similarly a negative value excludes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121] where\nd1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. >>> a = torch.randn(3, 3)\n>>> a\ntensor([[-1.0813, -0.8619,  0.7105],\n        [ 0.0935,  0.1380,  2.2112],\n        [-0.3409, -0.9828,  0.0289]])\n>>> torch.tril(a)\ntensor([[-1.0813,  0.0000,  0.0000],\n        [ 0.0935,  0.1380,  0.0000],\n        [-0.3409, -0.9828,  0.0289]])\n\n>>> b = torch.randn(4, 6)\n>>> b\ntensor([[ 1.2219,  0.5653, -0.2521, -0.2345,  1.2544,  0.3461],\n        [ 0.4785, -0.4477,  0.6049,  0.6368,  0.8775,  0.7145],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.3615,  0.6864],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0978]])\n>>> torch.tril(b, diagonal=1)\ntensor([[ 1.2219,  0.5653,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785, -0.4477,  0.6049,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716, -1.1243, -0.5413,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164, -0.7648, -1.4024,  0.0000]])\n>>> torch.tril(b, diagonal=-1)\ntensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 0.4785,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n        [ 1.1502,  3.2716,  0.0000,  0.0000,  0.0000,  0.0000],\n        [-0.0614, -0.7344, -1.3164,  0.0000,  0.0000,  0.0000]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril"
    },
    {
        "X": "How to use torch.i0, give an example?",
        "Y": ">>> torch.i0(torch.arange(5, dtype=torch.float32))\ntensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])",
        "Z": ">>> torch.i0(torch.arange(5, dtype=torch.float32))\ntensor([ 1.0000,  1.2661,  2.2796,  4.8808, 11.3019])",
        "source": "https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0"
    },
    {
        "X": "How to use torch.hstack, give an example?",
        "Y": ">>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.hstack((a,b))\ntensor([1, 2, 3, 4, 5, 6])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.hstack((a,b))\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])",
        "Z": "This is equivalent to concatenation along the first axis for 1-D tensors, and along the second axis for all other tensors. >>> a = torch.tensor([1, 2, 3])\n>>> b = torch.tensor([4, 5, 6])\n>>> torch.hstack((a,b))\ntensor([1, 2, 3, 4, 5, 6])\n>>> a = torch.tensor([[1],[2],[3]])\n>>> b = torch.tensor([[4],[5],[6]])\n>>> torch.hstack((a,b))\ntensor([[1, 4],\n        [2, 5],\n        [3, 6]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.hstack.html#torch.hstack"
    },
    {
        "X": "How to use torch.pow, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.4331,  1.2475,  0.6834, -0.2791])\n>>> torch.pow(a, 2)\ntensor([ 0.1875,  1.5561,  0.4670,  0.0779])\n>>> exp = torch.arange(1., 5.)\n\n>>> a = torch.arange(1., 5.)\n>>> a\ntensor([ 1.,  2.,  3.,  4.])\n>>> exp\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.pow(a, exp)\ntensor([   1.,    4.,   27.,  256.])",
        "Z": "When exponent is a tensor, the shapes of input\nand exponent must be broadcastable. >>> a = torch.randn(4)\n>>> a\ntensor([ 0.4331,  1.2475,  0.6834, -0.2791])\n>>> torch.pow(a, 2)\ntensor([ 0.1875,  1.5561,  0.4670,  0.0779])\n>>> exp = torch.arange(1., 5.)\n\n>>> a = torch.arange(1., 5.)\n>>> a\ntensor([ 1.,  2.,  3.,  4.])\n>>> exp\ntensor([ 1.,  2.,  3.,  4.])\n>>> torch.pow(a, exp)\ntensor([   1.,    4.,   27.,  256.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow"
    },
    {
        "X": "How  The operation applied is:, give an example?",
        "Y": ">>> exp = torch.arange(1., 5.)\n>>> base = 2\n>>> torch.pow(base, exp)\ntensor([  2.,   4.,   8.,  16.])",
        "Z": "The operation applied is: >>> exp = torch.arange(1., 5.)\n>>> base = 2\n>>> torch.pow(base, exp)\ntensor([  2.,   4.,   8.,  16.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow"
    },
    {
        "X": "How to use torch.floor, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\n>>> torch.floor(a)\ntensor([-1.,  1., -1., -1.])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.8166,  1.5308, -0.2530, -0.2091])\n>>> torch.floor(a)\ntensor([-1.,  1., -1., -1.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor"
    },
    {
        "X": "How to use inference mode, give an example?",
        "Y": ">>> import torch\n>>> x = torch.ones(1, 2, 3, requires_grad=True)\n>>> with torch.inference_mode():\n...   y = x * x\n>>> y.requires_grad\nFalse\n>>> y._version\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: Inference tensors do not track version counter.\n>>> @torch.inference_mode()\n... def func(x):\n...   return x * x\n>>> out = func(x)\n>>> out.requires_grad\nFalse",
        "Z": ">>> import torch\n>>> x = torch.ones(1, 2, 3, requires_grad=True)\n>>> with torch.inference_mode():\n...   y = x * x\n>>> y.requires_grad\nFalse\n>>> y._version\nTraceback (most recent call last):\nFile \"<stdin>\", line 1, in <module>\nRuntimeError: Inference tensors do not track version counter.\n>>> @torch.inference_mode()\n... def func(x):\n...   return x * x\n>>> out = func(x)\n>>> out.requires_grad\nFalse",
        "source": "https://pytorch.org/docs/stable/generated/torch.inference_mode.html#torch.inference_mode"
    },
    {
        "X": "How to use torch.as_tensor, give an example?",
        "Y": ">>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n\n>>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a, device=torch.device('cuda'))\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([1,  2,  3])",
        "Z": ">>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a)\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([-1,  2,  3])\n\n>>> a = numpy.array([1, 2, 3])\n>>> t = torch.as_tensor(a, device=torch.device('cuda'))\n>>> t\ntensor([ 1,  2,  3])\n>>> t[0] = -1\n>>> a\narray([1,  2,  3])",
        "source": "https://pytorch.org/docs/stable/generated/torch.as_tensor.html#torch.as_tensor"
    },
    {
        "X": "How to use torch.trapz, give an example?",
        "Y": ">>> y = torch.randn((2, 3))\n>>> y\ntensor([[-2.1156,  0.6857, -0.2700],\n        [-1.2145,  0.5540,  2.0431]])\n>>> x = torch.tensor([[1, 3, 4], [1, 2, 3]])\n>>> torch.trapz(y, x)\ntensor([-1.2220,  0.9683])",
        "Z": ">>> y = torch.randn((2, 3))\n>>> y\ntensor([[-2.1156,  0.6857, -0.2700],\n        [-1.2145,  0.5540,  2.0431]])\n>>> x = torch.tensor([[1, 3, 4], [1, 2, 3]])\n>>> torch.trapz(y, x)\ntensor([-1.2220,  0.9683])",
        "source": "https://pytorch.org/docs/stable/generated/torch.trapz.html#torch.trapz"
    },
    {
        "X": "How to use torch.mul, give an example?",
        "Y": ">>> a = torch.randn(3)\n>>> a\ntensor([ 0.2015, -0.4255,  2.6087])\n>>> torch.mul(a, 100)\ntensor([  20.1494,  -42.5491,  260.8663])",
        "Z": "If input is of type FloatTensor or DoubleTensor, other\nshould be a real number, otherwise it should be an integer >>> a = torch.randn(3)\n>>> a\ntensor([ 0.2015, -0.4255,  2.6087])\n>>> torch.mul(a, 100)\ntensor([  20.1494,  -42.5491,  260.8663])",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    {
        "X": "How  The shapes of input and other must be\nbroadcastable., give an example?",
        "Y": ">>> a = torch.randn(4, 1)\n>>> a\ntensor([[ 1.1207],\n        [-0.3137],\n        [ 0.0700],\n        [ 0.8378]])\n>>> b = torch.randn(1, 4)\n>>> b\ntensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])\n>>> torch.mul(a, b)\ntensor([[ 0.5767,  0.1363, -0.5877,  2.5083],\n        [-0.1614, -0.0382,  0.1645, -0.7021],\n        [ 0.0360,  0.0085, -0.0367,  0.1567],\n        [ 0.4312,  0.1019, -0.4394,  1.8753]])",
        "Z": "The shapes of input and other must be\nbroadcastable. >>> a = torch.randn(4, 1)\n>>> a\ntensor([[ 1.1207],\n        [-0.3137],\n        [ 0.0700],\n        [ 0.8378]])\n>>> b = torch.randn(1, 4)\n>>> b\ntensor([[ 0.5146,  0.1216, -0.5244,  2.2382]])\n>>> torch.mul(a, b)\ntensor([[ 0.5767,  0.1363, -0.5877,  2.5083],\n        [-0.1614, -0.0382,  0.1645, -0.7021],\n        [ 0.0360,  0.0085, -0.0367,  0.1567],\n        [ 0.4312,  0.1019, -0.4394,  1.8753]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"
    },
    {
        "X": "How to use torch.logsumexp, give an example?",
        "Y": ">>> a = torch.randn(3, 3)\n>>> torch.logsumexp(a, 1)\ntensor([ 0.8442,  1.4322,  0.8711])",
        "Z": "If keepdim is True, the output tensor is of the same size\nas input except in the dimension(s) dim where it is of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in the\noutput tensor having 1 (or len(dim)) fewer dimension(s). >>> a = torch.randn(3, 3)\n>>> torch.logsumexp(a, 1)\ntensor([ 0.8442,  1.4322,  0.8711])",
        "source": "https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"
    },
    {
        "X": "How to use torch.max, give an example?",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6763,  0.7445, -2.2369]])\n>>> torch.max(a)\ntensor(0.7445)",
        "Z": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6763,  0.7445, -2.2369]])\n>>> torch.max(a)\ntensor(0.7445)",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "How  If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting\nin the output tensors having 1 fewer dimension than input., give an example?",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n        [ 1.1949, -1.1127, -2.2379, -0.6702],\n        [ 1.5717, -0.9207,  0.1297, -1.8768],\n        [-0.6172,  1.0036, -0.6060, -0.2432]])\n>>> torch.max(a, 1)\ntorch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))",
        "Z": "If keepdim is True, the output tensors are of the same size\nas input except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting\nin the output tensors having 1 fewer dimension than input. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[-1.2360, -0.2942, -0.1222,  0.8475],\n        [ 1.1949, -1.1127, -2.2379, -0.6702],\n        [ 1.5717, -0.9207,  0.1297, -1.8768],\n        [-0.6172,  1.0036, -0.6060, -0.2432]])\n>>> torch.max(a, 1)\ntorch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"
    },
    {
        "X": "How to use torch.utils.model_zoo.load_url, give an example?",
        "Y": ">>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')",
        "Z": "If the object is already present in model_dir, it\u2019s deserialized and\nreturned.\nThe default value of model_dir is <hub_dir>/checkpoints where\nhub_dir is the directory returned by get_dir(). >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')",
        "source": "https://pytorch.org/docs/stable/model_zoo.html"
    },
    {
        "X": "How to use torch.mean, give an example?",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.2294, -0.5481,  1.3288]])\n>>> torch.mean(a)\ntensor(0.3367)",
        "Z": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.2294, -0.5481,  1.3288]])\n>>> torch.mean(a)\ntensor(0.3367)",
        "source": "https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"
    },
    {
        "X": "How to use torch.quantile, give an example?",
        "Y": ">>> a = torch.randn(2, 3)\n>>> a\ntensor([[ 0.0795, -1.2117,  0.9765],\n        [ 1.1707,  0.6706,  0.4884]])\n>>> q = torch.tensor([0.25, 0.5, 0.75])\n>>> torch.quantile(a, q, dim=1, keepdim=True)\ntensor([[[-0.5661],\n        [ 0.5795]],\n\n        [[ 0.0795],\n        [ 0.6706]],\n\n        [[ 0.5280],\n        [ 0.9206]]])\n>>> torch.quantile(a, q, dim=1, keepdim=True).shape\ntorch.Size([3, 2, 1])\n>>> a = torch.arange(4.)\n>>> a\ntensor([0., 1., 2., 3.])",
        "Z": "If q is a 1D tensor, the first dimension of the output represents the quantiles and has size\nequal to the size of q, the remaining dimensions are what remains from the reduction. >>> a = torch.randn(2, 3)\n>>> a\ntensor([[ 0.0795, -1.2117,  0.9765],\n        [ 1.1707,  0.6706,  0.4884]])\n>>> q = torch.tensor([0.25, 0.5, 0.75])\n>>> torch.quantile(a, q, dim=1, keepdim=True)\ntensor([[[-0.5661],\n        [ 0.5795]],\n\n        [[ 0.0795],\n        [ 0.6706]],\n\n        [[ 0.5280],\n        [ 0.9206]]])\n>>> torch.quantile(a, q, dim=1, keepdim=True).shape\ntorch.Size([3, 2, 1])\n>>> a = torch.arange(4.)\n>>> a\ntensor([0., 1., 2., 3.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"
    },
    {
        "X": "How to use torch.vdot, give an example?",
        "Y": ">>> torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n>>> a = torch.tensor((1 +2j, 3 - 1j))\n>>> b = torch.tensor((2 +1j, 4 - 0j))\n>>> torch.vdot(a, b)\ntensor([16.+1.j])\n>>> torch.vdot(b, a)\ntensor([16.-1.j])",
        "Z": ">>> torch.vdot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)\n>>> a = torch.tensor((1 +2j, 3 - 1j))\n>>> b = torch.tensor((2 +1j, 4 - 0j))\n>>> torch.vdot(a, b)\ntensor([16.+1.j])\n>>> torch.vdot(b, a)\ntensor([16.-1.j])",
        "source": "https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"
    },
    {
        "X": "How to use torch.asinh, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.1606, -1.4267, -1.0899, -1.0250 ])\n>>> torch.asinh(a)\ntensor([ 0.1599, -1.1534, -0.9435, -0.8990 ])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.1606, -1.4267, -1.0899, -1.0250 ])\n>>> torch.asinh(a)\ntensor([ 0.1599, -1.1534, -0.9435, -0.8990 ])",
        "source": "https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh"
    },
    {
        "X": "How to use torch.lstsq, give an example?",
        "Y": "X = torch.linalg.lstsq(A, B).solution",
        "Z": "torch.lstsq() is deprecated in favor of torch.linalg.lstsq()\nand will be removed in a future PyTorch release. torch.linalg.lstsq()\nhas reversed arguments and does not return the QR decomposition in the returned tuple,\n(it returns other information about the problem).\nThe returned solution in torch.lstsq() stores the residuals of the solution in the\nlast m - n columns in the case m > n. In torch.linalg.lstsq(), the residuals\nare in the field \u2018residuals\u2019 of the returned named tuple.Unpacking the solution as``X = torch.lstsq(B, A).solution[:A.size(1)]`` should be replaced with X = torch.linalg.lstsq(A, B).solution",
        "source": "https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq"
    },
    {
        "X": "How  Returned tensor XXX has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k)(max(m,n)\u00d7k). The first nnn\nrows of XXX contains the solution. If m\u2265nm \\geq nm\u2265n, the residual sum of squares\nfor the solution in each column is given by the sum of squares of elements in the\nremaining m\u2212nm - nm\u2212n rows of that column., give an example?",
        "Y": ">>> A = torch.tensor([[1., 1, 1],\n...                   [2, 3, 4],\n...                   [3, 5, 2],\n...                   [4, 2, 5],\n...                   [5, 4, 3]])\n>>> B = torch.tensor([[-10., -3],\n...                   [ 12, 14],\n...                   [ 14, 12],\n...                   [ 16, 16],\n...                   [ 18, 16]])\n>>> X, _ = torch.lstsq(B, A)\n>>> X\ntensor([[  2.0000,   1.0000],\n        [  1.0000,   1.0000],\n        [  1.0000,   2.0000],\n        [ 10.9635,   4.8501],\n        [  8.9332,   5.2418]])",
        "Z": "Returned tensor XXX has shape (max\u2061(m,n)\u00d7k)(\\max(m, n) \\times k)(max(m,n)\u00d7k). The first nnn\nrows of XXX contains the solution. If m\u2265nm \\geq nm\u2265n, the residual sum of squares\nfor the solution in each column is given by the sum of squares of elements in the\nremaining m\u2212nm - nm\u2212n rows of that column. >>> A = torch.tensor([[1., 1, 1],\n...                   [2, 3, 4],\n...                   [3, 5, 2],\n...                   [4, 2, 5],\n...                   [5, 4, 3]])\n>>> B = torch.tensor([[-10., -3],\n...                   [ 12, 14],\n...                   [ 14, 12],\n...                   [ 16, 16],\n...                   [ 18, 16]])\n>>> X, _ = torch.lstsq(B, A)\n>>> X\ntensor([[  2.0000,   1.0000],\n        [  1.0000,   1.0000],\n        [  1.0000,   2.0000],\n        [ 10.9635,   4.8501],\n        [  8.9332,   5.2418]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.lstsq.html#torch.lstsq"
    },
    {
        "X": "How to use torch.view_as_real, give an example?",
        "Y": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.4737-0.3839j), (-0.2098-0.6699j), (0.3470-0.9451j), (-0.5174-1.3136j)])\n>>> torch.view_as_real(x)\ntensor([[ 0.4737, -0.3839],\n        [-0.2098, -0.6699],\n        [ 0.3470, -0.9451],\n        [-0.5174, -1.3136]])",
        "Z": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.4737-0.3839j), (-0.2098-0.6699j), (0.3470-0.9451j), (-0.5174-1.3136j)])\n>>> torch.view_as_real(x)\ntensor([[ 0.4737, -0.3839],\n        [-0.2098, -0.6699],\n        [ 0.3470, -0.9451],\n        [-0.5174, -1.3136]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.view_as_real.html#torch.view_as_real"
    },
    {
        "X": "How to use torch.isfinite, give an example?",
        "Y": ">>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([True,  False,  True,  False,  False])",
        "Z": "Real values are finite when they are not NaN, negative infinity, or infinity.\nComplex values are finite when both their real and imaginary parts are finite. >>> torch.isfinite(torch.tensor([1, float('inf'), 2, float('-inf'), float('nan')]))\ntensor([True,  False,  True,  False,  False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"
    },
    {
        "X": "How to use torch.flipud, give an example?",
        "Y": ">>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.flipud(x)\ntensor([[2, 3],\n        [0, 1]])",
        "Z": "Flip the entries in each column in the up/down direction.\nRows are preserved, but appear in a different order than before. >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.flipud(x)\ntensor([[2, 3],\n        [0, 1]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"
    },
    {
        "X": "How to use torch.full, give an example?",
        "Y": ">>> torch.full((2, 3), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416]])",
        "Z": ">>> torch.full((2, 3), 3.141592)\ntensor([[ 3.1416,  3.1416,  3.1416],\n        [ 3.1416,  3.1416,  3.1416]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.full.html#torch.full"
    },
    {
        "X": "How to use torch.Generator, give an example?",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cuda = torch.Generator(device='cuda')",
        "Z": ">>> g_cpu = torch.Generator()\n>>> g_cuda = torch.Generator(device='cuda')",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "How  Gets the current device of the generator., give an example?",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu.device\ndevice(type='cpu')",
        "Z": "Gets the current device of the generator. >>> g_cpu = torch.Generator()\n>>> g_cpu.device\ndevice(type='cpu')",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "How to use torch.Generator.get_state, give an example?",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu.get_state()",
        "Z": ">>> g_cpu = torch.Generator()\n>>> g_cpu.get_state()",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "How to use torch.Generator.initial_seed, give an example?",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu.initial_seed()\n2147483647",
        "Z": ">>> g_cpu = torch.Generator()\n>>> g_cpu.initial_seed()\n2147483647",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "How to use torch.Generator.manual_seed, give an example?",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu.manual_seed(2147483647)",
        "Z": ">>> g_cpu = torch.Generator()\n>>> g_cpu.manual_seed(2147483647)",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "How to use torch.Generator.seed, give an example?",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu.seed()\n1516516984916",
        "Z": ">>> g_cpu = torch.Generator()\n>>> g_cpu.seed()\n1516516984916",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "How to use torch.Generator.set_state, give an example?",
        "Y": ">>> g_cpu = torch.Generator()\n>>> g_cpu_other = torch.Generator()\n>>> g_cpu.set_state(g_cpu_other.get_state())",
        "Z": ">>> g_cpu = torch.Generator()\n>>> g_cpu_other = torch.Generator()\n>>> g_cpu.set_state(g_cpu_other.get_state())",
        "source": "https://pytorch.org/docs/stable/generated/torch.Generator.html#torch.Generator"
    },
    {
        "X": "How to use torch.deg2rad, give an example?",
        "Y": ">>> a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\n>>> torch.deg2rad(a)\ntensor([[ 3.1416, -3.1416],\n        [ 6.2832, -6.2832],\n        [ 1.5708, -1.5708]])",
        "Z": ">>> a = torch.tensor([[180.0, -180.0], [360.0, -360.0], [90.0, -90.0]])\n>>> torch.deg2rad(a)\ntensor([[ 3.1416, -3.1416],\n        [ 6.2832, -6.2832],\n        [ 1.5708, -1.5708]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad"
    },
    {
        "X": "How to use torch.var_mean, give an example?",
        "Y": ">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.var_mean(a, unbiased=False)\n(tensor(0.1754), tensor(-0.8509))",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. >>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.var_mean(a, unbiased=False)\n(tensor(0.1754), tensor(-0.8509))",
        "source": "https://pytorch.org/docs/stable/generated/torch.var_mean.html#torch.var_mean"
    },
    {
        "X": "How to use torch.complex, give an example?",
        "Y": ">>> real = torch.tensor([1, 2], dtype=torch.float32)\n>>> imag = torch.tensor([3, 4], dtype=torch.float32)\n>>> z = torch.complex(real, imag)\n>>> z\ntensor([(1.+3.j), (2.+4.j)])\n>>> z.dtype\ntorch.complex64",
        "Z": ">>> real = torch.tensor([1, 2], dtype=torch.float32)\n>>> imag = torch.tensor([3, 4], dtype=torch.float32)\n>>> z = torch.complex(real, imag)\n>>> z\ntensor([(1.+3.j), (2.+4.j)])\n>>> z.dtype\ntorch.complex64",
        "source": "https://pytorch.org/docs/stable/generated/torch.complex.html#torch.complex"
    },
    {
        "X": "How to use torch.triangular_solve, give an example?",
        "Y": ">>> A = torch.randn(2, 2).triu()\n>>> A\ntensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]])\n>>> b = torch.randn(2, 3)\n>>> b\ntensor([[-0.0210,  2.3513, -1.5492],\n        [ 1.5429,  0.7403, -1.0243]])\n>>> torch.triangular_solve(b, A)\ntorch.return_types.triangular_solve(\nsolution=tensor([[ 1.7841,  2.9046, -2.5405],\n        [ 1.9320,  0.9270, -1.2826]]),\ncloned_coefficient=tensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]]))",
        "Z": "Supports input of float, double, cfloat and cdouble data types. >>> A = torch.randn(2, 2).triu()\n>>> A\ntensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]])\n>>> b = torch.randn(2, 3)\n>>> b\ntensor([[-0.0210,  2.3513, -1.5492],\n        [ 1.5429,  0.7403, -1.0243]])\n>>> torch.triangular_solve(b, A)\ntorch.return_types.triangular_solve(\nsolution=tensor([[ 1.7841,  2.9046, -2.5405],\n        [ 1.9320,  0.9270, -1.2826]]),\ncloned_coefficient=tensor([[ 1.1527, -1.0753],\n        [ 0.0000,  0.7986]]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"
    },
    {
        "X": "How to use torch.ones, give an example?",
        "Y": ">>> torch.ones(2, 3)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n\n>>> torch.ones(5)\ntensor([ 1.,  1.,  1.,  1.,  1.])",
        "Z": ">>> torch.ones(2, 3)\ntensor([[ 1.,  1.,  1.],\n        [ 1.,  1.,  1.]])\n\n>>> torch.ones(5)\ntensor([ 1.,  1.,  1.,  1.,  1.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones"
    },
    {
        "X": "How to use torch.t, give an example?",
        "Y": ">>> x = torch.randn(())\n>>> x\ntensor(0.1995)\n>>> torch.t(x)\ntensor(0.1995)\n>>> x = torch.randn(3)\n>>> x\ntensor([ 2.4320, -0.4608,  0.7702])\n>>> torch.t(x)\ntensor([ 2.4320, -0.4608,  0.7702])\n>>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.4875,  0.9158, -0.5872],\n        [ 0.3938, -0.6929,  0.6932]])\n>>> torch.t(x)\ntensor([[ 0.4875,  0.3938],\n        [ 0.9158, -0.6929],\n        [-0.5872,  0.6932]])",
        "Z": "0-D and 1-D tensors are returned as is. When input is a 2-D tensor this\nis equivalent to transpose(input, 0, 1). >>> x = torch.randn(())\n>>> x\ntensor(0.1995)\n>>> torch.t(x)\ntensor(0.1995)\n>>> x = torch.randn(3)\n>>> x\ntensor([ 2.4320, -0.4608,  0.7702])\n>>> torch.t(x)\ntensor([ 2.4320, -0.4608,  0.7702])\n>>> x = torch.randn(2, 3)\n>>> x\ntensor([[ 0.4875,  0.9158, -0.5872],\n        [ 0.3938, -0.6929,  0.6932]])\n>>> torch.t(x)\ntensor([[ 0.4875,  0.3938],\n        [ 0.9158, -0.6929],\n        [-0.5872,  0.6932]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.t.html#torch.t"
    },
    {
        "X": "How to use torch.gt, give an example?",
        "Y": ">>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [False, False]])",
        "Z": "The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.gt(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [False, False]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"
    },
    {
        "X": "How to use torch.round, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.9920,  0.6077,  0.9734, -1.0362])\n>>> torch.round(a)\ntensor([ 1.,  1.,  1., -1.])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.9920,  0.6077,  0.9734, -1.0362])\n>>> torch.round(a)\ntensor([ 1.,  1.,  1., -1.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.round.html#torch.round"
    },
    {
        "X": "How to use torch.pca_lowrank, give an example?",
        "Y": "- Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n  structure with randomness: probabilistic algorithms for\n  constructing approximate matrix decompositions,\n  arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\n  `arXiv <http://arxiv.org/abs/0909.4061>`_).",
        "Z": "This function returns a namedtuple (U, S, V) which is the\nnearly optimal approximation of a singular value decomposition of\na centered matrix AAA such that A=Udiag(S)VTA = U diag(S) V^TA=Udiag(S)VT.References: - Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\n  structure with randomness: probabilistic algorithms for\n  constructing approximate matrix decompositions,\n  arXiv:0909.4061 [math.NA; math.PR], 2009 (available at\n  `arXiv <http://arxiv.org/abs/0909.4061>`_).",
        "source": "https://pytorch.org/docs/stable/generated/torch.pca_lowrank.html#torch.pca_lowrank"
    },
    {
        "X": "How to use torch.narrow, give an example?",
        "Y": ">>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> torch.narrow(x, 0, 0, 2)\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n>>> torch.narrow(x, 1, 1, 2)\ntensor([[ 2,  3],\n        [ 5,  6],\n        [ 8,  9]])",
        "Z": ">>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n>>> torch.narrow(x, 0, 0, 2)\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])\n>>> torch.narrow(x, 1, 1, 2)\ntensor([[ 2,  3],\n        [ 5,  6],\n        [ 8,  9]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"
    },
    {
        "X": "How to use torch.min, give an example?",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6750,  1.0857,  1.7197]])\n>>> torch.min(a)\ntensor(0.6750)",
        "Z": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.6750,  1.0857,  1.7197]])\n>>> torch.min(a)\ntensor(0.6750)",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "How  If keepdim is True, the output tensors are of the same size as\ninput except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension than input., give an example?",
        "Y": ">>> a = torch.randn(4, 4)\n>>> a\ntensor([[-0.6248,  1.1334, -1.1899, -0.2803],\n        [-1.4644, -0.2635, -0.3651,  0.6134],\n        [ 0.2457,  0.0384,  1.0128,  0.7015],\n        [-0.1153,  2.9849,  2.1458,  0.5788]])\n>>> torch.min(a, 1)\ntorch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))",
        "Z": "If keepdim is True, the output tensors are of the same size as\ninput except in the dimension dim where they are of size 1.\nOtherwise, dim is squeezed (see torch.squeeze()), resulting in\nthe output tensors having 1 fewer dimension than input. >>> a = torch.randn(4, 4)\n>>> a\ntensor([[-0.6248,  1.1334, -1.1899, -0.2803],\n        [-1.4644, -0.2635, -0.3651,  0.6134],\n        [ 0.2457,  0.0384,  1.0128,  0.7015],\n        [-0.1153,  2.9849,  2.1458,  0.5788]])\n>>> torch.min(a, 1)\ntorch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"
    },
    {
        "X": "How to use torch.multinomial, give an example?",
        "Y": ">>> weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights\n>>> torch.multinomial(weights, 2)\ntensor([1, 2])\n>>> torch.multinomial(weights, 4) # ERROR!\nRuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,\nnot enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320\n>>> torch.multinomial(weights, 4, replacement=True)\ntensor([ 2,  1,  1,  1])",
        "Z": "If not, they are drawn without replacement, which means that when a\nsample index is drawn for a row, it cannot be drawn again for that row. >>> weights = torch.tensor([0, 10, 3, 0], dtype=torch.float) # create a tensor of weights\n>>> torch.multinomial(weights, 2)\ntensor([1, 2])\n>>> torch.multinomial(weights, 4) # ERROR!\nRuntimeError: invalid argument 2: invalid multinomial distribution (with replacement=False,\nnot enough non-negative category to sample) at ../aten/src/TH/generic/THTensorRandom.cpp:320\n>>> torch.multinomial(weights, 4, replacement=True)\ntensor([ 2,  1,  1,  1])",
        "source": "https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"
    },
    {
        "X": "How to use To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019s Linear module.\nThis module applies an affine transformation to its input., give an example?",
        "Y": "import torch\nfrom torch import nn\n\nclass MyLinear(nn.Module):\n  def __init__(self, in_features, out_features):\n    super().__init__()\n    self.weight = nn.Parameter(torch.randn(in_features, out_features))\n    self.bias = nn.Parameter(torch.randn(out_features))\n\n  def forward(self, input):\n    return (input @ self.weight) + self.bias",
        "Z": "To get started, let\u2019s look at a simpler, custom version of PyTorch\u2019s Linear module.\nThis module applies an affine transformation to its input. import torch\nfrom torch import nn\n\nclass MyLinear(nn.Module):\n  def __init__(self, in_features, out_features):\n    super().__init__()\n    self.weight = nn.Parameter(torch.randn(in_features, out_features))\n    self.bias = nn.Parameter(torch.randn(out_features))\n\n  def forward(self, input):\n    return (input @ self.weight) + self.bias",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use This simple module has the following fundamental characteristics of modules:This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called:, give an example?",
        "Y": "m = MyLinear(4, 3)\nsample_input = torch.randn(4)\nm(sample_input)\n: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)",
        "Z": "This simple module has the following fundamental characteristics of modules:This simple module demonstrates how modules package state and computation together. Instances of this module can be\nconstructed and called: m = MyLinear(4, 3)\nsample_input = torch.randn(4)\nm(sample_input)\n: tensor([-0.3037, -1.0413, -4.2057], grad_fn=<AddBackward0>)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use Note that the module itself is callable, and that calling it invokes its forward() function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement a backward() function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail in\nNeural Network Training with Modules.The full set of parameters registered by the module can be iterated through via a call to\nparameters() or named_parameters(),\nwhere the latter includes each parameter\u2019s name:, give an example?",
        "Y": "for parameter in m.named_parameters():\n  print(parameter)\n: ('weight', Parameter containing:\ntensor([[ 1.0597,  1.1796,  0.8247],\n        [-0.5080, -1.2635, -1.1045],\n        [ 0.0593,  0.2469, -1.4299],\n        [-0.4926, -0.5457,  0.4793]], requires_grad=True))\n('bias', Parameter containing:\ntensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))",
        "Z": "Note that the module itself is callable, and that calling it invokes its forward() function.\nThis name is in reference to the concepts of \u201cforward pass\u201d and \u201cbackward pass\u201d, which apply to each module.\nThe \u201cforward pass\u201d is responsible for applying the computation represented by the module\nto the given input(s) (as shown in the above snippet). The \u201cbackward pass\u201d computes gradients of\nmodule outputs with respect to its inputs, which can be used for \u201ctraining\u201d parameters through gradient\ndescent methods. PyTorch\u2019s autograd system automatically takes care of this backward pass computation, so it\nis not required to manually implement a backward() function for each module. The process of training\nmodule parameters through successive forward / backward passes is covered in detail in\nNeural Network Training with Modules.The full set of parameters registered by the module can be iterated through via a call to\nparameters() or named_parameters(),\nwhere the latter includes each parameter\u2019s name: for parameter in m.named_parameters():\n  print(parameter)\n: ('weight', Parameter containing:\ntensor([[ 1.0597,  1.1796,  0.8247],\n        [-0.5080, -1.2635, -1.1045],\n        [ 0.0593,  0.2469, -1.4299],\n        [-0.4926, -0.5457,  0.4793]], requires_grad=True))\n('bias', Parameter containing:\ntensor([ 0.3634,  0.2015, -0.8525], requires_grad=True))",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using the Sequential module. It allows us to chain together\nmultiple modules:, give an example?",
        "Y": "net = nn.Sequential(\n  MyLinear(4, 3),\n  nn.ReLU(),\n  MyLinear(3, 1)\n)\n\nsample_input = torch.randn(4)\nnet(sample_input)\n: tensor([-0.6749], grad_fn=<AddBackward0>)",
        "Z": "Modules can contain other modules, making them useful building blocks for developing more elaborate functionality.\nThe simplest way to do this is using the Sequential module. It allows us to chain together\nmultiple modules: net = nn.Sequential(\n  MyLinear(4, 3),\n  nn.ReLU(),\n  MyLinear(3, 1)\n)\n\nsample_input = torch.randn(4)\nnet(sample_input)\n: tensor([-0.6749], grad_fn=<AddBackward0>)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation.For example, here\u2019s a simple neural network implemented as a custom module:, give an example?",
        "Y": "import torch.nn.functional as F\n\nclass Net(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l0 = MyLinear(4, 3)\n    self.l1 = MyLinear(3, 1)\n  def forward(self, x):\n    x = self.l0(x)\n    x = F.relu(x)\n    x = self.l1(x)\n    return x",
        "Z": "In general, it is recommended to define a custom module for anything beyond the simplest use cases, as this gives\nfull flexibility on how submodules are used for a module\u2019s computation.For example, here\u2019s a simple neural network implemented as a custom module: import torch.nn.functional as F\n\nclass Net(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l0 = MyLinear(4, 3)\n    self.l1 = MyLinear(3, 1)\n  def forward(self, x):\n    x = self.l0(x)\n    x = F.relu(x)\n    x = self.l1(x)\n    return x",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use For example, here\u2019s a simple neural network implemented as a custom module:This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0 and l1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019s forward() method. Immediate\nchildren of a module can be iterated through via a call to children() or\nnamed_children():, give an example?",
        "Y": "net = Net()\nfor child in net.named_children():\n  print(child)\n: ('l0', MyLinear())\n('l1', MyLinear())",
        "Z": "For example, here\u2019s a simple neural network implemented as a custom module:This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0 and l1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019s forward() method. Immediate\nchildren of a module can be iterated through via a call to children() or\nnamed_children(): net = Net()\nfor child in net.named_children():\n  print(child)\n: ('l0', MyLinear())\n('l1', MyLinear())",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0 and l1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019s forward() method. Immediate\nchildren of a module can be iterated through via a call to children() or\nnamed_children():To go deeper than just the immediate children, modules() and\nnamed_modules() recursively iterate through a module and its child modules:, give an example?",
        "Y": "class BigNet(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l1 = MyLinear(5, 4)\n    self.net = Net()\n  def forward(self, x):\n    return self.net(self.l1(x))\n\nbig_net = BigNet()\nfor module in big_net.named_modules():\n  print(module)\n: ('', BigNet(\n  (l1): MyLinear()\n  (net): Net(\n    (l0): MyLinear()\n    (l1): MyLinear()\n  )\n))\n('l1', MyLinear())\n('net', Net(\n  (l0): MyLinear()\n  (l1): MyLinear()\n))\n('net.l0', MyLinear())\n('net.l1', MyLinear())",
        "Z": "This module is composed of two \u201cchildren\u201d or \u201csubmodules\u201d (l0 and l1) that define the layers of\nthe neural network and are utilized for computation within the module\u2019s forward() method. Immediate\nchildren of a module can be iterated through via a call to children() or\nnamed_children():To go deeper than just the immediate children, modules() and\nnamed_modules() recursively iterate through a module and its child modules: class BigNet(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.l1 = MyLinear(5, 4)\n    self.net = Net()\n  def forward(self, x):\n    return self.net(self.l1(x))\n\nbig_net = BigNet()\nfor module in big_net.named_modules():\n  print(module)\n: ('', BigNet(\n  (l1): MyLinear()\n  (net): Net(\n    (l0): MyLinear()\n    (l1): MyLinear()\n  )\n))\n('l1', MyLinear())\n('net', Net(\n  (l0): MyLinear()\n  (l1): MyLinear()\n))\n('net.l0', MyLinear())\n('net.l1', MyLinear())",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use To go deeper than just the immediate children, modules() and\nnamed_modules() recursively iterate through a module and its child modules:Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nThe ModuleList and ModuleDict modules are useful here; they\nregister submodules from a list or dict:, give an example?",
        "Y": "class DynamicNet(nn.Module):\n  def __init__(self, num_layers):\n    super().__init__()\n    self.linears = nn.ModuleList(\n      [MyLinear(4, 4) for _ in range(num_layers)])\n    self.activations = nn.ModuleDict({\n      'relu': nn.ReLU(),\n      'lrelu': nn.LeakyReLU()\n    })\n    self.final = MyLinear(4, 1)\n  def forward(self, x, act):\n    for linear in self.linears:\n      x = linear(x)\n    x = self.activations[act](x)\n    x = self.final(x)\n    return x\n\ndynamic_net = DynamicNet(3)\nsample_input = torch.randn(4)\noutput = dynamic_net(sample_input, 'relu')",
        "Z": "To go deeper than just the immediate children, modules() and\nnamed_modules() recursively iterate through a module and its child modules:Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nThe ModuleList and ModuleDict modules are useful here; they\nregister submodules from a list or dict: class DynamicNet(nn.Module):\n  def __init__(self, num_layers):\n    super().__init__()\n    self.linears = nn.ModuleList(\n      [MyLinear(4, 4) for _ in range(num_layers)])\n    self.activations = nn.ModuleDict({\n      'relu': nn.ReLU(),\n      'lrelu': nn.LeakyReLU()\n    })\n    self.final = MyLinear(4, 1)\n  def forward(self, x, act):\n    for linear in self.linears:\n      x = linear(x)\n    x = self.activations[act](x)\n    x = self.final(x)\n    return x\n\ndynamic_net = DynamicNet(3)\nsample_input = torch.randn(4)\noutput = dynamic_net(sample_input, 'relu')",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nThe ModuleList and ModuleDict modules are useful here; they\nregister submodules from a list or dict:For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls to parameters() and named_parameters() will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:, give an example?",
        "Y": "for parameter in dynamic_net.named_parameters():\n  print(parameter)\n: ('linears.0.weight', Parameter containing:\ntensor([[-1.2051,  0.7601,  1.1065,  0.1963],\n        [ 3.0592,  0.4354,  1.6598,  0.9828],\n        [-0.4446,  0.4628,  0.8774,  1.6848],\n        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\n('linears.0.bias', Parameter containing:\ntensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\n('linears.1.weight', Parameter containing:\ntensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\n        [-0.0550,  1.5317,  1.1064, -0.5562],\n        [-0.4028, -0.6942,  1.5793, -1.0140],\n        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\n('linears.1.bias', Parameter containing:\ntensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\n('linears.2.weight', Parameter containing:\ntensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n        [-0.3526,  0.8756, -1.5847, -0.6016],\n        [-0.3269, -0.1608,  0.2897, -2.0829],\n        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\n('linears.2.bias', Parameter containing:\ntensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\n('final.weight', Parameter containing:\ntensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))\n('final.bias', Parameter containing:\ntensor([0.3381], requires_grad=True))",
        "Z": "Sometimes, it\u2019s necessary for a module to dynamically define submodules.\nThe ModuleList and ModuleDict modules are useful here; they\nregister submodules from a list or dict:For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls to parameters() and named_parameters() will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network: for parameter in dynamic_net.named_parameters():\n  print(parameter)\n: ('linears.0.weight', Parameter containing:\ntensor([[-1.2051,  0.7601,  1.1065,  0.1963],\n        [ 3.0592,  0.4354,  1.6598,  0.9828],\n        [-0.4446,  0.4628,  0.8774,  1.6848],\n        [-0.1222,  1.5458,  1.1729,  1.4647]], requires_grad=True))\n('linears.0.bias', Parameter containing:\ntensor([ 1.5310,  1.0609, -2.0940,  1.1266], requires_grad=True))\n('linears.1.weight', Parameter containing:\ntensor([[ 2.1113, -0.0623, -1.0806,  0.3508],\n        [-0.0550,  1.5317,  1.1064, -0.5562],\n        [-0.4028, -0.6942,  1.5793, -1.0140],\n        [-0.0329,  0.1160, -1.7183, -1.0434]], requires_grad=True))\n('linears.1.bias', Parameter containing:\ntensor([ 0.0361, -0.9768, -0.3889,  1.1613], requires_grad=True))\n('linears.2.weight', Parameter containing:\ntensor([[-2.6340, -0.3887, -0.9979,  0.0767],\n        [-0.3526,  0.8756, -1.5847, -0.6016],\n        [-0.3269, -0.1608,  0.2897, -2.0829],\n        [ 2.6338,  0.9239,  0.6943, -1.5034]], requires_grad=True))\n('linears.2.bias', Parameter containing:\ntensor([ 1.0268,  0.4489, -0.9403,  0.1571], requires_grad=True))\n('final.weight', Parameter containing:\ntensor([[ 0.2509], [-0.5052], [ 0.3088], [-1.4951]], requires_grad=True))\n('final.bias', Parameter containing:\ntensor([0.3381], requires_grad=True))",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls to parameters() and named_parameters() will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:It\u2019s also easy to move all parameters to a different device or change their precision using\nto():, give an example?",
        "Y": "# Move all parameters to a CUDA device\ndynamic_net.to(device='cuda')\n\n# Change precision of all parameters\ndynamic_net.to(dtype=torch.float64)\n\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)",
        "Z": "For any given module, its parameters consist of its direct parameters as well as the parameters of all submodules.\nThis means that calls to parameters() and named_parameters() will\nrecursively include child parameters, allowing for convenient optimization of all parameters within the network:It\u2019s also easy to move all parameters to a different device or change their precision using\nto(): # Move all parameters to a CUDA device\ndynamic_net.to(device='cuda')\n\n# Change precision of all parameters\ndynamic_net.to(dtype=torch.float64)\n\ndynamic_net(torch.randn(5, device='cuda', dtype=torch.float64))\n: tensor([6.5166], device='cuda:0', dtype=torch.float64, grad_fn=<AddBackward0>)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers from torch.optim:, give an example?",
        "Y": "# Create the network (from previous section) and optimizer\nnet = Net()\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\n# Run a sample training loop that \"teaches\" the network\n# to output the constant zero function\nfor _ in range(10000):\n  input = torch.randn(4)\n  output = net(input)\n  loss = torch.abs(output)\n  net.zero_grad()\n  loss.backward()\n  optimizer.step()",
        "Z": "Once a network is built, it has to be trained, and its parameters can be easily optimized with one of PyTorch\u2019s\nOptimizers from torch.optim: # Create the network (from previous section) and optimizer\nnet = Net()\noptimizer = torch.optim.SGD(net.parameters(), lr=1e-4, weight_decay=1e-2, momentum=0.9)\n\n# Run a sample training loop that \"teaches\" the network\n# to output the constant zero function\nfor _ in range(10000):\n  input = torch.randn(4)\n  output = net(input)\n  loss = torch.abs(output)\n  net.zero_grad()\n  loss.backward()\n  optimizer.step()",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employing torch.abs() as a loss function. While this is not a very interesting task, the\nkey parts of training are present:After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue of l1\u2019s weight parameter shows that its values are now much closer to 0 (as may be expected):, give an example?",
        "Y": "print(net.l1.weight)\n: Parameter containing:\ntensor([[-0.0013],\n        [ 0.0030],\n        [-0.0008]], requires_grad=True)",
        "Z": "In this simplified example, the network learns to simply output zero, as any non-zero output is \u201cpenalized\u201d according\nto its absolute value by employing torch.abs() as a loss function. While this is not a very interesting task, the\nkey parts of training are present:After the above snippet has been run, note that the network\u2019s parameters have changed. In particular, examining the\nvalue of l1\u2019s weight parameter shows that its values are now much closer to 0 (as may be expected): print(net.l1.weight)\n: Parameter containing:\ntensor([[-0.0013],\n        [ 0.0030],\n        [-0.0008]], requires_grad=True)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving its state_dict (i.e. \u201cstate dictionary\u201d):, give an example?",
        "Y": "# Save the module\ntorch.save(net.state_dict(), 'net.pt')\n\n...\n\n# Load the module later on\nnew_net = Net()\nnew_net.load_state_dict(torch.load('net.pt'))\n: <All keys matched successfully>",
        "Z": "In the previous section, we demonstrated training a module\u2019s \u201cparameters\u201d, or learnable aspects of computation.\nNow, if we want to save the trained model to disk, we can do so by saving its state_dict (i.e. \u201cstate dictionary\u201d): # Save the module\ntorch.save(net.state_dict(), 'net.pt')\n\n...\n\n# Load the module later on\nnew_net = Net()\nnew_net.load_state_dict(torch.load('net.pt'))\n: <All keys matched successfully>",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use A module\u2019s state_dict contains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have:As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019s state_dict so that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to use register_buffer() to accomplish this:, give an example?",
        "Y": "class RunningMean(nn.Module):\n  def __init__(self, num_features, momentum=0.9):\n    super().__init__()\n    self.momentum = momentum\n    self.register_buffer('mean', torch.zeros(num_features))\n  def forward(self, x):\n    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n    return self.mean",
        "Z": "A module\u2019s state_dict contains state that affects its computation. This includes, but is not limited to, the\nmodule\u2019s parameters. For some modules, it may be useful to have state beyond parameters that affects module\ncomputation but is not learnable. For such cases, PyTorch provides the concept of \u201cbuffers\u201d, both \u201cpersistent\u201d\nand \u201cnon-persistent\u201d. Following is an overview of the various types of state a module can have:As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019s state_dict so that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to use register_buffer() to accomplish this: class RunningMean(nn.Module):\n  def __init__(self, num_features, momentum=0.9):\n    super().__init__()\n    self.momentum = momentum\n    self.register_buffer('mean', torch.zeros(num_features))\n  def forward(self, x):\n    self.mean = self.momentum * self.mean + (1.0 - self.momentum) * x\n    return self.mean",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019s state_dict so that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to use register_buffer() to accomplish this:Now, the current value of the running mean is considered part of the module\u2019s state_dict\nand will be properly restored when loading the module from disk:, give an example?",
        "Y": "m = RunningMean(4)\nfor _ in range(10):\n  input = torch.randn(4)\n  m(input)\n\nprint(m.state_dict())\n: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))\n\n# Serialized form will contain the 'mean' tensor\ntorch.save(m.state_dict(), 'mean.pt')\n\nm_loaded = RunningMean(4)\nm_loaded.load_state_dict(torch.load('mean.pt'))\nassert(torch.all(m.mean == m_loaded.mean))",
        "Z": "As a motivating example for the use of buffers, consider a simple module that maintains a running mean. We want\nthe current value of the running mean to be considered part of the module\u2019s state_dict so that it will be\nrestored when loading a serialized form of the module, but we don\u2019t want it to be learnable.\nThis snippet shows how to use register_buffer() to accomplish this:Now, the current value of the running mean is considered part of the module\u2019s state_dict\nand will be properly restored when loading the module from disk: m = RunningMean(4)\nfor _ in range(10):\n  input = torch.randn(4)\n  m(input)\n\nprint(m.state_dict())\n: OrderedDict([('mean', tensor([ 0.1041, -0.1113, -0.0647,  0.1515]))]))\n\n# Serialized form will contain the 'mean' tensor\ntorch.save(m.state_dict(), 'mean.pt')\n\nm_loaded = RunningMean(4)\nm_loaded.load_state_dict(torch.load('mean.pt'))\nassert(torch.all(m.mean == m_loaded.mean))",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use Now, the current value of the running mean is considered part of the module\u2019s state_dict\nand will be properly restored when loading the module from disk:As mentioned previously, buffers can be left out of the module\u2019s state_dict by marking them as non-persistent:, give an example?",
        "Y": "self.register_buffer('unserialized_thing', torch.randn(5), persistent=False)",
        "Z": "Now, the current value of the running mean is considered part of the module\u2019s state_dict\nand will be properly restored when loading the module from disk:As mentioned previously, buffers can be left out of the module\u2019s state_dict by marking them as non-persistent: self.register_buffer('unserialized_thing', torch.randn(5), persistent=False)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How to use As mentioned previously, buffers can be left out of the module\u2019s state_dict by marking them as non-persistent:Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with\nto():, give an example?",
        "Y": "# Moves all module parameters and buffers to the specified device / dtype\nm.to(device='cuda', dtype=torch.float64)",
        "Z": "As mentioned previously, buffers can be left out of the module\u2019s state_dict by marking them as non-persistent:Both persistent and non-persistent buffers are affected by model-wide device / dtype changes applied with\nto(): # Moves all module parameters and buffers to the specified device / dtype\nm.to(device='cuda', dtype=torch.float64)",
        "source": "https://pytorch.org/docs/stable/notes/modules.html"
    },
    {
        "X": "How  A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor:, give an example?",
        "Y": ">>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: >>> torch.tensor([[1., -1.], [1., -1.]])\ntensor([[ 1.0000, -1.0000],\n        [ 1.0000, -1.0000]])\n>>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6]])",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "How  A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op:, give an example?",
        "Y": ">>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')",
        "Z": "A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: >>> torch.zeros([2, 4], dtype=torch.int32)\ntensor([[ 0,  0,  0,  0],\n        [ 0,  0,  0,  0]], dtype=torch.int32)\n>>> cuda0 = torch.device('cuda:0')\n>>> torch.ones([2, 4], dtype=torch.float64, device=cuda0)\ntensor([[ 1.0000,  1.0000,  1.0000,  1.0000],\n        [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64, device='cuda:0')",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "How  For more information about building Tensors, see Creation OpsThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:, give an example?",
        "Y": ">>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])",
        "Z": "For more information about building Tensors, see Creation OpsThe contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: >>> x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n>>> print(x[1][2])\ntensor(6)\n>>> x[0][1] = 8\n>>> print(x)\ntensor([[ 1,  8,  3],\n        [ 4,  5,  6]])",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "How  The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value:, give an example?",
        "Y": ">>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: >>> x = torch.tensor([[1]])\n>>> x\ntensor([[ 1]])\n>>> x.item()\n1\n>>> x = torch.tensor(2.5)\n>>> x\ntensor(2.5000)\n>>> x.item()\n2.5",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "How  For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation., give an example?",
        "Y": ">>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])",
        "Z": "For more information about indexing, see Indexing, Slicing, Joining, Mutating OpsA tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. >>> x = torch.tensor([[1., -1.], [1., 1.]], requires_grad=True)\n>>> out = x.pow(2).sum()\n>>> out.backward()\n>>> x.grad\ntensor([[ 2.0000, -2.0000],\n        [ 2.0000,  2.0000]])",
        "source": "https://pytorch.org/docs/stable/tensors.html#torch.Tensor"
    },
    {
        "X": "How to use torch.flip, give an example?",
        "Y": ">>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[ 0,  1],\n         [ 2,  3]],\n\n        [[ 4,  5],\n         [ 6,  7]]])\n>>> torch.flip(x, [0, 1])\ntensor([[[ 6,  7],\n         [ 4,  5]],\n\n        [[ 2,  3],\n         [ 0,  1]]])",
        "Z": ">>> x = torch.arange(8).view(2, 2, 2)\n>>> x\ntensor([[[ 0,  1],\n         [ 2,  3]],\n\n        [[ 4,  5],\n         [ 6,  7]]])\n>>> torch.flip(x, [0, 1])\ntensor([[[ 6,  7],\n         [ 4,  5]],\n\n        [[ 2,  3],\n         [ 0,  1]]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"
    },
    {
        "X": "How to use torch.svd, give an example?",
        "Y": "U, S, Vh = torch.linalg.svd(A, full_matrices=not some)\nV = Vh.transpose(-2, -1).conj()",
        "Z": "torch.svd() is deprecated in favor of torch.linalg.svd()\nand will be removed in a future PyTorch release.U, S, V = torch.svd(A, some=some, compute_uv=True) (default) should be replaced with U, S, Vh = torch.linalg.svd(A, full_matrices=not some)\nV = Vh.transpose(-2, -1).conj()",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "How  U, S, V = torch.svd(A, some=some, compute_uv=True) (default) should be replaced with_, S, _ = torch.svd(A, some=some, compute_uv=False) should be replaced with, give an example?",
        "Y": "S = torch.svdvals(A)",
        "Z": "U, S, V = torch.svd(A, some=some, compute_uv=True) (default) should be replaced with_, S, _ = torch.svd(A, some=some, compute_uv=False) should be replaced with S = torch.svdvals(A)",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "How  Supports input of float, double, cfloat and cdouble data types.\nThe dtypes of U and V are the same as input\u2019s. S will\nalways be real-valued, even if input is complex., give an example?",
        "Y": ">>> a = torch.randn(5, 3)\n>>> a\ntensor([[ 0.2364, -0.7752,  0.6372],\n        [ 1.7201,  0.7394, -0.0504],\n        [-0.3371, -1.0584,  0.5296],\n        [ 0.3550, -0.4022,  1.5569],\n        [ 0.2445, -0.0158,  1.1414]])\n>>> u, s, v = torch.svd(a)\n>>> u\ntensor([[ 0.4027,  0.0287,  0.5434],\n        [-0.1946,  0.8833,  0.3679],\n        [ 0.4296, -0.2890,  0.5261],\n        [ 0.6604,  0.2717, -0.2618],\n        [ 0.4234,  0.2481, -0.4733]])\n>>> s\ntensor([2.3289, 2.0315, 0.7806])\n>>> v\ntensor([[-0.0199,  0.8766,  0.4809],\n        [-0.5080,  0.4054, -0.7600],\n        [ 0.8611,  0.2594, -0.4373]])\n>>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))\ntensor(8.6531e-07)\n>>> a_big = torch.randn(7, 5, 3)\n>>> u, s, v = torch.svd(a_big)\n>>> torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))\ntensor(2.6503e-06)",
        "Z": "Supports input of float, double, cfloat and cdouble data types.\nThe dtypes of U and V are the same as input\u2019s. S will\nalways be real-valued, even if input is complex. >>> a = torch.randn(5, 3)\n>>> a\ntensor([[ 0.2364, -0.7752,  0.6372],\n        [ 1.7201,  0.7394, -0.0504],\n        [-0.3371, -1.0584,  0.5296],\n        [ 0.3550, -0.4022,  1.5569],\n        [ 0.2445, -0.0158,  1.1414]])\n>>> u, s, v = torch.svd(a)\n>>> u\ntensor([[ 0.4027,  0.0287,  0.5434],\n        [-0.1946,  0.8833,  0.3679],\n        [ 0.4296, -0.2890,  0.5261],\n        [ 0.6604,  0.2717, -0.2618],\n        [ 0.4234,  0.2481, -0.4733]])\n>>> s\ntensor([2.3289, 2.0315, 0.7806])\n>>> v\ntensor([[-0.0199,  0.8766,  0.4809],\n        [-0.5080,  0.4054, -0.7600],\n        [ 0.8611,  0.2594, -0.4373]])\n>>> torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))\ntensor(8.6531e-07)\n>>> a_big = torch.randn(7, 5, 3)\n>>> u, s, v = torch.svd(a_big)\n>>> torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))\ntensor(2.6503e-06)",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"
    },
    {
        "X": "How to use torch.real, give an example?",
        "Y": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.real\ntensor([ 0.3100, -0.5445, -1.6492, -0.0638])",
        "Z": ">>> x=torch.randn(4, dtype=torch.cfloat)\n>>> x\ntensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n>>> x.real\ntensor([ 0.3100, -0.5445, -1.6492, -0.0638])",
        "source": "https://pytorch.org/docs/stable/generated/torch.real.html#torch.real"
    },
    {
        "X": "How to use torch.xlogy, give an example?",
        "Y": ">>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.xlogy(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.xlogy(x, y)\ntensor([1.0986, 1.3863, 0.0000])\n>>> torch.xlogy(x, 4)\ntensor([1.3863, 2.7726, 4.1589])\n>>> torch.xlogy(2, y)\ntensor([2.1972, 1.3863, 0.0000])",
        "Z": "Similar to SciPy\u2019s scipy.special.xlogy. >>> x = torch.zeros(5,)\n>>> y = torch.tensor([-1, 0, 1, float('inf'), float('nan')])\n>>> torch.xlogy(x, y)\ntensor([0., 0., 0., 0., nan])\n>>> x = torch.tensor([1, 2, 3])\n>>> y = torch.tensor([3, 2, 1])\n>>> torch.xlogy(x, y)\ntensor([1.0986, 1.3863, 0.0000])\n>>> torch.xlogy(x, 4)\ntensor([1.3863, 2.7726, 4.1589])\n>>> torch.xlogy(2, y)\ntensor([2.1972, 1.3863, 0.0000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy"
    },
    {
        "X": "How to use torch.quantize_per_channel, give an example?",
        "Y": ">>> x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)\ntensor([[-1.,  0.],\n        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,\n       quantization_scheme=torch.per_channel_affine,\n       scale=tensor([0.1000, 0.0100], dtype=torch.float64),\n       zero_point=tensor([10,  0]), axis=0)\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()\ntensor([[  0,  10],\n        [100, 200]], dtype=torch.uint8)",
        "Z": ">>> x = torch.tensor([[-1.0, 0.0], [1.0, 2.0]])\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8)\ntensor([[-1.,  0.],\n        [ 1.,  2.]], size=(2, 2), dtype=torch.quint8,\n       quantization_scheme=torch.per_channel_affine,\n       scale=tensor([0.1000, 0.0100], dtype=torch.float64),\n       zero_point=tensor([10,  0]), axis=0)\n>>> torch.quantize_per_channel(x, torch.tensor([0.1, 0.01]), torch.tensor([10, 0]), 0, torch.quint8).int_repr()\ntensor([[  0,  10],\n        [100, 200]], dtype=torch.uint8)",
        "source": "https://pytorch.org/docs/stable/generated/torch.quantize_per_channel.html#torch.quantize_per_channel"
    },
    {
        "X": "How to use torch.enable_grad, give an example?",
        "Y": ">>> x = torch.tensor([1.], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue",
        "Z": "Also functions as a decorator. (Make sure to instantiate with parenthesis.) >>> x = torch.tensor([1.], requires_grad=True)\n>>> with torch.no_grad():\n...   with torch.enable_grad():\n...     y = x * 2\n>>> y.requires_grad\nTrue\n>>> y.backward()\n>>> x.grad\n>>> @torch.enable_grad()\n... def doubler(x):\n...     return x * 2\n>>> with torch.no_grad():\n...     z = doubler(x)\n>>> z.requires_grad\nTrue",
        "source": "https://pytorch.org/docs/stable/generated/torch.enable_grad.html#torch.enable_grad"
    },
    {
        "X": "How to use torch.eig, give an example?",
        "Y": "L_complex = torch.linalg.eigvals(A)",
        "Z": "torch.eig() is deprecated in favor of torch.linalg.eig()\nand will be removed in a future PyTorch release.\ntorch.linalg.eig() returns complex tensors of dtype cfloat or cdouble\nrather than real tensors mimicking complex tensors.L, _ = torch.eig(A) should be replaced with L_complex = torch.linalg.eigvals(A)",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "How  L, _ = torch.eig(A) should be replaced withL, V = torch.eig(A, eigenvectors=True) should be replaced with, give an example?",
        "Y": "L_complex, V_complex = torch.linalg.eig(A)",
        "Z": "L, _ = torch.eig(A) should be replaced withL, V = torch.eig(A, eigenvectors=True) should be replaced with L_complex, V_complex = torch.linalg.eig(A)",
        "source": "https://pytorch.org/docs/stable/generated/torch.eig.html#torch.eig"
    },
    {
        "X": "How to use At the heart of PyTorch data loading utility is the torch.utils.data.DataLoader\nclass.  It represents a Python iterable over a dataset, with support forThese options are configured by the constructor arguments of a\nDataLoader, which has signature:, give an example?",
        "Y": "DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)",
        "Z": "These options are configured by the constructor arguments of a\nDataLoader, which has signature: DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,\n           batch_sampler=None, num_workers=0, collate_fn=None,\n           pin_memory=False, drop_last=False, timeout=0,\n           worker_init_fn=None, *, prefetch_factor=2,\n           persistent_workers=False)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How to use After fetching a list of samples using the indices from sampler, the function\npassed as the collate_fn argument is used to collate lists of samples\ninto batches.In this case, loading from a map-style dataset is roughly equivalent with:, give an example?",
        "Y": "for indices in batch_sampler:\n    yield collate_fn([dataset[i] for i in indices])",
        "Z": "After fetching a list of samples using the indices from sampler, the function\npassed as the collate_fn argument is used to collate lists of samples\ninto batches.In this case, loading from a map-style dataset is roughly equivalent with: for indices in batch_sampler:\n    yield collate_fn([dataset[i] for i in indices])",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How to use In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:, give an example?",
        "Y": "dataset_iter = iter(dataset)\nfor indices in batch_sampler:\n    yield collate_fn([next(dataset_iter) for _ in indices])",
        "Z": "In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with: dataset_iter = iter(dataset)\nfor indices in batch_sampler:\n    yield collate_fn([next(dataset_iter) for _ in indices])",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How to use When automatic batching is disabled, the default collate_fn simply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.In this case, loading from a map-style dataset is roughly equivalent with:, give an example?",
        "Y": "for index in sampler:\n    yield collate_fn(dataset[index])",
        "Z": "When automatic batching is disabled, the default collate_fn simply\nconverts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.In this case, loading from a map-style dataset is roughly equivalent with: for index in sampler:\n    yield collate_fn(dataset[index])",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How  In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with:, give an example?",
        "Y": "for data in iter(dataset):\n    yield collate_fn(data)",
        "Z": "In this case, loading from a map-style dataset is roughly equivalent with:and loading from an iterable-style dataset is roughly equivalent with: for data in iter(dataset):\n    yield collate_fn(data)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How to use See the example below., give an example?",
        "Y": "class SimpleCustomBatch:\n    def __init__(self, data):\n        transposed_data = list(zip(*data))\n        self.inp = torch.stack(transposed_data[0], 0)\n        self.tgt = torch.stack(transposed_data[1], 0)\n\n    # custom memory pinning method on custom type\n    def pin_memory(self):\n        self.inp = self.inp.pin_memory()\n        self.tgt = self.tgt.pin_memory()\n        return self\n\ndef collate_wrapper(batch):\n    return SimpleCustomBatch(batch)\n\ninps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ntgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ndataset = TensorDataset(inps, tgts)\n\nloader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                    pin_memory=True)\n\nfor batch_ndx, sample in enumerate(loader):\n    print(sample.inp.is_pinned())\n    print(sample.tgt.is_pinned())",
        "Z": "See the example below. class SimpleCustomBatch:\n    def __init__(self, data):\n        transposed_data = list(zip(*data))\n        self.inp = torch.stack(transposed_data[0], 0)\n        self.tgt = torch.stack(transposed_data[1], 0)\n\n    # custom memory pinning method on custom type\n    def pin_memory(self):\n        self.inp = self.inp.pin_memory()\n        self.tgt = self.tgt.pin_memory()\n        return self\n\ndef collate_wrapper(batch):\n    return SimpleCustomBatch(batch)\n\ninps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ntgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)\ndataset = TensorDataset(inps, tgts)\n\nloader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,\n                    pin_memory=True)\n\nfor batch_ndx, sample in enumerate(loader):\n    print(sample.inp.is_pinned())\n    print(sample.tgt.is_pinned())",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How to use torch.utils.data.IterableDataset, give an example?",
        "Y": ">>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example code only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n\n>>> # Mult-process loading with two worker processes\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n[3, 4, 5, 6]",
        "Z": "When a subclass is used with DataLoader, each\nitem in the dataset will be yielded from the DataLoader\niterator. When num_workers > 0, each worker process will have a\ndifferent copy of the dataset object, so it is often desired to configure\neach copy independently to avoid having duplicate data returned from the\nworkers. get_worker_info(), when called in a worker\nprocess, returns information about the worker. It can be used in either the\ndataset\u2019s __iter__() method or the DataLoader \u2018s\nworker_init_fn option to modify each copy\u2019s behavior. >>> class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end > start, \"this example code only works with end >= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n>>> # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n>>> ds = MyIterableDataset(start=3, end=7)\n\n>>> # Single-process loading\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n\n>>> # Mult-process loading with two worker processes\n>>> # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 5, 4, 6]\n\n>>> # With even more workers\n>>> print(list(torch.utils.data.DataLoader(ds, num_workers=20)))\n[3, 4, 5, 6]",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How to use torch.utils.data.random_split, give an example?",
        "Y": ">>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))",
        "Z": "Randomly split a dataset into non-overlapping new datasets of given lengths.\nOptionally fix the generator for reproducible results, e.g.: >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How to use torch.utils.data.WeightedRandomSampler, give an example?",
        "Y": ">>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n[4, 4, 1, 4, 5]\n>>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n[0, 1, 4, 3, 2]",
        "Z": ">>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n[4, 4, 1, 4, 5]\n>>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n[0, 1, 4, 3, 2]",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How to use torch.utils.data.BatchSampler, give an example?",
        "Y": ">>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]",
        "Z": ">>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n>>> list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))\n[[0, 1, 2], [3, 4, 5], [6, 7, 8]]",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How to use torch.utils.data.distributed.DistributedSampler, give an example?",
        "Y": ">>> sampler = DistributedSampler(dataset) if is_distributed else None\n>>> loader = DataLoader(dataset, shuffle=(sampler is None),\n...                     sampler=sampler)\n>>> for epoch in range(start_epoch, n_epochs):\n...     if is_distributed:\n...         sampler.set_epoch(epoch)\n...     train(loader)",
        "Z": "It is especially useful in conjunction with\ntorch.nn.parallel.DistributedDataParallel. In such a case, each\nprocess can pass a DistributedSampler instance as a\nDataLoader sampler, and load a subset of the\noriginal dataset that is exclusive to it. >>> sampler = DistributedSampler(dataset) if is_distributed else None\n>>> loader = DataLoader(dataset, shuffle=(sampler is None),\n...                     sampler=sampler)\n>>> for epoch in range(start_epoch, n_epochs):\n...     if is_distributed:\n...         sampler.set_epoch(epoch)\n...     train(loader)",
        "source": "https://pytorch.org/docs/stable/data.html"
    },
    {
        "X": "How to use torch.sum, give an example?",
        "Y": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.1133, -0.9567,  0.2958]])\n>>> torch.sum(a)\ntensor(-0.5475)",
        "Z": ">>> a = torch.randn(1, 3)\n>>> a\ntensor([[ 0.1133, -0.9567,  0.2958]])\n>>> torch.sum(a)\ntensor(-0.5475)",
        "source": "https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"
    },
    {
        "X": "How to use torch.bitwise_not, give an example?",
        "Y": ">>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\ntensor([ 0,  1, -4], dtype=torch.int8)",
        "Z": ">>> torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))\ntensor([ 0,  1, -4], dtype=torch.int8)",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"
    },
    {
        "X": "How to use torch.vsplit, give an example?",
        "Y": ">>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.vsplit(t, 2)\n(tensor([[0., 1., 2., 3.],\n         [4., 5., 6., 7.]]),\n tensor([[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]))\n>>> torch.vsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]),\n tensor([[12., 13., 14., 15.]]),\n tensor([], size=(0, 4)))",
        "Z": ">>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.vsplit(t, 2)\n(tensor([[0., 1., 2., 3.],\n         [4., 5., 6., 7.]]),\n tensor([[ 8.,  9., 10., 11.],\n         [12., 13., 14., 15.]]))\n>>> torch.vsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.,  3.],\n         [ 4.,  5.,  6.,  7.],\n         [ 8.,  9., 10., 11.]]),\n tensor([[12., 13., 14., 15.]]),\n tensor([], size=(0, 4)))",
        "source": "https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"
    },
    {
        "X": "How to use torch.hsplit, give an example?",
        "Y": ">>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))",
        "Z": ">>> t = torch.arange(16.0).reshape(4,4)\n>>> t\ntensor([[ 0.,  1.,  2.,  3.],\n        [ 4.,  5.,  6.,  7.],\n        [ 8.,  9., 10., 11.],\n        [12., 13., 14., 15.]])\n>>> torch.hsplit(t, 2)\n(tensor([[ 0.,  1.],\n         [ 4.,  5.],\n         [ 8.,  9.],\n         [12., 13.]]),\n tensor([[ 2.,  3.],\n         [ 6.,  7.],\n         [10., 11.],\n         [14., 15.]]))\n>>> torch.hsplit(t, [3, 6])\n(tensor([[ 0.,  1.,  2.],\n         [ 4.,  5.,  6.],\n         [ 8.,  9., 10.],\n         [12., 13., 14.]]),\n tensor([[ 3.],\n         [ 7.],\n         [11.],\n         [15.]]),\n tensor([], size=(4, 0)))",
        "source": "https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"
    },
    {
        "X": "How to use torch.fliplr, give an example?",
        "Y": ">>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.fliplr(x)\ntensor([[1, 0],\n        [3, 2]])",
        "Z": "Flip the entries in each row in the left/right direction.\nColumns are preserved, but appear in a different order than before. >>> x = torch.arange(4).view(2, 2)\n>>> x\ntensor([[0, 1],\n        [2, 3]])\n>>> torch.fliplr(x)\ntensor([[1, 0],\n        [3, 2]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"
    },
    {
        "X": "How to use torch.cosh, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.1632,  1.1835, -0.6979, -0.7325])\n>>> torch.cosh(a)\ntensor([ 1.0133,  1.7860,  1.2536,  1.2805])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.1632,  1.1835, -0.6979, -0.7325])\n>>> torch.cosh(a)\ntensor([ 1.0133,  1.7860,  1.2536,  1.2805])",
        "source": "https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"
    },
    {
        "X": "How to use torch.rad2deg, give an example?",
        "Y": ">>> a = torch.tensor([[3.142, -3.142], [6.283, -6.283], [1.570, -1.570]])\n>>> torch.rad2deg(a)\ntensor([[ 180.0233, -180.0233],\n        [ 359.9894, -359.9894],\n        [  89.9544,  -89.9544]])",
        "Z": ">>> a = torch.tensor([[3.142, -3.142], [6.283, -6.283], [1.570, -1.570]])\n>>> torch.rad2deg(a)\ntensor([[ 180.0233, -180.0233],\n        [ 359.9894, -359.9894],\n        [  89.9544,  -89.9544]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg"
    },
    {
        "X": "How to use torch.broadcast_shapes, give an example?",
        "Y": ">>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\ntorch.Size([1, 3, 2])",
        "Z": "This is equivalent to\ntorch.broadcast_tensors(*map(torch.empty, shapes))[0].shape\nbut avoids the need create to intermediate tensors. This is useful for\nbroadcasting tensors of common batch shape but different rightmost shape,\ne.g. to broadcast mean vectors with covariance matrices. >>> torch.broadcast_shapes((2,), (3, 1), (1, 1, 1))\ntorch.Size([1, 3, 2])",
        "source": "https://pytorch.org/docs/stable/generated/torch.broadcast_shapes.html#torch.broadcast_shapes"
    },
    {
        "X": "How to use torch.get_default_dtype, give an example?",
        "Y": ">>> torch.get_default_dtype()  # initial default for floating point is torch.float32\ntorch.float32\n>>> torch.set_default_dtype(torch.float64)\n>>> torch.get_default_dtype()  # default is now changed to torch.float64\ntorch.float64\n>>> torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this\n>>> torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor\ntorch.float32",
        "Z": ">>> torch.get_default_dtype()  # initial default for floating point is torch.float32\ntorch.float32\n>>> torch.set_default_dtype(torch.float64)\n>>> torch.get_default_dtype()  # default is now changed to torch.float64\ntorch.float64\n>>> torch.set_default_tensor_type(torch.FloatTensor)  # setting tensor type also affects this\n>>> torch.get_default_dtype()  # changed to torch.float32, the dtype for torch.FloatTensor\ntorch.float32",
        "source": "https://pytorch.org/docs/stable/generated/torch.get_default_dtype.html#torch.get_default_dtype"
    },
    {
        "X": "How to use torch.cartesian_prod, give an example?",
        "Y": ">>> a = [1, 2, 3]\n>>> b = [4, 5]\n>>> list(itertools.product(a, b))\n[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n>>> tensor_a = torch.tensor(a)\n>>> tensor_b = torch.tensor(b)\n>>> torch.cartesian_prod(tensor_a, tensor_b)\ntensor([[1, 4],\n        [1, 5],\n        [2, 4],\n        [2, 5],\n        [3, 4],\n        [3, 5]])",
        "Z": ">>> a = [1, 2, 3]\n>>> b = [4, 5]\n>>> list(itertools.product(a, b))\n[(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]\n>>> tensor_a = torch.tensor(a)\n>>> tensor_b = torch.tensor(b)\n>>> torch.cartesian_prod(tensor_a, tensor_b)\ntensor([[1, 4],\n        [1, 5],\n        [2, 4],\n        [2, 5],\n        [3, 4],\n        [3, 5]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.cartesian_prod.html#torch.cartesian_prod"
    },
    {
        "X": "How to use torch.zeros_like, give an example?",
        "Y": ">>> input = torch.empty(2, 3)\n>>> torch.zeros_like(input)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])",
        "Z": ">>> input = torch.empty(2, 3)\n>>> torch.zeros_like(input)\ntensor([[ 0.,  0.,  0.],\n        [ 0.,  0.,  0.]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.zeros_like.html#torch.zeros_like"
    },
    {
        "X": "How to use torch.flatten, give an example?",
        "Y": ">>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.flatten(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n>>> torch.flatten(t, start_dim=1)\ntensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])",
        "Z": "Unlike NumPy\u2019s flatten, which always copies input\u2019s data, this function may return the original object, a view,\nor copy. If no dimensions are flattened, then the original object input is returned. Otherwise, if input can\nbe viewed as the flattened shape, then that view is returned. Finally, only if the input cannot be viewed as the\nflattened shape is input\u2019s data copied. See torch.Tensor.view() for details on when a view will be returned. >>> t = torch.tensor([[[1, 2],\n...                    [3, 4]],\n...                   [[5, 6],\n...                    [7, 8]]])\n>>> torch.flatten(t)\ntensor([1, 2, 3, 4, 5, 6, 7, 8])\n>>> torch.flatten(t, start_dim=1)\ntensor([[1, 2, 3, 4],\n        [5, 6, 7, 8]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten"
    },
    {
        "X": "How to use torch.bitwise_and, give an example?",
        "Y": ">>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([1, 0,  3], dtype=torch.int8)\n>>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ False, True, False])",
        "Z": ">>> torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))\ntensor([1, 0,  3], dtype=torch.int8)\n>>> torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))\ntensor([ False, True, False])",
        "source": "https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and"
    },
    {
        "X": "How to use torch.digamma, give an example?",
        "Y": ">>> a = torch.tensor([1, 0.5])\n>>> torch.digamma(a)\ntensor([-0.5772, -1.9635])",
        "Z": ">>> a = torch.tensor([1, 0.5])\n>>> torch.digamma(a)\ntensor([-0.5772, -1.9635])",
        "source": "https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma"
    },
    {
        "X": "How to use This feature is under a Beta release and its API may change.FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action:, give an example?",
        "Y": "import torch\n# Simple module for demonstration\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\nmodule = MyModule()\n\nfrom torch.fx import symbolic_trace\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\n# High-level intermediate representation (IR) - Graph representation\nprint(symbolic_traced.graph)\n\"\"\"\ngraph(x):\n    %param : [#users=1] = self.param\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %param), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %clamp_1 : [#users=1] = call_method[target=clamp](args = (%linear_1,), kwargs = {min: 0.0, max: 1.0})\n    return clamp_1\n\"\"\"\n\n# Code generation - valid Python code\nprint(symbolic_traced.code)\n\"\"\"\ndef forward(self, x):\n    param = self.param\n    add_1 = x + param;  x = param = None\n    linear_1 = self.linear(add_1);  add_1 = None\n    clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None\n    return clamp_1\n\"\"\"",
        "Z": "FX is a toolkit for developers to use to transform nn.Module\ninstances. FX consists of three main components: a symbolic tracer,\nan intermediate representation, and Python code generation. A\ndemonstration of these components in action: import torch\n# Simple module for demonstration\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return self.linear(x + self.param).clamp(min=0.0, max=1.0)\n\nmodule = MyModule()\n\nfrom torch.fx import symbolic_trace\n# Symbolic tracing frontend - captures the semantics of the module\nsymbolic_traced : torch.fx.GraphModule = symbolic_trace(module)\n\n# High-level intermediate representation (IR) - Graph representation\nprint(symbolic_traced.graph)\n\"\"\"\ngraph(x):\n    %param : [#users=1] = self.param\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %param), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %clamp_1 : [#users=1] = call_method[target=clamp](args = (%linear_1,), kwargs = {min: 0.0, max: 1.0})\n    return clamp_1\n\"\"\"\n\n# Code generation - valid Python code\nprint(symbolic_traced.code)\n\"\"\"\ndef forward(self, x):\n    param = self.param\n    add_1 = x + param;  x = param = None\n    linear_1 = self.linear(add_1);  add_1 = None\n    clamp_1 = linear_1.clamp(min = 0.0, max = 1.0);  linear_1 = None\n    return clamp_1\n\"\"\"",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use What is an FX transform? Essentially, it\u2019s a function that looks like this., give an example?",
        "Y": "import torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)",
        "Z": "What is an FX transform? Essentially, it\u2019s a function that looks like this. import torch\nimport torch.fx\n\ndef transform(m: nn.Module,\n              tracer_class : type = torch.fx.Tracer) -> torch.nn.Module:\n    # Step 1: Acquire a Graph representing the code in `m`\n\n    # NOTE: torch.fx.symbolic_trace is a wrapper around a call to\n    # fx.Tracer.trace and constructing a GraphModule. We'll\n    # split that out in our transform to allow the caller to\n    # customize tracing behavior.\n    graph : torch.fx.Graph = tracer_class().trace(m)\n\n    # Step 2: Modify this Graph or create a new one\n    graph = ...\n\n    # Step 3: Construct a Module to return\n    return torch.fx.GraphModule(m, graph)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use NoteIt is also possible to modify an existing GraphModule instead of\ncreating a new one, like so:, give an example?",
        "Y": "import torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module:\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm",
        "Z": "It is also possible to modify an existing GraphModule instead of\ncreating a new one, like so: import torch\nimport torch.fx\n\ndef transform(m : nn.Module) -> nn.Module:\n    gm : torch.fx.GraphModule = torch.fx.symbolic_trace(m)\n\n    # Modify gm.graph\n    # <...>\n\n    # Recompile the forward() method of `gm` from its Graph\n    gm.recompile()\n\n    return gm",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Full treatment of the semantics of graphs can be found in the Graph\ndocumentation, but we are going to cover the basics here. A Graph is\na data structure that represents a method on a GraphModule. The\ninformation that this requires is:All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example:, give an example?",
        "Y": "import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()",
        "Z": "All three of these concepts are represented with Node instances.\nLet\u2019s see what we mean by that with a short example: import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(\n            self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)\n\ngm.graph.print_tabular()",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls., give an example?",
        "Y": "import torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)",
        "Z": "One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls. import torch\nimport torch.fx\n\n# Sample module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return torch.add(x, y)\n\ndef transform(m: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph : fx.Graph = tracer_class().trace(m)\n    # FX represents its Graph as an ordered list of\n    # nodes, so we can iterate through them.\n    for node in graph.nodes:\n        # Checks if we're calling a function (i.e:\n        # torch.add)\n        if node.op == 'call_function':\n            # The target attribute is the function\n            # that call_function calls.\n            if node.target == torch.add:\n                node.target = torch.mul\n\n    graph.lint() # Does some checks to make sure the\n                 # Graph is well-formed.\n\n    return fx.GraphModule(m, graph)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use One approach to building this new Graph is to directly manipulate your old\none. To aid in this, we can simply take the Graph we obtain from symbolic\ntracing and modify it. For example, let\u2019s say we desire to replace\ntorch.add() calls with torch.mul() calls.We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below., give an example?",
        "Y": "# Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)",
        "Z": "We can also do more involved Graph rewrites, such as\ndeleting or appending nodes. To aid in these transformations,\nFX has utility functions for transforming the graph that can\nbe found in the Graph documentation. An\nexample of using these APIs to append a torch.relu() call\ncan be found below. # Specifies the insertion point. Any nodes added to the\n# Graph within this scope will be inserted after `node`\nwith traced.graph.inserting_after(node):\n    # Insert a new `call_function` node calling `torch.relu`\n    new_node = traced.graph.call_function(\n        torch.relu, args=(node,))\n\n    # We want all places that used the value of `node` to\n    # now use that value after the `relu` call we've added.\n    # We use the `replace_all_uses_with` API to do this.\n    node.replace_all_uses_with(new_node)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Another way of manipulating Graphs is by reusing the Proxy\nmachinery used in symbolic tracing. For example, let\u2019s\nimagine that we wanted to write a transformation that decomposed\nPyTorch functions into smaller operations. It would transform every\nF.relu(x) call into (x > 0) * x. One possibility would be to\nperform the requisite graph rewriting to insert the comparison and\nmultiplication after the F.relu, and then clean up the original\nF.relu. However, we can automate this process by using Proxy\nobjects to automatically record operations into the Graph.To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph., give an example?",
        "Y": "# Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name]) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)",
        "Z": "To use this method, we write the operations that we want inserted as regular\nPyTorch code and invoke that code with Proxy objects as arugments.\nThese Proxy objects will capture the operations that are performed\non them and append them to the Graph. # Note that this decomposition rule can be read as regular Python\ndef relu_decomposition(x):\n    return (x > 0) * x\n\ndecomposition_rules = {}\ndecomposition_rules[F.relu] = relu_decomposition\n\ndef decompose(model: torch.nn.Module,\n              tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    \"\"\"\n    Decompose `model` into smaller constituent operations.\n    Currently,this only supports decomposing ReLU into its\n    mathematical definition: (x > 0) * x\n    \"\"\"\n    graph : fx.Graph = tracer_class().trace(model)\n    new_graph = fx.Graph()\n    env = {}\n    for node in graph.nodes:\n        if node.op == 'call_function' and node.target in decomposition_rules:\n            # By wrapping the arguments with proxies,\n            # we can dispatch to the appropriate\n            # decomposition rule and implicitly add it\n            # to the Graph by symbolically tracing it.\n            proxy_args = [\n                fx.Proxy(env[x.name]) if isinstance(x, fx.Node) else x for x in node.args]\n            output_proxy = decomposition_rules[node.target](*proxy_args)\n\n            # Operations on `Proxy` always yield new `Proxy`s, and the\n            # return value of our decomposition rule is no exception.\n            # We need to extract the underlying `Node` from the `Proxy`\n            # to use it in subsequent iterations of this transform.\n            new_node = output_proxy.node\n            env[node.name] = new_node\n        else:\n            # Default case: we don't have a decomposition rule for this\n            # node, so just copy the node over into the new graph.\n            new_node = new_graph.node_copy(node, lambda x: env[x.name])\n            env[node.name] = new_node\n    return fx.GraphModule(model, new_graph)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like:, give an example?",
        "Y": "import torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)",
        "Z": "A useful code organizational pattern in FX is to loop over all the Nodes\nin a Graph and execute them. This can be used for several things including\nruntime analysis of values flowing through the graph or transformation of the code\nvia retracing with Proxys. For example, suppose we want to run a\nGraphModule and record the torch.Tensor shape and dtype\nproperties on the nodes as we see them at runtime. That might look like: import torch\nimport torch.fx\nfrom torch.fx.node import Node\n\nfrom typing import Dict\n\nclass ShapeProp:\n    \"\"\"\n    Shape propagation. This class takes a `GraphModule`.\n    Then, its `propagate` method executes the `GraphModule`\n    node-by-node with the given arguments. As each operation\n    executes, the ShapeProp class stores away the shape and\n    element type for the output values of each operation on\n    the `shape` and `dtype` attributes of the operation's\n    `Node`.\n    \"\"\"\n    def __init__(self, mod):\n        self.mod = mod\n        self.graph = mod.graph\n        self.modules = dict(self.mod.named_modules())\n\n    def propagate(self, *args):\n        args_iter = iter(args)\n        env : Dict[str, Node] = {}\n\n        def load_arg(a):\n            return torch.fx.graph.map_arg(a, lambda n: env[n.name])\n\n        def fetch_attr(target : str):\n            target_atoms = target.split('.')\n            attr_itr = self.mod\n            for i, atom in enumerate(target_atoms):\n                if not hasattr(attr_itr, atom):\n                    raise RuntimeError(f\"Node referenced nonexistant target {'.'.join(target_atoms[:i])}\")\n                attr_itr = getattr(attr_itr, atom)\n            return attr_itr\n\n        for node in self.graph.nodes:\n            if node.op == 'placeholder':\n                result = next(args_iter)\n            elif node.op == 'get_attr':\n                result = fetch_attr(node.target)\n            elif node.op == 'call_function':\n                result = node.target(*load_arg(node.args), **load_arg(node.kwargs))\n            elif node.op == 'call_method':\n                self_obj, *args = load_arg(node.args)\n                kwargs = load_arg(node.kwargs)\n                result = getattr(self_obj, node.target)(*args, **kwargs)\n            elif node.op == 'call_module':\n                result = self.modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))\n\n            # This is the only code specific to shape propagation.\n            # you can delete this `if` branch and this becomes\n            # a generic GraphModule interpreter.\n            if isinstance(result, torch.Tensor):\n                node.shape = result.shape\n                node.dtype = result.dtype\n\n            env[node.name] = result\n\n        return load_arg(self.graph.result)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample:, give an example?",
        "Y": "import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"",
        "Z": "Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample: import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef transform(m : torch.nn.Module) -> torch.nn.Module:\n    gm = torch.fx.symbolic_trace(m)\n\n    # Imagine we're doing some transforms here\n    # <...>\n\n    gm.recompile()\n\n    return gm\n\nresnet18 = models.resnet18()\ntransformed_resnet18 = transform(resnet18)\n\ninput_image = torch.randn(5, 3, 224, 224)\n\nassert resnet18(input_image) == transformed_resnet18(input_image)\n\"\"\"\nRuntimeError: Boolean value of Tensor with more than one value is ambiguous\n\"\"\"",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Because the output of most deep learning modules consists of floating\npoint torch.Tensor instances, checking for equivalence between\nthe results of two torch.nn.Module is not as straightforward\nas doing a simple equality check. To motivate this, let\u2019s use an\nexample:Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold:, give an example?",
        "Y": "assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))",
        "Z": "Here, we\u2019ve tried to check equality of the values of two deep learning\nmodels with the == equality operator. However, this is not well-\ndefined both due to the issue of that operator returning a tensor\nand not a bool, but also because comparison of floating point values\nshould use a margin of error (or epsilon) to account for the\nnon-commutativity of floating point operations (see\nhere for more\ndetails). We can use torch.allclose() instead, which will give\nus an approximate comparison taking into account a relative and\nabsolute tolerance threshold: assert torch.allclose(resnet18(input_image), transformed_resnet18(input_image))",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked., give an example?",
        "Y": "import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)",
        "Z": "Invoke pdb to step into the running program. Although the code that\nrepresents the Graph is not in any source file, we can still step\ninto it manually using pdb when the forward pass is invoked. import torch\nimport torch.fx\nimport torchvision.models as models\n\ndef my_pass(inp: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    graph = tracer_class().trace(inp)\n    # Transformation logic here\n    # <...>\n\n    # Return new Module\n    return fx.GraphModule(inp, graph)\n\nmy_module = models.resnet18()\nmy_module_transformed = my_pass(my_module)\n\ninput_value = torch.randn(5, 3, 224, 224)\n\n# When this line is executed at runtime, we will be dropped into an\n# interactive `pdb` prompt. We can use the `step` or `s` command to\n# step into the execution of the next line\nimport pdb; pdb.set_trace()\n\nmy_module_transformed(input_value)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there., give an example?",
        "Y": "# Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n    x = self.x\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM()",
        "Z": "If you\u2019d like to run the same code multiple times, then it can be\na bit tedious to step to the right code with pdb. In that case, one\napproach is to simply copy-paste the generated forward pass into\nyour code and examine it from there. # Assume that `traced` is a GraphModule that has undergone some\n# number of transforms\n\n# Copy this code for later\nprint(traced)\n# Print the code generated from symbolic tracing. This outputs:\n\"\"\"\ndef forward(self, y):\n    x = self.x\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Subclass the original Module\nclass SubclassM(M):\n    def __init__(self):\n        super().__init__()\n\n    # Paste the generated `forward` function (the one we printed and\n    # copied above) here\n    def forward(self, y):\n        x = self.x\n        add_1 = x + y;  x = y = None\n        return add_1\n\n# Create an instance of the original, untraced Module. Then, create an\n# instance of the Module with the copied `forward` function. We can\n# now compare the output of both the original and the traced version.\npre_trace = M()\npost_trace = SubclassM()",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder., give an example?",
        "Y": "m = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar()",
        "Z": "GraphModule.to_folder() is a method in GraphModule that allows\nyou to dump out the generated FX code to a folder. Although copying the\nforward pass into the code often suffices as in Print the Generated Code,\nit may be easier to examine modules and parameters using to_folder. m = symbolic_trace(M())\nm.to_folder(\"foo\", \"Bar\")\nfrom foo import Bar\ny = Bar()",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module:, give an example?",
        "Y": "# Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph(x, y):\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %y), kwargs = {})\n    return add_1\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args      kwargs\n-------------  ------  -----------------------  --------  --------\nplaceholder    x       x                        ()        {}\nplaceholder    y       y                        ()        {}\ncall_function  add_1   <built-in function add>  (x, y)    {}\n\"\"\"",
        "Z": "Now that we\u2019ve identified that a transformation is creating incorrect\ncode, it\u2019s time to debug the transformation itself. First, we\u2019ll check\nthe Limitations of Symbolic Tracing section in the documentation.\nOnce we verify that tracing is working as expected, the goal\nbecomes figuring out what went wrong during our GraphModule\ntransformation. There may be a quick answer in\nWriting Transformations, but, if not, there are several ways to\nexamine our traced module: # Sample Module\nclass M(torch.nn.Module):\n    def forward(self, x, y):\n        return x + y\n\n# Create an instance of `M`\nm = M()\n\n# Symbolically trace an instance of `M` (returns a GraphModule). In\n# this example, we'll only be discussing how to inspect a\n# GraphModule, so we aren't showing any sample transforms for the\n# sake of brevity.\ntraced = symbolic_trace(m)\n\n# Print the code produced by tracing the module.\nprint(traced)\n# The generated `forward` function is:\n\"\"\"\ndef forward(self, x, y):\n    add_1 = x + y;  x = y = None\n    return add_1\n\"\"\"\n\n# Print the internal Graph.\nprint(traced.graph)\n# This print-out returns:\n\"\"\"\ngraph(x, y):\n    %add_1 : [#users=1] = call_function[target=<built-in function add>](args = (%x, %y), kwargs = {})\n    return add_1\n\"\"\"\n\n# Print a tabular representation of the internal Graph.\ntraced.graph.print_tabular()\n# This gives us:\n\"\"\"\nopcode         name    target                   args      kwargs\n-------------  ------  -----------------------  --------  --------\nplaceholder    x       x                        ()        {}\nplaceholder    y       y                        ()        {}\ncall_function  add_1   <built-in function add>  (x, y)    {}\n\"\"\"",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.Going off of the example above, consider the following code:, give an example?",
        "Y": "# Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed)",
        "Z": "Using the utility functions above, we can compare our traced Module\nbefore and after we\u2019ve applied our transformations. Sometimes, a\nsimple visual comparison is enough to trace down a bug. If it\u2019s still\nnot clear what\u2019s going wrong, a debugger like pdb can be a good\nnext step.Going off of the example above, consider the following code: # Sample user-defined function\ndef transform_graph(module: torch.nn.Module, tracer_class : type = fx.Tracer) -> torch.nn.Module:\n    # Get the Graph from our traced Module\n    g = tracer_class().trace(module)\n\n    \"\"\"\n    Transformations on `g` go here\n    \"\"\"\n\n    return fx.GraphModule(module, g)\n\n# Transform the Graph\ntransformed = transform_graph(traced)\n\n# Print the new code after our transforms. Check to see if it was\n# what we expected\nprint(transformed)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use The main limitation of symbolic tracing is it does not currently support\ndynamic control flow. That is, loops or if statements where the\ncondition may depend on the input values of the program.For example, let\u2019s examine the following program:, give an example?",
        "Y": "def func_to_trace(x):\n    if x.sum() > 0:\n        return torch.relu(x)\n    else:\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n  <...>\n  File \"dyn.py\", line 6, in func_to_trace\n    if x.sum() > 0:\n  File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n    return self.tracer.to_bool(self)\n  File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\"",
        "Z": "For example, let\u2019s examine the following program: def func_to_trace(x):\n    if x.sum() > 0:\n        return torch.relu(x)\n    else:\n        return torch.neg(x)\n\ntraced = torch.fx.symbolic_trace(func_to_trace)\n\"\"\"\n  <...>\n  File \"dyn.py\", line 6, in func_to_trace\n    if x.sum() > 0:\n  File \"pytorch/torch/fx/proxy.py\", line 155, in __bool__\n    return self.tracer.to_bool(self)\n  File \"pytorch/torch/fx/proxy.py\", line 85, in to_bool\n    raise TraceError('symbolically traced variables cannot be used as inputs to control flow')\ntorch.fx.proxy.TraceError: symbolically traced variables cannot be used as inputs to control flow\n\"\"\"",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example:, give an example?",
        "Y": "import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    relu_1 = torch.relu(linear_1);  linear_1 = None\n    return relu_1\n\"\"\"",
        "Z": "On the other hand, so-called static control flow is supported. Static\ncontrol flow is loops or if statements whose value cannot change\nacross invocations. Typically, in PyTorch programs, this control flow\narises for code making decisions about a model\u2019s architecture based on\nhyper-parameters. As a concrete example: import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self, do_activation : bool = False):\n        super().__init__()\n        self.do_activation = do_activation\n        self.linear = torch.nn.Linear(512, 512)\n\n    def forward(self, x):\n        x = self.linear(x)\n        # This if-statement is so-called static control flow.\n        # Its condition does not depend on any input values\n        if self.do_activation:\n            x = torch.relu(x)\n        return x\n\nwithout_activation = MyModule(do_activation=False)\nwith_activation = MyModule(do_activation=True)\n\ntraced_without_activation = torch.fx.symbolic_trace(without_activation)\nprint(traced_without_activation.code)\n\"\"\"\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    return linear_1\n\"\"\"\n\ntraced_with_activation = torch.fx.symbolic_trace(with_activation)\nprint(traced_with_activation.code)\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    relu_1 = torch.relu(linear_1);  linear_1 = None\n    return relu_1\n\"\"\"",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing:, give an example?",
        "Y": "def f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\nfx.symbolic_trace(f, concrete_args={'flag': True})",
        "Z": "The if-statement if self.do_activation does not depend on any\nfunction inputs, thus it is static. do_activation can be considered\nto be a hyper-parameter, and the traces of different instances of\nMyModule with different values for that parameter have different\ncode. This is a valid pattern that is supported by symbolic tracing.Many instances of dynamic control flow are semantically static control\nflow. These instances can be made to support symbolic tracing by\nremoving the data dependencies on input values, for example by moving\nvalues to Module attributes or by binding concrete values to arguments\nduring symbolic tracing: def f(x, flag):\n    if flag: return x\n    else: return x*2\n\nfx.symbolic_trace(f) # Fails!\n\nfx.symbolic_trace(f, concrete_args={'flag': True})",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example:, give an example?",
        "Y": "import torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    \"\"\"\n    Normalize `x` by the size of the batch dimension\n    \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n  <...>\n  File \"sqrt.py\", line 9, in normalize\n    return x / sqrt(len(x))\n  File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\"",
        "Z": "FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example: import torch\nimport torch.fx\nfrom math import sqrt\n\ndef normalize(x):\n    \"\"\"\n    Normalize `x` by the size of the batch dimension\n    \"\"\"\n    return x / sqrt(len(x))\n\n# It's valid Python code\nnormalize(torch.rand(3, 4))\n\ntraced = torch.fx.symbolic_trace(normalize)\n\"\"\"\n  <...>\n  File \"sqrt.py\", line 9, in normalize\n    return x / sqrt(len(x))\n  File \"pytorch/torch/fx/proxy.py\", line 161, in __len__\n    raise RuntimeError(\"'len' is not supported in symbolic tracing by default. If you want \"\nRuntimeError: 'len' is not supported in symbolic tracing by default. If you want this call to be recorded, please call torch.fx.wrap('len') at module scope\n\"\"\"",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use FX uses __torch_function__ as the mechanism by which it intercepts\ncalls (see the technical\noverview\nfor more information about this). Some functions, such as builtin Python\nfunctions or those in the math module, are not covered by\n__torch_function__, but we would still like to capture them in\nsymbolic tracing. For example:The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API:, give an example?",
        "Y": "torch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n    len_1 = len(x)\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n    return truediv\n\"\"\"",
        "Z": "The error tells us that the built-in function len is not supported.\nWe can make it so that functions like this are recorded in the trace as\ndirect calls using the wrap() API: torch.fx.wrap('len')\ntorch.fx.wrap('sqrt')\n\ntraced = torch.fx.symbolic_trace(normalize)\n\nprint(traced.code)\n\"\"\"\nimport math\ndef forward(self, x):\n    len_1 = len(x)\n    sqrt_1 = math.sqrt(len_1);  len_1 = None\n    truediv = x / sqrt_1;  x = sqrt_1 = None\n    return truediv\n\"\"\"",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so:, give an example?",
        "Y": "class MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph)",
        "Z": "The Tracer class is the class that underlies the\nimplementation of symbolic_trace. The behavior of tracing can be\ncustomized by subclassing Tracer, like so: class MyCustomTracer(torch.fx.Tracer):\n    # Inside here you can override various methods\n    # to customize tracing. See the `Tracer` API\n    # reference\n    pass\n\n\n# Let's use this custom tracer to trace through this module\nclass MyModule(torch.nn.Module):\n    def forward(self, x):\n        return torch.relu(x) + torch.ones(3, 4)\n\nmod = MyModule()\n\ntraced_graph = MyCustomTracer().trace(mod)\n# trace() returns a Graph. Let's wrap it up in a\n# GraphModule to make it runnable\ntraced = torch.fx.GraphModule(mod, traced_graph)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example:, give an example?",
        "Y": "class MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    neg_1 = torch.neg(linear_1);  linear_1 = None\n    return neg_1\n\"\"\"",
        "Z": "Leaf Modules are the modules that appear as calls in the symbolic trace\nrather than being traced through. The default set of leaf modules is the\nset of standard torch.nn module instances. For example: class MySpecialSubmodule(torch.nn.Module):\n    def forward(self, x):\n        return torch.neg(x)\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = torch.nn.Linear(3, 4)\n        self.submod = MySpecialSubmodule()\n\n    def forward(self, x):\n        return self.submod(self.linear(x))\n\ntraced = torch.fx.symbolic_trace(MyModule())\nprint(traced.code)\n# `linear` is preserved as a call, yet `submod` is traced though.\n# This is because the default set of \"Leaf Modules\" includes all\n# standard `torch.nn` modules.\n\"\"\"\nimport torch\ndef forward(self, x):\n    linear_1 = self.linear(x);  x = None\n    neg_1 = torch.neg(linear_1);  linear_1 = None\n    return neg_1\n\"\"\"",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Miscellanea, give an example?",
        "Y": "@torch.fx.wrap\ndef torch_randn(x, shape):\n    return torch.randn(shape)\n\ndef f(x):\n    return x + torch_randn(x, 5)\nfx.symbolic_trace(f)",
        "Z": "@torch.fx.wrap\ndef torch_randn(x, shape):\n    return torch.randn(shape)\n\ndef f(x):\n    return x + torch_randn(x, 5)\nfx.symbolic_trace(f)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use torch.fx.symbolic_trace, give an example?",
        "Y": "def f(a, b):\n    if b == True:\n        return a\n    else:\n        return a*2",
        "Z": "concrete_args allows you to partially specialize your function, whether it\u2019s to remove control flow or data structures.For example: def f(a, b):\n    if b == True:\n        return a\n    else:\n        return a*2",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How  Note that although you can still pass in different values of b, they will be ignored.We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example:, give an example?",
        "Y": "def f(x):\n    out = 0\n    for v in x.values():\n        out += v\n    return out\nf = fx.symbolic_trace(f, concrete_args={'x': {'a': fx.PH, 'b': fx.PH, 'c': fx.PH}})\nassert f({'a': 1, 'b': 2, 'c': 4}) == 7",
        "Z": "Note that although you can still pass in different values of b, they will be ignored.We can also use concrete_args to eliminate data-structure handling from\nour function. This will use pytrees to flatten your input. To avoid\noverspecializing, pass in fx.PH for values that shouldn\u2019t be\nspecialized. For example: def f(x):\n    out = 0\n    for v in x.values():\n        out += v\n    return out\nf = fx.symbolic_trace(f, concrete_args={'x': {'a': fx.PH, 'b': fx.PH, 'c': fx.PH}})\nassert f({'a': 1, 'b': 2, 'c': 4}) == 7",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use torch.fx.wrap, give an example?",
        "Y": "# foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\ntorch.fx.wrap('my_custom_function')\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)",
        "Z": "This function can be called at module-level scope to register fn_or_name as a \u201cleaf function\u201d.\nA \u201cleaf function\u201d will be preserved as a CallFunction node in the FX trace instead of being\ntraced through: # foo/bar/baz.py\ndef my_custom_function(x, y):\n    return x * x + y * y\n\ntorch.fx.wrap('my_custom_function')\n\ndef fn_to_be_traced(x, y):\n    # When symbolic tracing, the below call to my_custom_function will be inserted into\n    # the graph rather than tracing it.\n    return my_custom_function(x, y)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How  This function can also equivalently be used as a decorator:, give an example?",
        "Y": "# foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y",
        "Z": "This function can also equivalently be used as a decorator: # foo/bar/baz.py\n@torch.fx.wrap\ndef my_custom_function(x, y):\n    return x * x + y * y",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use For example, the following code, give an example?",
        "Y": "import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)",
        "Z": "For example, the following code import torch\nimport torch.fx\n\nclass MyModule(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.param = torch.nn.Parameter(torch.rand(3, 4))\n        self.linear = torch.nn.Linear(4, 5)\n\n    def forward(self, x):\n        return torch.topk(torch.sum(self.linear(x + self.linear.weight).relu(), dim=-1), 3)\n\nm = MyModule()\ngm = torch.fx.symbolic_trace(m)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Will produce the following Graph:, give an example?",
        "Y": "print(gm.graph)",
        "Z": "For example, the following codeWill produce the following Graph: print(gm.graph)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use API Reference, give an example?",
        "Y": "graph(x):\n    %linear_weight : [#users=1] = self.linear.weight\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1",
        "Z": "graph(x):\n    %linear_weight : [#users=1] = self.linear.weight\n    %add_1 : [#users=1] = call_function[target=operator.add](args = (%x, %linear_weight), kwargs = {})\n    %linear_1 : [#users=1] = call_module[target=linear](args = (%add_1,), kwargs = {})\n    %relu_1 : [#users=1] = call_method[target=relu](args = (%linear_1,), kwargs = {})\n    %sum_1 : [#users=1] = call_function[target=torch.sum](args = (%relu_1,), kwargs = {dim: -1})\n    %topk_1 : [#users=1] = call_function[target=torch.topk](args = (%sum_1, 3), kwargs = {})\n    return topk_1",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use torch.fx.Graph.eliminate_dead_code, give an example?",
        "Y": "def forward(self, x):\n    a = x + 1\n    return x + self.attr_1",
        "Z": "Before dead code is eliminated, a from a = x + 1 below has no users\nand thus can be eliminated from the graph without having an effect. def forward(self, x):\n    a = x + 1\n    return x + self.attr_1",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How  Before dead code is eliminated, a from a = x + 1 below has no users\nand thus can be eliminated from the graph without having an effect.After dead code is eliminated, a = x + 1 has been removed, and the rest\nof forward remains., give an example?",
        "Y": "def forward(self, x):\n    return x + self.attr_1",
        "Z": "Before dead code is eliminated, a from a = x + 1 below has no users\nand thus can be eliminated from the graph without having an effect.After dead code is eliminated, a = x + 1 has been removed, and the rest\nof forward remains. def forward(self, x):\n    return x + self.attr_1",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use torch.fx.Graph.inserting_after, give an example?",
        "Y": "with g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently",
        "Z": "Set the point at which create_node and companion methods will insert into the graph.\nWhen used within a \u2018with\u2019 statement, this will temporary set the insert point and\nthen restore it when the with statement exits: with g.inserting_after(n):\n    ... # inserting after node n\n... # insert point restored to what it was previously\ng.inserting_after(n) #  set the insert point permanently",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use torch.fx.Graph.inserting_before, give an example?",
        "Y": "with g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently",
        "Z": "Set the point at which create_node and companion methods will insert into the graph.\nWhen used within a \u2018with\u2019 statement, this will temporary set the insert point and\nthen restore it when the with statement exits: with g.inserting_before(n):\n    ... # inserting before node n\n... # insert point restored to what it was previously\ng.inserting_before(n) #  set the insert point permanently",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use torch.fx.Graph.node_copy, give an example?",
        "Y": "# Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])",
        "Z": "# Copying all the nodes in `g` into `new_graph`\ng : torch.fx.Graph = ...\nnew_graph = torch.fx.graph()\nvalue_remap = {}\nfor node in g.nodes:\n    value_remap[node] = new_graph.node_copy(node, lambda n : value_remap[n])",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use torch.fx.Node.prepend, give an example?",
        "Y": "Before: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax",
        "Z": "Before: p -> self\n        bx -> x -> ax\nAfter:  p -> x -> self\n        bx -> ax",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Methods in the Interpreter class can be overridden to customize\nthe behavior of execution. The map of overrideable methods\nin terms of call hierarchy:, give an example?",
        "Y": "run()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()",
        "Z": "Methods in the Interpreter class can be overridden to customize\nthe behavior of execution. The map of overrideable methods\nin terms of call hierarchy: run()\n    +-- run_node\n        +-- placeholder()\n        +-- get_attr()\n        +-- call_function()\n        +-- call_method()\n        +-- call_module()\n        +-- output()",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Suppose we want to swap all instances of torch.neg with\ntorch.sigmoid and vice versa (including their Tensor\nmethod equivalents). We could subclass Interpreter like so:, give an example?",
        "Y": "class NegSigmSwapInterpreter(Interpreter):\n    def call_function(self, target : Target,\n                      args : Tuple, kwargs : Dict) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : Target,\n                    args : Tuple, kwargs : Dict) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_allclose(result, torch.neg(input).sigmoid())",
        "Z": "Suppose we want to swap all instances of torch.neg with\ntorch.sigmoid and vice versa (including their Tensor\nmethod equivalents). We could subclass Interpreter like so: class NegSigmSwapInterpreter(Interpreter):\n    def call_function(self, target : Target,\n                      args : Tuple, kwargs : Dict) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : Target,\n                    args : Tuple, kwargs : Dict) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\ninput = torch.randn(3, 4)\nresult = NegSigmSwapInterpreter(gm).run(input)\ntorch.testing.assert_allclose(result, torch.neg(input).sigmoid())",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use Suppose we want to swap all instances of torch.neg with\ntorch.sigmoid and vice versa (including their Tensor\nmethod equivalents). We could subclass Transformer like so:, give an example?",
        "Y": "class NegSigmSwapXformer(Transformer):\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())",
        "Z": "Suppose we want to swap all instances of torch.neg with\ntorch.sigmoid and vice versa (including their Tensor\nmethod equivalents). We could subclass Transformer like so: class NegSigmSwapXformer(Transformer):\n    def call_function(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == torch.sigmoid:\n            return torch.neg(*args, **kwargs)\n        return super().call_function(n)\n\n    def call_method(self, target : 'Target', args : Tuple[Argument, ...], kwargs : Dict[str, Any]) -> Any:\n        if target == 'neg':\n            call_self, *args_tail = args\n            return call_self.sigmoid(*args_tail, **kwargs)\n        return super().call_method(n)\n\ndef fn(x):\n    return torch.sigmoid(x).neg()\n\ngm = torch.fx.symbolic_trace(fn)\n\ntransformed : torch.nn.Module = NegSigmSwapXformer(gm).transform()\ninput = torch.randn(3, 4)\ntorch.testing.assert_allclose(transformed(input), torch.neg(input).sigmoid())",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use torch.fx.replace_pattern, give an example?",
        "Y": "class Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]",
        "Z": "A list of Match objects representing the places\nin the original graph that pattern was matched to. The list\nis empty if there are no matches. Match is defined as: class Match(NamedTuple):\n    # Node from which the match was found\n    anchor: Node\n    # Maps nodes in the pattern subgraph to nodes in the larger graph\n    nodes_map: Dict[Node, Node]",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How  When the pattern is matched, it will be removed from the larger\nfunction and replaced by replacement. If there are multiple\nmatches for pattern in the larger function, each non-overlapping\nmatch will be replaced. In the case of a match overlap, the first\nfound match in the set of overlapping matches will be replaced.\n(\u201cFirst\u201d here being defined as the first in a topological ordering\nof the Nodes\u2019 use-def relationships. In most cases, the first Node\nis the parameter that appears directly after self, while the\nlast Node is whatever the function returns.)One important thing to note is that the parameters of the\npattern Callable must be used in the Callable itself,\nand the parameters of the replacement Callable must match\nthe pattern. The first rule is why, in the above code block, the\nforward function has parameters x, w1, w2, but the\npattern function only has parameters w1, w2. pattern\ndoesn\u2019t use x, so it shouldn\u2019t specify x as a parameter.\nAs an example of the second rule, consider replacing, give an example?",
        "Y": "def pattern(x, y):\n    return torch.neg(x) + torch.relu(y)",
        "Z": "When the pattern is matched, it will be removed from the larger\nfunction and replaced by replacement. If there are multiple\nmatches for pattern in the larger function, each non-overlapping\nmatch will be replaced. In the case of a match overlap, the first\nfound match in the set of overlapping matches will be replaced.\n(\u201cFirst\u201d here being defined as the first in a topological ordering\nof the Nodes\u2019 use-def relationships. In most cases, the first Node\nis the parameter that appears directly after self, while the\nlast Node is whatever the function returns.)One important thing to note is that the parameters of the\npattern Callable must be used in the Callable itself,\nand the parameters of the replacement Callable must match\nthe pattern. The first rule is why, in the above code block, the\nforward function has parameters x, w1, w2, but the\npattern function only has parameters w1, w2. pattern\ndoesn\u2019t use x, so it shouldn\u2019t specify x as a parameter.\nAs an example of the second rule, consider replacing def pattern(x, y):\n    return torch.neg(x) + torch.relu(y)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How  One important thing to note is that the parameters of the\npattern Callable must be used in the Callable itself,\nand the parameters of the replacement Callable must match\nthe pattern. The first rule is why, in the above code block, the\nforward function has parameters x, w1, w2, but the\npattern function only has parameters w1, w2. pattern\ndoesn\u2019t use x, so it shouldn\u2019t specify x as a parameter.\nAs an example of the second rule, consider replacingwith, give an example?",
        "Y": "def replacement(x, y):\n    return torch.relu(x)",
        "Z": "One important thing to note is that the parameters of the\npattern Callable must be used in the Callable itself,\nand the parameters of the replacement Callable must match\nthe pattern. The first rule is why, in the above code block, the\nforward function has parameters x, w1, w2, but the\npattern function only has parameters w1, w2. pattern\ndoesn\u2019t use x, so it shouldn\u2019t specify x as a parameter.\nAs an example of the second rule, consider replacingwith def replacement(x, y):\n    return torch.relu(x)",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How  In this case, replacement needs the same number of parameters\nas pattern (both x and y), even though the parameter\ny isn\u2019t used in replacement.After calling subgraph_rewriter.replace_pattern, the generated\nPython code looks like this:, give an example?",
        "Y": "def forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2",
        "Z": "In this case, replacement needs the same number of parameters\nas pattern (both x and y), even though the parameter\ny isn\u2019t used in replacement.After calling subgraph_rewriter.replace_pattern, the generated\nPython code looks like this: def forward(self, x, w1, w2):\n    stack_1 = torch.stack([w1, w2])\n    sum_1 = stack_1.sum()\n    stack_2 = torch.stack([w1, w2])\n    sum_2 = stack_2.sum()\n    max_1 = torch.max(sum_1)\n    add_1 = x + max_1\n    max_2 = torch.max(sum_2)\n    add_2 = add_1 + max_2\n    return add_2",
        "source": "https://pytorch.org/docs/stable/fx.html"
    },
    {
        "X": "How to use torch.split, give an example?",
        "Y": ">>> a = torch.arange(10).reshape(5,2)\n>>> a\ntensor([[0, 1],\n        [2, 3],\n        [4, 5],\n        [6, 7],\n        [8, 9]])\n>>> torch.split(a, 2)\n(tensor([[0, 1],\n         [2, 3]]),\n tensor([[4, 5],\n         [6, 7]]),\n tensor([[8, 9]]))\n>>> torch.split(a, [1,4])\n(tensor([[0, 1]]),\n tensor([[2, 3],\n         [4, 5],\n         [6, 7],\n         [8, 9]]))",
        "Z": "If split_size_or_sections is a list, then tensor will be split\ninto len(split_size_or_sections) chunks with sizes in dim according\nto split_size_or_sections. >>> a = torch.arange(10).reshape(5,2)\n>>> a\ntensor([[0, 1],\n        [2, 3],\n        [4, 5],\n        [6, 7],\n        [8, 9]])\n>>> torch.split(a, 2)\n(tensor([[0, 1],\n         [2, 3]]),\n tensor([[4, 5],\n         [6, 7]]),\n tensor([[8, 9]]))\n>>> torch.split(a, [1,4])\n(tensor([[0, 1]]),\n tensor([[2, 3],\n         [4, 5],\n         [6, 7],\n         [8, 9]]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"
    },
    {
        "X": "How to use torch.sign, give an example?",
        "Y": ">>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> a\ntensor([ 0.7000, -1.2000,  0.0000,  2.3000])\n>>> torch.sign(a)\ntensor([ 1., -1.,  0.,  1.])",
        "Z": ">>> a = torch.tensor([0.7, -1.2, 0., 2.3])\n>>> a\ntensor([ 0.7000, -1.2000,  0.0000,  2.3000])\n>>> torch.sign(a)\ntensor([ 1., -1.,  0.,  1.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign"
    },
    {
        "X": "How to use torch.utils.cpp_extension.CppExtension, give an example?",
        "Y": ">>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CppExtension\n>>> setup(\n        name='extension',\n        ext_modules=[\n            CppExtension(\n                name='extension',\n                sources=['extension.cpp'],\n                extra_compile_args=['-g']),\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. >>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CppExtension\n>>> setup(\n        name='extension',\n        ext_modules=[\n            CppExtension(\n                name='extension',\n                sources=['extension.cpp'],\n                extra_compile_args=['-g']),\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How to use torch.utils.cpp_extension.CUDAExtension, give an example?",
        "Y": ">>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n>>> setup(\n        name='cuda_extension',\n        ext_modules=[\n            CUDAExtension(\n                    name='cuda_extension',\n                    sources=['extension.cpp', 'extension_kernel.cu'],\n                    extra_compile_args={'cxx': ['-g'],\n                                        'nvcc': ['-O2']})\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })",
        "Z": "All arguments are forwarded to the setuptools.Extension\nconstructor. >>> from setuptools import setup\n>>> from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n>>> setup(\n        name='cuda_extension',\n        ext_modules=[\n            CUDAExtension(\n                    name='cuda_extension',\n                    sources=['extension.cpp', 'extension_kernel.cu'],\n                    extra_compile_args={'cxx': ['-g'],\n                                        'nvcc': ['-O2']})\n        ],\n        cmdclass={\n            'build_ext': BuildExtension\n        })",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How to use torch.utils.cpp_extension.load, give an example?",
        "Y": ">>> from torch.utils.cpp_extension import load\n>>> module = load(\n        name='extension',\n        sources=['extension.cpp', 'extension_kernel.cu'],\n        extra_cflags=['-O2'],\n        verbose=True)",
        "Z": "CUDA support with mixed compilation is provided. Simply pass CUDA source\nfiles (.cu or .cuh) along with other sources. Such files will be\ndetected and compiled with nvcc rather than the C++ compiler. This includes\npassing the CUDA lib64 directory as a library directory, and linking\ncudart. You can pass additional flags to nvcc via\nextra_cuda_cflags, just like with extra_cflags for C++. Various\nheuristics for finding the CUDA install directory are used, which usually\nwork fine. If not, setting the CUDA_HOME environment variable is the\nsafest option. >>> from torch.utils.cpp_extension import load\n>>> module = load(\n        name='extension',\n        sources=['extension.cpp', 'extension_kernel.cu'],\n        extra_cflags=['-O2'],\n        verbose=True)",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How to use torch.utils.cpp_extension.load_inline, give an example?",
        "Y": ">>> from torch.utils.cpp_extension import load_inline\n>>> source = \\'\\'\\'\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\n  return x.sin() + y.sin();\n}\n\\'\\'\\'\n>>> module = load_inline(name='inline_extension',\n                         cpp_sources=[source],\n                         functions=['sin_add'])",
        "Z": "See load() for a description of arguments omitted below. >>> from torch.utils.cpp_extension import load_inline\n>>> source = \\'\\'\\'\nat::Tensor sin_add(at::Tensor x, at::Tensor y) {\n  return x.sin() + y.sin();\n}\n\\'\\'\\'\n>>> module = load_inline(name='inline_extension',\n                         cpp_sources=[source],\n                         functions=['sin_add'])",
        "source": "https://pytorch.org/docs/stable/cpp_extension.html"
    },
    {
        "X": "How to use torch.repeat_interleave, give an example?",
        "Y": ">>> x = torch.tensor([1, 2, 3])\n>>> x.repeat_interleave(2)\ntensor([1, 1, 2, 2, 3, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.repeat_interleave(y, 2)\ntensor([1, 1, 2, 2, 3, 3, 4, 4])\n>>> torch.repeat_interleave(y, 3, dim=1)\ntensor([[1, 1, 1, 2, 2, 2],\n        [3, 3, 3, 4, 4, 4]])\n>>> torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)\ntensor([[1, 2],\n        [3, 4],\n        [3, 4]])",
        "Z": ">>> x = torch.tensor([1, 2, 3])\n>>> x.repeat_interleave(2)\ntensor([1, 1, 2, 2, 3, 3])\n>>> y = torch.tensor([[1, 2], [3, 4]])\n>>> torch.repeat_interleave(y, 2)\ntensor([1, 1, 2, 2, 3, 3, 4, 4])\n>>> torch.repeat_interleave(y, 3, dim=1)\ntensor([[1, 1, 1, 2, 2, 2],\n        [3, 3, 3, 4, 4, 4]])\n>>> torch.repeat_interleave(y, torch.tensor([1, 2]), dim=0)\ntensor([[1, 2],\n        [3, 4],\n        [3, 4]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"
    },
    {
        "X": "How to use torch.add, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n>>> torch.add(a, 20)\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])",
        "Z": "If input is of type FloatTensor or DoubleTensor, other must be\na real number, otherwise it should be an integer. >>> a = torch.randn(4)\n>>> a\ntensor([ 0.0202,  1.0985,  1.3506, -0.6056])\n>>> torch.add(a, 20)\ntensor([ 20.0202,  21.0985,  21.3506,  19.3944])",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "How  If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer., give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\n>>> b = torch.randn(4, 1)\n>>> b\ntensor([[ 0.3743],\n        [-1.7724],\n        [-0.5811],\n        [-0.8017]])\n>>> torch.add(a, b, alpha=10)\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n        [-18.6971, -18.0736, -17.0994, -17.3216],\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])",
        "Z": "If other is of type FloatTensor or DoubleTensor, alpha must be\na real number, otherwise it should be an integer. >>> a = torch.randn(4)\n>>> a\ntensor([-0.9732, -0.3497,  0.6245,  0.4022])\n>>> b = torch.randn(4, 1)\n>>> b\ntensor([[ 0.3743],\n        [-1.7724],\n        [-0.5811],\n        [-0.8017]])\n>>> torch.add(a, b, alpha=10)\ntensor([[  2.7695,   3.3930,   4.3672,   4.1450],\n        [-18.6971, -18.0736, -17.0994, -17.3216],\n        [ -6.7845,  -6.1610,  -5.1868,  -5.4090],\n        [ -8.9902,  -8.3667,  -7.3925,  -7.6147]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.add.html#torch.add"
    },
    {
        "X": "How to use torch.kron, give an example?",
        "Y": ">>> mat1 = torch.eye(2)\n>>> mat2 = torch.ones(2, 2)\n>>> torch.kron(mat1, mat2)\ntensor([[1., 1., 0., 0.],\n        [1., 1., 0., 0.],\n        [0., 0., 1., 1.],\n        [0., 0., 1., 1.]])\n\n>>> mat1 = torch.eye(2)\n>>> mat2 = torch.arange(1, 5).reshape(2, 2)\n>>> torch.kron(mat1, mat2)\ntensor([[1., 2., 0., 0.],\n        [3., 4., 0., 0.],\n        [0., 0., 1., 2.],\n        [0., 0., 3., 4.]])",
        "Z": "Supports real-valued and complex-valued inputs. >>> mat1 = torch.eye(2)\n>>> mat2 = torch.ones(2, 2)\n>>> torch.kron(mat1, mat2)\ntensor([[1., 1., 0., 0.],\n        [1., 1., 0., 0.],\n        [0., 0., 1., 1.],\n        [0., 0., 1., 1.]])\n\n>>> mat1 = torch.eye(2)\n>>> mat2 = torch.arange(1, 5).reshape(2, 2)\n>>> torch.kron(mat1, mat2)\ntensor([[1., 2., 0., 0.],\n        [3., 4., 0., 0.],\n        [0., 0., 1., 2.],\n        [0., 0., 3., 4.]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.kron.html#torch.kron"
    },
    {
        "X": "How to use torch.polar, give an example?",
        "Y": ">>> import numpy as np\n>>> abs = torch.tensor([1, 2], dtype=torch.float64)\n>>> angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\n>>> z = torch.polar(abs, angle)\n>>> z\ntensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128)",
        "Z": ">>> import numpy as np\n>>> abs = torch.tensor([1, 2], dtype=torch.float64)\n>>> angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\n>>> z = torch.polar(abs, angle)\n>>> z\ntensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128)",
        "source": "https://pytorch.org/docs/stable/generated/torch.polar.html#torch.polar"
    },
    {
        "X": "How to use torch.use_deterministic_algorithms, give an example?",
        "Y": ">>> torch.use_deterministic_algorithms(True)\n\n# Forward mode nondeterministic error\n>>> torch.randn(10).index_copy(0, torch.tensor([0]), torch.randn(1))\n...\nRuntimeError: index_copy does not have a deterministic implementation...\n\n# Backward mode nondeterministic error\n>>> torch.randn(10, requires_grad=True, device='cuda').index_select(0, torch.tensor([0], device='cuda')).backward()\n...\nRuntimeError: index_add_cuda_ does not have a deterministic implementation...",
        "Z": "Note that deterministic operations tend to have worse performance than\nnondeterministic operations. >>> torch.use_deterministic_algorithms(True)\n\n# Forward mode nondeterministic error\n>>> torch.randn(10).index_copy(0, torch.tensor([0]), torch.randn(1))\n...\nRuntimeError: index_copy does not have a deterministic implementation...\n\n# Backward mode nondeterministic error\n>>> torch.randn(10, requires_grad=True, device='cuda').index_select(0, torch.tensor([0], device='cuda')).backward()\n...\nRuntimeError: index_add_cuda_ does not have a deterministic implementation...",
        "source": "https://pytorch.org/docs/stable/generated/torch.use_deterministic_algorithms.html#torch.use_deterministic_algorithms"
    },
    {
        "X": "How to use torch.addcmul, give an example?",
        "Y": ">>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcmul(t, t1, t2, value=0.1)\ntensor([[-0.8635, -0.6391,  1.6174],\n        [-0.7617, -0.5879,  1.7388],\n        [-0.8353, -0.6249,  1.6511]])",
        "Z": "For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. >>> t = torch.randn(1, 3)\n>>> t1 = torch.randn(3, 1)\n>>> t2 = torch.randn(1, 3)\n>>> torch.addcmul(t, t1, t2, value=0.1)\ntensor([[-0.8635, -0.6391,  1.6174],\n        [-0.7617, -0.5879,  1.7388],\n        [-0.8353, -0.6249,  1.6511]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul"
    },
    {
        "X": "How to use torch.fmax, give an example?",
        "Y": ">>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])\n>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])\n>>> torch.fmax(a, b)\ntensor([9.7000, 0.5000, 3.1000,    nan])",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and floating-point inputs. >>> a = torch.tensor([9.7, float('nan'), 3.1, float('nan')])\n>>> b = torch.tensor([-2.2, 0.5, float('nan'), float('nan')])\n>>> torch.fmax(a, b)\ntensor([9.7000, 0.5000, 3.1000,    nan])",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"
    },
    {
        "X": "How to use torch.tensor_split, give an example?",
        "Y": ">>> x = torch.arange(8)\n>>> torch.tensor_split(x, 3)\n(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))\n\n>>> x = torch.arange(7)\n>>> torch.tensor_split(x, 3)\n(tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]))\n>>> torch.tensor_split(x, (1, 6))\n(tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6]))\n\n>>> x = torch.arange(14).reshape(2, 7)\n>>> x\ntensor([[ 0,  1,  2,  3,  4,  5,  6],\n        [ 7,  8,  9, 10, 11, 12, 13]])\n>>> torch.tensor_split(x, 3, dim=1)\n(tensor([[0, 1, 2],\n        [7, 8, 9]]),\n tensor([[ 3,  4],\n        [10, 11]]),\n tensor([[ 5,  6],\n        [12, 13]]))\n>>> torch.tensor_split(x, (1, 6), dim=1)\n(tensor([[0],\n        [7]]),\n tensor([[ 1,  2,  3,  4,  5],\n        [ 8,  9, 10, 11, 12]]),\n tensor([[ 6],\n        [13]]))",
        "Z": ">>> x = torch.arange(8)\n>>> torch.tensor_split(x, 3)\n(tensor([0, 1, 2]), tensor([3, 4, 5]), tensor([6, 7]))\n\n>>> x = torch.arange(7)\n>>> torch.tensor_split(x, 3)\n(tensor([0, 1, 2]), tensor([3, 4]), tensor([5, 6]))\n>>> torch.tensor_split(x, (1, 6))\n(tensor([0]), tensor([1, 2, 3, 4, 5]), tensor([6]))\n\n>>> x = torch.arange(14).reshape(2, 7)\n>>> x\ntensor([[ 0,  1,  2,  3,  4,  5,  6],\n        [ 7,  8,  9, 10, 11, 12, 13]])\n>>> torch.tensor_split(x, 3, dim=1)\n(tensor([[0, 1, 2],\n        [7, 8, 9]]),\n tensor([[ 3,  4],\n        [10, 11]]),\n tensor([[ 5,  6],\n        [12, 13]]))\n>>> torch.tensor_split(x, (1, 6), dim=1)\n(tensor([[0],\n        [7]]),\n tensor([[ 1,  2,  3,  4,  5],\n        [ 8,  9, 10, 11, 12]]),\n tensor([[ 6],\n        [13]]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"
    },
    {
        "X": "How to use torch.std, give an example?",
        "Y": ">>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.std(a, unbiased=False)\ntensor(0.4188)",
        "Z": "If unbiased is True, Bessel\u2019s correction will be used.\nOtherwise, the sample deviation is calculated, without any correction. >>> a = torch.tensor([[-0.8166, -1.3802, -0.3560]])\n>>> torch.std(a, unbiased=False)\ntensor(0.4188)",
        "source": "https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"
    },
    {
        "X": "How to use torch.cos, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n>>> torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n>>> torch.cos(a)\ntensor([ 0.1395,  0.2957,  0.6553,  0.5574])",
        "source": "https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos"
    },
    {
        "X": "How to use This is the simplest to apply form of quantization where the weights are\nquantized ahead of time but the activations are dynamically quantized\nduring inference. This is used for situations where the model execution time\nis dominated by loading weights from memory rather than computing the matrix\nmultiplications. This is true for for LSTM and Transformer type models with\nsmall batch size.Diagram:, give an example?",
        "Y": "# original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                 /\nlinear_weight_fp32\n\n# dynamically quantized model\n# linear and LSTM weights are in int8\nprevious_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32\n                     /\n   linear_weight_int8",
        "Z": "Diagram: # original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                 /\nlinear_weight_fp32\n\n# dynamically quantized model\n# linear and LSTM weights are in int8\nprevious_layer_fp32 -- linear_int8_w_fp32_inp -- activation_fp32 -- next_layer_fp32\n                     /\n   linear_weight_int8",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How to use Diagram:API example:, give an example?",
        "Y": "import torch\n\n# define a floating point model\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        self.fc = torch.nn.Linear(4, 4)\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n# create a quantized model instance\nmodel_int8 = torch.quantization.quantize_dynamic(\n    model_fp32,  # the original model\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8)  # the target dtype for quantized weights\n\n# run the model\ninput_fp32 = torch.randn(4, 4, 4, 4)\nres = model_int8(input_fp32)",
        "Z": "Diagram:API example: import torch\n\n# define a floating point model\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        self.fc = torch.nn.Linear(4, 4)\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n# create a quantized model instance\nmodel_int8 = torch.quantization.quantize_dynamic(\n    model_fp32,  # the original model\n    {torch.nn.Linear},  # a set of layers to dynamically quantize\n    dtype=torch.qint8)  # the target dtype for quantized weights\n\n# run the model\ninput_fp32 = torch.randn(4, 4, 4, 4)\nres = model_int8(input_fp32)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How to use Static quantization quantizes the weights and activations of the model.  It\nfuses activations into preceding layers where possible.  It requires\ncalibration with a representative dataset to determine optimal quantization\nparameters for activations. Post Training Quantization is typically used when\nboth memory bandwidth and compute savings are important with CNNs being a\ntypical use case.  Static quantization is also known as Post Training\nQuantization or PTQ.Diagram:, give an example?",
        "Y": "# original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                    /\n    linear_weight_fp32\n\n# statically quantized model\n# weights and activations are in int8\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                    /\n  linear_weight_int8",
        "Z": "Diagram: # original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                    /\n    linear_weight_fp32\n\n# statically quantized model\n# weights and activations are in int8\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                    /\n  linear_weight_int8",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How to use Diagram:, give an example?",
        "Y": "import torch\n\n# define a floating point model where some layers could be statically quantized\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # manually specify where tensors will be converted from floating\n        # point to quantized in the quantized model\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.relu(x)\n        # manually specify where tensors will be converted from quantized\n        # to floating point in the quantized model\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to eval mode for static quantization logic to work\nmodel_fp32.eval()\n\n# attach a global qconfig, which contains information about what kind\n# of observers to attach. Use 'fbgemm' for server inference and\n# 'qnnpack' for mobile inference. Other quantization configurations such\n# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n# calibration techniques can be specified here.\nmodel_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n\n# Fuse the activations to preceding layers, where applicable.\n# This needs to be done manually depending on the model architecture.\n# Common fusions include `conv + relu` and `conv + batchnorm + relu`\nmodel_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n\n# Prepare the model for static quantization. This inserts observers in\n# the model that will observe activation tensors during calibration.\nmodel_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n\n# calibrate the prepared model to determine quantization parameters for activations\n# in a real world setting, the calibration would be done with a representative dataset\ninput_fp32 = torch.randn(4, 1, 4, 4)\nmodel_fp32_prepared(input_fp32)\n\n# Convert the observed model to a quantized model. This does several things:\n# quantizes the weights, computes and stores the scale and bias value to be\n# used with each activation tensor, and replaces key operators with quantized\n# implementations.\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n\n# run the model, relevant calculations will happen in int8\nres = model_int8(input_fp32)",
        "Z": "Diagram: import torch\n\n# define a floating point model where some layers could be statically quantized\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # manually specify where tensors will be converted from floating\n        # point to quantized in the quantized model\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.relu(x)\n        # manually specify where tensors will be converted from quantized\n        # to floating point in the quantized model\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to eval mode for static quantization logic to work\nmodel_fp32.eval()\n\n# attach a global qconfig, which contains information about what kind\n# of observers to attach. Use 'fbgemm' for server inference and\n# 'qnnpack' for mobile inference. Other quantization configurations such\n# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n# calibration techniques can be specified here.\nmodel_fp32.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n\n# Fuse the activations to preceding layers, where applicable.\n# This needs to be done manually depending on the model architecture.\n# Common fusions include `conv + relu` and `conv + batchnorm + relu`\nmodel_fp32_fused = torch.quantization.fuse_modules(model_fp32, [['conv', 'relu']])\n\n# Prepare the model for static quantization. This inserts observers in\n# the model that will observe activation tensors during calibration.\nmodel_fp32_prepared = torch.quantization.prepare(model_fp32_fused)\n\n# calibrate the prepared model to determine quantization parameters for activations\n# in a real world setting, the calibration would be done with a representative dataset\ninput_fp32 = torch.randn(4, 1, 4, 4)\nmodel_fp32_prepared(input_fp32)\n\n# Convert the observed model to a quantized model. This does several things:\n# quantizes the weights, computes and stores the scale and bias value to be\n# used with each activation tensor, and replaces key operators with quantized\n# implementations.\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n\n# run the model, relevant calculations will happen in int8\nres = model_int8(input_fp32)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How to use Quantization Aware Training models the effects of quantization during training\nallowing for higher accuracy compared to other quantization methods.  During\ntraining, all calculations are done in floating point, with fake_quant modules\nmodeling the effects of quantization by clamping and rounding to simulate the\neffects of INT8.  After model conversion, weights and\nactivations are quantized, and activations are fused into the preceding layer\nwhere possible.  It is commonly used with CNNs and yields a higher accuracy\ncompared to static quantization.  Quantization Aware Training is also known as\nQAT.Diagram:, give an example?",
        "Y": "# original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                      /\n    linear_weight_fp32\n\n# model with fake_quants for modeling quantization numerics during training\nprevious_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32\n                           /\n   linear_weight_fp32 -- fq\n\n# quantized model\n# weights and activations are in int8\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                     /\n   linear_weight_int8",
        "Z": "Diagram: # original model\n# all tensors and computations are in floating point\nprevious_layer_fp32 -- linear_fp32 -- activation_fp32 -- next_layer_fp32\n                      /\n    linear_weight_fp32\n\n# model with fake_quants for modeling quantization numerics during training\nprevious_layer_fp32 -- fq -- linear_fp32 -- activation_fp32 -- fq -- next_layer_fp32\n                           /\n   linear_weight_fp32 -- fq\n\n# quantized model\n# weights and activations are in int8\nprevious_layer_int8 -- linear_with_activation_int8 -- next_layer_int8\n                     /\n   linear_weight_int8",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How  Diagram:, give an example?",
        "Y": "import torch\n\n# define a floating point model where some layers could benefit from QAT\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to train mode for QAT logic to work\nmodel_fp32.train()\n\n# attach a global qconfig, which contains information about what kind\n# of observers to attach. Use 'fbgemm' for server inference and\n# 'qnnpack' for mobile inference. Other quantization configurations such\n# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n# calibration techniques can be specified here.\nmodel_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n\n# fuse the activations to preceding layers, where applicable\n# this needs to be done manually depending on the model architecture\nmodel_fp32_fused = torch.quantization.fuse_modules(model_fp32,\n    [['conv', 'bn', 'relu']])\n\n# Prepare the model for QAT. This inserts observers and fake_quants in\n# the model that will observe weight and activation tensors during calibration.\nmodel_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n\n# run the training loop (not shown)\ntraining_loop(model_fp32_prepared)\n\n# Convert the observed model to a quantized model. This does several things:\n# quantizes the weights, computes and stores the scale and bias value to be\n# used with each activation tensor, fuses modules where appropriate,\n# and replaces key operators with quantized implementations.\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n\n# run the model, relevant calculations will happen in int8\nres = model_int8(input_fp32)",
        "Z": "Diagram: import torch\n\n# define a floating point model where some layers could benefit from QAT\nclass M(torch.nn.Module):\n    def __init__(self):\n        super(M, self).__init__()\n        # QuantStub converts tensors from floating point to quantized\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n        self.bn = torch.nn.BatchNorm2d(1)\n        self.relu = torch.nn.ReLU()\n        # DeQuantStub converts tensors from quantized to floating point\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        x = self.quant(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.relu(x)\n        x = self.dequant(x)\n        return x\n\n# create a model instance\nmodel_fp32 = M()\n\n# model must be set to train mode for QAT logic to work\nmodel_fp32.train()\n\n# attach a global qconfig, which contains information about what kind\n# of observers to attach. Use 'fbgemm' for server inference and\n# 'qnnpack' for mobile inference. Other quantization configurations such\n# as selecting symmetric or assymetric quantization and MinMax or L2Norm\n# calibration techniques can be specified here.\nmodel_fp32.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n\n# fuse the activations to preceding layers, where applicable\n# this needs to be done manually depending on the model architecture\nmodel_fp32_fused = torch.quantization.fuse_modules(model_fp32,\n    [['conv', 'bn', 'relu']])\n\n# Prepare the model for QAT. This inserts observers and fake_quants in\n# the model that will observe weight and activation tensors during calibration.\nmodel_fp32_prepared = torch.quantization.prepare_qat(model_fp32_fused)\n\n# run the training loop (not shown)\ntraining_loop(model_fp32_prepared)\n\n# Convert the observed model to a quantized model. This does several things:\n# quantizes the weights, computes and stores the scale and bias value to be\n# used with each activation tensor, fuses modules where appropriate,\n# and replaces key operators with quantized implementations.\nmodel_fp32_prepared.eval()\nmodel_int8 = torch.quantization.convert(model_fp32_prepared)\n\n# run the model, relevant calculations will happen in int8\nres = model_int8(input_fp32)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How to use There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function)., give an example?",
        "Y": "import torch.quantization.quantize_fx as quantize_fx\nimport copy\n\nmodel_fp = UserModel(...)\n\n#\n# post training dynamic/weight_only quantization\n#\n\n# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_to_quantize.eval()\nqconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig}\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\n# no calibration needed when we only have dynamici/weight_only quantization\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# post training static quantization\n#\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nqconfig_dict = {\"\": torch.quantization.get_default_qconfig('qnnpack')}\nmodel_to_quantize.eval()\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\n# calibrate (not shown)\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# quantization aware training for static quantization\n#\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nqconfig_dict = {\"\": torch.quantization.get_default_qat_qconfig('qnnpack')}\nmodel_to_quantize.train()\n# prepare\nmodel_prepared = quantize_fx.prepare_qat_fx(model_to_qunatize, qconfig_dict)\n# training loop (not shown)\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# fusion\n#\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_fused = quantize_fx.fuse_fx(model_to_quantize)",
        "Z": "There are multiple quantization types in post training quantization (weight only, dynamic and static) and the configuration is done through qconfig_dict (an argument of the prepare_fx function). import torch.quantization.quantize_fx as quantize_fx\nimport copy\n\nmodel_fp = UserModel(...)\n\n#\n# post training dynamic/weight_only quantization\n#\n\n# we need to deepcopy if we still want to keep model_fp unchanged after quantization since quantization apis change the input model\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_to_quantize.eval()\nqconfig_dict = {\"\": torch.quantization.default_dynamic_qconfig}\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\n# no calibration needed when we only have dynamici/weight_only quantization\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# post training static quantization\n#\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nqconfig_dict = {\"\": torch.quantization.get_default_qconfig('qnnpack')}\nmodel_to_quantize.eval()\n# prepare\nmodel_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_dict)\n# calibrate (not shown)\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# quantization aware training for static quantization\n#\n\nmodel_to_quantize = copy.deepcopy(model_fp)\nqconfig_dict = {\"\": torch.quantization.get_default_qat_qconfig('qnnpack')}\nmodel_to_quantize.train()\n# prepare\nmodel_prepared = quantize_fx.prepare_qat_fx(model_to_qunatize, qconfig_dict)\n# training loop (not shown)\n# quantize\nmodel_quantized = quantize_fx.convert_fx(model_prepared)\n\n#\n# fusion\n#\nmodel_to_quantize = copy.deepcopy(model_fp)\nmodel_fused = quantize_fx.fuse_fx(model_to_quantize)",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How to use If you see an error similar to:, give an example?",
        "Y": "RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...",
        "Z": "If you see an error similar to: RuntimeError: Could not run 'quantized::some_operator' with arguments from the 'CPU' backend...",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How to use If you see an error similar to:This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:, give an example?",
        "Y": "class M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv(x)\n        return x",
        "Z": "This means that you are trying to pass a non-quantized Tensor to a quantized\nkernel. A common workaround is to use torch.quantization.QuantStub to\nquantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: class M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.conv = torch.nn.Conv2d(1, 1, 1)\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv(x)\n        return x",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How  If you see an error similar to:, give an example?",
        "Y": "RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend.",
        "Z": "If you see an error similar to: RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend.",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How to use If you see an error similar to:This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example:, give an example?",
        "Y": "class M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.conv1 = torch.nn.Conv2d(1, 1, 1)\n        # this module will not be quantized (see `qconfig = None` logic below)\n        self.conv2 = torch.nn.Conv2d(1, 1, 1)\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv1(x)\n        # during the convert step, this will be replaced with a\n        # `dequantize` call\n        x = self.dequant(x)\n        x = self.conv2(x)\n        return x\n\nm = M()\nm.qconfig = some_qconfig\n# turn off quantization for conv2\nm.conv2.qconfig = None",
        "Z": "This means that you are trying to pass a quantized Tensor to a non-quantized\nkernel. A common workaround is to use torch.quantization.DeQuantStub to\ndequantize the tensor.  This needs to be done manually in Eager mode quantization.\nAn e2e example: class M(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.quant = torch.quantization.QuantStub()\n        self.conv1 = torch.nn.Conv2d(1, 1, 1)\n        # this module will not be quantized (see `qconfig = None` logic below)\n        self.conv2 = torch.nn.Conv2d(1, 1, 1)\n        self.dequant = torch.quantization.DeQuantStub()\n\n    def forward(self, x):\n        # during the convert step, this will be replaced with a\n        # `quantize_per_tensor` call\n        x = self.quant(x)\n        x = self.conv1(x)\n        # during the convert step, this will be replaced with a\n        # `dequantize` call\n        x = self.dequant(x)\n        x = self.conv2(x)\n        return x\n\nm = M()\nm.qconfig = some_qconfig\n# turn off quantization for conv2\nm.conv2.qconfig = None",
        "source": "https://pytorch.org/docs/stable/quantization.html"
    },
    {
        "X": "How to use torch.fake_quantize_per_tensor_affine, give an example?",
        "Y": ">>> x = torch.randn(4)\n>>> x\ntensor([ 0.0552,  0.9730,  0.3973, -1.0780])\n>>> torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)\ntensor([0.1000, 1.0000, 0.4000, 0.0000])",
        "Z": ">>> x = torch.randn(4)\n>>> x\ntensor([ 0.0552,  0.9730,  0.3973, -1.0780])\n>>> torch.fake_quantize_per_tensor_affine(x, 0.1, 0, 0, 255)\ntensor([0.1000, 1.0000, 0.4000, 0.0000])",
        "source": "https://pytorch.org/docs/stable/generated/torch.fake_quantize_per_tensor_affine.html#torch.fake_quantize_per_tensor_affine"
    },
    {
        "X": "How to use torch.baddbmm, give an example?",
        "Y": ">>> M = torch.randn(10, 3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.baddbmm(M, batch1, batch2).size()\ntorch.Size([10, 3, 5])",
        "Z": "This operator supports TensorFloat32. >>> M = torch.randn(10, 3, 5)\n>>> batch1 = torch.randn(10, 3, 4)\n>>> batch2 = torch.randn(10, 4, 5)\n>>> torch.baddbmm(M, batch1, batch2).size()\ntorch.Size([10, 3, 5])",
        "source": "https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"
    },
    {
        "X": "How to use torch.ge, give an example?",
        "Y": ">>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, True], [False, True]])",
        "Z": "The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.ge(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[True, True], [False, True]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge"
    },
    {
        "X": "How to use torch.all, give an example?",
        "Y": ">>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.all(a)\ntensor(False, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.all(a)\ntensor(False)",
        "Z": ">>> a = torch.rand(1, 2).bool()\n>>> a\ntensor([[False, True]], dtype=torch.bool)\n>>> torch.all(a)\ntensor(False, dtype=torch.bool)\n>>> a = torch.arange(0, 3)\n>>> a\ntensor([0, 1, 2])\n>>> torch.all(a)\ntensor(False)",
        "source": "https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"
    },
    {
        "X": "How to use torch.dot, give an example?",
        "Y": ">>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)",
        "Z": ">>> torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1]))\ntensor(7)",
        "source": "https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot"
    },
    {
        "X": "How to use Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs.The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example:, give an example?",
        "Y": "import torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\n\n# Writer will output to ./runs/ directory by default\nwriter = SummaryWriter()\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\nmodel = torchvision.models.resnet50(False)\n# Have ResNet model take in grayscale rather than RGB\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nimages, labels = next(iter(trainloader))\n\ngrid = torchvision.utils.make_grid(images)\nwriter.add_image('images', grid, 0)\nwriter.add_graph(model, images)\nwriter.close()",
        "Z": "Once you\u2019ve installed TensorBoard, these utilities let you log PyTorch models\nand metrics into a directory for visualization within the TensorBoard UI.\nScalars, images, histograms, graphs, and embedding visualizations are all\nsupported for PyTorch models and tensors as well as Caffe2 nets and blobs.The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example: import torch\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\n\n# Writer will output to ./runs/ directory by default\nwriter = SummaryWriter()\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\nmodel = torchvision.models.resnet50(False)\n# Have ResNet model take in grayscale rather than RGB\nmodel.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nimages, labels = next(iter(trainloader))\n\ngrid = torchvision.utils.make_grid(images)\nwriter.add_image('images', grid, 0)\nwriter.add_graph(model, images)\nwriter.close()",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example:This can then be visualized with TensorBoard, which should be installable\nand runnable with:, give an example?",
        "Y": "pip install tensorboard\ntensorboard --logdir=runs",
        "Z": "The SummaryWriter class is your main entry to log data for consumption\nand visualization by TensorBoard. For example:This can then be visualized with TensorBoard, which should be installable\nand runnable with: pip install tensorboard\ntensorboard --logdir=runs",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use This can then be visualized with TensorBoard, which should be installable\nand runnable with:Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface., give an example?",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nwriter = SummaryWriter()\n\nfor n_iter in range(100):\n    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)",
        "Z": "This can then be visualized with TensorBoard, which should be installable\nand runnable with:Lots of information can be logged for one experiment. To avoid cluttering\nthe UI and have better result clustering, we can group plots by naming them\nhierarchically. For example, \u201cLoss/train\u201d and \u201cLoss/test\u201d will be grouped\ntogether, while \u201cAccuracy/train\u201d and \u201cAccuracy/test\u201d will be grouped separately\nin the TensorBoard interface. from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nwriter = SummaryWriter()\n\nfor n_iter in range(100):\n    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.__init__, give an example?",
        "Y": "from torch.utils.tensorboard import SummaryWriter\n\n# create a summary writer with automatically generated folder name.\nwriter = SummaryWriter()\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n\n# create a summary writer using the specified folder name.\nwriter = SummaryWriter(\"my_experiment\")\n# folder location: my_experiment\n\n# create a summary writer with comment appended.\nwriter = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/",
        "Z": "from torch.utils.tensorboard import SummaryWriter\n\n# create a summary writer with automatically generated folder name.\nwriter = SummaryWriter()\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.local/\n\n# create a summary writer using the specified folder name.\nwriter = SummaryWriter(\"my_experiment\")\n# folder location: my_experiment\n\n# create a summary writer with comment appended.\nwriter = SummaryWriter(comment=\"LR_0.1_BATCH_16\")\n# folder location: runs/May04_22-14-54_s-MacBook-Pro.localLR_0.1_BATCH_16/",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_scalar, give an example?",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nx = range(100)\nfor i in x:\n    writer.add_scalar('y=2x', i * 2, i)\nwriter.close()",
        "Z": "from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nx = range(100)\nfor i in x:\n    writer.add_scalar('y=2x', i * 2, i)\nwriter.close()",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_scalars, give an example?",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nr = 5\nfor i in range(100):\n    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n                                    'xcosx':i*np.cos(i/r),\n                                    'tanx': np.tan(i/r)}, i)\nwriter.close()\n# This call adds three values to the same scalar plot with the tag\n# 'run_14h' in TensorBoard's scalar section.",
        "Z": "from torch.utils.tensorboard import SummaryWriter\nwriter = SummaryWriter()\nr = 5\nfor i in range(100):\n    writer.add_scalars('run_14h', {'xsinx':i*np.sin(i/r),\n                                    'xcosx':i*np.cos(i/r),\n                                    'tanx': np.tan(i/r)}, i)\nwriter.close()\n# This call adds three values to the same scalar plot with the tag\n# 'run_14h' in TensorBoard's scalar section.",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_histogram, give an example?",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nwriter = SummaryWriter()\nfor i in range(10):\n    x = np.random.random(1000)\n    writer.add_histogram('distribution centers', x + i, i)\nwriter.close()",
        "Z": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nwriter = SummaryWriter()\nfor i in range(10):\n    x = np.random.random(1000)\n    writer.add_histogram('distribution centers', x + i, i)\nwriter.close()",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_image, give an example?",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nimg_HWC = np.zeros((100, 100, 3))\nimg_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nwriter = SummaryWriter()\nwriter.add_image('my_image', img, 0)\n\n# If you have non-default dimension setting, set the dataformats argument.\nwriter.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\nwriter.close()",
        "Z": "Note that this requires the pillow package. from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nimg = np.zeros((3, 100, 100))\nimg[0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg[1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nimg_HWC = np.zeros((100, 100, 3))\nimg_HWC[:, :, 0] = np.arange(0, 10000).reshape(100, 100) / 10000\nimg_HWC[:, :, 1] = 1 - np.arange(0, 10000).reshape(100, 100) / 10000\n\nwriter = SummaryWriter()\nwriter.add_image('my_image', img, 0)\n\n# If you have non-default dimension setting, set the dataformats argument.\nwriter.add_image('my_image_HWC', img_HWC, 0, dataformats='HWC')\nwriter.close()",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_images, give an example?",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nimg_batch = np.zeros((16, 3, 100, 100))\nfor i in range(16):\n    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n\nwriter = SummaryWriter()\nwriter.add_images('my_image_batch', img_batch, 0)\nwriter.close()",
        "Z": "Note that this requires the pillow package. from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\n\nimg_batch = np.zeros((16, 3, 100, 100))\nfor i in range(16):\n    img_batch[i, 0] = np.arange(0, 10000).reshape(100, 100) / 10000 / 16 * i\n    img_batch[i, 1] = (1 - np.arange(0, 10000).reshape(100, 100) / 10000) / 16 * i\n\nwriter = SummaryWriter()\nwriter.add_images('my_image_batch', img_batch, 0)\nwriter.close()",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_text, give an example?",
        "Y": "writer.add_text('lstm', 'This is an lstm', 0)\nwriter.add_text('rnn', 'This is an rnn', 10)",
        "Z": "writer.add_text('lstm', 'This is an lstm', 0)\nwriter.add_text('rnn', 'This is an rnn', 10)",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_embedding, give an example?",
        "Y": "import keyword\nimport torch\nmeta = []\nwhile len(meta)<100:\n    meta = meta+keyword.kwlist # get some strings\nmeta = meta[:100]\n\nfor i, v in enumerate(meta):\n    meta[i] = v+str(i)\n\nlabel_img = torch.rand(100, 3, 10, 32)\nfor i in range(100):\n    label_img[i]*=i/100.0\n\nwriter.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), metadata=meta)",
        "Z": "import keyword\nimport torch\nmeta = []\nwhile len(meta)<100:\n    meta = meta+keyword.kwlist # get some strings\nmeta = meta[:100]\n\nfor i, v in enumerate(meta):\n    meta[i] = v+str(i)\n\nlabel_img = torch.rand(100, 3, 10, 32)\nfor i in range(100):\n    label_img[i]*=i/100.0\n\nwriter.add_embedding(torch.randn(100, 5), metadata=meta, label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), label_img=label_img)\nwriter.add_embedding(torch.randn(100, 5), metadata=meta)",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_pr_curve, give an example?",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nlabels = np.random.randint(2, size=100)  # binary label\npredictions = np.random.rand(100)\nwriter = SummaryWriter()\nwriter.add_pr_curve('pr_curve', labels, predictions, 0)\nwriter.close()",
        "Z": "from torch.utils.tensorboard import SummaryWriter\nimport numpy as np\nlabels = np.random.randint(2, size=100)  # binary label\npredictions = np.random.rand(100)\nwriter = SummaryWriter()\nwriter.add_pr_curve('pr_curve', labels, predictions, 0)\nwriter.close()",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_custom_scalars, give an example?",
        "Y": "layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\n             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n\nwriter.add_custom_scalars(layout)",
        "Z": "layout = {'Taiwan':{'twse':['Multiline',['twse/0050', 'twse/2330']]},\n             'USA':{ 'dow':['Margin',   ['dow/aaa', 'dow/bbb', 'dow/ccc']],\n                  'nasdaq':['Margin',   ['nasdaq/aaa', 'nasdaq/bbb', 'nasdaq/ccc']]}}\n\nwriter.add_custom_scalars(layout)",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_mesh, give an example?",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nvertices_tensor = torch.as_tensor([\n    [1, 1, 1],\n    [-1, -1, 1],\n    [1, -1, -1],\n    [-1, 1, -1],\n], dtype=torch.float).unsqueeze(0)\ncolors_tensor = torch.as_tensor([\n    [255, 0, 0],\n    [0, 255, 0],\n    [0, 0, 255],\n    [255, 0, 255],\n], dtype=torch.int).unsqueeze(0)\nfaces_tensor = torch.as_tensor([\n    [0, 2, 3],\n    [0, 3, 1],\n    [0, 1, 2],\n    [1, 3, 2],\n], dtype=torch.int).unsqueeze(0)\n\nwriter = SummaryWriter()\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\nwriter.close()",
        "Z": "from torch.utils.tensorboard import SummaryWriter\nvertices_tensor = torch.as_tensor([\n    [1, 1, 1],\n    [-1, -1, 1],\n    [1, -1, -1],\n    [-1, 1, -1],\n], dtype=torch.float).unsqueeze(0)\ncolors_tensor = torch.as_tensor([\n    [255, 0, 0],\n    [0, 255, 0],\n    [0, 0, 255],\n    [255, 0, 255],\n], dtype=torch.int).unsqueeze(0)\nfaces_tensor = torch.as_tensor([\n    [0, 2, 3],\n    [0, 3, 1],\n    [0, 1, 2],\n    [1, 3, 2],\n], dtype=torch.int).unsqueeze(0)\n\nwriter = SummaryWriter()\nwriter.add_mesh('my_mesh', vertices=vertices_tensor, colors=colors_tensor, faces=faces_tensor)\n\nwriter.close()",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.utils.tensorboard.writer.SummaryWriter.add_hparams, give an example?",
        "Y": "from torch.utils.tensorboard import SummaryWriter\nwith SummaryWriter() as w:\n    for i in range(5):\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})",
        "Z": "from torch.utils.tensorboard import SummaryWriter\nwith SummaryWriter() as w:\n    for i in range(5):\n        w.add_hparams({'lr': 0.1*i, 'bsize': i},\n                      {'hparam/accuracy': 10*i, 'hparam/loss': 10*i})",
        "source": "https://pytorch.org/docs/stable/tensorboard.html"
    },
    {
        "X": "How to use torch.linspace, give an example?",
        "Y": ">>> torch.linspace(3, 10, steps=5)\ntensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])\n>>> torch.linspace(-10, 10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n>>> torch.linspace(start=-10, end=10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n>>> torch.linspace(start=-10, end=10, steps=1)\ntensor([-10.])",
        "Z": ">>> torch.linspace(3, 10, steps=5)\ntensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])\n>>> torch.linspace(-10, 10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n>>> torch.linspace(start=-10, end=10, steps=5)\ntensor([-10.,  -5.,   0.,   5.,  10.])\n>>> torch.linspace(start=-10, end=10, steps=1)\ntensor([-10.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.linspace.html#torch.linspace"
    },
    {
        "X": "How to use torch.lcm, give an example?",
        "Y": ">>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.lcm(a, b)\ntensor([15, 20, 15])\n>>> c = torch.tensor([3])\n>>> torch.lcm(a, c)\ntensor([15, 30, 15])",
        "Z": "Both input and other must have integer types. >>> a = torch.tensor([5, 10, 15])\n>>> b = torch.tensor([3, 4, 5])\n>>> torch.lcm(a, b)\ntensor([15, 20, 15])\n>>> c = torch.tensor([3])\n>>> torch.lcm(a, c)\ntensor([15, 30, 15])",
        "source": "https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm"
    },
    {
        "X": "How to use The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts.In fact, resetting all .grads to None before each\naccumulation phase, e.g.:, give an example?",
        "Y": "for iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()",
        "Z": "The default behavior (letting .grads be None before the first\nbackward(), such that their layout is created according to 1 or 2,\nand retained over time according to 3 or 4) is recommended for best performance.\nCalls to model.zero_grad() or optimizer.zero_grad() will not affect .grad\nlayouts.In fact, resetting all .grads to None before each\naccumulation phase, e.g.: for iterations...\n    ...\n    for param in model.parameters():\n        param.grad = None\n    loss.backward()",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How to use torch.autograd.Function, give an example?",
        "Y": ">>> class Exp(Function):\n>>>\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> #Use it by calling the apply method:\n>>> output = Exp.apply(input)",
        "Z": "Normally, the only way users interact with functions is by creating\nsubclasses and defining new operations. This is a recommended way of\nextending torch.autograd. >>> class Exp(Function):\n>>>\n>>>     @staticmethod\n>>>     def forward(ctx, i):\n>>>         result = i.exp()\n>>>         ctx.save_for_backward(result)\n>>>         return result\n>>>\n>>>     @staticmethod\n>>>     def backward(ctx, grad_output):\n>>>         result, = ctx.saved_tensors\n>>>         return grad_output * result\n>>>\n>>> #Use it by calling the apply method:\n>>> output = Exp.apply(input)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How to use torch.autograd.profiler.profile, give an example?",
        "Y": ">>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>          y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------",
        "Z": ">>> x = torch.randn((1, 1), requires_grad=True)\n>>> with torch.autograd.profiler.profile() as prof:\n>>>     for _ in range(100):  # any normal python code, really!\n>>>         y = x ** 2\n>>          y.backward()\n>>> # NOTE: some columns were removed for brevity\n>>> print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))\n-----------------------------------  ---------------  ---------------  ---------------\nName                                 Self CPU total   CPU time avg     Number of Calls\n-----------------------------------  ---------------  ---------------  ---------------\nmul                                  32.048ms         32.048ms         200\npow                                  27.041ms         27.041ms         200\nPowBackward0                         9.727ms          55.483ms         100\ntorch::autograd::AccumulateGrad      9.148ms          9.148ms          100\ntorch::autograd::GraphRoot           691.816us        691.816us        100\n-----------------------------------  ---------------  ---------------  ---------------",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How to use It is useful when running the program under nvprof:, give an example?",
        "Y": "nvprof --profile-from-start off -o trace_name.prof -- <regular command here>",
        "Z": "It is useful when running the program under nvprof: nvprof --profile-from-start off -o trace_name.prof -- <regular command here>",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How to use torch.autograd.profiler.emit_nvtx, give an example?",
        "Y": ">>> with torch.cuda.profiler.profile():\n...     model(x) # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)",
        "Z": "Unfortunately, there\u2019s no way to force nvprof to flush the data it collected\nto disk, so for CUDA profiling one has to use this context manager to annotate\nnvprof traces and wait for the process to exit before inspecting them.\nThen, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or\ntorch.autograd.profiler.load_nvprof() can load the results for inspection\ne.g. in Python REPL. >>> with torch.cuda.profiler.profile():\n...     model(x) # Warmup CUDA memory allocator and profiler\n...     with torch.autograd.profiler.emit_nvtx():\n...         model(x)",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How to use torch.autograd.detect_anomaly, give an example?",
        "Y": ">>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward",
        "Z": "This does two things: >>> import torch\n>>> from torch import autograd\n>>> class MyFunc(autograd.Function):\n...     @staticmethod\n...     def forward(ctx, inp):\n...         return inp.clone()\n...     @staticmethod\n...     def backward(ctx, gO):\n...         # Error during the backward pass\n...         raise RuntimeError(\"Some error in backward\")\n...         return gO.clone()\n>>> def run_fn(a):\n...     out = MyFunc.apply(a)\n...     return out.sum()\n>>> inp = torch.rand(10, 10, requires_grad=True)\n>>> out = run_fn(inp)\n>>> out.backward()\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward\n>>> with autograd.detect_anomaly():\n...     inp = torch.rand(10, 10, requires_grad=True)\n...     out = run_fn(inp)\n...     out.backward()\n    Traceback of forward call that caused the error:\n      File \"tmp.py\", line 53, in <module>\n        out = run_fn(inp)\n      File \"tmp.py\", line 44, in run_fn\n        out = MyFunc.apply(a)\n    Traceback (most recent call last):\n      File \"<stdin>\", line 4, in <module>\n      File \"/your/pytorch/install/torch/_tensor.py\", line 93, in backward\n        torch.autograd.backward(self, gradient, retain_graph, create_graph)\n      File \"/your/pytorch/install/torch/autograd/__init__.py\", line 90, in backward\n        allow_unreachable=True)  # allow_unreachable flag\n      File \"/your/pytorch/install/torch/autograd/function.py\", line 76, in apply\n        return self._forward_cls.backward(self, *args)\n      File \"<stdin>\", line 8, in backward\n    RuntimeError: Some error in backward",
        "source": "https://pytorch.org/docs/stable/autograd.html"
    },
    {
        "X": "How to use torch.chain_matmul, give an example?",
        "Y": ">>> a = torch.randn(3, 4)\n>>> b = torch.randn(4, 5)\n>>> c = torch.randn(5, 6)\n>>> d = torch.randn(6, 7)\n>>> torch.chain_matmul(a, b, c, d)\ntensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])",
        "Z": ">>> a = torch.randn(3, 4)\n>>> b = torch.randn(4, 5)\n>>> c = torch.randn(5, 6)\n>>> d = torch.randn(6, 7)\n>>> torch.chain_matmul(a, b, c, d)\ntensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],\n        [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],\n        [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.chain_matmul.html#torch.chain_matmul"
    },
    {
        "X": "How to use torch.tanh, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.8986, -0.7279,  1.1745,  0.2611])\n>>> torch.tanh(a)\ntensor([ 0.7156, -0.6218,  0.8257,  0.2553])",
        "Z": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.8986, -0.7279,  1.1745,  0.2611])\n>>> torch.tanh(a)\ntensor([ 0.7156, -0.6218,  0.8257,  0.2553])",
        "source": "https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh"
    },
    {
        "X": "How to use torch.atan2, give an example?",
        "Y": ">>> a = torch.randn(4)\n>>> a\ntensor([ 0.9041,  0.0196, -0.3108, -2.4423])\n>>> torch.atan2(a, torch.randn(4))\ntensor([ 0.9833,  0.0811, -1.9743, -1.4151])",
        "Z": "The shapes of input and other must be\nbroadcastable. >>> a = torch.randn(4)\n>>> a\ntensor([ 0.9041,  0.0196, -0.3108, -2.4423])\n>>> torch.atan2(a, torch.randn(4))\ntensor([ 0.9833,  0.0811, -1.9743, -1.4151])",
        "source": "https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"
    },
    {
        "X": "How to use torch.ne, give an example?",
        "Y": ">>> torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [True, False]])",
        "Z": "The second argument can be a number or a tensor whose shape is\nbroadcastable with the first argument. >>> torch.ne(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[1, 1], [4, 4]]))\ntensor([[False, True], [True, False]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne"
    },
    {
        "X": "How to use torch.msort, give an example?",
        "Y": ">>> t = torch.randn(3, 4)\n>>> t\ntensor([[-0.1321,  0.4370, -1.2631, -1.1289],\n        [-2.0527, -1.1250,  0.2275,  0.3077],\n        [-0.0881, -0.1259, -0.5495,  1.0284]])\n>>> torch.msort(t)\ntensor([[-2.0527, -1.1250, -1.2631, -1.1289],\n        [-0.1321, -0.1259, -0.5495,  0.3077],\n        [-0.0881,  0.4370,  0.2275,  1.0284]])",
        "Z": ">>> t = torch.randn(3, 4)\n>>> t\ntensor([[-0.1321,  0.4370, -1.2631, -1.1289],\n        [-2.0527, -1.1250,  0.2275,  0.3077],\n        [-0.0881, -0.1259, -0.5495,  1.0284]])\n>>> torch.msort(t)\ntensor([[-2.0527, -1.1250, -1.2631, -1.1289],\n        [-0.1321, -0.1259, -0.5495,  0.3077],\n        [-0.0881,  0.4370,  0.2275,  1.0284]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort"
    },
    {
        "X": "How to use torch.utils.bottleneck is a tool that can be used as an initial step for\ndebugging bottlenecks in your program. It summarizes runs of your script with\nthe Python profiler and PyTorch\u2019s autograd profiler.Run it on the command line with, give an example?",
        "Y": "python -m torch.utils.bottleneck /path/to/source/script.py [args]",
        "Z": "Run it on the command line with python -m torch.utils.bottleneck /path/to/source/script.py [args]",
        "source": "https://pytorch.org/docs/stable/bottleneck.html"
    },
    {
        "X": "How to use torch.utils.checkpoint.checkpoint_sequential, give an example?",
        "Y": ">>> model = nn.Sequential(...)\n>>> input_var = checkpoint_sequential(model, chunks, input_var)",
        "Z": "See checkpoint() on how checkpointing works. >>> model = nn.Sequential(...)\n>>> input_var = checkpoint_sequential(model, chunks, input_var)",
        "source": "https://pytorch.org/docs/stable/checkpoint.html"
    },
    {
        "X": "How to use torch.unbind, give an example?",
        "Y": ">>> torch.unbind(torch.tensor([[1, 2, 3],\n>>>                            [4, 5, 6],\n>>>                            [7, 8, 9]]))\n(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))",
        "Z": "Returns a tuple of all slices along a given dimension, already without it. >>> torch.unbind(torch.tensor([[1, 2, 3],\n>>>                            [4, 5, 6],\n>>>                            [7, 8, 9]]))\n(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))",
        "source": "https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind"
    },
    {
        "X": "How to use torch.cholesky_inverse, give an example?",
        "Y": ">>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[  0.9935,  -0.6353,   1.5806],\n        [ -0.6353,   0.8769,  -1.7183],\n        [  1.5806,  -1.7183,  10.6618]])\n>>> torch.cholesky_inverse(u)\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n>>> a.inverse()\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])",
        "Z": "If upper is True or not provided, uuu is upper\ntriangular such that the returned tensor is >>> a = torch.randn(3, 3)\n>>> a = torch.mm(a, a.t()) + 1e-05 * torch.eye(3) # make symmetric positive definite\n>>> u = torch.cholesky(a)\n>>> a\ntensor([[  0.9935,  -0.6353,   1.5806],\n        [ -0.6353,   0.8769,  -1.7183],\n        [  1.5806,  -1.7183,  10.6618]])\n>>> torch.cholesky_inverse(u)\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])\n>>> a.inverse()\ntensor([[ 1.9314,  1.2251, -0.0889],\n        [ 1.2251,  2.4439,  0.2122],\n        [-0.0889,  0.2122,  0.1412]])",
        "source": "https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse"
    },
    {
        "X": "How to use torch.lu, give an example?",
        "Y": ">>> A = torch.randn(2, 3, 3)\n>>> A_LU, pivots = torch.lu(A)\n>>> A_LU\ntensor([[[ 1.3506,  2.5558, -0.0816],\n         [ 0.1684,  1.1551,  0.1940],\n         [ 0.1193,  0.6189, -0.5497]],\n\n        [[ 0.4526,  1.2526, -0.3285],\n         [-0.7988,  0.7175, -0.9701],\n         [ 0.2634, -0.9255, -0.3459]]])\n>>> pivots\ntensor([[ 3,  3,  3],\n        [ 3,  3,  3]], dtype=torch.int32)\n>>> A_LU, pivots, info = torch.lu(A, get_infos=True)\n>>> if info.nonzero().size(0) == 0:\n...   print('LU factorization succeeded for all samples!')\nLU factorization succeeded for all samples!",
        "Z": ">>> A = torch.randn(2, 3, 3)\n>>> A_LU, pivots = torch.lu(A)\n>>> A_LU\ntensor([[[ 1.3506,  2.5558, -0.0816],\n         [ 0.1684,  1.1551,  0.1940],\n         [ 0.1193,  0.6189, -0.5497]],\n\n        [[ 0.4526,  1.2526, -0.3285],\n         [-0.7988,  0.7175, -0.9701],\n         [ 0.2634, -0.9255, -0.3459]]])\n>>> pivots\ntensor([[ 3,  3,  3],\n        [ 3,  3,  3]], dtype=torch.int32)\n>>> A_LU, pivots, info = torch.lu(A, get_infos=True)\n>>> if info.nonzero().size(0) == 0:\n...   print('LU factorization succeeded for all samples!')\nLU factorization succeeded for all samples!",
        "source": "https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"
    },
    {
        "X": "How to use torch.igamma, give an example?",
        "Y": ">>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.igammac(a1, a2)\ntensor([0.3528, 0.5665, 0.7350])\ntensor([0.3528, 0.5665, 0.7350])\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\ntensor([1., 1., 1.])",
        "Z": "Supports broadcasting to a common shape\nand float inputs. >>> a1 = torch.tensor([4.0])\n>>> a2 = torch.tensor([3.0, 4.0, 5.0])\n>>> a = torch.igammac(a1, a2)\ntensor([0.3528, 0.5665, 0.7350])\ntensor([0.3528, 0.5665, 0.7350])\n>>> b = torch.igamma(a1, a2) + torch.igammac(a1, a2)\ntensor([1., 1., 1.])",
        "source": "https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"
    },
    {
        "X": "How to use torch.load, give an example?",
        "Y": ">>> torch.load('tensors.pt')\n# Load all tensors onto the CPU\n>>> torch.load('tensors.pt', map_location=torch.device('cpu'))\n# Load all tensors onto the CPU, using a function\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)\n# Load all tensors onto GPU 1\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))\n# Map tensors from GPU 1 to GPU 0\n>>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})\n# Load tensor from io.BytesIO object\n>>> with open('tensor.pt', 'rb') as f:\n...     buffer = io.BytesIO(f.read())\n>>> torch.load(buffer)\n# Load a module with 'ascii' encoding for unpickling\n>>> torch.load('module.pt', encoding='ascii')",
        "Z": "User extensions can register their own location tags and tagging and\ndeserialization methods using torch.serialization.register_package(). >>> torch.load('tensors.pt')\n# Load all tensors onto the CPU\n>>> torch.load('tensors.pt', map_location=torch.device('cpu'))\n# Load all tensors onto the CPU, using a function\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)\n# Load all tensors onto GPU 1\n>>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))\n# Map tensors from GPU 1 to GPU 0\n>>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})\n# Load tensor from io.BytesIO object\n>>> with open('tensor.pt', 'rb') as f:\n...     buffer = io.BytesIO(f.read())\n>>> torch.load(buffer)\n# Load a module with 'ascii' encoding for unpickling\n>>> torch.load('module.pt', encoding='ascii')",
        "source": "https://pytorch.org/docs/stable/generated/torch.load.html#torch.load"
    },
    {
        "X": "How many dimensions does each input tensor have?",
        "Y": "zero",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "X": "What is another name for a tuple of Tensors?",
        "Y": "output",
        "Z": "Returns a 3-dimensional view of each input tensor with zero dimensions.\nInput tensors with three or more dimensions are returned as-is. input (Tensor or list of Tensors) \u2013  output (Tensor or tuple of Tensors) Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.atleast_3d.html#torch.atleast_3d"
    },
    {
        "X": "What may use the Sleef library when input is on the CPU?",
        "Y": "torch.sinh",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "X": "Where can you find details about the Sleef library?",
        "Y": "here",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "X": "Returns what with the hyperbolic sine of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "X": "When input is on the CPU, the implementation of torch.sinh may use what library?",
        "Y": "Sleef library",
        "Z": "Returns a new tensor with the hyperbolic sine of the elements of\ninput. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Note When input is on the CPU, the implementation of torch.sinh may use\nthe Sleef library, which rounds very large results to infinity or negative\ninfinity. See here for details.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"
    },
    {
        "X": "What is shifts a tuple of?",
        "Y": "python:ints",
        "Z": "input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "X": "If shifts is a tuple, dims must be what?",
        "Y": "a tuple of the same size",
        "Z": "input (Tensor) \u2013 the input tensor. shifts (int or tuple of python:ints) \u2013 The number of places by which the elements\nof the tensor are shifted. If shifts is a tuple, dims must be a tuple of\nthe same size, and each dimension will be rolled by the corresponding\nvalue dims (int or tuple of python:ints) \u2013 Axis along which to roll Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"
    },
    {
        "X": "What is constructed by repeating the elements of input?",
        "Y": "tensor",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "If reps specifies how many dimensions than input has, then ones are prepended to reps until all dimensions are specified?",
        "Y": "fewer",
        "Z": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "If input has shape (8, 6, 4, 2) and reps is (2, 2), reps is treated as what?",
        "Y": "(1, 1, 2, 2).",
        "Z": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "If reps specifies how many dimensions than input has, then ones are prepended to reps until all dimensions are specified.",
        "Y": "fewer",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What are reps treated as if input has shape and reps is 2?",
        "Y": "(1, 1, 2, 2)",
        "Z": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What specifies fewer dimensions than input has?",
        "Y": "reps",
        "Z": "If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2).",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is the size of the input tensor along a given dimension?",
        "Y": "k largest elements",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "If what is not given, the last dimension of the input is chosen?",
        "Y": "dim",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "If largest is what, the smallest elements of the input tensor are returned?",
        "Y": "False",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "The boolean option sorted if True will make sure that the returned k elements are themselves sorted?",
        "Y": "input (Tensor)",
        "Z": "The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is the k in \"top-k\"?",
        "Y": "k",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "Returns what of the given input tensor along a given dimension?",
        "Y": "k largest elements",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "If largest is what, the k smallest elements are returned?",
        "Y": "False",
        "Z": "If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What returns the indices of the elements in the original input tensor?",
        "Y": "A namedtuple of (values, indices)",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "The boolean option sorted if True, will make sure that the returned k elements are themselves what?",
        "Y": "sorted input",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "If largest is what, the smallest elements are returned?",
        "Y": "False",
        "Z": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "The boolean option sorted if True will make sure that the returned k elements are themselves sorted input (Tensor)",
        "Y": "input tensor",
        "Z": "If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is chosen if dim is not given?",
        "Y": "the last dimension of the input",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is returned if largest is False?",
        "Y": "A namedtuple of",
        "Z": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is the dimension to sort along with the input tensor?",
        "Y": "k",
        "Z": "If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "The boolean option sorted if True will make sure that the returned k elements are themselves sorted what?",
        "Y": "input (Tensor)",
        "Z": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "Which dimension controls whether to return largest or smallest elements?",
        "Y": "k",
        "Z": "If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What are returned if largest is False?",
        "Y": "k smallest elements",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "A namedtuple of (values, indices, etc.) is returned, where the indices are the indices of",
        "Y": "indices",
        "Z": "A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What option makes sure that the returned k elements are themselves sorted input?",
        "Y": "boolean",
        "Z": "Returns the k largest elements of the given input tensor along\na given dimension. If dim is not given, the last dimension of the input is chosen. If largest is False then the k smallest elements are returned. A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What is returned where the indices are the indices of the elements in the original input tensor?",
        "Y": "A namedtuple of (values, indices)",
        "Z": "A namedtuple of (values, indices) is returned, where the indices are the indices\nof the elements in the original input tensor. The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "The dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or smallest elements sorted (bool, optional)",
        "Y": "k",
        "Z": "The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "The boolean option sorted if True, will ma what?",
        "Y": "k",
        "Z": "The boolean option sorted if True, will make sure that the returned\nk elements are themselves sorted input (Tensor) \u2013 the input tensor. k (int) \u2013 the k in \u201ctop-k\u201d dim (int, optional) \u2013 the dimension to sort along largest (bool, optional) \u2013 controls whether to return largest or\nsmallest elements sorted (bool, optional) \u2013 controls whether to return the elements\nin sorted order out (tuple, optional) \u2013 the output tuple of (Tensor, LongTensor) that can be\noptionally given to be used as output buffers Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"
    },
    {
        "X": "What type of operation is applied in kHkWkH times kWkHkW regions?",
        "Y": "2D average-pooling operation",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of operation is applied in kHkWkH times kWkHkW regions by step size?",
        "Y": "2D average-pooling operation",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of average-pooling operation is applied in kHkWkH times kWkHkW regions?",
        "Y": "2D",
        "Z": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is applied in kTkHkWkT times kH times kWkTk",
        "Y": "3D average-pooling operation",
        "Z": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of operation does kHkWkH times kWkHkW regions by step size?",
        "Y": "2D average-pooling",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of operation is applied in kTkHkWkT times kH times kWk",
        "Y": "3D average-pooling",
        "Z": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of average-pooling operation does kHkWkH times kWkHkW regions by step size?",
        "Y": "2D",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does kTkHkWkT times kH times kWkTkH",
        "Y": "3D average-pooling operation",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of average-pooling operation does kTkHkWkT times kH times",
        "Y": "3D",
        "Z": "Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of pooling operation does kHkWkH times kWkHkW regions by step size?",
        "Y": "2D average-pooling",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What operation does kHkWkH times kWkHkW regions by step size sHsWs",
        "Y": "2D average-pooling",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What operation does kTkHkWkT times kH times kWkTkH",
        "Y": "3D average-pooling",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the 3D pooling over an input signal consist of several input planes do?",
        "Y": "Computes a partial inverse of MaxPool2d",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a 3D pooling over an input signal consist of several input planes do?",
        "Y": "Computes a partial inverse of MaxPool1d",
        "Z": "Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the 3D max pooling over an input signal consist of?",
        "Y": "Computes a partial inverse of MaxPool1d",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the 3D max pooling over an input signal consist of several input planes do?",
        "Y": "Computes a partial inverse of MaxPool1d",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the result of the 3D max pooling over an input signal composed of several input planes?",
        "Y": "Computes a partial inverse of MaxPool2d",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does MaxPool1d do?",
        "Y": "Computes a partial inverse",
        "Z": "Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of power-average pooling over an input signal composed of several input planes?",
        "Y": "2D",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does MaxPool2d do?",
        "Y": "Computes a partial inverse of MaxPool3d",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does Compute a partial inverse of MaxPool3d do?",
        "Y": "Computes a partial inverse of MaxPool3d",
        "Z": "Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of each element of the input Tensor?",
        "Y": "Thresholds",
        "Z": "Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does threshold apply element-wise?",
        "Y": "rectified linear unit function",
        "Z": "Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the in-place version of threshold()?",
        "Y": "relu()",
        "Z": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the function that is element-wise?",
        "Y": "hardswish function",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What version of threshold() is used?",
        "Y": "In-place",
        "Z": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What element-wise function does threshold() use?",
        "Y": "rectified linear unit function",
        "Z": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the element-wise function of the input Tensor?",
        "Y": "hardswish function",
        "Z": "Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does threshold() apply element-wise?",
        "Y": "rectified linear unit function",
        "Z": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What element-wise function does relu() use?",
        "Y": "rectified linear unit function",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the element-wise function of the HardTanh function?",
        "Y": "hardswish function",
        "Z": "In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does relu do element-wise?",
        "Y": "rectified linear unit function",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the in-place version of the rectified linear unit function?",
        "Y": "relu()",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the in-place version of HardTanh?",
        "Y": "hardtanh()",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the in-place version of relu()?",
        "Y": "hardtanh()",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the element-wise function that is described in the paper?",
        "Y": "hardswish function",
        "Z": "Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "How does elu(x) apply?",
        "Y": "element-wise",
        "Z": "Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "In-place version of elu(). Applies what?",
        "Y": "element-wise",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What =max(0,x)+min(0,(exp(x/)1))?",
        "Y": "CELU(x)",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What =max(0,x)+negative_slopemin(0,x)?",
        "Y": "LeakyReLU(x)",
        "Z": "Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does x =max(0,x)+negative_slopemin(0,x)textLeaky",
        "Y": "LeakyReLU",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the value of the function PReLU(x)?",
        "Y": "0,x",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the in-place version of rrelu()?",
        "Y": "gated linear unit",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the in-place version of Randomized leaky ReLU?",
        "Y": "rrelu()",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of unit is the LeakyReLU?",
        "Y": "gated linear unit",
        "Z": "Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the in-place version of leaky_relu()?",
        "Y": "elu()",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of transformation does y=xAT+by = xAT + by=xAT+b?",
        "Y": "linear",
        "Z": "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b.   Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of transformation does y=x1TAx2+by apply to the incoming data?",
        "Y": "bilinear",
        "Z": "Applies a linear transformation to the incoming data: y=xAT+by = xA^T + by=xAT+b.   Applies a bilinear transformation to the incoming data: y=x1TAx2+by = x_1^T A x_2 + by=x1T\u200bAx2\u200b+b",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the probability of zeroing some of the elements of the input tensor?",
        "Y": "probability p",
        "Z": "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Randomly zeroes some of the elements of the input tensor with probability p using samples from what?",
        "Y": "a Bernoulli distribution",
        "Z": "During training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution.   Applies alpha dropout to the input.   Randomly masks out entire channels (a channel is a feature map, e.g.   Randomly zero out entire channels (a channel is a 2D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 2D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).   Randomly zero out entire channels (a channel is a 3D feature map, e.g., the jjj-th channel of the iii-th sample in the batched input is a 3D tensor input[i,j]\\text{input}[i, j]input[i,j]) of the input tensor).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Computes sums, means or maxes of bags of embeddings without what?",
        "Y": "instantiating the intermediate embeddings",
        "Z": "A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the tensor of shape have everywhere except where the index of last dimension matches the corresponding value of the input tensor?",
        "Y": "zeros",
        "Z": "A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the tensor that returns a tensor of shape?",
        "Y": "num_classes",
        "Z": "A simple lookup table that looks up embeddings in a fixed dictionary and size.   Computes sums, means or maxes of bags of embeddings, without instantiating the intermediate embeddings.   Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of similarity between x1 and x2 is returned by torch.nn.PairwiseDistance?",
        "Y": "cosine",
        "Z": "See torch.nn.PairwiseDistance for details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Computes the distance between every pair of row vectors in the input?",
        "Y": "p-norm",
        "Z": "See torch.nn.PairwiseDistance for details   Returns cosine similarity between x1 and x2, computed along dim.   Computes the p-norm distance between every pair of row vectors in the input.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of negative log likelihood loss does CosineEmbeddingLoss combine?",
        "Y": "Poisson",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What criterion combines log_softmax and nll_loss in a single function?",
        "Y": "CosineEmbeddingLoss",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "CosineEmbeddingLoss combines what two criterions in a single function?",
        "Y": "log_softmax and nll_loss",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the criterion that combines log_softmax and nll_loss in a single function?",
        "Y": "CosineEmbeddingLoss",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of negative log likelihood loss is HingeEmbeddingLoss?",
        "Y": "Gaussian",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is a function that measures Binary Cross Entropy between target and output logits?",
        "Y": "Poisson negative log likelihood loss",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the function that measures Poisson negative log likelihood loss?",
        "Y": "CosineEmbeddingLoss",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What criterion combines in a single function?",
        "Y": "log_softmax and nll_loss",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of loss is HingeEmbeddingLoss?",
        "Y": "Gaussian negative log likelihood loss",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of negative log likelihood loss does CosineEmbeddingLoss measure?",
        "Y": "Poisson",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "CosineEmbeddingLoss combines what two criterion in a single function?",
        "Y": "log_softmax and nll_loss",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the criterion for Gaussian negative log likelihood loss?",
        "Y": "Connectionist Temporal Classification loss",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the term for the loss of log_softmax and nll_loss in a single function?",
        "Y": "Connectionist Temporal Classification loss",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of loss does HingeEmbeddingLoss measure?",
        "Y": "Gaussian negative log likelihood loss",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "CosineEmbeddingLoss combines log_softmax and nll_loss in a single function?",
        "Y": "Poisson",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the criterion that combines log_softmax and nll_loss in a single function?",
        "Y": "Connectionist Temporal Classification loss",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the Kullback-Leibler divergence Loss Function measure?",
        "Y": "Measures the element-wise mean squared error",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the Kullback-Leibler divergence Loss Function that measures the element-wise mean squared error?",
        "Y": "MarginRankingLoss",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the loss that combines log_softmax and nll_loss in a single function?",
        "Y": "Poisson negative log likelihood loss",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the function that combines log_softmax and nll_loss in a single function?",
        "Y": "CosineEmbeddingLoss",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the Kullback-Leibler divergence Loss Function called?",
        "Y": "MultiLabelMarginLoss",
        "Z": "The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the Kullback-Leibler divergence Loss Function use for details?",
        "Y": "MultiLabelSoftMarginLoss",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "CosineEmbeddingLoss combines what in a single function?",
        "Y": "log_softmax and nll_loss",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does this criterion combine in a single function?",
        "Y": "log_softmax and nll_loss",
        "Z": "This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the Kullback-Leibler divergence Loss Function that takes the mean element-wise absolute value difference?",
        "Y": "MultiLabelMarginLoss",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the Kullback-Leibler divergence Loss Function?",
        "Y": "MultiLabelSoftMarginLoss",
        "Z": "The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the MultiLabelSoftMarginLoss function?",
        "Y": "MultiLabelSoftMarginLoss",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "This criterion combines log_softmax and what else in a single function?",
        "Y": "nll_loss",
        "Z": "This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is HingeEmbeddingLoss for details?",
        "Y": "Gaussian negative log likelihood loss",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is another name for Gaussian negative log likelihood loss?",
        "Y": "HingeEmbeddingLoss",
        "Z": "The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is HingeEmbeddingLoss?",
        "Y": "Gaussian negative log likelihood loss",
        "Z": "Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=N",
        "Y": "negative log likelihood loss",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average",
        "Y": "negative log likelihood loss",
        "Z": "See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the result of multi_margin_loss?",
        "Y": "negative log likelihood loss",
        "Z": "The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does MarginRankingLoss do?",
        "Y": "Measures the element-wise mean squared error",
        "Z": "Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does MarginRankingLoss measure?",
        "Y": "the element-wise mean squared error",
        "Z": "Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a function use if the absolute element-wise error falls below delta?",
        "Y": "a squared term",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a function use if the absolute element-wise error falls below beta and an L1 term otherwise?",
        "Y": "a squared term",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What operation is reversed by rearranging elements in a tensor of shape?",
        "Y": "PixelShuffle",
        "Z": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the downscale_factor of a tensor of shape?",
        "Y": "Pads tensor",
        "Z": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What ranges elements in a tensor of shape?",
        "Y": "r",
        "Z": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Reverses what operation by rearranging elements in a tensor of shape?",
        "Y": "PixelShuffle",
        "Z": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the downscale_factor of the PixelShuffle operation?",
        "Y": "r",
        "Z": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the tensor of the PixelShuffle operation?",
        "Y": "Pads",
        "Z": "Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the step that samples the input to the given size or the given scale_factor?",
        "Y": "Down/up",
        "Z": "Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is used to compute the output of a flow-field grid?",
        "Y": "input values and pixel locations",
        "Z": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the tensor that Down/up samples the input to either the given size or the given scale_factor?",
        "Y": "Pads tensor",
        "Z": "Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is computed from a flow-field grid?",
        "Y": "input values and pixel locations",
        "Z": "Rearranges elements in a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W) to a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r), where r is the upscale_factor.   Reverses the PixelShuffle operation by rearranging elements in a tensor of shape (\u2217,C,H\u00d7r,W\u00d7r)(*, C, H \\times r, W \\times r)(\u2217,C,H\u00d7r,W\u00d7r) to a tensor of shape (\u2217,C\u00d7r2,H,W)(*, C \\times r^2, H, W)(\u2217,C\u00d7r2,H,W), where r is the downscale_factor.   Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of theta is used to generate a flow field?",
        "Y": "affine matrices",
        "Z": "Pads tensor.   Down/up samples the input to either the given size or the given scale_factor   Upsamples the input to either the given size or the given scale_factor   Upsamples the input, using nearest neighbours\u2019 pixel values.   Upsamples the input, using bilinear upsampling.   Given an input and a flow-field grid, computes the output using input values and pixel locations from grid.   Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices theta.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What dimension does the dot product version of this function not support an out parameter?",
        "Y": "1-dimensional",
        "Z": "This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "What is an alias for?",
        "Y": "torch.acos",
        "Z": "Alias for torch.acos().",
        "source": "https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos"
    },
    {
        "X": "What does it do when a tensor input is not specified?",
        "Y": "Counts the number of non-zero values in the tensor input along the given dim",
        "Z": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "X": "What is a tuple of dims along which to count non-zeros?",
        "Y": "python:ints",
        "Z": "Counts the number of non-zero values in the tensor input along the given dim.\nIf no dim is specified then all non-zeros in the tensor are counted. input (Tensor) \u2013 the input tensor. dim (int or tuple of python:ints, optional) \u2013 Dim or tuple of dims along which to count non-zeros. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"
    },
    {
        "X": "How much higher performance is torch.linalg.svd() compared to the full-rank SVD implementation?",
        "Y": "10-fold",
        "Z": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What type of matrices will low-rank SVD be useful for?",
        "Y": "sparse matrices",
        "Z": "Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "Why should you use the full-rank SVD implementation for dense matrices?",
        "Y": "10-fold higher performance characteristics",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What type of matrices is low-rank SVD useful for?",
        "Y": "huge sparse matrices",
        "Z": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "How much higher performance is torch.linalg.svd() compared to full-rank SVD?",
        "Y": "10-fold",
        "Z": "The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What is a slightly overestimated rank of A?",
        "Y": "q",
        "Z": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "The low-rank SVD is useful for what type of matrices that torch.linalg.svd() cannot handle?",
        "Y": "huge sparse matrices",
        "Z": "The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What will be useful for huge sparse matrices that torch.linalg.svd() cannot handle?",
        "Y": "low-rank SVD",
        "Z": "Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What is useful for huge sparse matrices that torch.linalg.svd() cannot handle?",
        "Y": "low-rank SVD",
        "Z": "In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What does q mean?",
        "Y": "a slightly overestimated rank of A. conduct",
        "Z": "A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n). Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding\nstructure with randomness: probabilistic algorithms for\nconstructing approximate matrix decompositions,\narXiv:0909.4061 [math.NA; math.PR], 2009 (available at\narXiv).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What grid is defined by expanding the iii th input over dimensions defined by other inputs?",
        "Y": "iii th",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "What can NNN tensors be?",
        "Y": "scalar or 1-dimensional vector",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "What will be treated as tensors of size automatically?",
        "Y": "Scalars",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "What will be treated as tensors of size?",
        "Y": "Scalars",
        "Z": "tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "If the input has what of size (N1,),(N2,),...,(Nk,)(N_1,), (N_2,",
        "Y": "kkk tensors",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "What is the name for the sequence of Tensors?",
        "Y": "seq",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "Returns True if the input is a single element tensor which is not equal to what?",
        "Y": "torch.tensor([0]) or torch.tensor([False])",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero\nafter type conversions.\ni.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or\ntorch.tensor([False]).\nThrows a RuntimeError if torch.numel() != 1 (even in case\nof sparse tensors). input (Tensor) \u2013 the input tensor. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero"
    },
    {
        "X": "What throws if torch.numel()!= 1?",
        "Y": "RuntimeError",
        "Z": "Returns True if the input is a single element tensor which is not equal to zero\nafter type conversions.\ni.e. not equal to torch.tensor([0.]) or torch.tensor([0]) or\ntorch.tensor([False]).\nThrows a RuntimeError if torch.numel() != 1 (even in case\nof sparse tensors). input (Tensor) \u2013 the input tensor. Examples:",
        "source": "https://pytorch.org/docs/stable/generated/torch.is_nonzero.html#torch.is_nonzero"
    },
    {
        "X": "What are the two variants of Torch's tensor types?",
        "Y": "CPU and GPU",
        "Z": "A torch.Tensor is a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the complex of a BFloat16Tensor torch?",
        "Y": "128-bit",
        "Z": "Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of tensor is a 32-bit floating point torch?",
        "Y": "dtype CPU tensor GPU tensor",
        "Z": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the GPU tensor?",
        "Y": "32-bit floating point torch",
        "Z": "CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the GPU tensor?",
        "Y": "32-bit floating point torch",
        "Z": "GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What bit floating point torch is GPU tensor?",
        "Y": "32",
        "Z": "GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of torch is float32?",
        "Y": "torch",
        "Z": "torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for a double-tensor torch?",
        "Y": "double torch",
        "Z": "torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of complex torch is included in the BFloat16Tensor 32-bit complex torch?",
        "Y": "8-bit",
        "Z": "torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for a double torch?",
        "Y": "double torch",
        "Z": "torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the floating point of a DoubleTensor torch?",
        "Y": "16-bit",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is 1 torch.float16 or torch.half torch?",
        "Y": "16-bit floating point",
        "Z": "16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are the names of the two types of torch?",
        "Y": "torch.float16 or torch.half torch",
        "Z": "torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How many byte complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or",
        "Y": "32",
        "Z": "torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of HalfTensor 16-bit floating point 2 torch?",
        "Y": "torch.cuda",
        "Z": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the 32-bit complex torch.complex32 64-bit complex torch.complex64?",
        "Y": "32-bit complex torch.complex32 64-bit complex torch.complex64",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of complex torch is a BFloat16Tensor 32-bit complex torch?",
        "Y": "64-bit",
        "Z": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.byteTensor",
        "Y": "32-bit complex torch.complex32 64-bit complex torch.complex64",
        "Z": "torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of complex torch is a BFloat16Tensor?",
        "Y": "32-bit",
        "Z": "64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How many bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch",
        "Y": "32",
        "Z": "torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed)",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the torch that has a 64-bit integer?",
        "Y": "LongTensor",
        "Z": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of torch is LongTensor?",
        "Y": "long torch",
        "Z": "torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of complex torch is a torch?",
        "Y": "64-bit",
        "Z": "64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the Boolean?",
        "Y": "LongTensor Boolean",
        "Z": "64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of torch is a LongTensor?",
        "Y": "Boolean torch",
        "Z": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a LongTensor Boolean torch.bool?",
        "Y": "LongTensor torch.cuda",
        "Z": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of torch is.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch",
        "Y": "128-bit complex torch",
        "Z": "128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for unsigned 8-bit integer torch.uint8 torch?",
        "Y": "torch.cdouble",
        "Z": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a torch.uint8 torch?",
        "Y": "8-bit integer",
        "Z": "8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned)",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed)",
        "Y": "torch.int8",
        "Z": "torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a CharTensor 16-bit integer (signed)?",
        "Y": "torch.int16 or torch.short torch",
        "Z": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Int16 or torch.short torch.ShortTensor torch.cuda.IntTensor 32-bit",
        "Y": "16",
        "Z": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How many 2-bit integers are in a torch?",
        "Y": "3",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a torch.int32 or torch.int torch?",
        "Y": "32-bit integer",
        "Z": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What language can a tensor be constructed from?",
        "Y": "Python",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are two ways to avoid a copy of a Tensor?",
        "Y": "requires_grad_() or detach()",
        "Z": "torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "A tensor can be constructed from a Python list or sequence using what constructor?",
        "Y": "torch.tensor()",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are two ways to avoid a copy of a tensor?",
        "Y": "requires_grad_() or detach()",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "If you have a numpy array and want to avoid a copy, use what?",
        "Y": "torch.as_tensor()",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "A tensor of specific data type can be constructed by passing a what?",
        "Y": "torch.dtype",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What always copies data?",
        "Y": "Warning torch.tensor()",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of data does torch.as_tensor() copy?",
        "Y": "numpy array",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "For more information about building Tensors, see what?",
        "Y": "Creation Ops",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What language can be used to access and modify the contents of a tensor?",
        "Y": "Python",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "The contents of a tensor can be accessed and modified using Python\u2019s what?",
        "Y": "indexing",
        "Z": "A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "The contents of a tensor can be accessed and modified using Python's what?",
        "Y": "indexing and slicing notation",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is required to create a tensor?",
        "Y": "requires_grad=True",
        "Z": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What holds data for each tensor?",
        "Y": "an associated torch.Storage",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the tensor class that defines numeric operations on it?",
        "Y": "Note",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.Tensor.item() get a Python number from?",
        "Y": "a tensor",
        "Z": "Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What holds a tensor's data?",
        "Y": "torch.Storage",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for a tensor class?",
        "Y": "Note",
        "Z": "Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "For more information on the torch.dtype, torch.device, and torch.layout attributes of a torch.Tensor,",
        "Y": "Tensor Attributes",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.FloatTensor.abs() compute the result in?",
        "Y": "a new tensor",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.FloatTensor.abs() do?",
        "Y": "Note",
        "Z": "Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are methods that mutate a tensor marked with?",
        "Y": "underscore suffix",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What method is used to change an existing tensor's torch.device and/or torch.dtype?",
        "Y": "to() method",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does the to() method do to an existing tensor?",
        "Y": "Warning",
        "Z": "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "For more information on the torch.Tensor, see what?",
        "Y": "Tensor Attributes",
        "Z": "For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.FloatTensor.abs_() do?",
        "Y": "computes the absolute value in-place and returns the modified tensor",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the warning that is issued when a tensor is changed?",
        "Y": "Warning",
        "Z": "For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What method can be used to change an existing tensor's torch.device and/or torch.dtype?",
        "Y": "to() method",
        "Z": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What computes the absolute value in-place and returns the modified tensor?",
        "Y": "torch.FloatTensor.abs_()",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a warning about a mutated tensor?",
        "Y": "Warning",
        "Z": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does the to() method do?",
        "Y": "Warning",
        "Z": "Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the warning that a method that mutates a tensor is marked with an underscore suffix?",
        "Y": "Warning",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does the to() method do to change an existing tensor's torch.device and/or torch.dtype?",
        "Y": "Warning",
        "Z": "Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What kind of memory usage might be caused by the current implementation of torch.Tensor?",
        "Y": "unexpectedly high",
        "Z": "Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does the current implementation of torch.Tensor introduce?",
        "Y": "memory overhead",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.tensor() create a tensor with?",
        "Y": "pre-existing data",
        "Z": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the tensor creation ops?",
        "Y": "Creation Ops",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of tensor does torch create?",
        "Y": "tensor with the same size (and similar types) as another tensor",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of tensor is similar to another tensor but different in size?",
        "Y": "tensor",
        "Z": "There are a few main ways to create a tensor, depending on your use case. To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of tensor is created with the same size as another tensor?",
        "Y": "tensor",
        "Z": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of tensor is created with the same type but different size as another tensor?",
        "Y": "tensor",
        "Z": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch create a tensor with?",
        "Y": "specific size",
        "Z": "To create a tensor with pre-existing data, use torch.tensor(). To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for tensor creation ops?",
        "Y": "Creation Ops",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "To create a tensor with similar type but different size as another tensor, use what?",
        "Y": "tensor.new_* creation ops",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "If n is the number of dimensions in x, x.T is equivalent to what?",
        "Y": "x.permute",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "If n is the number of dimensions in what integer, x.T is equivalent to x.permute?",
        "Y": "x",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What returns a new Tensor with data as the tensor data?",
        "Y": "Tensor",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does tensor.new_full return a Tensor of size size filled with?",
        "Y": "fill_value",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does tensor.new_empty return a Tensor of size size filled with?",
        "Y": "uninitialized data",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What returns a Tensor of size size filled with uninitialized data?",
        "Y": "Tensor.device",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Tensor.new_full return a Tensor of size size filled with?",
        "Y": "fill_value",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What returns a tensor of size size filled with uninitialized data?",
        "Y": "Tensor.new_ones",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "If n is the number of dimensions in what, x.T is equivalent to x.permute(n-1, n-2",
        "Y": "x",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does the Tensor.new_empty return a Tensor of size size filled with?",
        "Y": "uninitialized data",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.new_ones Returns a Tensor of size size filled with what value?",
        "Y": "1",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.new_zeros Returns a Tensor of size size filled with what value?",
        "Y": "0.",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What data does Tensor.new_empty return a Tensor of size size filled with?",
        "Y": "uninitialized data",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.new_ones Returns a Tensor of size size filled with what?",
        "Y": "1",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.new_zeros Returns a Tensor of size size filled with what?",
        "Y": "0.",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What returns a Tensor of size size filled with 0.",
        "Y": "Tensor.is_cuda",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is x.T equivalent to?",
        "Y": "x.permute",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Tensor.new_empty return a Tensor of size size filled with?",
        "Y": "uninitialized data",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is the Tensor.is_cuda True or False if the Tensor is stored on the GPU?",
        "Y": "True",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does a Tensor.new_empty return a Tensor of size size filled with?",
        "Y": "uninitialized data",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the tensor of size size filled with?",
        "Y": "0.",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is Tensor.is_cuda True or False if the Tensor is stored on the GPU?",
        "Y": "True",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does the new Tensor return as the tensor data?",
        "Y": "data",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.is_cuda Is True if the Tensor is stored on what?",
        "Y": "GPU",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is the Tensor.is_quantized True or False if the Tensor is quantized?",
        "Y": "True",
        "Z": "Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Returns what with data as the tensor data?",
        "Y": "a new Tensor",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the Tensor true if it is a meta tensor?",
        "Y": "meta tensor",
        "Z": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is True if the Tensor is a meta tensor?",
        "Y": "meta",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the Tensor of size size filled with?",
        "Y": "uninitialized data",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.is_quantized Is what if the Tensor is quantized?",
        "Y": "True",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of Tensor is True if the Tensor is a meta tensor?",
        "Y": "meta tensor",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Returns a Tensor of size size filled with what?",
        "Y": "1",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the device where the Tensor is located?",
        "Y": "torch",
        "Z": "Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the torch.device where the Tensor is located?",
        "Y": "the torch.device where this Tensor is",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What device is used to store the Tensor?",
        "Y": "torch",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the torch.device where the Tensor is?",
        "Y": "Tensor.grad",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Tensor.new_ones return?",
        "Y": "Tensor of size size filled with 1.",
        "Z": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.new_zeros Returns a Tensor of size size filled with how many zeros?",
        "Y": "0.",
        "Z": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What device is used to store a Tensor?",
        "Y": "torch",
        "Z": "Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the device where the Tensor is stored?",
        "Y": "torch",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the size size of a Tensor of size size filled with?",
        "Y": "1",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the device where the Tensor is?",
        "Y": "torch",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the torch.device where this Tensor is?",
        "Y": "Tensor.grad",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the default value of Tensor.grad?",
        "Y": "None",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Alias for dim() Tensor.real?",
        "Y": "dim",
        "Z": "Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is True if the Tensor is a what?",
        "Y": "meta tensor",
        "Z": "Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Alias for dim() Tensor.n Returns a new tensor containing real values of the self tens",
        "Y": "dim",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is Tensor.is_quantized True or False if the Tensor is quantized?",
        "Y": "True",
        "Z": "Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Alias for dim() Tensor.real Returns a new tensor containing real values of the self tens",
        "Y": "dim",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is the Tensor quantized?",
        "Y": "True",
        "Z": "Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the default value of the attribute Tensor.grad?",
        "Y": "None",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is True if the Tensor is what?",
        "Y": "quantized",
        "Z": "Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the default attribute of Tensor.grad?",
        "Y": "None",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is the Tensor.is_meta True or False if the Tensor is a meta tensor?",
        "Y": "True",
        "Z": "If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the In-place version of abs()?",
        "Y": "abs()",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is Tensor.is_meta True or False if the Tensor is a meta tensor?",
        "Y": "True",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the In-place version of abs()?",
        "Y": "abs()",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is the Tensor a meta tensor?",
        "Y": "True",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the device where the Tensor is located?",
        "Y": "torch.device",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the in-place version of abs() Tensor.absolute?",
        "Y": "abs() Tensor.abs",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is True if the Tensor is a meta tensor?",
        "Y": "if the Tensor is a meta tensor",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the torch.abs() Tensor.abs_ In-place version of?",
        "Y": "abs() Tensor.absolute",
        "Z": "Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is used for abs() Tensor.absolute?",
        "Y": "Alias",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the device where this Tensor is?",
        "Y": "torch.device",
        "Z": "Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Tensor.ndim Alias for dim() Tensor.real Return a new tensor containing?",
        "Y": "real values of the self tensor",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Alias does torch.acos() provide for?",
        "Y": "abs_() Tensor.acos",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for",
        "Y": "Tensor.abs",
        "Z": "Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]).",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the Alias for abs?",
        "Y": "Tensor.acos",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the default value of this attribute?",
        "Y": "None by default",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Alias for abs() Tensor.absolute_ In-place version of absolute()?",
        "Y": "abs() Tensor.absolute",
        "Z": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Alias for abs() Tensor.absolute Returns a new tensor containing real values of the self",
        "Y": "abs",
        "Z": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Alias for dim() Tensor.real Returns a new tensor containing what?",
        "Y": "real values of the self tensor",
        "Z": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does a new tensor do?",
        "Y": "add",
        "Z": "Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Returns a new tensor containing real values of the self tensor?",
        "Y": "real",
        "Z": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Returns a new tensor containing imaginary values of the self tensor?",
        "Y": "Tensor.abs",
        "Z": "Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does add to self tensor?",
        "Y": "a scalar or tensor",
        "Z": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of tensor can be added to a self tensor?",
        "Y": "scalar",
        "Z": "Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the In-place version of add() Tensor?",
        "Y": "add",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is added to self tensor?",
        "Y": "scalar or tensor",
        "Z": "Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the In-place version of Tensor.add?",
        "Y": "add() Tensor",
        "Z": "Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does add add to self tensor?",
        "Y": "scalar or tensor",
        "Z": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the In-place version of abs() Tensor?",
        "Y": "abs() Tensor",
        "Z": "Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the In-place version of addbmm() Tensor?",
        "Y": "addbmm",
        "Z": "Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the in-place version of abs() Tensor?",
        "Y": "abs() Tensor",
        "Z": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolut",
        "Y": "abs() Tensor",
        "Z": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the in-place version of Tensor?",
        "Y": "abs() Tensor",
        "Z": "Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the in-place version of add() Tensor?",
        "Y": "addbmm",
        "Z": "In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_()",
        "Y": "absolute",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the In-place version of add() Tensor?",
        "Y": "add",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the In-place version of Alias for abs() Tensor.absolute?",
        "Y": "abs() Tensor.absolute",
        "Z": "In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Tensor.absolute use for abs?",
        "Y": "Alias",
        "Z": "Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the In-place version of addcdiv?",
        "Y": "addcdiv",
        "Z": "Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What do you add to a scalar or tensor to self tensor?",
        "Y": "a scalar or tensor to self tensor",
        "Z": "See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward Computes the gradient of current tensor w.r.t. Tensor.baddbmm See torch.baddbmm() Tensor.baddbmm_ In-place version of baddbmm() Tensor.bernoulli Returns a result tensor where each result[i]\\texttt{result[i]}result[i] is independently sampled from Bernoulli(self[i])\\text{Bernoulli}(\\texttt{self[i]})Bernoulli(self[i]).",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the in-place version of absolute() Alias for abs_() Tensor?",
        "Y": "Alias for abs() Tensor",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the function that adds a scalar or tensor to self tensor?",
        "Y": "addcmul",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does add add add?",
        "Y": "a scalar or tensor to self tensor",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the In-place version of absolute() Alias for abs() Tensor?",
        "Y": "absolute",
        "Z": "Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is used for abs_() Tensor?",
        "Y": "Alias",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the in-place version of addcdiv?",
        "Y": "addcmul",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the In-place version of absolute() Alias for abs_() Tensor.acos?",
        "Y": "acos() Tensor.arccos",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the In-place version of absolute() Alias for abs_() Tensor?",
        "Y": "absolute",
        "Z": "Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Alias for what?",
        "Y": "torch.mul()",
        "Z": "Alias for torch.mul().",
        "source": "https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply"
    },
    {
        "X": "In what state is the PyTorch API of sparse tensors?",
        "Y": "beta",
        "Z": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does PyTorch implement as one of the storage formats for implementing sparse tensors?",
        "Y": "Coordinate format",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is another name for Coordinate format?",
        "Y": "COO format",
        "Z": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "In what format are the indices of specified elements collected?",
        "Y": "indices tensor of size",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the storage format for sparse tensors?",
        "Y": "Coordinate format",
        "Z": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the indices of specified elements collected in?",
        "Y": "indices tensor of size",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the indices of specified elements in COO format?",
        "Y": "torch.int64",
        "Z": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Which API of sparse tensors is in beta?",
        "Y": "PyTorch",
        "Z": "The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch implements what format for sparse tensors?",
        "Y": "Coordinate format",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the element type in the COO format?",
        "Y": "torch",
        "Z": "PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the dimensionality of the tensor and nse is the number of specified elements?",
        "Y": "ndim",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The indices of specified elements are collected in indices tensor of size (ndim) and with element type torch.int",
        "Y": "nse",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is another name for indices tensor of size?",
        "Y": "nse",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "In indices tensor of size, what is the dimensionality of the tensor?",
        "Y": "ndim",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the dimensionality of the tensor?",
        "Y": "ndim",
        "Z": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The memory consumption of a sparse COO tensor is at least what?",
        "Y": "nse bytes",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the number of specified elements in a tensor?",
        "Y": "nse",
        "Z": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What tensor is at least product(tensor shape>) * size of element type in bytes>?",
        "Y": "strided tensor",
        "Z": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of tensor is at least product(tensor shape>) * size of element type in bytes>?",
        "Y": "strided tensor",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How much memory saving occurs when using the COO storage format?",
        "Y": "200 fold",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the memory consumption of a 10 000 x 10 000 tensor with COO tensor layout?",
        "Y": "2 000 000 bytes",
        "Z": "For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What should we note about the input i?",
        "Y": "the input i is NOT a list of index tuples",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What would we write to define a sparse tensor?",
        "Y": "i",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The input i is NOT a list of what?",
        "Y": "index tuples",
        "Z": "Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Which hybrid COO tensor extends the sparse COO tensor?",
        "Y": "PyTorch",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What extends the sparse COO tensor?",
        "Y": "PyTorch hybrid COO tensor",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The indices of specified elements are collected in what indices of size?",
        "Y": "tensor",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The indices of specified elements are collected in indices tensor of size (sparse_dims, dense_",
        "Y": "nse",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the corresponding values of nse, dense_dims, and with an arbitrary integer or floating point number element type?",
        "Y": "tensor",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the values tensor of size?",
        "Y": "nse, dense_dims",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M =?",
        "Y": "s.sparse_dim()",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Where is the entry [5, 6] in a sparse tensor?",
        "Y": "(1, 0",
        "Z": "Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of tensor is s?",
        "Y": "sparse COO tensor",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the sparse COO tensor?",
        "Y": "s.sparse_dim()",
        "Z": "We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the sum of the number of sparse and dense dimensions?",
        "Y": "dimensionality",
        "Z": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If s is a COO tensor and M = s.sparse_dim(), K = s",
        "Y": "sparse",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "s.values().layout == torch.strided - values are stored as what?",
        "Y": "strided tensors",
        "Z": "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the meaning of strided tensors?",
        "Y": "Note",
        "Z": "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the dimensionality of a tensor the sum of the number of sparse and dense dimensions?",
        "Y": "s.ndim",
        "Z": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What permits uncoalesced sparse tensors?",
        "Y": "PyTorch sparse COO tensor format",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What uncoalesced tensor can be created when multiple values are specified for the same index?",
        "Y": "1-D",
        "Z": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the value at an uncoalesced sparse COO tensor?",
        "Y": "the sum of all duplicate value entries",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the torch.Tensor.is_coalesced() return True?",
        "Y": "Note",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the properties of the output of torch.Tensor.coalesce() method?",
        "Y": "the indices of specified tensor elements are unique",
        "Z": "s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the result of the torch.Tensor.is_coalesced() method?",
        "Y": "Note",
        "Z": "while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.Tensor.is_coalesced() return?",
        "Y": "True",
        "Z": "torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of sparse tensor will most operations work identically given?",
        "Y": "coalesced or uncoalesced sparse tensor",
        "Z": "In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What should you do to your sparse tensors to prevent them from growing too large?",
        "Y": "coalesce",
        "Z": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is an example of a lexicographical ordering of indices?",
        "Y": "Let\u2019s consider the following example",
        "Z": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is a torch.Tensor instance?",
        "Y": "a sparse COO tensor",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of a sparse COO tensor?",
        "Y": "torch",
        "Z": "Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What methods can be used to acquire the COO format data of a sparse COO tensor?",
        "Y": "methods torch.Tensor.indices() and torch.Tensor.values()",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does a sparse COO tensor do?",
        "Y": "Note",
        "Z": "As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How can the number of sparse and dense dimensions be acquired?",
        "Y": "methods torch.Tensor.sparse_dim() and torch.Tensor.dense_dim()",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices():",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Currently, one can acquire the COO format data only when the tensor instance is what?",
        "Y": "coalesced",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What dimensions can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.d",
        "Y": "sparse and dense",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices():",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If s is a what, then its COO format data can be acquired using methods torch.Tensor.indices() and torch",
        "Y": "sparse COO tensor",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices():",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "For acquiring the COO format data of what tensor, use torch.Tensor._values() and torch.T",
        "Y": "uncoalesced",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices():",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of tensor is uncoalesced?",
        "Y": "coalesced",
        "Z": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can one construct of a sparse COO tensor using the torch.Tensor.coalesce() method?",
        "Y": "coalesced copy",
        "Z": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can be used to construct a coalesced copy of a sparse COO tensor?",
        "Y": "the torch",
        "Z": "but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What method can be used to create a coalesced copy of a sparse COO tensor?",
        "Y": "torch",
        "Z": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can one construct of a sparse COO tensor using the torch?",
        "Y": "coalesced copy",
        "Z": "Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can be used to build a coalesced copy of a sparse COO tensor?",
        "Y": "torch",
        "Z": "but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can be done with the fill value of a sparse tensor?",
        "Y": "operations that may interpret the fill value differently",
        "Z": "Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions: In PyTorch, the fill value of a sparse tensor cannot be specified\nexplicitly and is assumed to be zero in general. However, there exists\noperations that may interpret the fill value differently. For\ninstance, torch.sparse.softmax() computes the softmax with the\nassumption that the fill value is negative infinity.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the three 1-D tensors in a CSR sparse tensor?",
        "Y": "crow_indices, col_indices and values",
        "Z": "A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the crow_indices tensor consist of?",
        "Y": "compressed row indices",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the crow_indices tensor?",
        "Y": "1-D tensor of size size[0] + 1",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What encodes the index in values and col_indices depending on where the given row starts?",
        "Y": "tensor",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does each successive number in the tensor subtracted by the number before it denote?",
        "Y": "the number of elements in a given row",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the size of the crow_indices tensor?",
        "Y": "1-D tensor",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The crow_indices tensor encodes the index in values and col_indices depending on what?",
        "Y": "where the given row starts",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the last element of the crow_indices tensor?",
        "Y": "number of non-zeros",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The crow_indices tensor encodes the index in values and what else depending on where the given row starts?",
        "Y": "col_indices",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the col_indices tensor?",
        "Y": "1-D tensor of size nnz",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the col_indices tensor contain?",
        "Y": "column indices of each value",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What tensor contains the column indices of each value?",
        "Y": "col_indices",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The index tensors crow_indices and col_indices should have element type either what?",
        "Y": "torch.int64",
        "Z": "The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If you want to use MKL-enabled matrix operations, use what?",
        "Y": "torch.int32",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the col_indices tensor of size nnz?",
        "Y": "1-D tensor",
        "Z": "The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What tensor contains the values of the CSR tensor?",
        "Y": "values",
        "Z": "The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the size of the CSR tensor?",
        "Y": "1-D",
        "Z": "The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If you want to use MKL-enabled matrix operations, what should you use?",
        "Y": "torch.int32",
        "Z": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the default element type for crow_indices and col_indices?",
        "Y": "torch.int64",
        "Z": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the default element type for index tensors?",
        "Y": "torch.int64",
        "Z": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The index tensors crow_indices and col_indices should have element type what?",
        "Y": "torch.int64",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the method used to construct sparse CSR matrices?",
        "Y": "torch",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The size argument is optional and will be deduced from what if it is not present?",
        "Y": "crow_indices and col_indices",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the simplest way of constructing a sparse CSR from a strided or sparse COO tens",
        "Y": "tensor",
        "Z": "Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Is the size argument optional or optional?",
        "Y": "optional",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The sparse matrix-vector multiplication can be performed with what method?",
        "Y": "tensor.matmul()",
        "Z": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The sparse matrix-vector multiplication is the only math operation supported on what?",
        "Y": "CSR tensors",
        "Z": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Any zeros in the strided tensor will be interpreted as what?",
        "Y": "missing values",
        "Z": "The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[layout] denote?",
        "Y": "matrix",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What denotes a scalar?",
        "Y": "f",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is a PyTorch operation?",
        "Y": "Sparse grad?",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is Layout signature torch.mv()?",
        "Y": "no",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does T[layout] denote with a given layout?",
        "Y": "a tensor",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the Layout signature?",
        "Y": "torch.mv()",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the default value for M[sparse_coo] at V[strided] -> V[stride",
        "Y": "no",
        "Z": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.mv() no M[sparse_coo] @ V[strided] -> V[s",
        "Y": "Layout signature",
        "Z": "PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the term for a hybrid sparse?",
        "Y": "Sparse grad",
        "Z": "Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the Sparse grad at V[strided] -> V[strided] torch?",
        "Y": "M[sparse_coo]",
        "Z": "Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the value of M[sparse_coo] at V[strided] -> V[strided",
        "Y": "no",
        "Z": "Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is not present at V[strided] -> V[strided] torch?",
        "Y": "M[sparse_coo]",
        "Z": "Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.mv() say about M[sparse_coo] at V[strided] -> V[",
        "Y": "no",
        "Z": "torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.mv() no?",
        "Y": "M[sparse_coo]",
        "Z": "torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the number of M[sparse_coo] at V[strided] -> V[strided",
        "Y": "no",
        "Z": "no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does no M[sparse_coo] @ V[strided] -> V[strided] torch",
        "Y": "M[sparse_coo]",
        "Z": "no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does.mv() no M[sparse_csr] @ V[strided] -> V[",
        "Y": "M[sparse_coo] @ V[strided] -> V[strided] torch",
        "Z": "M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_coo] do at V[strided] -> V[strided] torch",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the value of the M[sparse_csr] at V[strided] -> V[stri",
        "Y": "no",
        "Z": "torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the node that does not have a T[sparse_coo] at T[strided",
        "Y": "addmm",
        "Z": "torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the number of M[sparse_csr] at V[strided] -> V[stride",
        "Y": "no",
        "Z": "no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does no M[sparse_coo] @ M[strided] -> M[strided] torch",
        "Y": "addmm",
        "Z": "no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does matmul() no M[sparse_coo] @ M[strided] -> M[stride",
        "Y": "M[sparse_csr]",
        "Z": "M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Matmul() no what @ M[strided] -> M[strided] torch?",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Where does M[sparse_coo] come from?",
        "Y": "M[strided]",
        "Z": "torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "When does no M[sparse_coo] occur?",
        "Y": "M[strided]",
        "Z": "no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_coo] do at M[strided] -> M[strided] torch",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Where does M[sparse_csr] come from?",
        "Y": "M[strided]",
        "Z": "M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "When does no M[sparse_csr] occur?",
        "Y": "M[strided]",
        "Z": "no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_csr] do at M[strided] -> M[strided]",
        "Y": "M[sparse_csr]",
        "Z": "M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What @ M[strided] -> M[strided] torch?",
        "Y": "M[sparse_csr]",
        "Z": "M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_coo] @ M[strided] -> M[strided] torch?",
        "Y": "M[sparse_coo]",
        "Z": "no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.sparse.mm() say at M[strided] -> M[strided] torch?",
        "Y": "M[sparse_coo]",
        "Z": "torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.sparse.mm() yes?",
        "Y": "M[sparse_coo]",
        "Z": "torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[strided] -> M[strided] torch have?",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does lobpcg stand for?",
        "Y": "lobpcg",
        "Z": "yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.smm() no at M[strided] -> M[sparse_coo] torch?",
        "Y": "M[sparse_coo]",
        "Z": "torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does not exist at M[strided] -> M[sparse_coo] torch?",
        "Y": "M[sparse_coo]",
        "Z": "no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_coo] do at M[strided] -> M[sparse_co",
        "Y": "M[sparse_coo]",
        "Z": "no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_coo] mean?",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Where is M[sparse_coo] at?",
        "Y": "M[strided]",
        "Z": "M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What @ M[strided] -> M[sparse_coo] torch?",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does not exist at M[strided] -> M[hybrid sparse_coo] torch?",
        "Y": "M[sparse_coo]",
        "Z": "no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[sparse_coo] @ M[strided] -> M[hybrid sparse",
        "Y": "no",
        "Z": "no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[hybrid sparse_coo] mean?",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What @ M[strided] -> M[hybrid sparse_coo] torch?",
        "Y": "M[sparse_coo]",
        "Z": "M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does f * mean?",
        "Y": "M[sparse_coo]",
        "Z": "f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does sspaddmm() no?",
        "Y": "f * M[sparse_coo]",
        "Z": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the answer to T[sparse_coo] at T[strided]?",
        "Y": "no",
        "Z": "no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is T[sparse_coo] @ T[strided] -> T[strided] torch?",
        "Y": "no",
        "Z": "no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the acronym for \"M[sparse_coo]\"?",
        "Y": "PCA",
        "Z": "T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does M[strided] + f * mean at M[strided]?",
        "Y": "M[sparse_coo]",
        "Z": "no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does f * M[strided] + f * (M[sparse_coo] @ M[s",
        "Y": "M[sparse_coo]",
        "Z": "f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does f * M[strided] + f * mean at M[strided]?",
        "Y": "M[sparse_coo]",
        "Z": "torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is M[sparse_coo]?",
        "Y": "SVD",
        "Z": "yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.sspaddmm() no f *?",
        "Y": "M[sparse_coo]",
        "Z": "torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does torch.sspaddmm() no f * (M[sparse_coo] @ M[stride",
        "Y": "M[sparse_coo]",
        "Z": "torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided]",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the number of dense dimensions in a sparse tensor self?",
        "Y": "Tensor.dense_dim",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the number of sparse dimensions in a sparse tensor self?",
        "Y": "Tensor.sparse_dim",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the values of the strided tensor self filtered by?",
        "Y": "indices of the sparse tensor mask",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.is_sparse use if the Tensor uses?",
        "Y": "sparse storage layout",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the number of dense dimensions in a sparse tensor self?",
        "Y": "dense",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is returned in a sparse tensor self?",
        "Y": "number of dense dimensions",
        "Z": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the values from a strided tensor self filtered by?",
        "Y": "indices of the sparse tensor mask",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the values of the sparse tensor mask filtered by?",
        "Y": "indices",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Is True if the Tensor uses sparse storage layout?",
        "Y": "False",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What _dim Return the number of sparse dimensions in a sparse tensor self?",
        "Y": "sparse",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns a new sparse tensor with values from a strided tensor self filtered by the indice",
        "Y": "Tensor.sparse_mask",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is used to convert a tensor to compressed row storage format?",
        "Y": "Tensor.indices",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the sparse tensor mask filter?",
        "Y": "indices",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the new sparse tensor return values from?",
        "Y": "a strided tensor self filtered by the indices of the sparse tensor mask",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns the number of dense dimensions in a sparse tensor self?",
        "Y": "Tensor.dense_dim",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What return the number of sparse dimensions in a sparse tensor self?",
        "Y": "sparse",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does a sparse tensor self return?",
        "Y": "the number of sparse dimensions",
        "Z": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Return the number of dense dimensions in a what tensor self?",
        "Y": "sparse",
        "Z": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_dim return?",
        "Y": "the number of sparse dimensions in a sparse tensor self",
        "Z": "Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the return value of a sparse tensor?",
        "Y": "number of sparse dimensions in a sparse tensor self",
        "Z": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Where are the values of a new sparse tensor retrieved from?",
        "Y": "a strided tensor self filtered by the indices of the sparse tensor mask",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the sparse tensor mask filtered by?",
        "Y": "indices",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the following Tensor methods specific to sparse COO tensors?",
        "Y": "Tensor.coalesce",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of copy of self does Tensor.coalesce return if self is an uncoalesced tensor?",
        "Y": "coalesced",
        "Z": "Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Tensor method returns a coalesced copy of self if self is an uncoalesced tensor?",
        "Y": "Tensor.sparse_resize_",
        "Z": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is returned if self is an uncoalesced tensor?",
        "Y": "self",
        "Z": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns a coalesced copy of self if self is an uncoalesced tensor?",
        "Y": "Tensor.coalesce",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of copy of the tensor does Tensor._to_sparse_csr return?",
        "Y": "sparse",
        "Z": "Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What resizes self sparse tensor to the desired size and the number of sparse and dense dimensions?",
        "Y": "Tensor.sparse_resize",
        "Z": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Returns a coalesced copy of self if self is an what type of tensor?",
        "Y": "uncoalesced",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of tensor is self?",
        "Y": "uncoalesced",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_resize_ Resize self sparse tensor to?",
        "Y": "the desired size and the number of sparse and dense dimensions",
        "Z": "Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the Tensor method that resizes a sparse tensor?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is an uncoalesced tensor, what is returned?",
        "Y": "if self is an uncoalesced tensor",
        "Z": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the tensor method that resizes a sparse tensor?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_resize_ Resize?",
        "Y": "self sparse tensor",
        "Z": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the Tensor method that resizes a sparse COO tensor?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is another name for sparse tensors?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Return the tensor of a sparse COO tensor?",
        "Y": "indices",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_resize resize?",
        "Y": "self sparse tensor",
        "Z": "Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What happens when a sparse tensor is removed?",
        "Y": "resizes self to the desired size and the number of sparse and dense dimensions",
        "Z": "Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does return the value of a sparse COO tensor?",
        "Y": "values tensor of a sparse COO tensor",
        "Z": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What removes all specified elements from a sparse tensor?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is returned by the value tensor of a sparse COO tensor?",
        "Y": "Return the values tensor of a sparse COO tensor",
        "Z": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Returns a coalesced copy of self if self is an what tensor?",
        "Y": "uncoalesced",
        "Z": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor, what type of tensor is it?",
        "Y": "uncoalesced",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is _coalesced?",
        "Y": "Tensor.is",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "When does Tensor.coalesce return a coalesced copy of self?",
        "Y": "if self is an uncoalesced tensor",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what returns true?",
        "Y": "False",
        "Z": "Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns true if self is a sparse COO tensor that is coalesced?",
        "Y": "Tensor.to_dense",
        "Z": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what type of tensor is returned?",
        "Y": "uncoalesced",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what return does Tensor.is_co",
        "Y": "True",
        "Z": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of copy of self is returned if self is an uncoalesced tensor?",
        "Y": "coalesced",
        "Z": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Returns what type of self if self is an uncoalesced tensor?",
        "Y": "coalesced copy",
        "Z": "Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.sparse_resize_and_clear_ do to a sparse tensor?",
        "Y": "Removes all specified elements",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The following methods are specific to what?",
        "Y": "sparse CSR tensors",
        "Z": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what value does Tensor.is_co",
        "Y": "True",
        "Z": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What resizes to the desired size and the number of sparse and dense dimensions?",
        "Y": "self sparse tensor",
        "Z": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Is self a sparse COO tensor that is coalesced?",
        "Y": "False",
        "Z": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What methods are specific to sparse CSR tensors?",
        "Y": "Tensor.crow_indices",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Resizes self sparse tensor to what size?",
        "Y": "the desired size",
        "Z": "Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What happens to a sparse tensor?",
        "Y": "resizes self to the desired size and the number of sparse and dense dimensions",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, what return value is returned?",
        "Y": "False",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The following methods are specific to sparse what?",
        "Y": "CSR tensors",
        "Z": "Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What removes all specified elements from a sparse tensor self?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does resize self to the desired size and the number of sparse and dense dimensions?",
        "Y": "Removes all specified elements",
        "Z": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does tensor.col_indices return when self is a sparse CSR tensor of layout sparse",
        "Y": "column indices of the self tensor",
        "Z": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the tensor return when self is a sparse CSR tensor of layout sparse_csr",
        "Y": "column indices",
        "Z": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Returns True if self is a sparse COO tensor that is coalesced, what else?",
        "Y": "False",
        "Z": "Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the tensor containing the compressed row indices of the self tensor when self is a sparse C",
        "Y": "Tensor.col_indices",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Returns what if self is a sparse COO tensor that is coalesced?",
        "Y": "True",
        "Z": "Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the tensor containing the column indices of the self tensor when self is a sparse CSR",
        "Y": "Tensor.col_indices",
        "Z": "The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.crow_indices return when self is a sparse CSR tensor of layout sparse",
        "Y": "tensor containing the compressed row indices of the self tensor",
        "Z": "Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does Tensor.col_indices return when self is a sparse CSR tensor of layout sparse_",
        "Y": "column indices of the self tensor",
        "Z": "Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The following methods are specific to what type of CSR tensors?",
        "Y": "sparse",
        "Z": "Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What indices of the self tensor is returned when self is a sparse CSR tensor of layout spars",
        "Y": "column",
        "Z": "Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Creates what copy of self?",
        "Y": "strided copy",
        "Z": "Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What indices does the tensor return when self is a sparse CSR tensor of layout sparse_",
        "Y": "column",
        "Z": "Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "When self is a sparse CSR tensor of layout what?",
        "Y": "sparse_csr",
        "Z": "Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does return when self is a sparse CSR tensor of layout sparse_csr?",
        "Y": "the tensor containing the compressed row indices",
        "Z": "Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Returns the tensor containing what indices of the self tensor when self is a sparse CSR",
        "Y": "column",
        "Z": "Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Self is a sparse CSR tensor of what?",
        "Y": "layout sparse_csr",
        "Z": "Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "When self is a sparse CSR tensor of layout, what does Tensor.col_indices return?",
        "Y": "sparse_csr",
        "Z": "Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is a sparse CSR tensor of layout sparse_csr?",
        "Y": "self",
        "Z": "Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Returns what when self is a sparse CSR tensor of layout sparse_csr?",
        "Y": "the tensor containing the column indices of the self tensor",
        "Z": "Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the method that supports sparse COO tensors?",
        "Y": "t_()",
        "Z": "The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the specified values for a sparse tensor in CSR?",
        "Y": "crow_indices and col_indices",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What ensions dim does sparse.sum return the sum of each row of the sparse tensor input in?",
        "Y": "dim",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does sparse.addmm do exactly the same thing as?",
        "Y": "torch.addmm()",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the function that supports backward for sparse matrix mat1?",
        "Y": "sparse.mm",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the specified values for the sparse tensor?",
        "Y": "crow_indices and col_indices",
        "Z": "Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of ensions dim does sparse.sum return?",
        "Y": "dim",
        "Z": "Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the sum of each row of the sparse tensor input in the given dimensions dim?",
        "Y": "sparse.sum",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does sparse.mm perform a matrix multiplication of?",
        "Y": "sparse matrix mat1 and the (sparse or strided) matrix mat2",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What multiplies a sparse tensor mat1 with a dense tensor mat2?",
        "Y": "Matrix",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Returns the sum of each row of the sparse tensor input in what ensions dim?",
        "Y": "dim",
        "Z": "Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does sparse.addmm return?",
        "Y": "the sum of each row of the sparse tensor input",
        "Z": "Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does sparse.mm perform of the sparse matrix mat1 and the (sparse or strided) matrix mat2",
        "Y": "matrix multiplication",
        "Z": "sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What performs a sparse COO matrix mat1 and a strided matrix mat2?",
        "Y": "matrix multiplication",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Matrix performs a matrix multiplication of a sparse COO matrix mat1 and what else?",
        "Y": "a strided matrix mat2",
        "Z": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does this function do exactly the same thing as in the forward?",
        "Y": "torch.addmm()",
        "Z": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of matrix does matrix multiplication perform?",
        "Y": "sparse",
        "Z": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does this function support backward for?",
        "Y": "sparse matrix mat1",
        "Z": "This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2",
        "Y": "sparse.mm",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does sparse.mm perform of a sparse COO matrix mat1 and a strided matrix mat2?",
        "Y": "matrix multiplication",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is used to perform a matrix multiplication of the sparse matrix input?",
        "Y": "dense matrix mat",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does sparse.log_apply?",
        "Y": "softmax",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What do sparse.mm perform a matrix multiplication of?",
        "Y": "sparse COO matrix mat1 and a strided matrix mat2",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does sparse.mm perform a matrix multiplication of with the dense matrix mat?",
        "Y": "sparse matrix input",
        "Z": "sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What applies a softmax function?",
        "Y": "softmax",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is performed of the sparse matrix mat1 and the (sparse or strided) matrix mat2?",
        "Y": "matrix multiplication",
        "Z": "Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Matrix performs a matrix multiplication of what?",
        "Y": "sparse COO matrix mat1 and a strided matrix mat2",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Matrix multiplies sparse matrix input with what?",
        "Y": "dense matrix mat",
        "Z": "Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Applies a softmax function followed by logarithm?",
        "Y": "sparse.log_softmax",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Which torch function supports sparse tensors?",
        "Y": "hstack()",
        "Z": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is_signed() is_tensor()?",
        "Y": "same_size()",
        "Z": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the torch function that supports sparse tensors?",
        "Y": "svd_lowrank()",
        "Z": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What do the following torch functions support?",
        "Y": "sparse tensors",
        "Z": "The following torch functions support sparse tensors: cat()\ndstack()\nempty()\nempty_like()\nhstack()\nindex_select()\nis_complex()\nis_floating_point()\nis_nonzero()\nis_same_size()\nis_signed()\nis_tensor()\nlobpcg()\nmm()\nnative_norm()\npca_lowrank()\nselect()\nstack()\nsvd_lowrank()\nunsqueeze()\nvstack()\nzeros()\nzeros_like()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Returns what with the sine of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the sine of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin"
    },
    {
        "X": "What does torch.full_like(input, fill_value, layout=input.layout, device=input.device",
        "Y": "fill_value",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "What is equivalent to torch.full?",
        "Y": "torch.full_like",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "What is returned with the same size as input filled with fill_value?",
        "Y": "tensor",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "What is equivalent to torch.full(input.size(), fill_value, dtype=input.dtype, layout=",
        "Y": "torch.full",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "What is fill_value?",
        "Y": "the number to fill the output tensor with",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "dtype (torch.dtype, optional) \u2013 what?",
        "Y": "desired data type of returned tensor",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What happens if None?",
        "Y": "defaults to the dtype of input",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "What determines size of the output tensor?",
        "Y": "input",
        "Z": "input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "The STFT computes the Fourier transform of short overlapping windows of the input to give what of the signal as they change over time?",
        "Y": "frequency components",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression:",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What does the STFT compute without the optional batch dimension?",
        "Y": "the following expression",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression:",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Input must be either a time sequence or a 2-D batch of time sequences?",
        "Y": "1-D time sequence or a 2-D batch of time sequences",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4).",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If hop_length is None, it is treated as equal to what?",
        "Y": "floor(n_fft / 4)",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If hop_length is None (default), it is treated as equal to what?",
        "Y": "floor(n_fft / 4)",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Input must be a what?",
        "Y": "1-D time sequence or a 2-D batch of time sequences",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_length is what, it is treated as equal to n_fft?",
        "Y": "None",
        "Z": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_length is None (default), it is treated as equal to what?",
        "Y": "n_fft",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_length is None, it is treated as equal to what?",
        "Y": "n_fft",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the win_length of a window?",
        "Y": "1-D tensor of size",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If window is None (default), it is treated as if having what number everywhere in the window?",
        "Y": "111",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What happens to window if win_lengthn_ffttextwin_lengthn_fft?",
        "Y": "padded on both sides to length n_fft",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What can a window be a size of win_length?",
        "Y": "1-D tensor",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_lengthn_ffttextwin_lengthn_fft, window will be what on both sides?",
        "Y": "padded",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What happens to window if win_lengthn_ffttextwin_length  textn",
        "Y": "padded on both sides to length n_fft",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What can a window be of size win_length?",
        "Y": "1-D tensor",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_lengthn_ffttextwin_lengthn_fft, window will be what on both sides",
        "Y": "padded",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If window is None (default), it is treated as if it has what number everywhere in the window?",
        "Y": "111",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If window is what (default) value, it is treated as if having 111 everywhere in the window?",
        "Y": "None",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is centered at time thop_lengtht times texthop_lengththop_length",
        "Y": "ttt-th frame",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What begins at time thop_lengtht times texthop_lengththop_length?",
        "Y": "ttt-th frame",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What determines the padding method used on input when center is True?",
        "Y": "pad_mode",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the name of the function that determines the padding method used on input when center is True?",
        "Y": "torch.nn.functional.pad()",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default padding method used when center is True?",
        "Y": "reflect",
        "Z": "pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If center is True, input will be padded on both sides so that which frame is centered at time thop_lengtht",
        "Y": "ttt-th frame",
        "Z": "If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the name of the padding method used on input when center is True?",
        "Y": "torch.nn.functional.pad()",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default setting for padding when center is True?",
        "Y": "\"reflect\"",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If onesided is what (default for real input)?",
        "Y": "True",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If what is true, the function returns the normalized STFT results?",
        "Y": "normalized is True",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If return_complex is True, the return is what?",
        "Y": "input.dim()",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If return_complex is False, the output is what?",
        "Y": "input.dim()",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4))",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value of normalized?",
        "Y": "False",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the return if return_complex is True?",
        "Y": "input.dim()",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4))",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the last dimension of the input.dim() + 2 dimensional real tensor?",
        "Y": "the last dimension represents the real and imaginary components",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If what is true, returns a complex tensor of size?",
        "Y": "return_complex",
        "Z": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the total number of frames used?",
        "Y": "TTT",
        "Z": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Returns either a complex tensor of size (NT)(* times N times T)",
        "Y": "if return_complex is true",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for Win_length?",
        "Y": "floor(n_fft / 4)",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for the size of window frame and STFT filter?",
        "Y": "None",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is window (Tensor, optional)?",
        "Y": "optional window function",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default: None (treated as window of all how many s)",
        "Y": "111",
        "Z": "hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default window of all 111 s?",
        "Y": "None",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\"",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "n_fft (int) \u2013 size of what?",
        "Y": "Fourier transform hop_length",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is n_fft equal to?",
        "Y": "floor",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default: None (treated as equal to n_fft) window (Tensor, what) \u2013 the optional window function",
        "Y": "optional",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is hop_length equal to?",
        "Y": "floor(n_fft / 4)",
        "Z": "hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default: None (treated as window of all?",
        "Y": "111 s",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What controls the padding method used when center is True?",
        "Y": "True pad_mode",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default name of the padding method used when center is True?",
        "Y": "\"reflect\"",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\"",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for the padding method used when center is True?",
        "Y": "reflect",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\"",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the window function?",
        "Y": "optional",
        "Z": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Window (Tensor) \u2013 the optional window function. Default: None (treated as window of all 111 s)",
        "Y": "optional",
        "Z": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default setting for return normalized STFT results?",
        "Y": "False",
        "Z": "window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is center?",
        "Y": "whether to pad input on both sides",
        "Z": "center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What do you want to avoid with real inputs?",
        "Y": "redundancy",
        "Z": "pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default for real input and window?",
        "Y": "True",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for normalized STFT results?",
        "Y": "False",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default: False onesided (bool, optional) \u2013 controls whether to return what percentage of results?",
        "Y": "half",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for real input and window?",
        "Y": "True",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "A tensor containing the STFT result with what described above Tensor containing the STFT result with?",
        "Y": "shape",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Default: False onesided (bool, optional) \u2013 controls whether to return half of results to avoid what for real inputs?",
        "Y": "redundancy",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What bool controls whether to return a complex tensor, or a real tensor with an extra last dimension for the",
        "Y": "return_complex",
        "Z": "normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What does dim represent?",
        "Y": "the dimension or dimensions to approximate the gradient over",
        "Z": "This function is analogous to NumPy\u2019s gradient function. {input} \u2013  spacing (scalar, list of scalar, list of Tensor, optional) \u2013 implicitly or explicitly represents coordinates the function is evaluated at (the) \u2013  dim (int, list of python:int, optional) \u2013 the dimension or dimensions to approximate the gradient over. edge_order (int, optional) \u2013 unsupported (must be equal to its default value which is 1.) Example",
        "source": "https://pytorch.org/docs/stable/generated/torch.gradient.html#torch.gradient"
    },
    {
        "X": "What does the number of bins in an array of non-negative ints do?",
        "Y": "Count the frequency of each value in an array of non-negative ints",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "If what is specified, the number of bins is at least minlength and if input is empty, the result is tensor of size",
        "Y": "minlength",
        "Z": "The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What is the value at position i?",
        "Y": "n",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What does the number of bins in an array of non-negative ints need to be one larger than the largest value in input?",
        "Y": "Note",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What is the result of the number of bins (size 1) being one larger than the largest value in input unless input is empty?",
        "Y": "tensor of size 0.",
        "Z": "The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What does the number of bins (size 1) have to be larger than the largest value in input?",
        "Y": "Note",
        "Z": "The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What should the minimum number of bins be?",
        "Y": "non-negative",
        "Z": "Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What happens to a tensor of shape Size([max(input) + 1])?",
        "Y": "if input is non-empty",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What does torch.conj() do?",
        "Y": "Computes the element-wise conjugate",
        "Z": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"
    },
    {
        "X": "What has a non-complex dtype?",
        "Y": "input",
        "Z": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"
    },
    {
        "X": "What may return a non-writeable view for an input of non-complex dtype?",
        "Y": "torch.conj()",
        "Z": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"
    },
    {
        "X": "What function returns a non-writeable view for an input of non-complex dtype?",
        "Y": "torch.conj()",
        "Z": "Computes the element-wise conjugate of the given input tensor. If input has a non-complex dtype,\nthis function just returns input. Warning In the future, torch.conj() may return a non-writeable view for an input of\nnon-complex dtype. It\u2019s recommended that programs not modify the tensor returned by torch.conj()\nwhen input is of non-complex dtype to be compatible with this change. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"
    },
    {
        "X": "What is the name of the alias?",
        "Y": "torch.vstack()",
        "Z": "Alias of torch.vstack().",
        "source": "https://pytorch.org/docs/stable/generated/torch.row_stack.html#torch.row_stack"
    },
    {
        "X": "Fills self tensor with elements samples from the normal distribution parameterized by what?",
        "Y": "mean and std",
        "Z": "Fills self tensor with elements samples from the normal distribution\nparameterized by mean and std.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"
    },
    {
        "X": "What fills with elements from the normal distribution?",
        "Y": "self tensor",
        "Z": "Fills self tensor with elements samples from the normal distribution\nparameterized by mean and std.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"
    },
    {
        "X": "Fills what with elements samples from the normal distribution parameterized by mean and std?",
        "Y": "self tensor",
        "Z": "Fills self tensor with elements samples from the normal distribution\nparameterized by mean and std.",
        "source": "https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"
    },
    {
        "X": "What does torch.as_tensor() copy data from?",
        "Y": "NumPy ndarray",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.tensor() always copy data?",
        "Y": "a Tensor data",
        "Z": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of data does torch.Tensor.requires_grad_() or torch.Tensor.detach",
        "Y": "Tensor",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.as_tensor() use if you want to avoid a copy of data?",
        "Y": "NumPy ndarray",
        "Z": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.tensor always copy data?",
        "Y": "Warning",
        "Z": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.tensor do?",
        "Y": "always copies data",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What is a warning about a copy of data by a Tensor?",
        "Y": "Warning",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What do you use if you want to avoid a copy of a Tensor data?",
        "Y": "torch.Tensor.requires_grad_() or torch.Tensor.detach()",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What ndarray does torch.as_tensor() avoid a copy of?",
        "Y": "NumPy",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.as_tensor() do?",
        "Y": "Warning",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What variable does torch.tensor() construct when data is a tensor x?",
        "Y": "leaf",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What is equivalent to x.clone().detach()?",
        "Y": "torch.tensor(x)",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What are the equivalents of torch.tensor?",
        "Y": "clone() and detach()",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What can data (array_like) \u2013 Initial data for the tensor be?",
        "Y": "a list, tuple, NumPy ndarray, scalar, and other types",
        "Z": "Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What can initial data for the tensor be?",
        "Y": "a list, tuple, NumPy ndarray, scalar, and other types",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What infers data type from data?",
        "Y": "if None",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What will device be for CPU tensor types?",
        "Y": "CPU",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "Default: if what, infers data type from data. device (torch.device, optional) \u2013 the desired device",
        "Y": "None",
        "Z": "When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What is the device for CPU tensor types?",
        "Y": "CPU",
        "Z": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors.",
        "Y": "pin_memory",
        "Z": "requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What is the default setting for pin_memory?",
        "Y": "False",
        "Z": "requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. pin_memory (bool, optional) \u2013 If set, returned tensor would be allocated in\nthe pinned memory. Works only for CPU tensors. Default: False. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What is an example of a return tensor?",
        "Y": "Example:",
        "Z": "device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "Returns what of the upper triangular part of a row by col matrix in a 2-by-N Tensor?",
        "Y": "the indices",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "The dividend and divisor may contain both for what?",
        "Y": "integer and floating point numbers",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled.",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "The remainder of division has the same what as the divisor other?",
        "Y": "sign",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled.",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What is supported to a common shape, type promotion, and integer and float inputs?",
        "Y": "broadcasting",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled.",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What has the same sign as the divisor other?",
        "Y": "The remainder",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "Supports what type of programming?",
        "Y": "broadcasting",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the divisor other. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What is the input for the dividend?",
        "Y": "Tensor",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "Supports what to a common shape, type promotion, and integer and float inputs?",
        "Y": "broadcasting",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "In some cases, it is not mathematically possible to satisfy the definition of a modulo operation with what?",
        "Y": "complex numbers",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What is the input (Tensor) \u2013 the divisor out (Tensor, optional) \u2013 the output tens",
        "Y": "the dividend",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What computes the element-wise remainder of division equivalently to the C library function fmod()?",
        "Y": "torch.fmod()",
        "Z": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod().",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What is the input (Tensor) referred to as?",
        "Y": "the dividend",
        "Z": "Note Complex inputs are not supported. In some cases, it is not mathematically\npossible to satisfy the definition of a modulo operation with complex numbers.\nSee torch.fmod() for how division by zero is handled. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example: See also torch.fmod(), which computes the element-wise remainder of\ndivision equivalently to the C library function fmod().",
        "source": "https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"
    },
    {
        "X": "What type of tensor does torch return?",
        "Y": "2-D",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the number of rows in the output tensor?",
        "Y": "n",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the default value of torch.set_default_tensor_type()?",
        "Y": "if None",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the default setting of the returned Tensor?",
        "Y": "torch.strided",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is returned with ones on the diagonal and zeros elsewhere?",
        "Y": "a 2-D tensor",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the default number of columns in the output tensor?",
        "Y": "n out",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "Default: if None, uses a what default?",
        "Y": "global",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the default setting for a 2-D tensor?",
        "Y": "torch.strided",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What does n out represent?",
        "Y": "the output tensor",
        "Z": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the default setting for the returned Tensor?",
        "Y": "torch.strided",
        "Z": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "n (int) \u2013 the nu of rows m (int, optional) \u2013 the number of columns with default being n",
        "Y": "m",
        "Z": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "If None, uses a global default (see torch.set_default_tensor_type()).",
        "Y": "Default",
        "Z": "out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the default setting of returned Tensor?",
        "Y": "torch.strided",
        "Z": "n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "Out (Tensor, optional) - what is the name of the output tensor?",
        "Y": "output tensor",
        "Z": "out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the default setting for the output tensor?",
        "Y": "torch.strided",
        "Z": "out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the default setting for the current device for the default tensor type?",
        "Y": "if None",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is a tensor with ones on the diagonal and zeros elsewhere?",
        "Y": "2-D tensor",
        "Z": "device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "Returns what with the truncated integer values of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the truncated integer values of\nthe elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc"
    },
    {
        "X": "What is an example of a new tensor?",
        "Y": "Example",
        "Z": "Returns a new tensor with the truncated integer values of\nthe elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc"
    },
    {
        "X": "Returns what with the logarithm to the base 2 of the elements of input?",
        "Y": "a new tensor",
        "Z": "Returns a new tensor with the logarithm to the base 2 of the elements\nof input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2"
    },
    {
        "X": "What is the name of the Alias for?",
        "Y": "torch.special.expm1",
        "Z": "Alias for torch.special.expm1().",
        "source": "https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1"
    },
    {
        "X": "What is the window_length?",
        "Y": "window length",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Computes the Kaiser window with window length window_length and what other parameter?",
        "Y": "shape parameter beta",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes:",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What does torch.i0() do?",
        "Y": "Let I_0 be the zeroth order modified Bessel function of the first kind",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Computes the Kaiser window with what parameter?",
        "Y": "window length",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes:",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the zeroth order modified Bessel function of the first kind?",
        "Y": "L",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is equivalent to calling torch.kaiser_window?",
        "Y": "Calling torch.kaiser_window",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the periodic argument intended for?",
        "Y": "a helpful shorthand",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the periodic argument intended to produce a periodic window as input to functions like torch.stft()?",
        "Y": "Note",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the name of the zeroth order modified Bessel function of the first kind?",
        "Y": "L",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1]?",
        "Y": "torch.kaiser_window(L, B, periodic=True)",
        "Z": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the periodic argument intended as a useful shorthand to produce a periodic window as input to functions like torch.stft()?",
        "Y": "Note",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)?",
        "Y": "Calling torch.kaiser_window",
        "Z": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the returned window if window_length is one?",
        "Y": "a single element tensor containing a one",
        "Z": "Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "The periodic argument is intended as a useful shorthand to produce a periodic window as input to functions like what?",
        "Y": "torch.stft()",
        "Z": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is window_length?",
        "Y": "length of the window",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the window returned if window_length is one?",
        "Y": "a single element tensor",
        "Z": "Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What does window_length (int) mean?",
        "Y": "length of the window",
        "Z": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What does beta (float, optional) represent for the window?",
        "Y": "shape parameter",
        "Z": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "If window_length is one, what is the returned window?",
        "Y": "a single element tensor containing a one",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the name of the window that is suitable for use in spectral analysis?",
        "Y": "periodic",
        "Z": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What does torch.set_default_tensor_type() use?",
        "Y": "global default",
        "Z": "periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the periodic window suitable for?",
        "Y": "spectral analysis",
        "Z": "periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What does beta (float, optional) provide for the window?",
        "Y": "shape parameter",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is dtype?",
        "Y": "the desired data type of returned tensor",
        "Z": "periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the only option that supports dense layout?",
        "Y": "torch.strided",
        "Z": "beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What parameter does beta (float, optional) provide for the window?",
        "Y": "shape parameter",
        "Z": "beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Default: if None, uses a what?",
        "Y": "global default",
        "Z": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the term for dense layout?",
        "Y": "torch.strided",
        "Z": "Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the name for a dense layout?",
        "Y": "torch.strided",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the name of the desired layout of returned window tensor?",
        "Y": "torch.layout",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the default layout of a window tensor?",
        "Y": "torch.strided",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the layout of returned window tensor?",
        "Y": "desired layout",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the default layout of returned window tensor?",
        "Y": "torch.strided",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Device will be what for CPU tensor types and the current CUDA device for CUDA tensor types?",
        "Y": "CPU",
        "Z": "layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Computes what of a square matrix for an integer n?",
        "Y": "n-th power",
        "Z": "Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "What is the first column of a product of Householder matrices?",
        "Y": "first n columns",
        "Z": "Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "What is the power of a square matrix for an integer n?",
        "Y": "n-th power",
        "Z": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "Efficiently multiplies two or more matrices by what?",
        "Y": "reordering",
        "Z": "Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "What does torch.tensordot() compute?",
        "Y": "multiplicative inverse",
        "Z": "Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "Computes the solution X to the system torch.tensordot(A, X) = what?",
        "Y": "B",
        "Z": "Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "What type of tensor does this function return?",
        "Y": "uncoalesced tensor",
        "Z": "Note This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What types of indices can be used for the tensor?",
        "Y": "a list, tuple, NumPy ndarray, scalar, and other types",
        "Z": "Note This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What is cast to the LongTensor internally?",
        "Y": "torch",
        "Z": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What is the first dimension of the indices?",
        "Y": "the number of tensor dimensions",
        "Z": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What can indices be?",
        "Y": "a list, tuple, NumPy ndarray, scalar, and other types",
        "Z": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What is the name of the internal tensor?",
        "Y": "LongTensor",
        "Z": "Note This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "Can be a list, NumPy ndarray, scalar, and other types?",
        "Y": "tuple",
        "Z": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What is the name of the tensor internally?",
        "Y": "LongTensor",
        "Z": "This function returns an uncoalesced tensor. indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What are some other types of indices?",
        "Y": "tuple, NumPy ndarray, scalar",
        "Z": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What will be cast to the LongTensor internally?",
        "Y": "torch",
        "Z": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What is the initial value for?",
        "Y": "tensor",
        "Z": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What are some other types of initial values for the tensor?",
        "Y": "tuple, NumPy ndarray, scalar",
        "Z": "indices (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. Will be cast to a torch.LongTensor\ninternally. The indices are the coordinates of the non-zero values in the matrix, and thus\nshould be two-dimensional where the first dimension is the number of tensor dimensions and\nthe second dimension is the number of non-zero values. values (array_like) \u2013 Initial values for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What is optional for a sparse tensor?",
        "Y": "size",
        "Z": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What default infers data type from values?",
        "Y": "if None",
        "Z": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What is the size of the sparse tensor if not provided?",
        "Y": "minimum size big enough to hold all non-zero elements",
        "Z": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "Default: if what, infers data type from values?",
        "Y": "None",
        "Z": "size (list, tuple, or torch.Size, optional) \u2013 Size of the sparse tensor. If not\nprovided the size will be inferred as the minimum size big enough to hold all non-zero\nelements. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What infers data type from values?",
        "Y": "if None",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "Default: if what, infers data type from values. device (torch.device, optional) \u2013 the desired device",
        "Y": "None",
        "Z": "dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from values. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.sparse_coo_tensor.html#torch.sparse_coo_tensor"
    },
    {
        "X": "What is treated as False?",
        "Y": "Zeros",
        "Z": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "X": "What is the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor?",
        "Y": "other",
        "Z": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "X": "What is an example of a tensor to compute OR with out?",
        "Y": "Example",
        "Z": "Computes the element-wise logical OR of the given input tensors. Zeros are treated as False and nonzeros are\ntreated as True. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the tensor to compute OR with out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"
    },
    {
        "X": "What contains L and U factors for LU factorization of A?",
        "Y": "LU",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What type of inputs can torch.solve(B, A) take?",
        "Y": "2D",
        "Z": "LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What types of inputs does torch.solve(B, A) support?",
        "Y": "real-valued and complex-valued inputs",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is the LU factorization of A in order?",
        "Y": "namedtuple solution",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does LU contain for LU factorization of A?",
        "Y": "L and U factors",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does torch.solve(B, A) do when it takes inputs that are batches of 2D matrices?",
        "Y": "Warning",
        "Z": "LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What type of inputs can torch.solve(B, A) take in?",
        "Y": "2D",
        "Z": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does torch.solve(B, A) provide?",
        "Y": "Warning",
        "Z": "torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does torch.linalg.solve() not return?",
        "Y": "LU factorization",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What can be used with torch.lu_solve() and torch.lu_unpack() to get the LU factorization?",
        "Y": "torch.lu()",
        "Z": "Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What should torch.solve(B, A) be replaced with?",
        "Y": "Note",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What types of inputs does PyTorch support?",
        "Y": "real-valued and complex-valued inputs",
        "Z": "Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is the replacement for torch.solve()?",
        "Y": "torch.linalg.solve()",
        "Z": "Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What does torch.linalg.solve() do?",
        "Y": "does not return the LU factorization of the input",
        "Z": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is used to get the LU factorization of the input?",
        "Y": "torch.lu()",
        "Z": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What should X = torch.solve(B, A)solution be replaced with?",
        "Y": "Note",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is torch.solve() replaced with?",
        "Y": "torch.linalg.solve()",
        "Z": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is torch.solve() replaced by?",
        "Y": "torch.linalg.solve()",
        "Z": "torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is the name of the strides used to transpose the returned matrices?",
        "Y": "B.contiguous().transpose(-1, -2)",
        "Z": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is *?",
        "Y": "zero or more batch dimensions",
        "Z": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is the input square matrix of size (,m,m)(*, m, m)(,m,m)",
        "Y": "A",
        "Z": "X = torch.solve(B, A).solution should be replaced with Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What will be transposed regardless of the original strides?",
        "Y": "the returned matrices solution and LU",
        "Z": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is the name of the input square matrix of size (,m,m)(*, m, m)(,m",
        "Y": "A",
        "Z": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "Out ((Tensor, Tensor)) \u2013 optional output tuple.",
        "Y": "optional",
        "Z": "Note Irrespective of the original strides, the returned matrices\nsolution and LU will be transposed, i.e. with strides like\nB.contiguous().transpose(-1, -2).stride() and\nA.contiguous().transpose(-1, -2).stride() respectively. input (Tensor) \u2013 input matrix BBB of size (\u2217,m,k)(*, m, k)(\u2217,m,k) , where \u2217*\u2217\nis zero or more batch dimensions. A (Tensor) \u2013 input square matrix of size (\u2217,m,m)(*, m, m)(\u2217,m,m), where\n\u2217*\u2217 is zero or more batch dimensions. out ((Tensor, Tensor), optional) \u2013 optional output tuple.",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is the pointer to current cuBLAS handle?",
        "Y": "cublasHandle_t",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is checked if the Context-manager selects a given stream?",
        "Y": "peer access between two devices",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does cublasHandle_t return for a given device?",
        "Y": "currently selected Stream",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does the cublasHandle_t pointer return for a given device?",
        "Y": "the currently selected Stream",
        "Z": "Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does the Context-manager return for a given device?",
        "Y": "default Stream",
        "Z": "Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does this library get?",
        "Y": "the cuda capability of a device",
        "Z": "It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does the library get from a device?",
        "Y": "properties",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What was this library compiled with?",
        "Y": "NVCC gencode flags",
        "Z": "Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.   Wrapper around the Context-manager StreamContext that selects a given stream.   Waits for all kernels in all streams on a CUDA device to complete.   Returns the random number generator state of the specified GPU as a ByteTensor.   Returns a list of ByteTensor representing the random number states of all devices.   Sets the random number generator state of the specified GPU.   Sets the random number generator state of all devices.   Sets the seed for generating random numbers for the current GPU.   Sets the seed for generating random numbers on all GPUs.   Sets the seed for generating random numbers to a random number for the current GPU.   Sets the seed for generating random numbers to a random number on all GPUs.   Returns the current random seed of the current GPU. comm.broadcast Broadcasts a tensor to specified GPU devices. comm.broadcast_coalesced Broadcasts a sequence tensors to the specified GPUs. comm.reduce_add Sums tensors from multiple GPUs. comm.scatter Scatters tensor across multiple GPUs. comm.gather Gathers tensors from multiple GPU devices.   Wrapper around a CUDA stream.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does PyTorch return for a given device?",
        "Y": "default Stream",
        "Z": "Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What type of memory allocator statistics does nvidia-smi return?",
        "Y": "CUDA",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does memory_reserved do?",
        "Y": "Set memory fraction for a process",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is memory_reserved()?",
        "Y": "Deprecated",
        "Z": "Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is a deprecated function that returns the maximum GPU memory occupied by tensors in bytes for a given device",
        "Y": "max_memory_reserved",
        "Z": "Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does memory_reserved() do for a process?",
        "Y": "Set memory fraction",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Set memory fraction for a process. Deprecated; see what?",
        "Y": "memory_reserved()",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Deprecated; see what?",
        "Y": "max_memory_reserved()",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is a deprecated function that returns the maximum GPU memory occupied by tensors for a given device?",
        "Y": "max_memory_reserved()",
        "Z": "Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does memory_reserved do for a process?",
        "Y": "Set memory fraction",
        "Z": "Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is the name of the deprecated function that returns the maximum GPU memory managed by the caching allocator for a given device",
        "Y": "max_memory_reserved()",
        "Z": "Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What is modeled after SciPy's special module?",
        "Y": "The torch.special module",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the entropy on input in what way?",
        "Y": "elementwise",
        "Z": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What does the torch.special module compute on input?",
        "Y": "entropy",
        "Z": "The torch.special module, modeled after SciPy\u2019s special module. This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "When may some functions change?",
        "Y": "PyTorch releases",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the name of each function in PyTorch?",
        "Y": "documentation",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes what on input?",
        "Y": "entropy",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Out (Tensor, optional) \u2013 the output tensor.",
        "Y": "optional",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What does Compute the error function of input do?",
        "Y": "Computes the complementary error function of input",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What error function is defined in the range (1,1)(-1, 1)(1,1)?",
        "Y": "inverse error function",
        "Z": "Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as:",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is defined in the range (1,1)(-1, 1)(1,1)?",
        "Y": "inverse error function",
        "Z": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is out (Tensor, optional) defined as?",
        "Y": "output tensor",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the inverse error function of input defined as?",
        "Y": "input (Tensor)",
        "Z": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is defined in the range (1,1)(-1, 1)(1,1) as: input (Tensor) \u2013 the input",
        "Y": "inverse error function",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the exponential of the elements minus what number of input?",
        "Y": "1",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What function is defined in the range (1,1)(-1, 1)(1,1)?",
        "Y": "inverse error function",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the inverse error function of input called?",
        "Y": "input (Tensor)",
        "Z": "Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What function is defined in the range (1,1)(-1, 1)(1,1) as: input (Tensor) \u2013 the",
        "Y": "inverse error function",
        "Z": "Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the exponential of the elements minus what amount of input?",
        "Y": "1",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Out (Tensor) - the output tensor.",
        "Y": "optional",
        "Z": "Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes the natural what of the absolute value of the gamma function on input?",
        "Y": "logarithm",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the natural value of the absolute value of the gamma function on input?",
        "Y": "logarithm",
        "Z": "Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What natural function is computed of the absolute value of the gamma function on input?",
        "Y": "logarithm",
        "Z": "This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What order modified Bessel function of the first kind is computed for each element of input?",
        "Y": "zeroth",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What type of Tensor is the output tensor?",
        "Y": "optional",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes what of input?",
        "Y": "base two exponential function",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Computes what of the absolute value of the gamma function on input?",
        "Y": "natural logarithm",
        "Z": "Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the absolute value of the gamma function on input?",
        "Y": "natural logarithm",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier other (Number or Tensor) \u2013 Argument Note At least one of input or other must be a tensor. out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "Out (Tensor, optional) - the output tensor.",
        "Y": "optional",
        "Z": "Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is input clamped to when eps is not None?",
        "Y": "eps",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "When eps is None and input  0 or input > 1, the function will yield what?",
        "Y": "NaN",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What does the function do for each element of input?",
        "Y": "Computes the exponentially scaled zeroth order modified Bessel function of the first kind",
        "Z": "Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the input with the following cases?",
        "Y": "* log1p(other)",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is scipy.special.xlog1py similar to?",
        "Y": "SciPy",
        "Z": "Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the default output tensor?",
        "Y": "None out",
        "Z": "Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What does the function compute with the following cases?",
        "Y": "* log1p(other)",
        "Z": "Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What does input (Number or Tensor) represent?",
        "Y": "Multiplier",
        "Z": "Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Returns a new tensor with the logit of the elements of input.\ninput is clamped to [eps, 1 - eps] when eps is not None.\nWhen eps is None and input < 0 or input > 1, the function will yields NaN. input (Tensor) \u2013 the input tensor. eps (float, optional) \u2013 the epsilon for input clamp bound. Default: None out (Tensor, optional) \u2013 the output tensor. Example: Computes input * log1p(other) with the following cases. Similar to SciPy\u2019s scipy.special.xlog1py. input (Number or Tensor) \u2013 Multiplier",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "If increasing is true, the order of the columns is reversed x0,x1,...,x(N1),x(N",
        "Y": "True",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What type of input tensor is x (Tensor)?",
        "Y": "1-D",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "If increasing is what, the order of the columns is reversed?",
        "Y": "True",
        "Z": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What are the powers of the input vector?",
        "Y": "elementwise",
        "Z": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "If increasing is what, the order of the columns is reversed x0,x1,...,x(N1)x0,",
        "Y": "True",
        "Z": "The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is x (Tensor)?",
        "Y": "1-D input tensor",
        "Z": "x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is increasing (bool, optional)?",
        "Y": "Order of the powers of the columns",
        "Z": "Generates a Vandermonde matrix. The columns of the output matrix are elementwise powers of the input vector x(N\u22121),x(N\u22122),...,x0x^{(N-1)}, x^{(N-2)}, ..., x^0x(N\u22121),x(N\u22122),...,x0.\nIf increasing is True, the order of the columns is reversed x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Such a\nmatrix with a geometric progression in each row is named for Alexandre-Theophile Vandermonde. x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed. Vandermonde matrix. If increasing is False, the first column is x(N\u22121)x^{(N-1)}x(N\u22121),\nthe second x(N\u22122)x^{(N-2)}x(N\u22122) and so forth. If increasing is True, the columns\nare x0,x1,...,x(N\u22121)x^0, x^1, ..., x^{(N-1)}x0,x1,...,x(N\u22121). Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What does N stand for?",
        "Y": "Number of columns in the output",
        "Z": "x (Tensor) \u2013 1-D input tensor. N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is N (int, optional)?",
        "Y": "Number of columns in the output",
        "Z": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What is N?",
        "Y": "Number of columns in the output",
        "Z": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What does increasing (bool, optional) mean?",
        "Y": "Order of the powers of the columns",
        "Z": "N (int, optional) \u2013 Number of columns in the output. If N is not specified,\na square array is returned (N=len(x))(N = len(x))(N=len(x)). increasing (bool, optional) \u2013 Order of the powers of the columns. If True,\nthe powers increase from left to right, if False (the default) they are reversed.",
        "source": "https://pytorch.org/docs/stable/generated/torch.vander.html#torch.vander"
    },
    {
        "X": "What does mantissa and exponent tensors do?",
        "Y": "Decomposes input",
        "Z": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "X": "What type of input does mantissa support?",
        "Y": "float inputs",
        "Z": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "X": "What is the input tensor out?",
        "Y": "input (Tensor)",
        "Z": "Decomposes input into mantissa and exponent tensors\nsuch that input=mantissa\u00d72exponent\\text{input} = \\text{mantissa} \\times 2^{\\text{exponent}}input=mantissa\u00d72exponent. The range of mantissa is the open interval (-1, 1). Supports float inputs. input (Tensor) \u2013 the input tensor out (tuple, optional) \u2013 the output tensors Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"
    },
    {
        "X": "What is the name of the function that performs the element-wise division of tensor1 by tensor2?",
        "Y": "Warning",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "How does the element-wise division of tensor1 by tensor2 work?",
        "Y": "multiply the result by the scalar value",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What is the result of the element-wise division of tensor1 by tensor2?",
        "Y": "Warning",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "The shapes of input, tensor1, and tensor2 must be what?",
        "Y": "broadcastable",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "Input (Tensor) \u2013 what is to be added tensor1 (Tensor)?",
        "Y": "tensor",
        "Z": "The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What is an example of a tensor that must be broadcastable?",
        "Y": "Example",
        "Z": "The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What does the Future type encapsulate?",
        "Y": "asynchronous execution",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the Future type primarily used by?",
        "Y": "Distributed RPC Framework",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is an example of an asynchronous execution of a callable?",
        "Y": "rpc_async()",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the Future type expose to add callback functions and set results?",
        "Y": "APIs",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does torch._C.Future encapsulate?",
        "Y": "asynchronous execution of a callable",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does torch._C.Future expose to add callback functions and set results?",
        "Y": "APIs",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What version of Warning GPU support is subject to changes?",
        "Y": "beta",
        "Z": "Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When will the given callback function run?",
        "Y": "when the Future is completed",
        "Z": "GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What can be added to the same Future, but the order in which they will be executed cannot be guaranteed?",
        "Y": "Multiple callbacks",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the Future is already completed, the given callback will be run what?",
        "Y": "inline",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is GPU support a beta feature?",
        "Y": "Warning",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When will the given callback function be run?",
        "Y": "when the Future is completed",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What cannot be guaranteed when multiple callbacks are added to the same Future?",
        "Y": "the order in which they will be executed",
        "Z": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What can a callback function use to get the value of a Future?",
        "Y": "value() method",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Which method provides a way to synchronize after your callback has completed?",
        "Y": "then()",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What can be cheaper if your callback does not return anything?",
        "Y": "add_done_callback",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What kind of callback registration API do both then() and add_done_callback use?",
        "Y": "same",
        "Z": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does add_done_callback behave in the same way as then()?",
        "Y": "GPU tensors",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a Callable that takes in one argument, is the reference to this Future?",
        "Y": "callback",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the name of the callable that takes in one argument?",
        "Y": "Note",
        "Z": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What method provides a way to synchronize after your callback has completed?",
        "Y": "then()",
        "Z": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What do both then() and add_done_callback use under the hood?",
        "Y": "same",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the name of the callback that takes in one argument?",
        "Y": "Future",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the name of the callback that takes in one argument, is the reference to this Future?",
        "Y": "which",
        "Z": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does this method behave in the same way as then()?",
        "Y": "GPU tensors",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the name of the Callable that references the Future?",
        "Y": "Note",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a callback for a Callable that takes in one argument, is the reference to this Future?",
        "Y": "Future",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a callback that takes in one argument, is the reference to this Future?",
        "Y": "Future",
        "Z": "callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a callback that takes in one argument?",
        "Y": "Note",
        "Z": "callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does Future.done() return if the Future is done?",
        "Y": "True",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "A Future is done if it has what?",
        "Y": "a result or an exception",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does Future.done() return if this Future is done?",
        "Y": "True",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When is a Future done?",
        "Y": "if it has a result or an exception",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the result is already usable, what does Future.done() do?",
        "Y": "synchronizations",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains what that reside on GPUs, Future.done() will return True even if the asynchronous kernels that are popul",
        "Y": "tensors",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will return True if the value contains tensors that reside on GPUs?",
        "Y": "Future.done()",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does Future.done() do to ensure that the result is already usable?",
        "Y": "synchronizations",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What function returns True if the value contains tensors that reside on GPUs?",
        "Y": "Future.done()",
        "Z": "callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will mark this Future as completed with an error and trigger all attached callbacks?",
        "Y": "this Future",
        "Z": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When calling wait()/value() on this Future, the exception set here will be raised what?",
        "Y": "inline",
        "Z": "Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the exception for this Future?",
        "Y": "result",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Set the result for this Future, which will mark this Future as what?",
        "Y": "completed",
        "Z": "Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does a Future cannot be marked completed twice?",
        "Y": "a Future cannot be marked completed twice",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the result for this Future do?",
        "Y": "mark this Future as completed",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will mark this Future as completed and trigger all attached callbacks?",
        "Y": "Set the result for this Future",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How many times can a Future be marked completed?",
        "Y": "twice",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens when a Future is marked as completed?",
        "Y": "trigger all attached callbacks",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does it mean that a Future cannot be marked completed twice?",
        "Y": "a Future cannot be marked completed twice",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the result object of the Future?",
        "Y": "object",
        "Z": "result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the result object of this Future?",
        "Y": "object",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a Callable that takes this Future as the only argument?",
        "Y": "Callable",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "A new Future object that holds what of the callback will be marked as completed when the given callback finishes?",
        "Y": "return value",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does a Callable that takes this Future as the only argument do?",
        "Y": "Note",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the only argument for a callback?",
        "Y": "Future",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When a new Future object holds the return value of the callback is marked as what?",
        "Y": "completed",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does a Callable that takes this Future object hold the return value of the callback and will be marked as completed when the given callback finishes",
        "Y": "Note",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a new Future object that holds the return value of a callback that will be marked as completed when the given callback finishes?",
        "Y": "Note",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When will a new Future object be marked as completed?",
        "Y": "completed when the given callback finishes",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What holds the return value of the callback and will be marked as completed when the given callback finishes?",
        "Y": "A new Future object",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a new Future object that holds the return value of the callback and will be marked as completed when the given callback finishes?",
        "Y": "Note",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "The future returned by fut.wait() will be marked appropriately with what?",
        "Y": "the encountered error",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens if the callback function throws an error?",
        "Y": "the future returned by then will be marked appropriately",
        "Z": "Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Obtain the value of an already-completed future. This method should only be called after a call to what?",
        "Y": "wait() has completed",
        "Z": "A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What could happen if the method is called after a call to wait() has completed?",
        "Y": "Future may not yet hold a value",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Obtain the value of an already-completed future. This method should only be called after a call to what function has completed?",
        "Y": "wait()",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Where should a method be called to obtain the value of an already-completed future?",
        "Y": "inside a callback function",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What may not yet hold a value?",
        "Y": "Future",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Where do tensors reside?",
        "Y": "GPUs",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What method should be used to perform additional synchronization if the value contains tensors that reside on GPUs?",
        "Y": "wait()",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Where should this method be called after a call to wait() has completed?",
        "Y": "inside a callback function",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What could happen if a call to value() is called after a call to wait() has completed?",
        "Y": "Future may not yet hold a value",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, then this method will not perform any additional synchronization. This should be done",
        "Y": "wait()",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "The synchronization of tensors that reside on GPUs should be done separately through a call to what?",
        "Y": "wait()",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens to the value held by this Future?",
        "Y": "the function (callback or RPC) creating the value has thrown an error",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the value() method do until the value of this Future is ready?",
        "Y": "Block",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the name of the method that performs synchronization of tensors that reside on GPUs?",
        "Y": "wait()",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the value held by this Future?",
        "Y": "The value held by this Future",
        "Z": "Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How long should the value() method block?",
        "Y": "until the value of this Future is ready",
        "Z": "If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How long is the value held by this Future blocked?",
        "Y": "until the value of this Future is ready",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the name of the block until the value of the Future is ready?",
        "Y": "Block",
        "Z": "Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How long until the value of this Future is ready?",
        "Y": "Block",
        "Z": "Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the wait method return if the function creating the value has thrown an error?",
        "Y": "The value held by this Future",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What functions create the value of a Future?",
        "Y": "callback or RPC",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the function that collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed",
        "Y": "Collects the provided Future objects into a single combined Future that is completed when all of the sub-futures are completed",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a list of Future objects?",
        "Y": "list",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does this return a Future object to?",
        "Y": "Returns a Future object to a list of the passed in Futures",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the value held by a Future?",
        "Y": "value held by this Future",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What function created the value has thrown an error?",
        "Y": "callback or RPC",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is done when all of the sub-futures are completed?",
        "Y": "Collects the provided Future objects into a single combined Future",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Returns a Future object to what?",
        "Y": "a list of the passed in Futures",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When is a single combined Future completed?",
        "Y": "when all of the sub-futures are completed",
        "Z": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a futures (list)?",
        "Y": "list of Future object",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does futures return?",
        "Y": "Returns a Future object to a list of the passed in Futures",
        "Z": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens if any of the futures encounter an error?",
        "Y": "exit early",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the method do?",
        "Y": "Waits for all provided futures to be complete",
        "Z": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If any of the futures encounter an error, the method will do what?",
        "Y": "exit early",
        "Z": "Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is futures (list)?",
        "Y": "a list of Future object",
        "Z": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When will this method throw an error?",
        "Y": "if wait on any Future throws",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Futures (list) \u2013 a list of what?",
        "Y": "completed Future results",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does this method do if wait on any Future throws?",
        "Y": "throw an error",
        "Z": "futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the dividend and divisor compute?",
        "Y": "the element-wise remainder of division",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "What does the dividend support?",
        "Y": "broadcasting",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "When the divisor is zero, returns what for floating point dtypes on both CPU and GPU?",
        "Y": "NaN",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "What is the dividend input?",
        "Y": "input (Tensor)",
        "Z": "Computes the element-wise remainder of division. The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "The remainder of the dividend and divisor has the same sign as what?",
        "Y": "dividend input",
        "Z": "The dividend and divisor may contain both for integer and floating point\nnumbers. The remainder has the same sign as the dividend input. Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "What is input (Tensor) \u2013?",
        "Y": "the dividend",
        "Z": "Supports broadcasting to a common shape,\ntype promotion, and integer and float inputs. Note When the divisor is zero, returns NaN for floating point dtypes\non both CPU and GPU; raises RuntimeError for integer division by\nzero on CPU; Integer division by zero on GPU may return any value. input (Tensor) \u2013 the dividend other (Tensor or Scalar) \u2013 the divisor out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"
    },
    {
        "X": "Returns what of (input - other) The shapes of input and other must be broadcastable?",
        "Y": "p-norm",
        "Z": "Returns the p-norm of (input - other) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the Right-hand-side input tensor p (float, optional) \u2013 the norm to be computed Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist"
    },
    {
        "X": "What is the other (Tensor)?",
        "Y": "the Right-hand-side input tensor",
        "Z": "Returns the p-norm of (input - other) The shapes of input and other must be\nbroadcastable. input (Tensor) \u2013 the input tensor. other (Tensor) \u2013 the Right-hand-side input tensor p (float, optional) \u2013 the norm to be computed Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist"
    },
    {
        "X": "What is reps treated as if input has shape and reps is 2?",
        "Y": "(1, 1, 2, 2).",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "If input has fewer dimensions than reps specifies, then input is treated as if it were unsqueezed at dimension zero until it has",
        "Y": "if input has fewer dimensions than reps specifies",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "If input has shape (what is the number of dimensions) and reps is (3, 3, 2, 2), then input is treated as if it had",
        "Y": "4, 2)",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What does reps represent?",
        "Y": "the number of repetitions per dimension",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "What is an example of a tensor whose elements to repeat?",
        "Y": "Example",
        "Z": "Constructs a tensor by repeating the elements of input.\nThe reps argument specifies the number of repetitions\nin each dimension. If reps specifies fewer dimensions than input has, then\nones are prepended to reps until all dimensions are specified.\nFor example, if input has shape (8, 6, 4, 2) and reps\nis (2, 2), then reps is treated as (1, 1, 2, 2). Analogously, if input has fewer dimensions than reps\nspecifies, then input is treated as if it were unsqueezed at\ndimension zero until it has as many dimensions as reps specifies.\nFor example, if input has shape (4, 2) and reps\nis (3, 3, 2, 2), then input is treated as if it had the\nshape (1, 1, 4, 2). Note This function is similar to NumPy\u2019s tile function. input (Tensor) \u2013 the tensor whose elements to repeat. reps (tuple) \u2013 the number of repetitions per dimension. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"
    },
    {
        "X": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes called what?",
        "Y": "deconvolution",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does deconvolution combine an array of sliding local blocks into?",
        "Y": "large containing tensor",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is a partial inverse of MaxPool3d?",
        "Y": "Computes a partial inverse of MaxPool3d",
        "Z": "Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does Compute a partial inverse of MaxPool2d?",
        "Y": "Computes a partial inverse of MaxPool3d",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does Computes a partial inverse of MaxPool1d?",
        "Y": "Computes a partial inverse of MaxPool2d",
        "Z": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies a 1D power-average pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the array of sliding local blocks combine into?",
        "Y": "a large containing tensor",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a 3D pooling over an input signal comprised of several input planes do?",
        "Y": "Computes a partial inverse of MaxPool1d",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does the 3D pooling function do?",
        "Y": "Computes a partial inverse of MaxPool3d",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies what operation in kTkHkWkT times kH times kWkT",
        "Y": "3D average-pooling",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does 3D fractional max pooling over an input signal composed of several input planes?",
        "Y": "2D fractional max pooling",
        "Z": "Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies the element-wise function ReLU6(x)=min(max(0,x),6)textReLU6",
        "Y": "hardswish",
        "Z": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the in-place version of elu()?",
        "Y": "elu()",
        "Z": "Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the scale of elu()?",
        "Y": "1.0507009873554804934193349852946",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the in-place version of elu()?",
        "Y": "LeakyReLU(x)",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is another term for Gaussian negative log likelihood loss?",
        "Y": "Gaussian negative log likelihood loss",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "A squared term is used if the absolute element-wise error falls below what?",
        "Y": "beta",
        "Z": "Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the loss that combines log_softmax and nll_loss in a single function?",
        "Y": "Poisson negative log likelihood loss",
        "Z": "Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What function uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise?",
        "Y": "SoftMarginLoss",
        "Z": "See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "If the first argument is 2-dimensional and the second argument is what, the matrix-vector product is returned?",
        "Y": "1-dimensional",
        "Z": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "If the first argument is 2-dimensional and the second argument is 1-dimensional, what dimension is prepended to its dimension for the purpose of the batched matrix",
        "Y": "1",
        "Z": "If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "What type of dimensions are broadcasted?",
        "Y": "non-matrix",
        "Z": "The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "If the second argument is at least what dimension, a 1 is appended to its dimension for the purpose of the batched matrix multiple and removed after",
        "Y": "1-dimensional",
        "Z": "If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor.",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "Return the singular value decomposition of a matrix, batches of matrices, or a sparse matrix AAA such that A",
        "Y": "U, S, V",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What is computed for the matrix AMA - MAM?",
        "Y": "SVD",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What is low-rank SVD useful for?",
        "Y": "huge sparse matrices",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What must niter be?",
        "Y": "a nonnegative integer",
        "Z": "Return the singular value decomposition (U, S, V) of a matrix,\nbatches of matrices, or a sparse matrix AAA such that\nA\u2248Udiag(S)VTA \\approx U diag(S) V^TA\u2248Udiag(S)VT. In case MMM is given, then\nSVD is computed for the matrix A\u2212MA - MA\u2212M. Note The implementation is based on the Algorithm 5.1 from\nHalko et al, 2009. Note To obtain repeatable results, reset the seed for the\npseudorandom number generator Note The input is assumed to be a low-rank matrix. Note In general, use the full-rank SVD implementation\ntorch.linalg.svd() for dense matrices due to its 10-fold\nhigher performance characteristics. The low-rank SVD\nwill be useful for huge sparse matrices that\ntorch.linalg.svd() cannot handle. A (Tensor): the input tensor of size (\u2217,m,n)(*, m, n)(\u2217,m,n) q (int, optional): a slightly overestimated rank of A. conduct; niter must be a nonnegative\ninteger, and defaults to 2 (\u2217,1,n)(*, 1, n)(\u2217,1,n).",
        "source": "https://pytorch.org/docs/stable/generated/torch.svd_lowrank.html#torch.svd_lowrank"
    },
    {
        "X": "What do NNN tensors create?",
        "Y": "NNN N-dimensional grids",
        "Z": "Take NNN tensors, each of which can be either scalar or 1-dimensional\nvector, and create NNN N-dimensional grids, where the iii th grid is defined by\nexpanding the iii th input over dimensions defined by other inputs. tensors (list of Tensor) \u2013 list of scalars or 1 dimensional tensors. Scalars will be\ntreated as tensors of size (1,)(1,)(1,) automatically If the input has kkk tensors of size\n(N1,),(N2,),\u2026,(Nk,)(N_1,), (N_2,), \\ldots , (N_k,)(N1\u200b,),(N2\u200b,),\u2026,(Nk\u200b,), then the output would also have kkk tensors,\nwhere all tensors are of size (N1,N2,\u2026,Nk)(N_1, N_2, \\ldots , N_k)(N1\u200b,N2\u200b,\u2026,Nk\u200b). seq (sequence of Tensors) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.meshgrid.html#torch.meshgrid"
    },
    {
        "X": "What is a torch.Tensor?",
        "Y": "multi-dimensional matrix containing elements of a single data type",
        "Z": "A torch.Tensor is a multi-dimensional matrix containing elements of\na single data type. Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of tensor does Torch define?",
        "Y": "LongTensor Boolean",
        "Z": "Torch defines 10 tensor types with CPU and GPU variants which are as follows: Data type dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for a doubletensor torch?",
        "Y": "double torch",
        "Z": "torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the sign of an IntTensor 6?",
        "Y": "4-bit integer",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is a bfloat16 torch?",
        "Y": "16-bit floating point 2 torch",
        "Z": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the unsigned name of the IntTensor torch?",
        "Y": "/ quantized 4-bit integer",
        "Z": "torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor /",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the sign for torch.int8 torch?",
        "Y": "8-bit integer",
        "Z": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is an alias for the default tensor type?",
        "Y": "torch.Tensor",
        "Z": "32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor).",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What can be used to create a tensor?",
        "Y": "requires_grad=True",
        "Z": "A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What can be constructed by passing a torch.dtype and/or a torch.device to a constructor or tensor",
        "Y": "A tensor of specific data type",
        "Z": "Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are the contents of a tensor accessed and modified using?",
        "Y": "indexing and slicing notation",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.Storage do?",
        "Y": "holds its data",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are the three attributes of a torch.Tensor?",
        "Y": "torch.dtype, torch.device, and torch.layout attributes",
        "Z": "The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are some attributes of a torch.Tensor?",
        "Y": "torch.dtype, torch.device, and torch.layout attributes",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does to() method on a tensor do?",
        "Y": "Warning",
        "Z": "For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What can a tensor be created with?",
        "Y": "requires_grad=True",
        "Z": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does the to() method on a tensor do?",
        "Y": "Warning",
        "Z": "A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation. Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What kind of view does the tensor class provide?",
        "Y": "multi-dimensional",
        "Z": "Each tensor has an associated torch.Storage, which holds its data.\nThe tensor class also provides multi-dimensional, strided\nview of a storage and defines numeric operations on it. Note For more information on tensor views, see Tensor Views. Note For more information on the torch.dtype, torch.device, and\ntorch.layout attributes of a torch.Tensor, see\nTensor Attributes. Note Methods which mutate a tensor are marked with an underscore suffix.\nFor example, torch.FloatTensor.abs_() computes the absolute value\nin-place and returns the modified tensor, while torch.FloatTensor.abs()\ncomputes the result in a new tensor. Note To change an existing tensor\u2019s torch.device and/or torch.dtype, consider using\nto() method on the tensor. Warning Current implementation of torch.Tensor introduces memory overhead,\nthus it might lead to unexpectedly high memory usage in the applications with many tiny tensors.\nIf this is your case, consider using one large structure.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Returns a new Tensor with data as the tensor data?",
        "Y": "Tensor.new_tensor",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.imag Returns a new tensor containing what values of the self tensor?",
        "Y": "imaginary values",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs()?",
        "Y": "abs()",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.new_full Returns a Tensor of size size filled with what?",
        "Y": "fill_value",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is returned a Tensor of size size filled with?",
        "Y": "uninitialized data",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "In-place version of what Alias for abs() Tensor.absolute?",
        "Y": "abs() Tensor.absolute",
        "Z": "Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the device where the Tensor is stored?",
        "Y": "torch.device",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What can be added to a self tensor?",
        "Y": "a scalar or tensor",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the tensor that is stored on the GPU?",
        "Y": "addbmm",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Add what to self tensor?",
        "Y": "scalar or tensor",
        "Z": "Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What Add a scalar or tensor to self tensor?",
        "Y": "add",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does the Alias for dim() Tensor.ndim return?",
        "Y": "real values of the self tensor",
        "Z": "Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does abs() Tensor.absolute Alias for abs() Tensor.absolute Alias for ab",
        "Y": "abs() Tensor",
        "Z": "Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What provides torch.Tensor to represent a multi-dimensional array containing elements of a single data type?",
        "Y": "PyTorch",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is stored contiguously in memory?",
        "Y": "array elements",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is a sparse storage format?",
        "Y": "Note",
        "Z": "PyTorch provides torch.Tensor to represent a\nmulti-dimensional array containing elements of a single data type. By\ndefault, array elements are stored contiguously in memory leading to\nefficient implementations of various array processing algorithms that\nrelay on the fast access to array elements.  However, there exists an\nimportant class of multi-dimensional arrays, so-called sparse arrays,\nwhere the contiguous memory storage of array elements turns out to be\nsuboptimal. Sparse arrays have a property of having a vast portion of\nelements being equal to zero which means that a lot of memory as well\nas processor resources can be spared if only the non-zero elements are\nstored or/and processed. Various sparse storage formats (such as COO,\nCSR/CSC, LIL, etc.) have been developed that are optimized for a\nparticular structure of non-zero elements in sparse arrays as well as\nfor specific operations on the arrays. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What format can be advantageous only when the size and sparsity levels of arrays are high?",
        "Y": "sparse storage",
        "Z": "Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the storage format for sparse tensors implemented by PyTorch?",
        "Y": "Coordinate format",
        "Z": "When talking about storing only non-zero elements of a sparse\narray, the usage of adjective \u201cnon-zero\u201d is not strict: one is\nallowed to store also zeros in the sparse array data\nstructure. Hence, in the following, we use \u201cspecified elements\u201d for\nthose array elements that are actually stored. In addition, the\nunspecified elements are typically assumed to have zero value, but\nnot only, hence we use the term \u201cfill value\u201d to denote such\nelements. Note Using a sparse storage format for storing sparse arrays can be\nadvantageous only when the size and sparsity levels of arrays are\nhigh. Otherwise, for small-sized or low-sparsity arrays using the\ncontiguous memory storage format is likely the most efficient\napproach. Warning The PyTorch API of sparse tensors is in beta and may change in the near future. PyTorch implements the so-called Coordinate format, or COO\nformat, as one of the storage formats for implementing sparse\ntensors.  In COO format, the specified elements are stored as tuples\nof element indices and the corresponding values. In particular, the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the indices of specified elements called?",
        "Y": "ndim",
        "Z": "the indices of specified elements are collected in indices\ntensor of size (ndim, nse) and with element type\ntorch.int64, the corresponding values are collected in values tensor of\nsize (nse,) and with an arbitrary integer or floating point\nnumber element type, where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the number of specified elements in a sparse COO tensor?",
        "Y": "nse",
        "Z": "where ndim is the dimensionality of the tensor and nse is the\nnumber of specified elements. Note The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is at least product(tensor shape>) * size of element type in bytes>?",
        "Y": "strided tensor",
        "Z": "The memory consumption of a sparse COO tensor is at least (ndim *\n8 + <size of element type in bytes>) * nse bytes (plus a constant\noverhead from storing other tensor data). The memory consumption of a strided tensor is at least\nproduct(<tensor shape>) * <size of element type in bytes>. For example, the memory consumption of a 10 000 x 10 000 tensor\nwith 100 000 non-zero 32-bit floating point numbers is at least\n(2 * 8 + 4) * 100 000 = 2 000 000 bytes when using COO tensor\nlayout and 10 000 * 10 000 * 4 = 400 000 000 bytes when using\nthe default strided tensor layout. Notice the 200 fold memory\nsaving from using the COO storage format. A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is NOT a list of index tuples?",
        "Y": "the input i",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the input i of a sparse COO tensor?",
        "Y": "i",
        "Z": "A sparse COO tensor can be constructed by providing the two tensors of\nindices and values, as well as the size of the sparse tensor (when it\ncannot be inferred from the indices and values tensors) to a function\ntorch.sparse_coo_tensor(). Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What would we use to define a sparse tensor?",
        "Y": "i",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the input i NOT a list of?",
        "Y": "index tuples",
        "Z": "Suppose we want to define a sparse tensor with the entry 3 at location\n(0, 2), entry 4 at location (1, 0), and entry 5 at location (1, 2).\nUnspecified elements are assumed to have the same value, fill value,\nwhich is zero by default. We would then write: Note that the input i is NOT a list of index tuples.  If you want\nto write your indices this way, you should transpose before passing them to\nthe sparse constructor: An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What extends the sparse COO tensor by allowing the values tensor to be a multi-dimensional ten",
        "Y": "PyTorch",
        "Z": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How does PyTorch hybrid COO tensor extend the sparse COO tensor?",
        "Y": "allowing the values tensor to be a multi-dimensional tensor",
        "Z": "An empty sparse COO tensor can be constructed by specifying its size\nonly: Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing the values tens",
        "Y": "multi-dimensional tensor",
        "Z": "Pytorch implements an extension of sparse tensors with scalar values\nto sparse tensors with (contiguous) tensor values. Such tensors are\ncalled hybrid tensors. PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the invariants of a sparse COO tensor?",
        "Y": "s.sparse_dim(), K = s.dense_dim()",
        "Z": "PyTorch hybrid COO tensor extends the sparse COO tensor by allowing\nthe values tensor to be a multi-dimensional tensor so that we\nhave: the indices of specified elements are collected in indices\ntensor of size (sparse_dims, nse) and with element type\ntorch.int64, the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the location of the entry [5, 6] in a 2 + 1-dimensional tensor?",
        "Y": "(1, 0",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the corresponding tensor values collected with?",
        "Y": "arbitrary integer or floating point number element type",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If s is a sparse COO tensor and M = s.sparse_dim(), what",
        "Y": "K",
        "Z": "the corresponding (tensor) values are collected in values\ntensor of size (nse, dense_dims) and with an arbitrary integer\nor floating point number element type. Note We use (M + K)-dimensional tensor to denote a N-dimensional hybrid\nsparse tensor, where M and K are the numbers of sparse and dense\ndimensions, respectively, such that M + K == N holds. Suppose we want to create a (2 + 1)-dimensional tensor with the entry\n[3, 4] at location (0, 2), entry [5, 6] at location (1, 0), and entry\n[7, 8] at location (1, 2). We would write In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does PyTorch sparse COO tensor format permit?",
        "Y": "uncoalesced sparse tensors",
        "Z": "In general, if s is a sparse COO tensor and M =\ns.sparse_dim(), K = s.dense_dim(), then we have the following\ninvariants: M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors where there may",
        "Y": "duplicate coordinates",
        "Z": "s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What will accumulate the multi-valued elements into a single value using summation?",
        "Y": "the coalescing process",
        "Z": "M + K == len(s.shape) == s.ndim - dimensionality of a tensor\nis the sum of the number of sparse and dense dimensions, s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are the indices of specified tensor elements?",
        "Y": "unique",
        "Z": "s.indices().shape == (M, nse) - sparse indices are stored\nexplicitly, s.values().shape == (nse,) + s.shape[M : M + K] - the values\nof a hybrid tensor are K-dimensional tensors, s.values().layout == torch.strided - values are stored as\nstrided tensors. Note Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique,",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch sparse COO tensor format permits what?",
        "Y": "uncoalesced sparse tensors",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How many values can be specified for the same index?",
        "Y": "1",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What are two values that lead to a 1-D uncoalesced tensor?",
        "Y": "3 and 4",
        "Z": "Dense dimensions always follow sparse dimensions, that is, mixing\nof dense and sparse dimensions is not supported. PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How many values can be specified for the same index 1?",
        "Y": "3 and 4",
        "Z": "PyTorch sparse COO tensor format permits uncoalesced sparse tensors,\nwhere there may be duplicate coordinates in the indices; in this case,\nthe interpretation is that the value at that index is the sum of all\nduplicate value entries. For example, one can specify multiple values,\n3 and 4, for the same index 1, that leads to an 1-D\nuncoalesced tensor: while the coalescing process will accumulate the multi-valued elements\ninto a single value using summation: In general, the output of torch.Tensor.coalesce() method is a\nsparse tensor with the following properties: the indices of specified tensor elements are unique, the indices are sorted in lexicographical order, torch.Tensor.is_coalesced() returns True. Note For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "How are sparse COO tensors implemented?",
        "Y": "by simply concatenating the indices and values tensors",
        "Z": "However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is a sparse COO tensor a instance of?",
        "Y": "torch.Tensor",
        "Z": "For the most part, you shouldn\u2019t have to care whether or not a\nsparse tensor is coalesced or not, as most operations will work\nidentically given a coalesced or uncoalesced sparse tensor. However, some operations can be implemented more efficiently on\nuncoalesced tensors, and some on coalesced tensors. For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What should you do to prevent sparse tensors from growing too large?",
        "Y": "coalesce",
        "Z": "If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What type of COO tensor is a torch.Tensor instance?",
        "Y": "sparse",
        "Z": "For instance, addition of sparse COO tensors is implemented by\nsimply concatenating the indices and values tensors: If you repeatedly perform an operation that can produce duplicate\nentries (e.g., torch.Tensor.add()), you should occasionally\ncoalesce your sparse tensors to prevent them from growing too large. On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can be acquired using methods torch.Tensor.indices() and torch.Tensor.values()?",
        "Y": "COO format data",
        "Z": "On the other hand, the lexicographical ordering of indices can be\nadvantageous for implementing algorithms that involve many element\nselection operations, such as slicing or matrix products. Let\u2019s consider the following example: As mentioned above, a sparse COO tensor is a torch.Tensor\ninstance and to distinguish it from the Tensor instances that use\nsome other layout, on can use torch.Tensor.is_sparse or\ntorch.Tensor.layout properties: The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What dimension can be acquired using methods torch.Tensor.sparse_dim() and torch.Tensor.d",
        "Y": "sparse",
        "Z": "The number of sparse and dense dimensions can be acquired using\nmethods torch.Tensor.sparse_dim() and\ntorch.Tensor.dense_dim(), respectively. For instance: If s is a sparse COO tensor then its COO format data can be\nacquired using methods torch.Tensor.indices() and\ntorch.Tensor.values(). Note Currently, one can acquire the COO format data only when the tensor\ninstance is coalesced: For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does not hold in general?",
        "Y": "sqrt(a + b) == sqrt(a) + sqrt(b)",
        "Z": "When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general. Slicing (with positive step) of a sparse COO tensor is supported only\nfor dense dimensions. Indexing is supported for both sparse and dense\ndimensions:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The values of the same indices are what?",
        "Y": "terms of a sum",
        "Z": "For acquiring the COO format data of an uncoalesced tensor, use\ntorch.Tensor._values() and torch.Tensor._indices(): Constructing a new sparse COO tensor results a tensor that is not\ncoalesced: but one can construct a coalesced copy of a sparse COO tensor using\nthe torch.Tensor.coalesce() method: When working with uncoalesced sparse COO tensors, one must take into\nan account the additive nature of uncoalesced data: the values of the\nsame indices are the terms of a sum that evaluation gives the value of\nthe corresponding tensor element. For example, the scalar\nmultiplication on an uncoalesced sparse tensor could be implemented by\nmultiplying all the uncoalesced values with the scalar because c *\n(a + b) == c * a + c * b holds. However, any nonlinear operation,\nsay, a square root, cannot be implemented by applying the operation to\nuncoalesced data because sqrt(a + b) == sqrt(a) + sqrt(b) does not\nhold in general.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The values tensor contains the values of the CSR tensor. This is a what?",
        "Y": "1-D tensor",
        "Z": "The CSR (Compressed Sparse Row) sparse tensor format implements the CSR format\nfor storage of 2 dimensional tensors. Although there is no support for N-dimensional\ntensors, the primary advantage over the COO format is better use of storage and\nmuch faster computation operations such as sparse matrix-vector multiplication\nusing MKL and MAGMA backends. CUDA support does not exist as of now. A CSR sparse tensor consists of three 1-D tensors: crow_indices, col_indices\nand values: The crow_indices tensor consists of compressed row indices. This is a 1-D tensor\nof size size[0] + 1. The last element is the number of non-zeros. This tensor\nencodes the index in values and col_indices depending on where the given row\nstarts. Each successive number in the tensor subtracted by the number before it denotes\nthe number of elements in a given row. The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the CSR tensor of size nnz?",
        "Y": "1-D tensor",
        "Z": "The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the default element type for index tensors crow_indices and col_indices?",
        "Y": "torch.int64",
        "Z": "The col_indices tensor contains the column indices of each value. This is a 1-D\ntensor of size nnz. The values tensor  contains the values of the CSR tensor. This is a 1-D tensor\nof size nnz. Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What can be directly constructed by using the torch._sparse_csr_tensor() method?",
        "Y": "Sparse CSR matrices",
        "Z": "Note The index tensors crow_indices and col_indices should have element type either\ntorch.int64 (default) or torch.int32. If you want to use MKL-enabled matrix\noperations, use torch.int32. This is as a result of the default linking of pytorch\nbeing with MKL LP64, which uses 32 bit integer indexing. Sparse CSR matrices can be directly constructed by using the torch._sparse_csr_tensor()\nmethod. The user must supply the row and column indices and values tensors separately.\nThe size argument is optional and will be deduced from the the crow_indices\nand col_indices if it is not present. The simplest way of constructing a sparse CSR tensor from a strided or sparse COO\ntensor is to use tensor._to_sparse_csr(). Any zeros in the (strided) tensor will\nbe interpreted as missing values in the sparse tensor: The sparse matrix-vector multiplication can be performed with the\ntensor.matmul() method. This is currently the only math operation\nsupported on CSR tensors.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What function is no longer used for Linear Algebra operations on sparse matrices?",
        "Y": "addmm()",
        "Z": "The following table summarizes supported Linear Algebra operations on\nsparse matrices where the operands layouts may vary. Here\nT[layout] denotes a tensor with a given layout. Similarly,\nM[layout] denotes a matrix (2-D PyTorch tensor), and V[layout]\ndenotes a vector (1-D PyTorch tensor). In addition, f denotes a\nscalar (float or 0-D PyTorch tensor), * is element-wise\nmultiplication, and @ is matrix multiplication. PyTorch operation Sparse grad? Layout signature torch.mv() no M[sparse_coo] @ V[strided] -> V[strided] torch.mv() no M[sparse_csr] @ V[strided] -> V[strided] torch.matmul() no M[sparse_coo] @ M[strided] -> M[strided] torch.matmul() no M[sparse_csr] @ M[strided] -> M[strided] torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does svd_lowrank() yes indicate if the PyTorch operation supports backward with respect to sparse matrix argument",
        "Y": "SVD",
        "Z": "torch.mm() no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the PyTorch operation support backward with respect to sparse matrix argument?",
        "Y": "All PyTorch operations",
        "Z": "no M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the only PyTorch operation that supports backward with respect to sparse matrix argument?",
        "Y": "torch",
        "Z": "M[sparse_coo] @ M[strided] -> M[strided] torch.sparse.mm() yes M[sparse_coo] @ M[strided] -> M[strided] torch.smm() no M[sparse_coo] @ M[strided] -> M[sparse_coo] torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "All PyTorch operations, except torch.smm(), support backward with respect to what?",
        "Y": "strided matrix arguments",
        "Z": "torch.hspmm() no M[sparse_coo] @ M[strided] -> M[hybrid sparse_coo] torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch does not support matrix multiplication with what layout signature?",
        "Y": "M[strided] @ M[sparse_coo]",
        "Z": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "PyTorch applications can still compute this using what?",
        "Y": "matrix relation",
        "Z": "torch.bmm() no T[sparse_coo] @ T[strided] -> T[strided] torch.addmm() no f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sparse.addmm() yes f * M[strided] + f * (M[sparse_coo] @ M[strided]) -> M[strided] torch.sspaddmm() no f * M[sparse_coo] + f * (M[sparse_coo] @ M[strided]) -> M[sparse_coo] torch.lobpcg() no GENEIG(M[sparse_coo]) -> M[strided], M[strided] torch.pca_lowrank() yes PCA(M[sparse_coo]) -> M[strided], M[strided], M[strided] torch.svd_lowrank() yes SVD(M[sparse_coo]) -> M[strided], M[strided], M[strided] where \u201cSparse grad?\u201d column indicates if the PyTorch operation supports\nbackward with respect to sparse matrix argument. All PyTorch operations,\nexcept torch.smm(), support backward with respect to strided\nmatrix arguments. Note Currently, PyTorch does not support matrix multiplication with the\nlayout signature M[strided] @ M[sparse_coo]. However,\napplications can still compute this using the matrix relation D @\nS == (S.t() @ D.t()).t().",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Where are the values of a sparse tensor filtered by the indices of the sparse tensor mask",
        "Y": "a strided tensor self",
        "Z": "The following Tensor methods are related to sparse tensors: Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What _ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions?",
        "Y": "Tensor.sparse_resize",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the method that resizes a sparse tensor?",
        "Y": "Tensor.sparse_resize_and_clear",
        "Z": "Tensor.is_sparse Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What is the name of the number of sparse dimensions in a sparse tensor self?",
        "Y": "sparse",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does the Tensor.sparse_mask return?",
        "Y": "a new sparse tensor",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Return the number of dense dimensions in a what self?",
        "Y": "sparse tensor",
        "Z": "Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns a new sparse tensor with values from a strided tensor self filtered by the",
        "Y": "Tensor.sparse_mask",
        "Z": "Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor that is coalesced, return a coalesced copy of self",
        "Y": "uncoalesced",
        "Z": "Is True if the Tensor uses sparse storage layout, False otherwise. Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Returns what with values from a strided tensor self filtered by the indices of the sparse tens",
        "Y": "a new sparse tensor",
        "Z": "Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor, what type of tensor is returned?",
        "Y": "uncoalesced",
        "Z": "Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What returns the indices of the self tensor when self is a sparse CSR tensor of layout spars",
        "Y": "Tensor.col_indices",
        "Z": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Convert a tensor to what format?",
        "Y": "compressed row storage format",
        "Z": "Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Return the tensor of a sparse COO tensor. Tensor.values Return the values tens",
        "Y": "indices",
        "Z": "Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "If self is a sparse COO tensor, return a coalesced copy of self if self is what?",
        "Y": "uncoalesced",
        "Z": "Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns the tensor containing the compressed row indices of the self tensor when self is a sparse",
        "Y": "Tensor.crow_indices",
        "Z": "Tensor.dense_dim Return the number of dense dimensions in a sparse tensor self. Tensor.sparse_dim Return the number of sparse dimensions in a sparse tensor self. Tensor.sparse_mask Returns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask. Tensor.to_sparse Returns a sparse copy of the tensor. Tensor._to_sparse_csr Convert a tensor to compressed row storage format. Tensor.indices Return the indices tensor of a sparse COO tensor. Tensor.values Return the values tensor of a sparse COO tensor. The following Tensor methods are specific to sparse COO tensors: Tensor.coalesce Returns a coalesced copy of self if self is an uncoalesced tensor. Tensor.sparse_resize_ Resizes self sparse tensor to the desired size and the number of sparse and dense dimensions. Tensor.sparse_resize_and_clear_ Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "The following methods are specific to what CSR tensors?",
        "Y": "sparse",
        "Z": "Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors: add()\nadd_()\naddmm()\naddmm_()\nany()\nasin()\nasin_()\narcsin()\narcsin_()\nbmm()\nclone()\ndeg2rad()\ndeg2rad_()\ndetach()\ndetach_()\ndim()\ndiv()\ndiv_()\nfloor_divide()\nfloor_divide_()\nget_device()\nindex_select()\nisnan()\nlog1p()\nlog1p_()\nmm()\nmul()\nmul_()\nmv()\nnarrow_copy()\nneg()\nneg_()\nnegative()\nnegative_()\nnumel()\nrad2deg()\nrad2deg_()\nresize_as_()\nsize()\npow()\nsqrt()\nsquare()\nsmm()\nsspaddmm()\nsub()\nsub_()\nt()\nt_()\ntranspose()\ntranspose_()\nzero_()",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What does a sparse tensor self do?",
        "Y": "Removes all specified elements",
        "Z": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "What Returns the tensor containing the column indices of the self tensor when self is a sparse C",
        "Y": "Tensor.col_indices",
        "Z": "Removes all specified elements from a sparse tensor self and resizes self to the desired size and the number of sparse and dense dimensions. Tensor.is_coalesced Returns True if self is a sparse COO tensor that is coalesced, False otherwise. Tensor.to_dense Creates a strided copy of self. The following methods are specific to sparse CSR tensors: Tensor.crow_indices Returns the tensor containing the compressed row indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. Tensor.col_indices Returns the tensor containing the column indices of the self tensor when self is a sparse CSR tensor of layout sparse_csr. The following Tensor methods support sparse COO tensors:",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Performs a matrix multiplication of what?",
        "Y": "sparse matrix input with the dense matrix mat",
        "Z": "Constructs a sparse tensor in COO(rdinate) format with specified values at the given indices.   Constructs a sparse tensor in CSR (Compressed Sparse Row) with specified values at the given crow_indices and col_indices. sparse.sum Returns the sum of each row of the sparse tensor input in the given dimensions dim. sparse.addmm This function does exact same thing as torch.addmm() in the forward, except that it supports backward for sparse matrix mat1. sparse.mm Performs a matrix multiplication of the sparse matrix mat1 and the (sparse or strided) matrix mat2.   Matrix multiplies a sparse tensor mat1 with a dense tensor mat2, then adds the sparse tensor input to the result.   Performs a matrix multiplication of a sparse COO matrix mat1 and a strided matrix mat2.   Performs a matrix multiplication of the sparse matrix input with the dense matrix mat. sparse.softmax Applies a softmax function. sparse.log_softmax Applies a softmax function followed by logarithm.",
        "source": "https://pytorch.org/docs/stable/sparse.html"
    },
    {
        "X": "Default: if None, what does torch.full_like(input.size(), fill_value, input.dtype",
        "Y": "defaults to the dtype of input",
        "Z": "Returns a tensor with the same size as input filled with fill_value.\ntorch.full_like(input, fill_value) is equivalent to\ntorch.full(input.size(), fill_value, dtype=input.dtype, layout=input.layout, device=input.device). input (Tensor) \u2013 the size of input will determine size of the output tensor. fill_value \u2013 the number to fill the output tensor with. dtype (torch.dtype, optional) \u2013 the desired data type of returned Tensor.\nDefault: if None, defaults to the dtype of input. layout (torch.layout, optional) \u2013 the desired layout of returned tensor.\nDefault: if None, defaults to the layout of input. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, defaults to the device of input. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. memory_format (torch.memory_format, optional) \u2013 the desired memory format of\nreturned Tensor. Default: torch.preserve_format.",
        "source": "https://pytorch.org/docs/stable/generated/torch.full_like.html#torch.full_like"
    },
    {
        "X": "What is the name of the short-time Fourier transform function that will only return complex tensors?",
        "Y": "return_complex=True",
        "Z": "Short-time Fourier transform (STFT). Warning From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "The interface of the STFT computes the Fourier transform of short overlapping windows of the input is modeled after what?",
        "Y": "librosa stft function",
        "Z": "From version 1.8.0, return_complex must always be given\nexplicitly for real inputs and return_complex=False has been\ndeprecated. Strongly prefer return_complex=True as in a future\npytorch release, this function will only return complex tensors. Note that torch.view_as_real() can be used to recover a real\ntensor with an extra last dimension for real and imaginary components. The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What must the input be?",
        "Y": "1-D time sequence or a 2-D batch of time sequences",
        "Z": "The STFT computes the Fourier transform of short overlapping windows of the\ninput. This giving frequency components of the signal as they change over\ntime. The interface of this function is modeled after the librosa stft function. Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If window is None (default), it is treated as if having what everywhere in the window?",
        "Y": "111",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_length is None, window will be padded on both sides to length n_fft before being applied.",
        "Y": "n_fft",
        "Z": "Ignoring the optional batch dimension, this method computes the following\nexpression: where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_length is None, window will be padded on both sides to length n_fft before being applied?",
        "Y": "n_fft",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If center is what, input will be padded on both sides so that the ttt-th frame is centered?",
        "Y": "True",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for input when center is True?",
        "Y": "reflect",
        "Z": "input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If True, input will be padded on both sides so that the ttt-th frame is centered at time thop_",
        "Y": "center",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for the padding method used on input when center is True?",
        "Y": "reflect",
        "Z": "If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If what is the default, input will be padded on both sides so that the ttt-th frame is centered?",
        "Y": "center is True",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default setting for a window when center is True?",
        "Y": "reflect",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If win_length  textn_fftwin_lengthn_fft, window will be",
        "Y": "n_fft",
        "Z": "window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\".",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default setting for the padding method used on input when center is True?",
        "Y": "\"reflect\"",
        "Z": "where mmm is the index of the sliding window, and \u03c9\\omega\u03c9 is\nthe frequency 0\u2264\u03c9<n_fft0 \\leq \\omega < \\text{n\\_fft}0\u2264\u03c9<n_fft for onesided=False,\nor 0\u2264\u03c9<\u230an_fft/2\u230b+10 \\leq \\omega < \\lfloor \\text{n\\_fft} / 2 \\rfloor + 10\u2264\u03c9<\u230an_fft/2\u230b+1 for onesided=True. input must be either a 1-D time sequence or a 2-D batch of time\nsequences. If hop_length is None (default), it is treated as equal to\nfloor(n_fft / 4). If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "If normalized is True, what is the default?",
        "Y": "If normalized is True",
        "Z": "If win_length is None (default), it is treated as equal to\nn_fft. window can be a 1-D tensor of size win_length, e.g., from\ntorch.hann_window(). If window is None (default), it is\ntreated as if having 111 everywhere in the window. If\nwin_length<n_fft\\text{win\\_length} < \\text{n\\_fft}win_length<n_fft, window will be padded on\nboth sides to length n_fft before being applied. If center is True (default), input will be padded on\nboth sides so that the ttt-th frame is centered at time\nt\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. Otherwise, the ttt-th frame\nbegins at time  t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length. pad_mode determines the padding method used on input when\ncenter is True. See torch.nn.functional.pad() for\nall available options. Default is \"reflect\". If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "When does the function return the normalized STFT results?",
        "Y": "If normalized is True",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Returns what if return_complex is true?",
        "Y": "a complex tensor of size",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the number of frequencies where STFT is applied?",
        "Y": "NNN",
        "Z": "If onesided is True (default for real input), only values for\n\u03c9\\omega\u03c9 in [0,1,2,\u2026,\u230an_fft2\u230b+1]\\left[0, 1, 2, \\dots, \\left\\lfloor\n\\frac{\\text{n\\_fft}}{2} \\right\\rfloor + 1\\right][0,1,2,\u2026,\u230a2n_fft\u200b\u230b+1] are returned because\nthe real-to-complex Fourier transform satisfies the conjugate symmetry,\ni.e., X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217X[m, \\omega] = X[m, \\text{n\\_fft} - \\omega]^*X[m,\u03c9]=X[m,n_fft\u2212\u03c9]\u2217.\nNote if the input or window tensors are complex, then onesided\noutput is not possible. If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Calling with what may cause error or return incorrect result?",
        "Y": "previous signature",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "The input tensor n_fft (int) \u2013 the input tensor n_fft (",
        "Y": "Fourier transform",
        "Z": "If normalized is True (default is False), the function\nreturns the normalized STFT results, i.e., multiplied by (frame_length)\u22120.5(\\text{frame\\_length})^{-0.5}(frame_length)\u22120.5. If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "Returns either a complex tensor of size (NT)(* times N times T",
        "Y": "a real tensor of size",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4))",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for the window function?",
        "Y": "None",
        "Z": "Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s)",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for window of all 111 s?",
        "Y": "None",
        "Z": "If return_complex is True (default if input is complex), the\nreturn is a input.dim() + 1 dimensional complex tensor. If False,\nthe output is a input.dim() + 2 dimensional real tensor where the last\ndimension represents the real and imaginary components. Returns either a complex tensor of size (\u2217\u00d7N\u00d7T)(* \\times N \\times T)(\u2217\u00d7N\u00d7T) if\nreturn_complex is true, or a real tensor of size (\u2217\u00d7N\u00d7T\u00d72)(* \\times N\n\\times T \\times 2)(\u2217\u00d7N\u00d7T\u00d72). Where \u2217*\u2217 is the optional batch size of\ninput, NNN is the number of frequencies where STFT is applied\nand TTT is the total number of frames used. Warning This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What frame is centered at time thop_lengtht times texthop_lengththop_",
        "Y": "ttt-th",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default value for return normalized STFT results?",
        "Y": "False",
        "Z": "This function changed signature at version 0.4.1. Calling with the\nprevious signature may cause error or return incorrect result. input (Tensor) \u2013 the input tensor n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default to return half of results to avoid redundancy for real inputs?",
        "Y": "False onesided",
        "Z": "n_fft (int) \u2013 size of Fourier transform hop_length (int, optional) \u2013 the distance between neighboring sliding window\nframes. Default: None (treated as equal to floor(n_fft / 4)) win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise.",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the default for STFT results?",
        "Y": "\"reflect\" normalized",
        "Z": "win_length (int, optional) \u2013 the size of window frame and STFT filter.\nDefault: None  (treated as equal to n_fft) window (Tensor, optional) \u2013 the optional window function.\nDefault: None (treated as window of all 111 s) center (bool, optional) \u2013 whether to pad input on both sides so\nthat the ttt-th frame is centered at time t\u00d7hop_lengtht \\times \\text{hop\\_length}t\u00d7hop_length.\nDefault: True pad_mode (string, optional) \u2013 controls the padding method used when\ncenter is True. Default: \"reflect\" normalized (bool, optional) \u2013 controls whether to return the normalized STFT results\nDefault: False onesided (bool, optional) \u2013 controls whether to return half of results to\navoid redundancy for real inputs.\nDefault: True for real input and window, False otherwise. return_complex (bool, optional) \u2013 whether to return a complex tensor, or\na real tensor with an extra last dimension for the real and\nimaginary components. A tensor containing the STFT result with shape described above Tensor",
        "source": "https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"
    },
    {
        "X": "What is the result if the number of bins is one larger than the largest value in input unless input is empty?",
        "Y": "a tensor of size 0.",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "If what is specified, the number of bins is at least minlength and if input is empty, the result is a tensor",
        "Y": "minlength",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Input (Tensor) \u2013 optional, weight for each value in the input tensor. Should be of same size as input",
        "Y": "1-d int tensor weights",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What should the weight for each value in the input tensor be of?",
        "Y": "same size",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Minlength (int) \u2013 what?",
        "Y": "optional, minimum number of bins",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "Minlength (int) \u2013 optional, minimum number of bins. Should be what?",
        "Y": "non-negative",
        "Z": "Count the frequency of each value in an array of non-negative ints. The number of bins (size 1) is one larger than the largest value in\ninput unless input is empty, in which case the result is a\ntensor of size 0. If minlength is specified, the number of bins is at least\nminlength and if input is empty, then the result is tensor of size\nminlength filled with zeros. If n is the value at position i,\nout[n] += weights[i] if weights is specified else\nout[n] += 1. Note This operation may produce nondeterministic gradients when given tensors on a CUDA device. See Reproducibility for more information. input (Tensor) \u2013 1-d int tensor weights (Tensor) \u2013 optional, weight for each value in the input tensor.\nShould be of same size as input tensor. minlength (int) \u2013 optional, minimum number of bins. Should be non-negative. a tensor of shape Size([max(input) + 1]) if\ninput is non-empty, else Size(0) output (Tensor) Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"
    },
    {
        "X": "What type of data does torch.as_tensor() avoid a copy of?",
        "Y": "NumPy ndarray",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What does torch.tensor() construct when data is a tensor x?",
        "Y": "a leaf variable",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What do you use if you have a Tensor data and want to avoid a copy?",
        "Y": "torch.Tensor.requires_grad_() or torch.Tensor.detach()",
        "Z": "torch.tensor() always copies data. If you have a Tensor\ndata and want to avoid a copy, use torch.Tensor.requires_grad_()\nor torch.Tensor.detach().\nIf you have a NumPy ndarray and want to avoid a copy, use\ntorch.as_tensor(). Warning When data is a tensor x, torch.tensor() reads out \u2018the data\u2019 from whatever it is passed,\nand constructs a leaf variable. Therefore torch.tensor(x) is equivalent to x.clone().detach()\nand torch.tensor(x, requires_grad=True) is equivalent to x.clone().detach().requires_grad_(True).\nThe equivalents using clone() and detach() are recommended. data (array_like) \u2013 Initial data for the tensor. Can be a list, tuple,\nNumPy ndarray, scalar, and other types. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, infers data type from data.",
        "source": "https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"
    },
    {
        "X": "What is defined as the elements on and above the diagonal?",
        "Y": "The upper triangular part",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What value controls which diagonal to consider?",
        "Y": "offset = 0,",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is the set of indices (i,i)lbrace (i, i) rbrace",
        "Y": "main diagonal",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What does row (int) mean?",
        "Y": "row (int) \u2013 number of rows in the 2-D matrix",
        "Z": "Returns the indices of the upper triangular part of a row by\ncol matrix in a 2-by-N Tensor, where the first row contains row\ncoordinates of all indices and the second row contains column coordinates.\nIndices are ordered based on rows and then columns. The upper triangular part of the matrix is defined as the elements on and\nabove the diagonal. The argument offset controls which diagonal to consider. If\noffset = 0, all elements on and above the main diagonal are\nretained. A positive value excludes just as many diagonals above the main\ndiagonal, and similarly a negative value includes just as many diagonals below\nthe main diagonal. The main diagonal are the set of indices\n{(i,i)}\\lbrace (i, i) \\rbrace{(i,i)} for i\u2208[0,min\u2061{d1,d2}\u22121]i \\in [0, \\min\\{d_{1}, d_{2}\\} - 1]i\u2208[0,min{d1\u200b,d2\u200b}\u22121]\nwhere d1,d2d_{1}, d_{2}d1\u200b,d2\u200b are the dimensions of the matrix. Note When running on CUDA, row * col must be less than 2592^{59}259 to\nprevent overflow during calculation. row (int) \u2013 number of rows in the 2-D matrix.",
        "source": "https://pytorch.org/docs/stable/generated/torch.triu_indices.html#torch.triu_indices"
    },
    {
        "X": "What is the default number of columns in a 2-D tensor?",
        "Y": "n out",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the global default of a 2-D tensor?",
        "Y": "Default",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is the default setting of a 2-D tensor?",
        "Y": "torch.strided",
        "Z": "Returns a 2-D tensor with ones on the diagonal and zeros elsewhere. n (int) \u2013 the number of rows m (int, optional) \u2013 the number of columns with default being n out (Tensor, optional) \u2013 the output tensor. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned Tensor.\nDefault: torch.strided. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False. A 2-D tensor with ones on the diagonal and zeros elsewhere Tensor Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.eye.html#torch.eye"
    },
    {
        "X": "What is equivalent to calling torch.kaiser_window(L, B, periodic=True)[:-1])?",
        "Y": "torch.kaiser_window(L, B, periodic=True)",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the name of the window suitable for use in spectral analysis?",
        "Y": "periodic",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What type of window is returned if False?",
        "Y": "symmetric",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Default: if None, uses what?",
        "Y": "global default",
        "Z": "Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()).",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is equivalent to calling torch.kaiser_window(L + 1, B, periodic=False)[:-1])?",
        "Y": "torch.kaiser_window(L, B, periodic=True)",
        "Z": "Computes the Kaiser window with window length window_length and shape parameter beta. Let I_0 be the zeroth order modified Bessel function of the first kind (see torch.i0()) and\nN = L - 1 if periodic is False and L if periodic is True,\nwhere L is the window_length. This function computes: Calling torch.kaiser_window(L, B, periodic=True) is equivalent to calling\ntorch.kaiser_window(L + 1, B, periodic=False)[:-1]).\nThe periodic argument is intended as a helpful shorthand\nto produce a periodic window as input to functions like torch.stft(). Note If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What is the default layout of a returned window tensor?",
        "Y": "torch.strided",
        "Z": "window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types. requires_grad (bool, optional) \u2013 If autograd should record operations on the\nreturned tensor. Default: False.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "What device will be the CPU for CPU tensor types?",
        "Y": "current CUDA device",
        "Z": "If window_length is one, then the returned window is a single element tensor containing a one. window_length (int) \u2013 length of the window. periodic (bool, optional) \u2013 If True, returns a periodic window suitable for use in spectral analysis.\nIf False, returns a symmetric window suitable for use in filter design. beta (float, optional) \u2013 shape parameter for the window. dtype (torch.dtype, optional) \u2013 the desired data type of returned tensor.\nDefault: if None, uses a global default (see torch.set_default_tensor_type()). layout (torch.layout, optional) \u2013 the desired layout of returned window tensor. Only\ntorch.strided (dense layout) is supported. device (torch.device, optional) \u2013 the desired device of returned tensor.\nDefault: if None, uses the current device for the default tensor type\n(see torch.set_default_tensor_type()). device will be the CPU\nfor CPU tensor types and the current CUDA device for CUDA tensor types.",
        "source": "https://pytorch.org/docs/stable/generated/torch.kaiser_window.html#torch.kaiser_window"
    },
    {
        "X": "Computes the what of a square matrix for an integer n?",
        "Y": "n-th power",
        "Z": "Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.   Computes the Cholesky decomposition of a complex Hermitian or real symmetric positive-definite matrix.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "Computes the solution to the system torch.tensordot(A, X) = B.",
        "Y": "X",
        "Z": "Computes the eigenvalue decomposition of a complex Hermitian or real symmetric matrix.   Computes the eigenvalues of a complex Hermitian or real symmetric matrix.   Computes the singular value decomposition (SVD) of a matrix.   Computes the singular values of a matrix.   Computes the solution of a square system of linear equations with a unique solution.   Computes a solution to the least squares problem of a system of linear equations.   Computes the inverse of a square matrix if it exists.   Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.   Computes the n-th power of a square matrix for an integer n.   Efficiently multiplies two or more matrices by reordering the multiplications so that the fewest arithmetic operations are performed.   Computes the first n columns of a product of Householder matrices.   Computes the multiplicative inverse of torch.tensordot().   Computes the solution X to the system torch.tensordot(A, X) = B.",
        "source": "https://pytorch.org/docs/stable/linalg.html"
    },
    {
        "X": "What function replaces torch.solve()?",
        "Y": "torch.linalg.solve()",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What function returns the LU factorization of the input?",
        "Y": "torch.lu()",
        "Z": "This function returns the solution to the system of linear\nequations represented by AX=BAX = BAX=B and the LU factorization of\nA, in order as a namedtuple solution, LU. LU contains L and U factors for LU factorization of A. torch.solve(B, A) can take in 2D inputs B, A or inputs that are\nbatches of 2D matrices. If the inputs are batches, then returns\nbatched outputs solution, LU. Supports real-valued and complex-valued inputs. Warning torch.solve() is deprecated in favor of torch.linalg.solve()\nand will be removed in a future PyTorch release.\ntorch.linalg.solve() has its arguments reversed and does not return the\nLU factorization of the input. To get the LU factorization see torch.lu(),\nwhich may be used with torch.lu_solve() and torch.lu_unpack(). X = torch.solve(B, A).solution should be replaced with Note",
        "source": "https://pytorch.org/docs/stable/generated/torch.solve.html#torch.solve"
    },
    {
        "X": "What is the name of a device. Gets the properties of a device?",
        "Y": "Gets the cuda capability of a device",
        "Z": "This package adds support for CUDA tensor types, that implement the same\nfunction as CPU tensors, but they utilize GPUs for computation. It is lazily initialized, so you can always import it, and use\nis_available() to determine if your system supports CUDA. CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Returns a bool indicating if CUDA is currently available.",
        "Y": "whether PyTorch\u2019s CUDA state has been initialized",
        "Z": "CUDA semantics has more details about working with CUDA.   Context-manager that selects a given stream.   Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "Sets the current device. This is a wrapper API to set the stream.",
        "Y": "current stream",
        "Z": "Checks if peer access between two devices is possible.   Returns cublasHandle_t pointer to current cuBLAS handle   Returns the index of a currently selected device.   Returns the currently selected Stream for a given device.   Returns the default Stream for a given device.   Context-manager that changes the selected device.   Returns the number of GPUs available.   Context-manager that changes the current device to that of given object.   Returns list CUDA architectures this library was compiled for.   Gets the cuda capability of a device.   Gets the name of a device.   Gets the properties of a device.   Returns NVCC gencode flags this library was compiled with.   Initialize PyTorch\u2019s CUDA state.   Force collects GPU memory after it has been released by CUDA IPC.   Returns a bool indicating if CUDA is currently available.   Returns whether PyTorch\u2019s CUDA state has been initialized.   Sets the current device.   Sets the current stream.This is a wrapper API to set the stream.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does the caching allocator do for a process?",
        "Y": "Set memory fraction",
        "Z": "Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.   Returns a human-readable printout of the running processes and their GPU memory use for a given device.   Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does memory_reserved() do?",
        "Y": "Set memory fraction for a process",
        "Z": "Returns a dictionary of CUDA memory allocator statistics for a given device.   Returns a human-readable printout of the current memory allocator statistics for a given device.   Returns a snapshot of the CUDA memory allocator state across all devices.   Returns the current GPU memory occupied by tensors in bytes for a given device.   Returns the maximum GPU memory occupied by tensors in bytes for a given device.   Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.   Returns the current GPU memory managed by the caching allocator in bytes for a given device.   Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.   Set memory fraction for a process.   Deprecated; see memory_reserved().   Deprecated; see max_memory_reserved().   Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.   Resets the \u201cpeak\u201d stats tracked by the CUDA memory allocator.",
        "source": "https://pytorch.org/docs/stable/cuda.html"
    },
    {
        "X": "What does Computes the error function of input do?",
        "Y": "Computes the complementary error function of input",
        "Z": "This module is in BETA. New functions are still being added, and some\nfunctions may change in future PyTorch releases. See the documentation of each\nfunction for details. Computes the entropy on input (as defined below), elementwise. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What does Computes the exponential of the elements minus 1 of input?",
        "Y": "Computes the exponential of the elements minus 1 of input",
        "Z": "out (Tensor, optional) \u2013 the output tensor. Computes the error function of input. The error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the function that provides greater precision than exp(x) - 1 for small values of x?",
        "Y": "Computes the exponential of the elements minus 1 of input",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What does Computes the exponential of the elements minus 1 of input provide?",
        "Y": "greater precision",
        "Z": "input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the complementary error function of input.\nThe complementary error function is defined as follows: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the inverse error function of input.\nThe inverse error function is defined in the range (\u22121,1)(-1, 1)(\u22121,1) as: input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What is the exponentially scaled zeroth order modified Bessel function?",
        "Y": "the first kind",
        "Z": "Computes the expit (also known as the logistic sigmoid function) of the elements of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponential of the elements minus 1\nof input. Note This function provides greater precision than exp(x) - 1 for small values of x. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the base two exponential function of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the natural logarithm of the absolute value of the gamma function on input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor. Example: Computes the exponentially scaled zeroth order modified Bessel function of the first kind (as defined below)\nfor each element of input. input (Tensor) \u2013 the input tensor. out (Tensor, optional) \u2013 the output tensor.",
        "source": "https://pytorch.org/docs/stable/special.html#torch.special.logit"
    },
    {
        "X": "What value does addcdiv multiply the result of the element-wise division of tensor1 by tensor2?",
        "Y": "scalar",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What is no longer supported with addcdiv?",
        "Y": "Warning Integer division",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "For what type of inputs can addcdiv be implemented as (input + value * tensor1 / tensor2)",
        "Y": "float inputs",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What is the tensor to be added tensor1 (Tensor)?",
        "Y": "input",
        "Z": "Performs the element-wise division of tensor1 by tensor2,\nmultiply the result by the scalar value and add it to input. Warning Integer division with addcdiv is no longer supported, and in a future\nrelease addcdiv will perform a true division of tensor1 and tensor2.\nThe historic addcdiv behavior can be implemented as\n(input + value * torch.trunc(tensor1 / tensor2)).to(input.dtype)\nfor integer inputs and as (input + value * tensor1 / tensor2) for float inputs.\nThe future addcdiv behavior is just the latter implementation:\n(input + value * tensor1 / tensor2), for all dtypes. The shapes of input, tensor1, and tensor2 must be\nbroadcastable. For inputs of type FloatTensor or DoubleTensor, value must be\na real number, otherwise an integer. input (Tensor) \u2013 the tensor to be added tensor1 (Tensor) \u2013 the numerator tensor tensor2 (Tensor) \u2013 the denominator tensor value (Number, optional) \u2013 multiplier for tensor1/tensor2\\text{tensor1} / \\text{tensor2}tensor1/tensor2 out (Tensor, optional) \u2013 the output tensor. Example:",
        "source": "https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"
    },
    {
        "X": "What does the torch._C.Future expose to add callback functions and set results?",
        "Y": "APIs",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the torch._C.Future encapsulate?",
        "Y": "asynchronous execution of a callable",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the torch._C.Future expose?",
        "Y": "APIs",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will be run when the Future is completed?",
        "Y": "Append the given callback function to this Future",
        "Z": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What API does add_done_callback use?",
        "Y": "callback registration",
        "Z": "Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is the name of the callback function that will be run when the Future is completed?",
        "Y": "Append",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If this Future is already completed, the given callback will be run what?",
        "Y": "inline",
        "Z": "Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is another name for callback?",
        "Y": "Future",
        "Z": "We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens if a Future is done?",
        "Y": "Return True",
        "Z": "With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens if this Future is done?",
        "Y": "Return True",
        "Z": "This package provides a Future type that encapsulates\nan asynchronous execution and a set of utility functions to simplify operations\non Future objects. Currently, the\nFuture type is primarily used by the\nDistributed RPC Framework. Wrapper around a torch._C.Future which encapsulates an asynchronous\nexecution of a callable, e.g. rpc_async(). It\nalso exposes a set of APIs to add callback functions and set results. Warning GPU support is a beta feature, subject to changes. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed. The callback must take one argument, which is the\nreference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run inline. We recommend that you use the then() method as it provides a way\nto synchronize after your callback has completed. add_done_callback\ncan be cheaper if your callback does not return anything. But both\nthen() and add_done_callback use the same callback\nregistration API under the hood. With respect to GPU tensors, this method behaves in the same way as\nthen(). callback (Future) \u2013 a Callable that takes in one argument, is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains tensors that reside on GPUs, what will return True even if the asynchronous kernels that are popul",
        "Y": "Future.done()",
        "Z": "is the reference to this Future. (which) \u2013  Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback,\nerror handling must be carefully taken care of. For example, if\nthis callback later completes additional futures, those futures are\nnot marked as completed with an error and the user is responsible\nfor handling completion/waiting on those futures independently. Return True if this Future is done. A Future is done if it\nhas a result or an exception. If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does a Future not do?",
        "Y": "a Future cannot be marked completed twice",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the result is already usable, what can be performed?",
        "Y": "synchronizations",
        "Z": "If the value contains tensors that reside on GPUs, Future.done()\nwill return True even if the asynchronous kernels that are\npopulating those tensors haven\u2019t yet completed running on the device,\nbecause at such stage the result is already usable, provided one\nperforms the appropriate synchronizations (see wait()). Set an exception for this Future, which will mark this Future as\ncompleted with an error and trigger all attached callbacks. Note that\nwhen calling wait()/value() on this Future, the exception set here\nwill be raised inline. result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens when the result is set for this Future?",
        "Y": "mark this Future as completed and trigger all attached callbacks",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is safe to call this method immediately after launching kernels?",
        "Y": "change streams in between",
        "Z": "result (BaseException) \u2013 the exception for this Future. Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "This method will record events on what?",
        "Y": "all the relevant current streams",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Set the result for this Future, which will do what?",
        "Y": "mark this Future as completed and trigger all attached callbacks",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "Is a Future able to be completed twice?",
        "Y": "a Future cannot be marked completed twice",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will this method record events on?",
        "Y": "all the relevant current streams",
        "Z": "Set the result for this Future, which will mark this Future as\ncompleted and trigger all attached callbacks. Note that a Future\ncannot be marked completed twice. If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the Future is already completed, the given callback will be run immediately what?",
        "Y": "inline",
        "Z": "If the result contains tensors that reside on GPUs, this method can be\ncalled even if the asynchronous kernels that are populating those\ntensors haven\u2019t yet completed running on the device, provided that the\nstreams on which those kernels were enqueued are set as the current ones\nwhen this method is called. Put simply, it\u2019s safe to call this method\nimmediately after launching those kernels, without any additional\nsynchronization, as long as one doesn\u2019t change streams in between. This\nmethod will record events on all the relevant current streams and will\nuse them to ensure proper scheduling for all the consumers of this\nFuture. result (object) \u2013 the result object of this Future. Append the given callback function to this Future, which will be run\nwhen the Future is completed.  Multiple callbacks can be added to\nthe same Future, but the order in which they will be executed cannot\nbe guaranteed (to enforce a certain order consider chaining:\nfut.then(cb1).then(cb2)). The callback must take one argument, which\nis the reference to this Future. The callback function can use the\nvalue() method to get the value. Note that if this Future is\nalready completed, the given callback will be run immediately inline. If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the Future's value contain that reside on GPUs?",
        "Y": "tensors",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What are the dedicated streams set as?",
        "Y": "current",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What non-blocking behavior is similar to the non-blocking behavior of?",
        "Y": "wait()",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait().",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What holds the return value of the callback?",
        "Y": "A new Future object",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does a callback do?",
        "Y": "Obtain the value of an already-completed future",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What could cause calling value() to fail?",
        "Y": "this Future may not yet hold a value",
        "Z": "callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is a method that should only be called after a call to wait() has completed?",
        "Y": "Obtain the value of an already-completed future",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What could cause value() to fail?",
        "Y": "this Future may not yet hold a value",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does this method do to obtain the value of an already-completed future?",
        "Y": "Obtain the value of an already-completed future",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When should the Obtain the value of an already-completed future be called?",
        "Y": "after a call to wait() has completed",
        "Z": "If the Future\u2019s value contains tensors that reside on GPUs, the\ncallback might be invoked while the async kernels that are populating\nthose tensors haven\u2019t yet finished executing on the device. However, the\ncallback will be invoked with some dedicated streams set as current\n(fetched from a global pool) which will be synchronized with those\nkernels. Hence any operation performed by the callback on these tensors\nwill be scheduled on the device after the kernels complete. In other\nwords, as long as the callback doesn\u2019t switch streams, it can safely\nmanipulate the result without any additional synchronization. This is\nsimilar to the non-blocking behavior of wait(). Similarly, if the callback returns a value that contains tensors that\nreside on a GPU, it can do so even if the kernels that are producing\nthese tensors are still running on the device, as long as the callback\ndidn\u2019t change streams during its execution. If one wants to change\nstreams, one must be careful to re-synchronize them with the original\nstreams, that is, those that were current when the callback was invoked. callback (Callable) \u2013 a Callable that takes this Future as\nthe only argument. A new Future object that holds the return value of the\ncallback and will be marked as completed when the given\ncallback finishes. Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What could happen if the method is called after a call to wait() has completed or inside a callback function passed to then()?",
        "Y": "Future may not yet hold a value",
        "Z": "Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What could happen if a call to value() fails?",
        "Y": "Future may not yet hold a value",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains what that reside on GPUs, this method will not perform any additional synchronization?",
        "Y": "tensors",
        "Z": "Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()).",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the value() method do?",
        "Y": "Block until the value of this Future is ready",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How long should the value() method be blocked?",
        "Y": "until the value of this Future is ready",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What could cause the call to value() to fail?",
        "Y": "this Future may not yet hold a value",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "How long should the value() method be called?",
        "Y": "until the value of this Future is ready",
        "Z": "This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "When does Block occur?",
        "Y": "until the value of this Future is ready",
        "Z": "Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the value contains what that reside on GPUs, then an additional synchronization is performed with the kernels (executing on the device) which may",
        "Y": "tensors",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What will insert the necessary instructions in the current streams to ensure that further operations enqueued on those streams will be properly scheduled after the as",
        "Y": "wait()",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What is required when accessing and using the values, as long as one doesn't change streams?",
        "Y": "No further synchronization",
        "Z": "Note Note that if the callback function throws, either\nthrough the original future being completed with an exception and\ncalling fut.wait(), or through other code in the callback, the\nfuture returned by then will be marked appropriately with the\nencountered error. However, if this callback later completes\nadditional futures, those futures are not marked as completed with\nan error and the user is responsible for handling completion/waiting\non those futures independently. Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If the function (callback or RPC) creating the value has thrown an error, what method will also throw an error?",
        "Y": "this wait method",
        "Z": "Obtain the value of an already-completed future. This method should only be called after a call to wait() has\ncompleted, or inside a callback function passed to then(). In\nother cases this Future may not yet hold a value and calling\nvalue() could fail. If the value contains tensors that reside on GPUs, then this method will\nnot perform any additional synchronization. This should be done\nbeforehand, separately, through a call to wait() (except within\ncallbacks, for which it\u2019s already being taken care of by then()). The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this value() method will\nalso throw an error. Block until the value of this Future is ready. If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What happens when all of the sub-futures are completed?",
        "Y": "Collects the provided Future objects into a single combined Future",
        "Z": "If the value contains tensors that reside on GPUs, then an additional\nsynchronization is performed with the kernels (executing on the device)\nwhich may be asynchronously populating those tensors. Such sync is\nnon-blocking, which means that wait() will insert the necessary\ninstructions in the current streams to ensure that further operations\nenqueued on those streams will be properly scheduled after the async\nkernels but, once that is done, wait() will return, even if those\nkernels are still running. No further synchronization is required when\naccessing and using the values, as long as one doesn\u2019t change streams. The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the wait method throw an error on?",
        "Y": "The value held by this Future",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What does the wait method do?",
        "Y": "Waits for all provided futures to be complete",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "If any of the futures encounter an error, the method will report the error not waiting for other futures to complete?",
        "Y": "exit early",
        "Z": "The value held by this Future. If the function (callback or RPC)\ncreating the value has thrown an error, this wait method will\nalso throw an error. Collects the provided Future objects into a single\ncombined Future that is completed when all of the\nsub-futures are completed. futures (list) \u2013 a list of Future objects. Returns a Future object to a list of the passed\nin Futures. Waits for all provided futures to be complete, and returns\nthe list of completed values. If any of the futures encounters an error,\nthe method will exit early and report the error not waiting for other\nfutures to complete. futures (list) \u2013 a list of Future object. A list of the completed Future results. This\nmethod will throw an error if wait on any\nFuture throws.",
        "source": "https://pytorch.org/docs/stable/futures.html"
    },
    {
        "X": "What type of convolution is applied to an input signal composed of several input planes?",
        "Y": "1D convolution",
        "Z": "Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies what operation in kTkHkWkT times kWkTkHkW regions by step",
        "Y": "3D average-pooling",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies a pooling over an input signal composed of several input planes. Applies a pooling over an input signal composed of several input",
        "Y": "3D max",
        "Z": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the result of a 2D max pooling over an input signal composed of several input planes?",
        "Y": "Computes a partial inverse of MaxPool1d",
        "Z": "Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies what type of convolution over an input image composed of several input planes?",
        "Y": "3D",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is another name for a 2D transposed convolution operator over an input image composed of several input planes?",
        "Y": "deconvolution",
        "Z": "Applies a 1D convolution over an input signal composed of several input planes.   Applies a 2D convolution over an input image composed of several input planes.   Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies what operation in kHkWkH times kWkHkW regions by step size sHs",
        "Y": "2D average-pooling",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies what over an input image composed of several input planes?",
        "Y": "3D convolution",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What operation in kTkHkWkT times kWkTkHkW regions by step size",
        "Y": "3D average-pooling",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the result of a 3D max pooling over an input signal composed of several input planes?",
        "Y": "Computes a partial inverse of MaxPool1d",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the name of the pooling over an input signal?",
        "Y": "3D adaptive max",
        "Z": "Applies a 3D convolution over an input image composed of several input planes.   Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What operation in kHkWkH times kWkHkW regions by step size sHsWs",
        "Y": "2D average-pooling",
        "Z": "Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d.   Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of pooling over an input signal composed of several input planes?",
        "Y": "1D power-average",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a 3D transposed convolution operator combine an array of sliding local blocks into?",
        "Y": "large containing tensor",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a 2D max pooling over an input signal consist of?",
        "Y": "several input planes",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "2D power-average pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is a 2D fractional max pooling over an input signal composed of several input planes?",
        "Y": "2D fractional max pooling over",
        "Z": "Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called \u201cdeconvolution\u201d   Extracts sliding local blocks from a batched input tensor.   Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does an array of sliding local blocks combine into?",
        "Y": "a large containing tensor",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the input signal composed of?",
        "Y": "several input planes",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a 1D adaptive max pooling over an input signal consist of?",
        "Y": "several input planes",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a 2D adaptive max pooling over an input signal consist of?",
        "Y": "several input planes",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does each element of the input Tensor have?",
        "Y": "Thresholds",
        "Z": "Combines an array of sliding local blocks into a large containing tensor.   Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is each element of the input Tensor?",
        "Y": "Thresholds",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the In-place version of threshold()?",
        "Y": "In-place version of threshold()",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of linear unit function does threshold() apply?",
        "Y": "rectified linear unit function element-wise",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the rectified linear unit function element-wise?",
        "Y": "In-place",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Computes a partial inverse of MaxPool2d. Computes a partial inverse of MaxPool",
        "Y": "Computes a partial inverse of MaxPool2d",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is element-wise?",
        "Y": "rectified linear unit function",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies what element-wise function?",
        "Y": "hardswish function",
        "Z": "Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What type of max pooling over an input signal composed of several input planes?",
        "Y": "3D",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the result of the 2D max pooling over an input signal composed of several input planes?",
        "Y": "Computes a partial inverse of MaxPool1d",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "2D adaptive max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a 3D adaptive max pooling over an input signal consist of?",
        "Y": "several input planes",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a 1D adaptive average pooling over an input signal consist of?",
        "Y": "several input planes",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "2D adaptive average pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What function element-wise. In-place version of hardtanh(). Applies the hardswish function, element-wise?",
        "Y": "HardTanh",
        "Z": "Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "3D adaptive max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "3D adaptive average pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.   Thresholds each element of the input Tensor.   In-place version of threshold().   Applies the rectified linear unit function element-wise.   In-place version of relu().   Applies the HardTanh function element-wise.   In-place version of hardtanh().   Applies the hardswish function, element-wise, as described in the paper:   Applies the element-wise function ReLU6(x)=min\u2061(max\u2061(0,x),6)\\text{ReLU6}(x) = \\min(\\max(0,x), 6)ReLU6(x)=min(max(0,x),6).   Applies element-wise, ELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121))\\text{ELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1))ELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121)).   In-place version of elu().",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What does a partial inverse of MaxPool1d. Computes a partial inverse of MaxPool2",
        "Y": "Computes a partial inverse of MaxPool3d",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is applied to an input signal composed of several input planes?",
        "Y": "1D power-average pooling",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "Applies 2D fractional max pooling over an input signal composed of what?",
        "Y": "several input planes",
        "Z": "Applies a 1D average pooling over an input signal composed of several input planes.   Applies 2D average-pooling operation in kH\u00d7kWkH \\times kWkH\u00d7kW regions by step size sH\u00d7sWsH \\times sWsH\u00d7sW steps.   Applies 3D average-pooling operation in kT\u00d7kH\u00d7kWkT \\times kH \\times kWkT\u00d7kH\u00d7kW regions by step size sT\u00d7sH\u00d7sWsT \\times sH \\times sWsT\u00d7sH\u00d7sW steps.   Applies a 1D max pooling over an input signal composed of several input planes.   Applies a 2D max pooling over an input signal composed of several input planes.   Applies a 3D max pooling over an input signal composed of several input planes.   Computes a partial inverse of MaxPool1d.   Computes a partial inverse of MaxPool2d.   Computes a partial inverse of MaxPool3d.   Applies a 1D power-average pooling over an input signal composed of several input planes.   Applies a 2D power-average pooling over an input signal composed of several input planes.   Applies a 1D adaptive max pooling over an input signal composed of several input planes.   Applies a 2D adaptive max pooling over an input signal composed of several input planes.   Applies a 3D adaptive max pooling over an input signal composed of several input planes.   Applies a 1D adaptive average pooling over an input signal composed of several input planes.   Applies a 2D adaptive average pooling over an input signal composed of several input planes.   Applies a 3D adaptive average pooling over an input signal composed of several input planes.   Applies 2D fractional max pooling over an input signal composed of several input planes.   Applies 3D fractional max pooling over an input signal composed of several input planes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "In-place version of what?",
        "Y": "leaky_relu()",
        "Z": "In-place version of elu().   Applies element-wise, SELU(x)=scale\u2217(max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x)\u22121)))\\text{SELU}(x) = scale * (\\max(0,x) + \\min(0, \\alpha * (\\exp(x) - 1)))SELU(x)=scale\u2217(max(0,x)+min(0,\u03b1\u2217(exp(x)\u22121))), with \u03b1=1.6732632423543772848170429916717\\alpha=1.6732632423543772848170429916717\u03b1=1.6732632423543772848170429916717 and scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946scale=1.0507009873554804934193349852946.   Applies element-wise, CELU(x)=max\u2061(0,x)+min\u2061(0,\u03b1\u2217(exp\u2061(x/\u03b1)\u22121))\\text{CELU}(x) = \\max(0,x) + \\min(0, \\alpha * (\\exp(x/\\alpha) - 1))CELU(x)=max(0,x)+min(0,\u03b1\u2217(exp(x/\u03b1)\u22121)).   Applies element-wise, LeakyReLU(x)=max\u2061(0,x)+negative_slope\u2217min\u2061(0,x)\\text{LeakyReLU}(x) = \\max(0, x) + \\text{negative\\_slope} * \\min(0, x)LeakyReLU(x)=max(0,x)+negative_slope\u2217min(0,x)   In-place version of leaky_relu().   Applies element-wise the function PReLU(x)=max\u2061(0,x)+weight\u2217min\u2061(0,x)\\text{PReLU}(x) = \\max(0,x) + \\text{weight} * \\min(0,x)PReLU(x)=max(0,x)+weight\u2217min(0,x) where weight is a learnable parameter.   Randomized leaky ReLU.   In-place version of rrelu().   The gated linear unit.   Applies element-wise the function GELU(x)=x\u2217\u03a6(x)\\text{GELU}(x) = x * \\Phi(x)GELU(x)=x\u2217\u03a6(x)   Applies element-wise LogSigmoid(xi)=log\u2061(11+exp\u2061(\u2212xi))\\text{LogSigmoid}(x_i) = \\log \\left(\\frac{1}{1 + \\exp(-x_i)}\\right)LogSigmoid(xi\u200b)=log(1+exp(\u2212xi\u200b)1\u200b)   Applies the hard shrinkage function element-wise   Applies element-wise, Tanhshrink(x)=x\u2212Tanh(x)\\text{Tanhshrink}(x) = x - \\text{Tanh}(x)Tanhshrink(x)=x\u2212Tanh(x)   Applies element-wise, the function SoftSign(x)=x1+\u2223x\u2223\\text{SoftSign}(x) = \\frac{x}{1 + |x|}SoftSign(x)=1+\u2223x\u2223x\u200b   Applies element-wise, the function Softplus(x)=1\u03b2\u2217log\u2061(1+exp\u2061(\u03b2\u2217x))\\text{Softplus}(x) = \\frac{1}{\\beta} * \\log(1 + \\exp(\\beta * x))Softplus(x)=\u03b21\u200b\u2217log(1+exp(\u03b2\u2217x)).   Applies a softmin function.   Applies a softmax function.   Applies the soft shrinkage function elementwise   Samples from the Gumbel-Softmax distribution (Link 1 Link 2) and optionally discretizes.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What function uses a squared term if the absolute element-wise error falls below delta and an L1 term otherwise?",
        "Y": "SoftMarginLoss",
        "Z": "Function that measures the Binary Cross Entropy between the target and the output.   Function that measures Binary Cross Entropy between target and output logits.   Poisson negative log likelihood loss.   See CosineEmbeddingLoss for details.   This criterion combines log_softmax and nll_loss in a single function.   The Connectionist Temporal Classification loss.   Gaussian negative log likelihood loss.   See HingeEmbeddingLoss for details.   The Kullback-Leibler divergence Loss   Function that takes the mean element-wise absolute value difference.   Measures the element-wise mean squared error.   See MarginRankingLoss for details.   See MultiLabelMarginLoss for details.   See MultiLabelSoftMarginLoss for details.   multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,   The negative log likelihood loss.   Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.   Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.   See SoftMarginLoss for details.   See TripletMarginLoss for details   See TripletMarginWithDistanceLoss for details.",
        "source": "https://pytorch.org/docs/stable/nn.functional.html"
    },
    {
        "X": "What is the result of two tensors?",
        "Y": "Matrix product of two tensors",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "The non-matrix dimensions are what?",
        "Y": "broadcasted",
        "Z": "Matrix product of two tensors. The behavior depends on the dimensionality of the tensors as follows: If both tensors are 1-dimensional, the dot product (scalar) is returned. If both arguments are 2-dimensional, the matrix-matrix product is returned. If the first argument is 1-dimensional and the second argument is 2-dimensional,\na 1 is prepended to its dimension for the purpose of the matrix multiply.\nAfter the matrix multiply, the prepended dimension is removed. If the first argument is 2-dimensional and the second argument is 1-dimensional,\nthe matrix-vector product is returned. If both arguments are at least 1-dimensional and at least one argument is\nN-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\nargument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\nbatched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\nThe non-matrix (i.e. batch) dimensions are broadcasted (and thus\nmust be broadcastable).  For example, if input is a\n(j\u00d71\u00d7n\u00d7n)(j \\times 1 \\times n \\times n)(j\u00d71\u00d7n\u00d7n) tensor and other is a (k\u00d7n\u00d7n)(k \\times n \\times n)(k\u00d7n\u00d7n)\ntensor, out will be a (j\u00d7k\u00d7n\u00d7n)(j \\times k \\times n \\times n)(j\u00d7k\u00d7n\u00d7n) tensor. Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\nare broadcastable, and not the matrix dimensions. For example, if input is a\n(j\u00d71\u00d7n\u00d7m)(j \\times 1 \\times n \\times m)(j\u00d71\u00d7n\u00d7m) tensor and other is a (k\u00d7m\u00d7p)(k \\times m \\times p)(k\u00d7m\u00d7p)\ntensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\nmatrix dimensions) are different. out will be a (j\u00d7k\u00d7n\u00d7p)(j \\times k \\times n \\times p)(j\u00d7k\u00d7n\u00d7p) tensor. This operator supports TensorFloat32. Note The 1-dimensional dot product version of this function does not support an out parameter. input (Tensor) \u2013 the first tensor to be multiplied other (Tensor) \u2013 the second tensor to be multiplied",
        "source": "https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"
    },
    {
        "X": "What is a dtype CPU tensor or a GPU tensor?",
        "Y": "dtype CPU tensor GPU tensor",
        "Z": "dtype CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of tensor is 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.",
        "Y": "CPU tensor GPU tensor",
        "Z": "CPU tensor GPU tensor 32-bit floating point torch.float32 or torch.float torch.FloatTensor torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How many bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.Hal",
        "Y": "16",
        "Z": "16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another term for binary16?",
        "Y": "Sometimes referred",
        "Z": "torch.cuda.FloatTensor 64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor().",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is torch.float64 or torch.double torch?",
        "Y": "64-bit floating point",
        "Z": "64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor().",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the term used to describe a device that uses 1 sign, 5 exponent, and 10 significand bits?",
        "Y": "Brain Floating Point",
        "Z": "64-bit floating point torch.float64 or torch.double torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor().",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Brain Floating Point: uses 1 sign, 8 exponent, and 7 significand bits. Useful when precision is important at the expense of",
        "Y": "Useful",
        "Z": "torch.DoubleTensor torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor().",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "How many bits does a doubletensor have?",
        "Y": "16",
        "Z": "torch.cuda.DoubleTensor 16-bit floating point 1 torch.float16 or torch.half torch.HalfTensor torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor().",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is torch.cuda.HalfTensor?",
        "Y": "16-bit floating point 2",
        "Z": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is Brain Floating Point stored as?",
        "Y": "8-bit signed integer",
        "Z": "torch.cuda.HalfTensor 16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor",
        "Y": "16-bit floating point 2",
        "Z": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the same number of exponent bits as float32 quantized 4-bit integer?",
        "Y": "8-bit signed integer",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the torch.Tensor?",
        "Y": "torch.Tensor",
        "Z": "16-bit floating point 2 torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is an alias?",
        "Y": "torch.Tensor",
        "Z": "torch.bfloat16 torch.BFloat16Tensor torch.cuda.BFloat16Tensor 32-bit complex torch.complex32 64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is torch.Tensor an alias for?",
        "Y": "default tensor type (torch.FloatTensor)",
        "Z": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "A tensor can be constructed from a what?",
        "Y": "Python list or sequence using the torch",
        "Z": "64-bit complex torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What can be constructed from a Python list or sequence using the torch.tensor type?",
        "Y": "A tensor",
        "Z": "torch.complex64 128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.By",
        "Y": "128-bit complex",
        "Z": "128-bit complex torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What can be constructed from a Python list or sequence using the torch.tensor() constructor?",
        "Y": "A tensor",
        "Z": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the tensor that uses 1 sign, 5 exponent, and 10 significand bits?",
        "Y": "binary16",
        "Z": "torch.complex128 or torch.cdouble 8-bit integer (unsigned) torch.uint8 torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value:",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does a Tensor data need to change?",
        "Y": "requires_grad flag",
        "Z": "torch.ByteTensor torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "If you want to change the required_grad flag of a Tensor, use what?",
        "Y": "requires_grad_() or detach()",
        "Z": "torch.cuda.ByteTensor 8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What are two ways to avoid a copy of a Tensor data?",
        "Y": "requires_grad_() or detach()",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "If you have a Tensor data and just want to change its requires_grad flag, use requires_grad_() or detach()",
        "Y": "numpy",
        "Z": "8-bit integer (signed) torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "If you have a Tensor data and want to avoid a copy, use requires_grad_() or detach() to avoid",
        "Y": "numpy array",
        "Z": "torch.int8 torch.CharTensor torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the signature of torch.cuda.CharTensor?",
        "Y": "16-bit integer",
        "Z": "torch.cuda.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.",
        "Y": "16-bit",
        "Z": "16-bit integer (signed) torch.int16 or torch.short torch.ShortTensor torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "A tensor of specific data type can be constructed by passing what to a constructor?",
        "Y": "torch.dtype and/or a torch.device",
        "Z": "torch.cuda.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.IntTensor torch.cuda.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.LongTensor torch.cuda.LongTensor Boolean torch.bool torch.BoolTensor torch.cuda.BoolTensor quantized 8-bit integer (unsigned) torch.quint8 torch.ByteTensor / quantized 8-bit integer (signed) torch.qint8 torch.CharTensor / quantized 32-bit integer (signed) torch.qfint32 torch.IntTensor / quantized 4-bit integer (unsigned) 3 torch.quint4x2 torch.ByteTensor / Sometimes referred to as binary16: uses 1 sign, 5 exponent, and 10\nsignificand bits. Useful when precision is important at the expense of range. Sometimes referred to as Brain Floating Point: uses 1 sign, 8 exponent, and 7\nsignificand bits. Useful when range is important, since it has the same\nnumber of exponent bits as float32 quantized 4-bit integer is stored as a 8-bit signed integer. Currently it\u2019s only supported in EmbeddingBag operator. torch.Tensor is an alias for the default tensor type (torch.FloatTensor). A tensor can be constructed from a Python list or sequence using the\ntorch.tensor() constructor: Warning torch.tensor() always copies data. If you have a Tensor\ndata and just want to change its requires_grad flag, use\nrequires_grad_() or\ndetach() to avoid a copy.\nIf you have a numpy array and want to avoid a copy, use\ntorch.as_tensor(). A tensor of specific data type can be constructed by passing a\ntorch.dtype and/or a torch.device to a\nconstructor or tensor creation op: For more information about building Tensors, see Creation Ops The contents of a tensor can be accessed and modified using Python\u2019s indexing\nand slicing notation: Use torch.Tensor.item() to get a Python number from a tensor containing a\nsingle value: For more information about indexing, see Indexing, Slicing, Joining, Mutating Ops A tensor can be created with requires_grad=True so that\ntorch.autograd records operations on them for automatic differentiation.",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "To create a tensor with specific size, use what?",
        "Y": "torch",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch. * tensor do?",
        "Y": "creation ops",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "To create a tensor with the same size (and similar types) as another tensor, use what?",
        "Y": "torch",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch. *_like tensor do?",
        "Y": "creation ops",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is the Tensor.is_cuda true?",
        "Y": "True if the Tensor is stored on the GPU",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is True if the Tensor is quantized?",
        "Y": "Tensor.is_quantized",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is Tensor.is_meta True if the Tensor is a meta tensor?",
        "Y": "True if the Tensor is a meta tensor",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.grad This attribute is what by default?",
        "Y": "None",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing what?",
        "Y": "real values of the self tensor",
        "Z": "Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.abs() Tensor.abs_ In-place version of abs() Tensor.abs_ In-place version of",
        "Y": "Tensor.abs",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for creation ops?",
        "Y": "Creation Ops",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing?",
        "Y": "real values of the self tensor",
        "Z": "To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is the Tensor.is_cuda True?",
        "Y": "True if the Tensor is stored on the GPU",
        "Z": "To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does torch.abs() Tensor.abs stand for?",
        "Y": "Tensor.abs",
        "Z": "Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.new_empty Returns a Tensor of size size filled with what?",
        "Y": "uninitialized data",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.is_cuda Is what?",
        "Y": "True if the Tensor is stored on the GPU",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.is_meta Is True if the Tensor is a what?",
        "Y": "meta tensor",
        "Z": "Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Where is the Tensor.device?",
        "Y": "torch.device",
        "Z": "Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does a scalar or tensor do to self tensor?",
        "Y": "Add a scalar or tensor to self tensor",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for add() Tensor?",
        "Y": "addbmm",
        "Z": "Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does new_empty return a Tensor of size size filled with?",
        "Y": "uninitialized data",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the tensor that returns a new Tensor?",
        "Y": "Tensor.abs",
        "Z": "Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.imag Returns a new tensor containing what of the self tensor?",
        "Y": "imaginary values",
        "Z": "Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the feature that adds a scalar or tensor to self tensor?",
        "Y": "addbmm",
        "Z": "Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for addbmm() Tensor?",
        "Y": "addbmm",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What type of data does a Tensor.new_empty return a Tensor of size size filled with?",
        "Y": "uninitialized data",
        "Z": "Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is another name for Tensor.abs?",
        "Y": "torch.abs",
        "Z": "To create a tensor with specific size, use torch.* tensor creation\nops (see Creation Ops). To create a tensor with the same size (and similar types) as another tensor,\nuse torch.*_like tensor creation ops\n(see Creation Ops). To create a tensor with similar type but different size as another tensor,\nuse tensor.new_* creation ops. Is this Tensor with its dimensions reversed. If n is the number of dimensions in x,\nx.T is equivalent to x.permute(n-1, n-2, ..., 0). Tensor.new_tensor Returns a new Tensor with data as the tensor data. Tensor.new_full Returns a Tensor of size size filled with fill_value. Tensor.new_empty Returns a Tensor of size size filled with uninitialized data. Tensor.new_ones Returns a Tensor of size size filled with 1. Tensor.new_zeros Returns a Tensor of size size filled with 0. Tensor.is_cuda Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Is True if the Tensor is a meta tensor, False otherwise?",
        "Y": "meta",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.ndim Alias for dim() Tensor.ndim Returns a new tensor containing what?",
        "Y": "real values of the self tensor",
        "Z": "Is True if the Tensor is stored on the GPU, False otherwise. Tensor.is_quantized Is True if the Tensor is quantized, False otherwise. Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan()",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "Tensor.is_meta Is what?",
        "Y": "True if the Tensor is a meta tensor",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does Tensor.ndim Alias for dim() Tensor.ndim Returns a new tensor containing",
        "Y": "real values of the self tensor",
        "Z": "Tensor.is_meta Is True if the Tensor is a meta tensor, False otherwise. Tensor.device Is the torch.device where this Tensor is. Tensor.grad This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What does the tensor.ndim Alias for dim() Tensor.real Returns a new tensor",
        "Y": "real values of the self tensor",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self?",
        "Y": "Tensor.abs",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    },
    {
        "X": "What is the name of the attribute that becomes a Tensor the first time a call to backward() computes gradients for self?",
        "Y": "addbmm",
        "Z": "This attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self. Tensor.ndim Alias for dim() Tensor.real Returns a new tensor containing real values of the self tensor. Tensor.imag Returns a new tensor containing imaginary values of the self tensor. Tensor.abs See torch.abs() Tensor.abs_ In-place version of abs() Tensor.absolute Alias for abs() Tensor.absolute_ In-place version of absolute() Alias for abs_() Tensor.acos See torch.acos() Tensor.acos_ In-place version of acos() Tensor.arccos See torch.arccos() Tensor.arccos_ In-place version of arccos() Tensor.add Add a scalar or tensor to self tensor. Tensor.add_ In-place version of add() Tensor.addbmm See torch.addbmm() Tensor.addbmm_ In-place version of addbmm() Tensor.addcdiv See torch.addcdiv() Tensor.addcdiv_ In-place version of addcdiv() Tensor.addcmul See torch.addcmul() Tensor.addcmul_ In-place version of addcmul() Tensor.addmm See torch.addmm() Tensor.addmm_ In-place version of addmm() Tensor.sspaddmm See torch.sspaddmm() Tensor.addmv See torch.addmv() Tensor.addmv_ In-place version of addmv() Tensor.addr See torch.addr() Tensor.addr_ In-place version of addr() Tensor.allclose See torch.allclose() Tensor.amax See torch.amax() Tensor.amin See torch.amin() Tensor.angle See torch.angle() Tensor.apply_ Applies the function callable to each element in the tensor, replacing each element with the value returned by callable. Tensor.argmax See torch.argmax() Tensor.argmin See torch.argmin() Tensor.argsort See torch.argsort() Tensor.asin See torch.asin() Tensor.asin_ In-place version of asin() Tensor.arcsin See torch.arcsin() Tensor.arcsin_ In-place version of arcsin() Tensor.as_strided See torch.as_strided() Tensor.atan See torch.atan() Tensor.atan_ In-place version of atan() Tensor.arctan See torch.arctan() Tensor.arctan_ In-place version of arctan() Tensor.atan2 See torch.atan2() Tensor.atan2_ In-place version of atan2() Tensor.all See torch.all() Tensor.any See torch.any() Tensor.backward",
        "source": "https://pytorch.org/docs/stable/tensors.html"
    }
]